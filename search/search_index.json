{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6df1\u5165\u6d45\u51fa\u200bPyTorch","text":"<p>\u200b\u5728\u7ebf\u200b\u9605\u8bfb\u200b\u5730\u5740\u200b\uff1ahttps://eanyang7.github.io/pytorch_docs/</p> <p>\u200b\u914d\u5957\u200b\u89c6\u9891\u6559\u7a0b\u200b\uff1ahttps://www.bilibili.com/video/BV1L44y1472Z</p> <p>\u200b\u8be5\u200b\u9879\u76ee\u200b\u662f\u200b\u5728\u200bdatawhalechina/thorough-pytorch\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u52a0\u5de5\u200b\u6539\u7f16\u200b</p>"},{"location":"#_1","title":"\u7ae0\u8282\u200b\u76ee\u5f55","text":"<ul> <li>\u200b\u7b2c\u96f6\u200b\u7ae0\u200b\uff1a\u200b\u524d\u7f6e\u200b\u77e5\u8bc6\u200b</li> <li>\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7b80\u53f2\u200b</li> <li>\u200b\u76f8\u5173\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b</li> <li>\u200b\u5e38\u7528\u200b\u5305\u200b\u7684\u200b\u5b66\u4e60\u200b</li> <li>Jupyter\u200b\u76f8\u5173\u200b\u64cd\u4f5c\u200b</li> <li>\u200b\u7b2c\u4e00\u7ae0\u200b\uff1aPyTorch\u200b\u7684\u200b\u7b80\u4ecb\u200b\u548c\u200b\u5b89\u88c5\u200b</li> <li>PyTorch\u200b\u7b80\u4ecb\u200b</li> <li>PyTorch\u200b\u7684\u200b\u5b89\u88c5\u200b</li> <li>PyTorch\u200b\u76f8\u5173\u200b\u8d44\u6e90\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u7b2c\u4e8c\u7ae0\u200b\uff1aPyTorch\u200b\u57fa\u7840\u77e5\u8bc6\u200b</li> <li>\u200b\u5f20\u91cf\u200b\u53ca\u5176\u200b\u8fd0\u7b97\u200b</li> <li>\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u3001CUDA\u200b\u548c\u200bcuDNN\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u7b2c\u4e09\u7ae0\u200b\uff1aPyTorch\u200b\u7684\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u200b\u6a21\u5757\u200b</li> <li>\u200b\u601d\u8003\u200b\uff1a\u200b\u5b8c\u6210\u200b\u4e00\u5957\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6d41\u7a0b\u200b\u9700\u8981\u200b\u54ea\u4e9b\u200b\u5173\u952e\u73af\u8282\u200b</li> <li>\u200b\u57fa\u672c\u200b\u914d\u7f6e\u200b</li> <li>\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b</li> <li>\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b</li> <li>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b</li> <li>\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200b</li> <li>\u200b\u7b2c\u56db\u7ae0\u200b\uff1aPyTorch\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b</li> <li>\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014Fashion-MNIST\u200b\u65f6\u88c5\u200b\u5206\u7c7b\u200b</li> <li>\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014\u200b\u679c\u852c\u200b\u5206\u7c7b\u200b\u5b9e\u6218\u200b\uff08notebook\uff09</li> <li>\u200b\u7b2c\u4e94\u7ae0\u200b\uff1aPyTorch\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b</li> <li>\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b</li> <li>\u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u5feb\u901f\u200b\u642d\u5efa\u200b\u590d\u6742\u200b\u7f51\u7edc\u200b</li> <li>\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b</li> <li>\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u4e0e\u200b\u8bfb\u53d6\u200b</li> <li>\u200b\u7b2c\u516d\u7ae0\u200b\uff1aPyTorch\u200b\u8fdb\u9636\u200b\u8bad\u7ec3\u200b\u6280\u5de7\u200b</li> <li>\u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b</li> <li>\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b</li> <li>\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b-torchvision</li> <li>\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b-timm</li> <li>\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b</li> <li>\u200b\u6570\u636e\u200b\u6269\u5145\u200b</li> <li>\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u4fee\u6539\u200b\u53ca\u200b\u4fdd\u5b58\u200b</li> <li>PyTorch\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4e0e\u200b\u8fdb\u9636\u200b\u8bad\u7ec3\u200b\u6280\u5de7\u200b</li> <li>\u200b\u7b2c\u4e03\u7ae0\u200b\uff1aPyTorch\u200b\u53ef\u89c6\u5316\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200bCNN\u200b\u5377\u79ef\u200b\u5c42\u200b</li> <li>\u200b\u4f7f\u7528\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b</li> <li>\u200b\u4f7f\u7528\u200bwandb\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b</li> <li>\u200b\u7b2c\u516b\u7ae0\u200b\uff1aPyTorch\u200b\u751f\u6001\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u56fe\u50cf\u200b\u2014torchvision</li> <li>\u200b\u89c6\u9891\u200b\u2014PyTorchVideo</li> <li>\u200b\u6587\u672c\u200b\u2014torchtext</li> <li>\u200b\u97f3\u9891\u200b-torchaudio</li> <li>\u200b\u7b2c\u4e5d\u7ae0\u200b\uff1a\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b</li> <li>\u200b\u4f7f\u7528\u200bONNX\u200b\u8fdb\u884c\u200b\u90e8\u7f72\u200b\u5e76\u200b\u63a8\u7406\u200b</li> <li>\u200b\u7b2c\u5341\u7ae0\u200b\uff1a\u200b\u5e38\u89c1\u200b\u7f51\u7edc\u200b\u4ee3\u7801\u200b\u7684\u200b\u89e3\u8bfb\u200b(\u200b\u63a8\u8fdb\u200b\u4e2d\u200b)</li> <li>\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b<ul> <li>\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b</li> <li>ResNet\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b</li> <li>Swin Transformer\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b</li> <li>Vision Transformer\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b</li> <li>RNN\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b</li> <li>LSTM\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b\u53ca\u5176\u200b\u5b9e\u6218\u200b</li> <li>\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b</li> <li>YOLO\u200b\u7cfb\u5217\u200b\u89e3\u8bfb\u200b\uff08\u200b\u4e0e\u200bMMYOLO\u200b\u5408\u4f5c\u200b\uff09</li> <li>\u200b\u56fe\u50cf\u200b\u5206\u5272\u200b</li> </ul> </li> <li>\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b<ul> <li>RNN\u200b\u6e90\u7801\u200b\u89e3\u8bfb\u200b</li> </ul> </li> <li>\u200b\u97f3\u9891\u200b\u5904\u7406\u200b</li> <li>\u200b\u89c6\u9891\u200b\u5904\u7406\u200b</li> <li>\u200b\u5176\u4ed6\u200b</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/","title":"\u4eba\u5de5\u667a\u80fd\u200b\u7b80\u53f2","text":"<p>\u200b\u81ea\u4ece\u200b\u56fe\u7075\u200b\u5728\u200b1950\u200b\u5e74\u200b\u7b2c\u4e00\u6b21\u200b\u63d0\u51fa\u200b\u201c\u200b\u673a\u5668\u200b\u667a\u80fd\u200b\uff08Machine Intelligence\uff09\u201d\u200b\u8fd9\u4e2a\u200b\u6982\u5ff5\u200b\u4ee5\u6765\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5df2\u7ecf\u200b\u7ecf\u5386\u200b\u4e86\u200b\u4e03\u5341\u4f59\u5e74\u200b\u7684\u200b\u53d1\u5c55\u200b\u3002\u200b\u5728\u200b\u8fd9\u200b\u4e03\u5341\u591a\u5e74\u200b\u4e2d\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5148\u540e\u200b\u7ecf\u5386\u200b\u4e86\u200b\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u6b21\u200b\u6d6a\u6f6e\u200b\u5bf9\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u6765\u8bf4\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u5177\u6709\u200b\u91cc\u7a0b\u7891\u200b\u610f\u4e49\u200b\u7684\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u8fd9\u200b\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\u4e3a\u4e3b\u200b\u7ebf\u200b\uff0c\u200b\u4e3a\u200b\u5927\u5bb6\u200b\u4ecb\u7ecd\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5386\u7a0b\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5c06\u200b\u4f1a\u200b\u7ed9\u200b\u5927\u5bb6\u200b\u4ecb\u7ecd\u200b\u73b0\u5728\u200b\u5e38\u8bf4\u200b\u7684\u200bDeep learning\uff0cMachine Learning\u200b\u548c\u200bAI\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002</p> <p>[* ]\u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e86\u89e3\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b</li> <li>\u200b\u4e86\u89e3\u200bDeep learning\uff0cMachine learning\u200b\u548c\u200bAI\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#11","title":"1.1 \u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u4e09\u6b21\u200b\u6d6a\u6f6e","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#111","title":"1.1.1 \u200b\u7b2c\u4e00\u6b21\u200b\u6d6a\u6f6e","text":"<p>1950\u200b\u5e74\u200b\uff0c\u200b\u963f\u5170\u200b\u00b7\u200b\u56fe\u7075\u200b\u53d1\u8868\u200b\u8457\u540d\u200b\u8bba\u6587\u200b\u300a\u200b\u8ba1\u7b97\u673a\u200b\u5668\u200b\u4e0e\u200b\u667a\u80fd\u200b\u300b\uff0c\u200b\u5728\u200b\u8fd9\u7bc7\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0c\u200b\u4ed6\u200b\u63d0\u51fa\u200b\u4e86\u200b\u673a\u5668\u200b\u601d\u7ef4\u200b\u7684\u200b\u6982\u5ff5\u200b\u548c\u200b\u56fe\u7075\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u6807\u5fd7\u200b\u7740\u200b\u201c\u200b\u673a\u5668\u200b\u7684\u200b\u667a\u80fd\u5316\u200b\u201d\u200b\u6b63\u5f0f\u200b\u8fdb\u5165\u200b\u4eba\u7c7b\u200b\u7684\u200b\u79d1\u6280\u200b\u6811\u200b\u3002\u200b\u5728\u6b64\u4e4b\u540e\u200b\u7684\u200b\u6570\u200b\u5e74\u95f4\u200b\uff0c\u200b\u673a\u5668\u200b\u667a\u80fd\u200b\u6709\u200b\u4e86\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u53d1\u5c55\u200b\u3002\u200b\u4e24\u5e74\u200b\u540e\u200b\u7684\u200b1952\u200b\u5e74\u200b\uff0c\u200b\u8ba1\u7b97\u673a\u200b\u79d1\u5b66\u5bb6\u200b\u963f\u745f\u200b\u00b7\u200b\u8428\u7f2a\u5c14\u200b\u5f00\u53d1\u200b\u51fa\u200b\u4e00\u6b3e\u200b\u8df3\u68cb\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u5e76\u200b\u63d0\u51fa\u200b\u4e86\u200b\u201c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u201d\u200b\u8fd9\u4e2a\u200b\u6982\u5ff5\u200b\u3002\u200b\u5728\u6b64\u4e4b\u540e\u200b\u7684\u200b4\u200b\u5e74\u91cc\u200b\uff0c\u200b\u673a\u5668\u200b\u667a\u80fd\u5316\u200b\u4e5f\u200b\u53d6\u5f97\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u7684\u200b\u8fdb\u6b65\u200b\uff0c\u200b\u76f4\u5230\u200b1956\u200b\u5e74\u200b\u7684\u200b\u8fbe\u7279\u8305\u65af\u200b\u4f1a\u8bae\u200b\u4e0a\u200b\uff0c\u200b\u7ea6\u7ff0\u200b\u00b7\u200b\u9ea6\u5361\u9521\u200b\u6b63\u5f0f\u200b\u63d0\u51fa\u200b\u4e86\u200b\u201c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u201d\u200b\u8fd9\u4e2a\u200b\u8bcd\u8bed\u200b\uff0c1956\u200b\u5e74\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u200b\u6210\u4e3a\u200b\u4e86\u200b\u5b9e\u9645\u610f\u4e49\u200b\u4e0a\u200b\u7684\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5143\u5e74\u200b\u3002</p> <p>\u200b\u8fbe\u7279\u8305\u65af\u200b\u4f1a\u8bae\u200b\u4e4b\u540e\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u8fdb\u5165\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u9ad8\u901f\u200b\u53d1\u5c55\u200b\u7684\u200b\u65f6\u671f\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u201c\u200b\u7b2c\u4e00\u6b21\u200b\u6d6a\u6f6e\u200b\u201d\u3002\u200b\u8fd9\u6b21\u200b\u6d6a\u6f6e\u200b\u4e00\u76f4\u200b\u6301\u7eed\u200b\u5230\u200b\u4e8c\u5341\u4e16\u7eaa\u200b\u516d\u5341\u5e74\u4ee3\u200b\u4e2d\u671f\u200b\u3002\u200b\u5728\u200b\u8fd9\u8fd1\u200b10\u200b\u5e74\u200b\u7684\u200b\u65f6\u95f4\u200b\u91cc\u200b\uff0c\u200b\u8ba1\u7b97\u673a\u672c\u8eab\u200b\u7684\u200b\u201c\u200b\u667a\u80fd\u200b\u201d\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5f97\u5230\u200b\u53d1\u5c55\u200b\uff0c\u200b\u5feb\u901f\u200b\u8fdb\u6b65\u200b\u7684\u200b\u662f\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u4e00\u4e9b\u200b\u7406\u8bba\u200b\u4e0e\u200b\u7b97\u6cd5\u200b\u65b9\u9762\u200b\u3002\u200b\u5f88\u591a\u200b\u5bf9\u200b\u540e\u6765\u200b\u4eba\u5de5\u667a\u80fd\u200b\u53d1\u5c55\u200b\u8d77\u5230\u200b\u5960\u57fa\u200b\u4f5c\u7528\u200b\u7684\u200b\u7b97\u6cd5\u200b\u2014\u2014\u200b\u5982\u200b\u7f57\u68ee\u5e03\u200b\u62c9\u7279\u200b\u5728\u200b1957\u200b\u5e74\u200b\u53d1\u660e\u200b\u611f\u77e5\u673a\u200b\u2014\u2014\u200b\u5c31\u662f\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u65f6\u95f4\u6bb5\u200b\u8bde\u751f\u200b\u7684\u200b\u3002\u200b\u611f\u77e5\u673a\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u200b\u7406\u8bba\u200b\u4e2d\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6700\u65e9\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u6a21\u578b\u200b\u4e5f\u200b\u4f7f\u5f97\u200b\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u200b\u7406\u8bba\u200b\u5f97\u5230\u200b\u4e86\u200b\u5de8\u5927\u200b\u7684\u200b\u7a81\u7834\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5f3a\u5316\u200b\u5b66\u4e60\u200b\u7684\u200b\u96cf\u5f62\u200b\u4e5f\u200b\u662f\u200b\u5728\u200b\u90a3\u200b\u6bb5\u65f6\u95f4\u200b\u63d0\u51fa\u200b\u7684\u200b\u3002\u200b\u5f7c\u65f6\u200b\u7684\u200b\u79d1\u5b66\u754c\u200b\u90fd\u200b\u5f25\u6f2b\u7740\u200b\u5feb\u4e50\u200b\u7684\u200b\u6c14\u6c1b\u200b\uff0c\u200b\u5927\u5bb6\u200b\u90fd\u200b\u8ba4\u4e3a\u200b\uff0c\u200b\u53ea\u8981\u200b\u575a\u6301\u200b\u8d70\u200b\u4e0b\u53bb\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5c31\u200b\u4e00\u5b9a\u200b\u4f1a\u200b\u5f97\u5230\u200b\u8de8\u8d8a\u5f0f\u200b\u7684\u200b\u53d1\u5c55\u200b\u3002\u200b\u4f46\u200b\u4e8b\u4e0e\u613f\u8fdd\u200b\uff0c\u200b\u4e0d\u4e45\u200b\u540e\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u7b2c\u4e00\u6b21\u200b\u5bd2\u51ac\u200b\uff08AI Winter\uff09\u200b\u5c31\u200b\u5230\u6765\u200b\u4e86\u200b\u3002</p> <p>1966\u200b\u5e74\u200b\u524d\u540e\u200b\uff0cAI\u200b\u906d\u9047\u200b\u4e86\u200b\u74f6\u9888\u200b\u3002\u200b\u4eba\u4eec\u200b\u53d1\u73b0\u200b\u903b\u8f91\u200b\u8bc1\u660e\u200b\u5668\u200b\u3001\u200b\u611f\u77e5\u5668\u200b\u3001\u200b\u5f3a\u5316\u200b\u5b66\u4e60\u200b\u7b49\u7b49\u200b\u53ea\u80fd\u200b\u505a\u200b\u5f88\u200b\u7b80\u5355\u200b\u3001\u200b\u975e\u5e38\u200b\u4e13\u95e8\u200b\u4e14\u200b\u5f88\u7a84\u200b\u7684\u200b\u4efb\u52a1\u200b\uff0c\u200b\u7a0d\u5fae\u200b\u8d85\u51fa\u8303\u56f4\u200b\u5c31\u200b\u65e0\u6cd5\u200b\u5e94\u5bf9\u200b\u3002\u200b\u5f53\u65f6\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u6709\u9650\u200b\u7684\u200b\u5185\u5b58\u200b\u548c\u200b\u5904\u7406\u901f\u5ea6\u200b\u4e0d\u8db3\u4ee5\u200b\u89e3\u51b3\u200b\u4efb\u4f55\u200b\u5b9e\u9645\u200b\u7684\u200bAI\u200b\u95ee\u9898\u200b\u3002\u200b\u7814\u7a76\u8005\u200b\u4eec\u200b\u5f88\u5feb\u200b\u5c31\u200b\u610f\u8bc6\u200b\u5230\u200b\uff0c\u200b\u8981\u6c42\u200b\u7a0b\u5e8f\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u4e16\u754c\u200b\u5177\u6709\u200b\u513f\u7ae5\u200b\u6c34\u5e73\u200b\u7684\u200b\u8ba4\u8bc6\u200b\u8fd9\u4e2a\u200b\u8981\u6c42\u200b\u90fd\u200b\u592a\u200b\u9ad8\u200b\u4e86\u200b\u2014\u2014\u200b\u90a3\u65f6\u200b\u6ca1\u200b\u4eba\u200b\u80fd\u591f\u200b\u505a\u51fa\u200b\u4eba\u5de5\u667a\u80fd\u200b\u9700\u8981\u200b\u7684\u200b\u5de8\u5927\u200b\u6570\u636e\u5e93\u200b\uff0c\u200b\u4e5f\u200b\u6ca1\u4eba\u200b\u77e5\u9053\u200b\u4e00\u4e2a\u200b\u7a0b\u5e8f\u200b\u600e\u6837\u624d\u80fd\u200b\u5b66\u5230\u200b\u5982\u6b64\u200b\u4e30\u5bcc\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u53e6\u4e00\u65b9\u9762\u200b\uff0c\u200b\u6709\u200b\u5f88\u591a\u200b\u8ba1\u7b97\u200b\u590d\u6742\u5ea6\u200b\u4ee5\u200b\u6307\u6570\u200b\u7a0b\u5ea6\u200b\u589e\u52a0\u200b\uff0c\u200b\u8fd9\u200b\u6210\u4e3a\u200b\u4e86\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u5b8c\u6210\u200b\u7684\u200b\u8ba1\u7b97\u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u8bf4\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u7b2c\u4e00\u6b21\u200b\u6d6a\u6f6e\u200b\u5728\u200b\u53d1\u5c55\u200b\u5230\u200b\u201c\u200b\u975e\u200b\u667a\u80fd\u200b\u5bf9\u8bdd\u200b\u673a\u5668\u200b\u201d\u200b\u7684\u200b\u667a\u80fd\u5316\u200b\u521d\u7ea7\u9636\u6bb5\u200b\u65f6\u200b\uff0c\u200b\u5c31\u200b\u56e0\u4e3a\u200b\u5f53\u65f6\u200b\u7684\u200b\u6280\u672f\u200b\u9650\u5236\u200b\u4e0d\u5f97\u4e0d\u200b\u505c\u6446\u200b\u3002\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u4f3c\u4e4e\u200b\u9677\u5165\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65e0\u89e3\u200b\u7684\u200b\u201c\u200b\u6b7b\u80e1\u540c\u200b\u201d\u200b\u91cc\u200b\uff0c\u200b\u5e76\u200b\u88ab\u200b\u8ba1\u7b97\u673a\u200b\u79d1\u5b66\u5bb6\u200b\u4eec\u200b\u9010\u6e10\u200b\u51b7\u843d\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#112","title":"1.1.2 \u200b\u7b2c\u4e8c\u6b21\u200b\u6d6a\u6f6e","text":"<p>\u200b\u65f6\u95f4\u200b\u6765\u5230\u200b\u4e86\u200b20\u200b\u4e16\u7eaa\u200b80\u200b\u5e74\u4ee3\u200b\u3002\u200b\u7ecf\u8fc7\u200b\u4e86\u200b\u6570\u5341\u5e74\u200b\u7684\u200b\u7814\u7a76\u200b\uff0c\u200b\u79d1\u5b66\u5bb6\u200b\u4eec\u200b\u9010\u6e10\u200b\u653e\u5f03\u200b\u4e86\u200b\u521d\u4ee3\u200b\u7684\u200b\u7b26\u53f7\u200b\u5b66\u6d3e\u200b\u601d\u8def\u200b\uff0c\u200b\u6539\u7528\u200b\u7edf\u8ba1\u5b66\u200b\u7684\u200b\u601d\u8def\u200b\u6765\u200b\u7814\u7a76\u200b\u4eba\u5de5\u667a\u80fd\u200b\u3002\u200b\u7814\u7a76\u200b\u601d\u8def\u200b\u7684\u200b\u6539\u53d8\u200b\u518d\u200b\u52a0\u4e0a\u200b\u786c\u4ef6\u200b\u6280\u672f\u200b\u7684\u200b\u5347\u7ea7\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u53c8\u200b\u4e00\u6b21\u200b\u8fce\u6765\u200b\u7684\u200b\u65b0\u200b\u7684\u200b\u5951\u673a\u200b\u3002\u200b\u5728\u200b\u90a3\u4e2a\u200b\u65f6\u4ee3\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u201c\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u201d\u200b\u53d7\u5230\u200b\u4e86\u200b\u7edd\u5bf9\u200b\u7684\u200b\u70ed\u200b\u6367\u200b\u3002\u200b\u7279\u5b9a\u200b\u9886\u57df\u200b\u7684\u200b\u201c\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u201d\u200b\u7a0b\u5e8f\u200b\u88ab\u200b\u66f4\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u91c7\u7eb3\u200b\uff0c\u200b\u8be5\u200b\u7cfb\u7edf\u200b\u80fd\u591f\u200b\u6839\u636e\u200b\u9886\u57df\u200b\u5185\u200b\u7684\u200b\u4e13\u4e1a\u77e5\u8bc6\u200b\uff0c\u200b\u63a8\u7406\u200b\u51fa\u200b\u4e13\u4e1a\u200b\u95ee\u9898\u200b\u7684\u200b\u7b54\u6848\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u4e5f\u200b\u7531\u6b64\u200b\u53d8\u5f97\u200b\u66f4\u52a0\u200b\u201c\u200b\u5b9e\u7528\u200b\u201d\uff0c\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u6240\u200b\u4f9d\u8d56\u200b\u7684\u200b\u77e5\u8bc6\u5e93\u200b\u7cfb\u7edf\u200b\u548c\u200b\u77e5\u8bc6\u200b\u5de5\u7a0b\u200b\u6210\u4e3a\u200b\u4e86\u200b\u5f53\u65f6\u200b\u4e3b\u8981\u200b\u7684\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u4ec5\u200b\u9002\u7528\u200b\u4e8e\u200b\u67d0\u4e9b\u200b\u7279\u5b9a\u200b\u573a\u666f\u200b\uff0c\u200b\u5f88\u5feb\u200b\u4eba\u4eec\u200b\u5c31\u200b\u5bf9\u200b\u8fd9\u200b\u4e00\u200b\u7cfb\u7edf\u200b\u7531\u200b\u72c2\u70ed\u200b\u7684\u200b\u8ffd\u6367\u200b\u9010\u6e10\u200b\u8d70\u5411\u200b\u5de8\u5927\u200b\u7684\u200b\u5931\u671b\u200b\u3002\u200b\u4e0e\u6b64\u540c\u65f6\u200b\uff0c\u200b\u73b0\u4ee3\u200b\u7535\u5b50\u8ba1\u7b97\u673a\u200b\u7684\u200b\u51fa\u73b0\u200b\u8ba9\u200b\u201c\u200b\u77e5\u8bc6\u200b\u67e5\u8be2\u200b\u201d\u200b\u7684\u200b\u8d39\u7528\u200b\u8fdb\u4e00\u6b65\u200b\u964d\u4f4e\u200b\uff0c\u200b\u4eba\u4eec\u200b\u66f4\u52a0\u200b\u6df1\u523b\u200b\u7684\u200b\u610f\u8bc6\u200b\u5230\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u662f\u200b\u5982\u6b64\u200b\u7684\u200b\u53e4\u8001\u200b\u9648\u65e7\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u653f\u5e9c\u90e8\u95e8\u200b\u4e0b\u8c03\u200b\u4e86\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u7684\u200b\u7814\u53d1\u200b\u8d44\u91d1\u200b\u3002\u200b\u7f3a\u5c11\u200b\u4e86\u200b\u8d44\u91d1\u200b\u7684\u200b\u652f\u6301\u200b\uff0c\u200b\u7531\u200b\u4e13\u5bb6\u7cfb\u7edf\u200b\u518d\u6b21\u200b\u5174\u8d77\u200b\u7684\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7814\u7a76\u200b\u53c8\u200b\u4e00\u6b21\u200b\u9677\u5165\u200b\u4e86\u200b\u4f4e\u8c37\u200b\u4e4b\u4e2d\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u7b2c\u4e8c\u6b21\u200b\u6d6a\u6f6e\u200b\u6301\u7eed\u200b\u7684\u200b\u65f6\u95f4\u200b\u6bd4\u8f83\u200b\u77ed\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u5728\u200b\u6574\u4e2a\u200b\u4eba\u5de5\u667a\u80fd\u200b\u53d1\u5c55\u200b\u5386\u53f2\u200b\u4e2d\u200b\u4ecd\u7136\u200b\u8d77\u5230\u200b\u4e86\u200b\u4e3e\u8db3\u8f7b\u91cd\u200b\u7684\u200b\u4f5c\u7528\u200b\u3002\u200b\u5b83\u200b\u5f7b\u5e95\u6539\u53d8\u200b\u4e86\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7814\u7a76\u200b\u7684\u200b\u5927\u200b\u601d\u8def\u200b\uff0c\u200b\u5c06\u200b\u7edf\u8ba1\u5b66\u200b\u601d\u60f3\u200b\u5f15\u5165\u200b\u7814\u7a76\u200b\u4e4b\u4e2d\u200b\uff0c\u200b\u4e3a\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5728\u200b\u672a\u6765\u200b\u51e0\u5341\u5e74\u200b\u7684\u200b\u53d1\u5c55\u200b\u6253\u4e0b\u200b\u4e86\u200b\u57fa\u7840\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5728\u200b\u8fd9\u6b21\u200b\u6d6a\u6f6e\u200b\u4e2d\u200b\u63d0\u51fa\u200b\u7684\u200bBP\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u4e3a\u200b\u4e4b\u540e\u200b\u673a\u5668\u200b\u611f\u77e5\u200b\u3001\u200b\u4ea4\u4e92\u200b\u7684\u200b\u80fd\u529b\u200b\u5960\u5b9a\u200b\u4e86\u200b\u57fa\u7840\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#113","title":"1.1.3 \u200b\u7b2c\u4e09\u6b21\u200b\u6d6a\u6f6e","text":"<p>1993\u200b\u5e74\u200b\u540e\u200b\uff0c\u200b\u65b0\u200b\u7684\u200b\u6570\u5b66\u200b\u5de5\u5177\u200b\uff0c\u200b\u7406\u8bba\u200b\u548c\u200b\u6469\u5c14\u5b9a\u5f8b\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u8ba1\u7b97\u673a\u200b\u7684\u200b\u7b97\u529b\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\uff0c\u200b\u4ee5\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e3a\u200b\u6838\u5fc3\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u83b7\u5f97\u200b\u53d1\u5c55\u200b\uff0c\u200b\u65b0\u200b\u7684\u200b\u82af\u7247\u200b\u548c\u200b\u4e91\u200b\u8ba1\u7b97\u200b\u7684\u200b\u53d1\u5c55\u200b\u4f7f\u5f97\u200b\u53ef\u7528\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u83b7\u5f97\u200b\u98de\u8dc3\u200b\u5f0f\u200b\u63d0\u9ad8\u200b\uff0c\u200b\u5927\u200b\u6570\u636e\u200b\u7684\u200b\u53d1\u5c55\u200b\u4f7f\u5f97\u200b\u6d77\u91cf\u200b\u6570\u636e\u200b\u7684\u200b\u50a8\u5b58\u200b\u548c\u200b\u5206\u6790\u200b\u6210\u4e3a\u200b\u53ef\u80fd\u200b\u3002\u200b\u5728\u200b\u8fd9\u6837\u200b\u7684\u200b\u6280\u672f\u200b\u80cc\u666f\u200b\u4e0b\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u7b2c\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\u5373\u5c06\u200b\u5230\u6765\u200b\u3002</p> <p>\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u7b2c\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\u6709\u200b\u4e24\u4e2a\u200b\u91cd\u8981\u200b\u7684\u200b\u65f6\u95f4\u200b\u8282\u70b9\u200b\uff1a2006\u200b\u5e74\u200b\u548c\u200b2016\u200b\u5e74\u200b\u30022006\u200b\u5e74\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u53d1\u5c55\u53f2\u200b\u7684\u200b\u5206\u6c34\u5cad\u200b\u3002\u200b\u6770\u5f17\u91cc\u200b\u8f9b\u987f\u200b\u5728\u200b\u8fd9\u200b\u4e00\u5e74\u200b\u53d1\u8868\u200b\u4e86\u200b\u300a\u200b\u4e00\u79cd\u200b\u6df1\u5ea6\u200b\u7f6e\u4fe1\u200b\u7f51\u7edc\u200b\u7684\u200b\u5feb\u901f\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u300b\uff0c\u200b\u5176\u4ed6\u200b\u91cd\u8981\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5b66\u672f\u200b\u6587\u7ae0\u200b\u4e5f\u200b\u5728\u200b\u8fd9\u200b\u4e00\u5e74\u200b\u88ab\u200b\u53d1\u5e03\u200b\uff0c\u200b\u5728\u200b\u57fa\u672c\u200b\u7406\u8bba\u200b\u5c42\u9762\u200b\u53d6\u5f97\u200b\u4e86\u200b\u82e5\u5e72\u200b\u91cd\u5927\u7a81\u7834\u200b\u3002\u200b\u800c\u200b2016\u200b\u5e74\u200b3\u200b\u6708\u200b\uff0c\u200b\u8c37\u6b4c\u200bDeepMind\u200b\u7814\u53d1\u200b\u7684\u200bAlphaGo\u200b\u5728\u200b\u56f4\u68cb\u200b\u4eba\u673a\u200b\u5927\u6218\u200b\u4e2d\u200b\u51fb\u8d25\u200b\u97e9\u56fd\u200b\u804c\u4e1a\u200b\u4e5d\u6bb5\u200b\u68cb\u624b\u200b\u674e\u4e16\u200b\u4e6d\u200b\uff0c\u201c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u201d\u200b\u4e00\u8bcd\u200b\u6b63\u5f0f\u200b\u8fdb\u5165\u200b\u666e\u901a\u200b\u6c11\u4f17\u200b\u7684\u200b\u89c6\u91ce\u200b\u5e76\u200b\u88ab\u200b\u9010\u6e10\u200b\u719f\u77e5\u200b\u3002\u200b\u81f3\u6b64\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u6b63\u5f0f\u200b\u8fc8\u5411\u200b\u4e86\u200b\u4ece\u200b\u201c\u200b\u79d1\u7814\u200b\u9886\u57df\u200b\u7684\u200b\u5e94\u7528\u578b\u200b\u5de5\u5177\u200b\u201d\u200b\u5230\u200b\u201c\u200b\u5b9e\u7528\u6027\u200b\uff0c\u200b\u529f\u80fd\u6027\u200b\u5de5\u5177\u200b\u201d\u200b\u7684\u200b\u8f6c\u53d8\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u6709\u200b\u4e86\u200b\u65b0\u200b\u7684\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u548c\u200b\u7814\u7a76\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5373\u200b\u4ece\u200b\u8fc7\u53bb\u200b\u7684\u200b\u5b66\u672f\u200b\u4e3b\u5bfc\u578b\u200b\u7814\u7a76\u200b\u9010\u6e10\u200b\u8d70\u5411\u200b\u4e86\u200b\u5546\u4e1a\u200b\u4e3b\u5bfc\u578b\u200b\u7814\u7a76\u200b\u3002\u200b\u968f\u7740\u200b\u4eba\u7c7b\u200b\u793e\u4f1a\u200b\u5bf9\u200b\u667a\u80fd\u5316\u200b\u5de5\u5177\u200b\u7684\u200b\u4e0d\u65ad\u200b\u8ffd\u6c42\u200b\u548c\u200b\u63a2\u7d22\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u8fce\u6765\u200b\u4e86\u200b\u5168\u65b0\u200b\u7684\u200b\u65f6\u4ee3\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#114","title":"1.1.4 \u200b\u603b\u7ed3","text":"<p>\u200b\u4e0a\u56fe\u200b\u662f\u200b\u5bf9\u200b\u4eba\u5de5\u667a\u80fd\u200b\u53d1\u5c55\u200b\u4e2d\u200b\u7ecf\u5386\u200b\u7684\u200b\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\u548c\u200b\u4e24\u6b21\u200b\u5bd2\u51ac\u200b\u7684\u200b\u5f62\u8c61\u200b\u603b\u7ed3\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6709\u200b\u89c2\u70b9\u200b\u8ba4\u4e3a\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u5e26\u6765\u200b\u7684\u200b\u201c\u200b\u6280\u672f\u200b\u7ea2\u5229\u200b\u201d\uff0c\u200b\u5c06\u200b\u652f\u6491\u200b\u6211\u4eec\u200b\u518d\u200b\u53d1\u5c55\u200b5~10\u200b\u5e74\u200b\u65f6\u95f4\u200b\uff0c\u200b\u968f\u540e\u200b\u5c31\u200b\u4f1a\u200b\u9047\u5230\u200b\u74f6\u9888\u200b\u3002\u200b\u4eba\u5de5\u667a\u80fd\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4ece\u200b1\u200b\u5230\u200b100\u200b\u8fdb\u6b65\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u5b83\u200b\u5f80\u5f80\u200b\u8d8b\u5411\u4e8e\u200b\u4e24\u4e2a\u200b\u6781\u7aef\u200b\uff1a\u200b\u8981\u4e48\u200b90\u200b\u5206\u200b\u4ee5\u4e0a\u200b\uff0c\u200b\u5176\u5b83\u200b\u7684\u200b\u90fd\u200b\u662f\u200b10\u200b\u5206\u200b\u4ee5\u4e0b\u200b\u3002\u200b\u76ee\u524d\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u6025\u9700\u200b\u5bfb\u627e\u200b\u5230\u200b\u4e00\u4e2a\u200b\u201c\u200b\u6280\u672f\u200b\u5947\u70b9\u200b\u201d\uff0c\u200b\u8ba9\u200b\u4eba\u5de5\u667a\u80fd\u200b\u8fc5\u901f\u200b\u53d1\u5c55\u200b\u5230\u200b\u901a\u7528\u200b\u4eba\u5de5\u667a\u80fd\u200b\u751a\u81f3\u200b\u662f\u200b\u8d85\u7ea7\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u6c34\u5e73\u200b\u3002\u200b\u5426\u5219\u200b\uff0c\u200b\u5728\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7814\u7a76\u200b\u5546\u4e1a\u5316\u200b\u7684\u200b\u4eca\u5929\u200b\uff0c\u200b\u65e0\u6cd5\u200b\u4ece\u4e2d\u200b\u83b7\u5229\u200b\u7684\u200b\u6295\u8d44\u200b\u4eba\u4eec\u200b\u5c06\u200b\u5feb\u901f\u200b\u64a4\u8d44\u200b\u9000\u573a\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u6216\u200b\u5c06\u200b\u8fdb\u5165\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u5bd2\u51ac\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#12-dlmlai","title":"1.2 DL,ML,AI\u200b\u4e09\u8005\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb","text":"<p>\u200b\u5927\u5bb6\u200b\u5bf9\u200b\u201c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u201d\u200b\u8fd9\u4e2a\u200b\u8bcd\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u6240\u8c13\u200b\u7684\u200b\u201cAI\u201d\uff08Artificial Intelligence\uff09\u200b\u60f3\u5fc5\u200b\u662f\u200b\u975e\u5e38\u200b\u719f\u6089\u200b\uff0c\u200b\u65e0\u8bba\u662f\u200b\u8fd1\u51e0\u5e74\u200b\u5404\u884c\u5404\u4e1a\u200b\u90fd\u200b\u559c\u6b22\u200b\u7528\u4f5c\u200b\u8425\u9500\u200b\u5671\u5934\u200b\u7684\u200b\u201c\u200b\u667a\u80fd\u5316\u200b\u201d\u200b\u8fd8\u662f\u200b\u65e9\u671f\u200b\u7535\u5f71\u200b\u5982\u200b\u300a\u200b\u9ed1\u5ba2\u5e1d\u56fd\u200b\u300b\u3001\u300a\u200b\u7ec8\u7ed3\u8005\u200b\u300b\u200b\u7b49\u200b\uff0c\u200b\u90fd\u200b\u8ba9\u200bAI\u200b\u8fd9\u4e2a\u200b\u6982\u5ff5\u200b\u6df1\u5165\u4eba\u5fc3\u200b\u3002\u200b\u4f46\u200b\u8fd1\u51e0\u5e74\u200b\uff0c\u200b\u53e6\u5916\u200b\u4e24\u4e2a\u200b\u8bcd\u8bed\u200b\u4e5f\u200b\u5728\u200b\u9010\u6b65\u200b\u8fdb\u5165\u200b\u6211\u4eec\u200b\u7684\u200b\u751f\u6d3b\u200b\uff0c\u200b\u5373\u200b\u5c31\u662f\u200b\u201c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff08Machine Learning\uff0cML\uff09\u201d\u200b\u548c\u200b\u201c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff08Deep Learning\uff0cDL\uff09\u201d\u3002\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u53d9\u8ff0\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5c06\u200b\u4e86\u89e3\u200bDL\u200b\u548c\u200bML\u200b\u7a76\u7adf\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5b83\u4eec\u200b\u548c\u200bAI\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#121-dlml","title":"1.2.1 DL\u200b\u548c\u200bML\u200b\u662f\u200b\u4ec0\u4e48","text":"<p>Machine Learning\uff08\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff09\u3002\u200b\u5b83\u200b\u5728\u200b1959\u200b\u5e74\u200b\u88ab\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u5148\u9a71\u8005\u200b\u4e4b\u4e00\u200b\u7684\u200b\u963f\u745f\u200b\u00b7\u200b\u585e\u7f2a\u5c14\u200b\u5b9a\u4e49\u200b\u4e3a\u200b\uff1a\u200b\u4e00\u95e8\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\uff0c\u200b\u5b83\u200b\u8d4b\u4e88\u200b\u8ba1\u7b97\u673a\u200b\u65e0\u9700\u200b\u660e\u786e\u200b\u7f16\u7a0b\u200b\u5c31\u200b\u80fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u80fd\u529b\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7a0b\u5e8f\u200b\u4e0d\u540c\u4e8e\u200b\u4f20\u7edf\u200b\u7f16\u7a0b\u200b\u90a3\u6837\u200b\uff0c\u200b\u4f7f\u7528\u200bif-then\u200b\u8bed\u53e5\u200b\u90a3\u6837\u200b\u660e\u786e\u200b\u5730\u200b\u8f93\u5165\u200b\u5230\u200b\u8ba1\u7b97\u673a\u200b\u4e2d\u200b\u4ee5\u4fbf\u200b\u5b83\u200b\u6839\u636e\u200b\u6761\u4ef6\u200b\u6267\u884c\u200b\u3002\u200b\u5728\u200b\u67d0\u79cd\u610f\u4e49\u200b\u4e0a\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7a0b\u5e8f\u200b\u8d4b\u4e88\u200b\u673a\u5668\u200b\u6839\u636e\u200b\u6240\u200b\u63a5\u89e6\u200b\u5230\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u81ea\u6211\u200b\u8c03\u6574\u200b\u7684\u200b\u80fd\u529b\u200b\u3002\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u66f4\u200b\u50cf\u662f\u200b\u4e00\u79cd\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5728\u200b\u4e8b\u5148\u200b\u5c31\u200b\u5bf9\u200b\u5b83\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6b63\u786e\u200b\u7684\u200b\u8c03\u6574\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u5c31\u200b\u4f1a\u200b\u5728\u200b\u4e00\u904d\u200b\u53c8\u200b\u4e00\u904d\u200b\u7684\u200b\u5c1d\u8bd5\u200b\u548c\u200b\u731c\u6d4b\u200b\u4e4b\u4e2d\u200b\u4e0d\u65ad\u200b\u51cf\u5c11\u200b\u5b83\u200b\u7684\u200b\u9519\u8bef\u200b\uff0c\u200b\u4ee5\u200b\u65e0\u9650\u200b\u903c\u8fd1\u200b\u4e8e\u200b\u6700\u7ec8\u200b\u7684\u200b\u6b63\u786e\u200b\u7ed3\u679c\u200b\u3002\u200b\u800c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u672c\u601d\u8def\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u5c06\u200b\u73b0\u5b9e\u200b\u95ee\u9898\u200b\u62bd\u8c61\u200b\u6210\u4e3a\u200b\u4e00\u4e2a\u200b\u6570\u5b66\u200b\u95ee\u9898\u200b\uff0c\u200b\u673a\u5668\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5bfb\u627e\u200b\u5230\u200b\u89e3\u51b3\u200b\u6570\u5b66\u200b\u95ee\u9898\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fdb\u800c\u200b\u89e3\u51b3\u200b\u73b0\u5b9e\u200b\u95ee\u9898\u200b\u3002</p> <p>Deep Learning\uff08\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff09\u3002\u200b\u5b83\u200b\u5728\u200b2006\u200b\u5e74\u200b\u88ab\u200b\u63d0\u51fa\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u8fd1\u4e9b\u5e74\u200b\u5f97\u5230\u200b\u4e86\u200b\u8fc5\u901f\u200b\u7684\u200b\u53d1\u5c55\u200b\u3002\u200b\u5b83\u200b\u901a\u8fc7\u200b\u5efa\u7acb\u200b\u3001\u200b\u6a21\u62df\u200b\u4eba\u8111\u200b\u8fdb\u884c\u200b\u5206\u6790\u200b\u5b66\u4e60\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5e76\u200b\u6a21\u4eff\u200b\u4eba\u8111\u200b\u7684\u200b\u673a\u5236\u200b\u6765\u200b\u89e3\u91ca\u200b\u6570\u636e\u200b\u3002\u200b\u674e\u5f00\u590d\u200b\u6559\u6388\u200b\u5728\u200b\u300a\u200b\u4eba\u5de5\u667a\u80fd\u200b\u300b\u200b\u4e00\u4e66\u4e2d\u200b\u8fd9\u6837\u200b\u89e3\u91ca\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff1a\u201c\u200b\u5047\u8bbe\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u8981\u200b\u5904\u7406\u200b\u7684\u200b\u4fe1\u606f\u200b\u662f\u200b\u201c\u200b\u6c34\u6d41\u200b\u201d\uff0c\u200b\u800c\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7f51\u7edc\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7531\u200b\u7ba1\u9053\u200b\u548c\u200b\u9600\u95e8\u200b\u7ec4\u6210\u200b\u7684\u200b\u5de8\u5927\u200b\u6c34\u7ba1\u200b\u7f51\u7edc\u200b\u3002\u200b\u7f51\u7edc\u200b\u7684\u200b\u5165\u53e3\u200b\u662f\u200b\u82e5\u5e72\u200b\u7ba1\u9053\u200b\u5f00\u53e3\u200b\uff0c\u200b\u7f51\u7edc\u200b\u7684\u200b\u51fa\u53e3\u200b\u4e5f\u200b\u662f\u200b\u82e5\u5e72\u200b\u7ba1\u9053\u200b\u5f00\u53e3\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u6c34\u7ba1\u200b\u7f51\u7edc\u200b\u6709\u200b\u8bb8\u591a\u200b\u5c42\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7531\u200b\u8bb8\u591a\u200b\u4e2a\u200b\u53ef\u4ee5\u200b\u63a7\u5236\u200b\u6c34\u6d41\u200b\u6d41\u5411\u200b\u4e0e\u200b\u6d41\u91cf\u200b\u7684\u200b\u8c03\u8282\u9600\u200b\u3002\u200b\u6839\u636e\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\u7684\u200b\u9700\u8981\u200b\uff0c\u200b\u6c34\u7ba1\u200b\u7f51\u7edc\u200b\u7684\u200b\u5c42\u6570\u200b\u3001\u200b\u6bcf\u5c42\u200b\u7684\u200b\u8c03\u8282\u9600\u200b\u6570\u91cf\u200b\u53ef\u4ee5\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u53d8\u5316\u200b\u7ec4\u5408\u200b\u3002\u200b\u5bf9\u200b\u590d\u6742\u200b\u4efb\u52a1\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8c03\u8282\u9600\u200b\u7684\u200b\u603b\u6570\u200b\u53ef\u4ee5\u200b\u6210\u5343\u4e0a\u4e07\u200b\u751a\u81f3\u200b\u66f4\u200b\u591a\u200b\u3002\u200b\u6c34\u7ba1\u200b\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u8c03\u8282\u9600\u200b\u90fd\u200b\u901a\u8fc7\u200b\u6c34\u7ba1\u200b\u4e0e\u200b\u4e0b\u200b\u4e00\u5c42\u200b\u7684\u200b\u6240\u6709\u200b\u8c03\u8282\u9600\u200b\u8fde\u63a5\u8d77\u6765\u200b\uff0c\u200b\u7ec4\u6210\u200b\u4e00\u4e2a\u200b\u4ece\u524d\u200b\u5230\u200b\u540e\u200b\uff0c\u200b\u9010\u5c42\u200b\u5b8c\u5168\u200b\u8fde\u901a\u200b\u7684\u200b\u6c34\u6d41\u200b\u7cfb\u7edf\u200b\u3002\u201d</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.1%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%80%E5%8F%B2/#122-ai","title":"1.2.2 \u200b\u5b83\u4eec\u200b\u548c\u200bAI\u200b\u7684\u200b\u5173\u7cfb","text":"<p>\u200b\u4f17\u6240\u5468\u77e5\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u662f\u200b\u7814\u7a76\u200b\u3001\u200b\u5f00\u53d1\u200b\u7528\u4e8e\u200b\u6a21\u62df\u200b\u3001\u200b\u5ef6\u4f38\u200b\u548c\u200b\u6269\u5c55\u200b\u4eba\u200b\u7684\u200b\u667a\u80fd\u200b\u7684\u200b\u7406\u8bba\u200b\u3001\u200b\u65b9\u6cd5\u200b\u3001\u200b\u6280\u672f\u200b\u53ca\u200b\u5e94\u7528\u200b\u7cfb\u7edf\u200b\u7684\u200b\u4e00\u95e8\u200b\u6280\u672f\u200b\u79d1\u5b66\u200b\u3002\u200b\u65e2\u7136\u5982\u6b64\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8ba1\u7b97\u5668\u200b\u7b97\u662f\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5417\u200b\uff1f\u200b\u4e25\u683c\u200b\u5730\u200b\u8bf4\u200b\u662f\u200b\u7b97\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u81f3\u5c11\u200b\u505a\u200b\u4e86\u200b\u201c\u200b\u6a21\u62df\u200b\u201d\u200b\u4eba\u200b\u5728\u200b\u8ba1\u7b97\u200b\u65b9\u9762\u200b\u7684\u200b\u667a\u80fd\u200b\uff0c\u200b\u5e76\u200b\u6269\u5c55\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u80fd\u529b\u200b\uff08\u200b\u6bd4\u4eba\u7b97\u200b\u5f97\u200b\u66f4\u200b\u5feb\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4ee3\u7801\u200b\u9a71\u52a8\u200b\u8ba1\u7b97\u673a\u200b\u53bb\u200b\u5e2e\u200b\u6211\u4eec\u200b\u5e72\u6d3b\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u7b97\u662f\u200b\u4eba\u5de5\u667a\u80fd\u200b\u5417\u200b\uff1f\u200b\u4e5f\u200b\u7b97\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u770b\u5230\u200b\u7684\u200b\u8c8c\u4f3c\u200b\u5f88\u200b\u9ad8\u7aef\u200b\u7684\u200b\u6280\u672f\u200b\uff0c\u200b\u5982\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u3001NLP\uff0c\u200b\u5176\u5b9e\u200b\u4f9d\u7136\u200b\u6ca1\u6709\u200b\u8131\u79bb\u200b\u8fd9\u4e2a\u200b\u8303\u56f4\u200b\uff0c\u200b\u8bf4\u767d\u4e86\u200b\uff0c\u200b\u5c31\u662f\u200b\u201c\u200b\u6a21\u62df\u200b\u4eba\u200b\u5728\u200b\u770b\u56fe\u200b\u65b9\u9762\u200b\u7684\u200b\u667a\u80fd\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u6a21\u62df\u200b\u4eba\u200b\u5728\u200b\u542c\u8bdd\u200b\u65b9\u9762\u200b\u7684\u200b\u667a\u80fd\u200b\u201d\uff0c\u200b\u672c\u8d28\u200b\u4e0a\u200b\u548c\u200b\u201c\u200b\u6a21\u62df\u200b\u4eba\u200b\u5728\u200b\u8ba1\u7b97\u200b\u65b9\u9762\u200b\u7684\u200b\u667a\u80fd\u200b\u201d\u200b\u6ca1\u200b\u5565\u200b\u4e24\u6837\u200b\uff0c\u200b\u867d\u7136\u200b\u96be\u5ea6\u200b\u6709\u200b\u9ad8\u4f4e\u200b\uff0c\u200b\u4f46\u200b\u76ee\u7684\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\u2014\u2014\u200b\u6a21\u62df\u200b\u3001\u200b\u5ef6\u4f38\u200b\u548c\u200b\u6269\u5c55\u200b\u4eba\u200b\u7684\u200b\u667a\u80fd\u200b\u3002</p> <p>\u200b\u968f\u7740\u200b\u4eba\u200b\u5bf9\u200b\u8ba1\u7b97\u673a\u200b\u7684\u200b\u671f\u671b\u200b\u8d8a\u6765\u8d8a\u200b\u9ad8\u200b\uff0c\u200b\u8981\u6c42\u200b\u5b83\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u8d8a\u6765\u8d8a\u200b\u590d\u6742\u200b\uff0c\u200b\u4ec5\u4ec5\u200b\u7b97\u200b\u7684\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u770b\u200b\u7684\u200b\u66f4\u51c6\u200b\u5df2\u7ecf\u200b\u8fdc\u8fdc\u200b\u4e0d\u80fd\u200b\u6ee1\u8db3\u200b\u4eba\u4eec\u200b\u7684\u200b\u8bc9\u6c42\u200b\u4e86\u200b\u3002\u200b\u8981\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u57df\u200b\u8d8a\u6765\u8d8a\u200b\u590d\u6742\u200b\uff0c\u200b\u5373\u4f7f\u200b\u662f\u200b\u540c\u4e00\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u5176\u200b\u9762\u5bf9\u200b\u7684\u200b\u573a\u666f\u200b\u4e5f\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u3002\u200b\u4f20\u7edf\u200b\u7684\u200b\u601d\u8def\u200b\u5c31\u662f\u200b\u67e5\u627e\u200b\u95ee\u9898\u200b\u7684\u200b\u6761\u4ef6\u200b\u548c\u200b\u89e3\u51b3\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u200b\u4e2d\u200b\u518d\u200b\u52a0\u5165\u200b\u4e00\u4e2a\u200bif-then\u3002\u200b\u4f46\u200b\u8fd9\u200b\u53ea\u662f\u200b\u6cbb\u6807\u4e0d\u6cbb\u672c\u200b\u3002\u200b\u968f\u7740\u200b\u6211\u4eec\u200b\u671f\u5f85\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\uff0c\u200b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u200b\u5c06\u200b\u8d8a\u6765\u8d8a\u200b\u590d\u6742\u200b\uff0c\u200b\u8d8a\u6765\u8d8a\u200b\u96be\u4ee5\u200b\u7ef4\u62a4\u200b\u3002\u200b\u90a3\u200b\u600e\u4e48\u529e\u200b\u5462\u200b\uff1f\u200b\u4e8e\u662f\u200b\u6709\u4eba\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u601d\u8def\u200b\u2014\u2014\u200b\u80fd\u5426\u200b\u4e0d\u200b\u4e3a\u96be\u200b\u7801\u519c\u200b\uff0c\u200b\u8ba9\u200b\u673a\u5668\u200b\u81ea\u5df1\u200b\u53bb\u200b\u5b66\u4e60\u200b\u5462\u200b\uff1f</p> <p>\u200b\u81f3\u6b64\u200b\uff0c\u201c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u201d\u200b\u7684\u200b\u6982\u5ff5\u200b\uff0c\u200b\u6b63\u5f0f\u200b\u8bde\u751f\u200b\u3002\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5c31\u662f\u200b\u7528\u200b\u7b97\u6cd5\u200b\u89e3\u6790\u200b\u6570\u636e\u200b\uff0c\u200b\u4e0d\u65ad\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5bf9\u200b\u4e16\u754c\u200b\u4e2d\u200b\u53d1\u751f\u200b\u7684\u200b\u4e8b\u200b\u505a\u51fa\u200b\u5224\u65ad\u200b\u548c\u200b\u9884\u6d4b\u200b\u7684\u200b\u4e00\u9879\u200b\u6280\u672f\u200b\u3002\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u4e0d\u4f1a\u200b\u4eb2\u624b\u200b\u7f16\u5199\u200b\u8f6f\u4ef6\u200b\u3001\u200b\u786e\u5b9a\u200b\u7279\u6b8a\u200b\u6307\u4ee4\u96c6\u200b\u3001\u200b\u7136\u540e\u200b\u8ba9\u200b\u7a0b\u5e8f\u200b\u5b8c\u6210\u200b\u7279\u6b8a\u4efb\u52a1\u200b\uff1b\u200b\u76f8\u53cd\u200b\uff0c\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u4f1a\u200b\u7528\u200b\u5927\u91cf\u200b\u6570\u636e\u200b\u548c\u200b\u7b97\u6cd5\u200b\u201c\u200b\u8bad\u7ec3\u200b\u201d\u200b\u673a\u5668\u200b\uff0c\u200b\u8ba9\u200b\u673a\u5668\u200b\u81ea\u884c\u200b\u5b66\u4f1a\u200b\u5982\u4f55\u200b\u6267\u884c\u200b\u4efb\u52a1\u200b\u3002\u200b\u8bf4\u767d\u4e86\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u53ea\u662f\u200b\u4eba\u4eec\u200b\u5b9e\u73b0\u200b\u8ba9\u200b\u673a\u5668\u200b\u201c\u200b\u6a21\u62df\u200b\u3001\u200b\u5ef6\u4f38\u200b\u548c\u200b\u6269\u5c55\u200b\u4eba\u200b\u7684\u200b\u667a\u80fd\u200b\u201d\u200b\u7684\u200b\u4e00\u79cd\u200b\u8f83\u4e3a\u200b\u8f7b\u677e\u200b\u7684\u200b\u65b9\u6cd5\u200b\u7f62\u4e86\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u6210\u529f\u200b\u4e0e\u5426\u200b\u53d6\u51b3\u4e8e\u200b\u6211\u4eec\u200b\u5582\u7ed9\u200b\u673a\u5668\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u662f\u5426\u200b\u51c6\u786e\u200b\u4e14\u200b\u6709\u6548\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u662f\u200b\u5927\u200b\u6570\u636e\u200b\u6280\u672f\u200b\u9886\u57df\u200b\u5185\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5e94\u7528\u200b\uff0c\u200b\u4eba\u4eec\u200b\u53ea\u662f\u200b\u501f\u7528\u200b\u8fd9\u4e2a\u200b\u5e94\u7528\u200b\uff0c\u200b\u6765\u200b\u53d1\u5c55\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7f62\u4e86\u200b\u3002\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u53d1\u5c55\u200b\u4e86\u200b\u51e0\u5341\u5e74\u200b\u4e4b\u540e\u200b\uff0c\u200b\u518d\u6b21\u200b\u9047\u5230\u200b\u4e86\u200b\u74f6\u9888\u200b\u671f\u200b\u3002\u200b\u968f\u7740\u200b\u95ee\u9898\u200b\u573a\u666f\u200b\u7684\u200b\u66f4\u52a0\u200b\u590d\u6742\u591a\u53d8\u200b\uff0c\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u5224\u65ad\u200b\u7684\u200b\u6761\u4ef6\u200b\u66f4\u52a0\u200b\u82db\u523b\u200b\uff0c\u200b\u4eba\u4eec\u200b\u4e0d\u5f97\u4e0d\u200b\u91cd\u65b0\u200b\u601d\u8003\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u6765\u200b\u4f18\u5316\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u3002\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5c31\u662f\u200b\u5e26\u200b\u7740\u200b\u8fd9\u4e2a\u200b\u76ee\u7684\u200b\u88ab\u200b\u63d0\u51fa\u200b\u7684\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u6709\u200b\u4e00\u4e2a\u200b\u6982\u5ff5\u200b\u53eb\u200b\u201c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u201d\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6b63\u662f\u200b\u901a\u8fc7\u200b\u4f18\u5316\u200b\u8fd9\u4e2a\u200b\u7f51\u7edc\u200b\u6765\u200b\u66f4\u597d\u200b\u7684\u200b\u89e3\u51b3\u200b\u901a\u8fc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u96be\u4ee5\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u57fa\u672c\u200b\u7279\u70b9\u200b\uff0c\u200b\u5c31\u662f\u200b\u8bd5\u56fe\u200b\u6a21\u4eff\u200b\u5927\u8111\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u4e4b\u95f4\u200b\u4f20\u9012\u200b\uff0c\u200b\u5904\u7406\u200b\u4fe1\u606f\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4e0d\u540c\u200b\u7684\u200b\u201c\u200b\u5c42\u200b\u201d\u200b\u6765\u200b\u62c6\u5206\u200b\u95ee\u9898\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u89e3\u51b3\u95ee\u9898\u200b\u7684\u200b\u4e00\u4e2a\u200b\u90e8\u5206\u200b\u3002\u200b\u6bd4\u5982\u200b\u5728\u200b\u5229\u7528\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u89e3\u51b3\u200b\u667a\u80fd\u200b\u9a7e\u9a76\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u7b2c\u4e00\u5c42\u200b\u53ef\u80fd\u200b\u7528\u4e8e\u200b\u8bc6\u522b\u200b\u8f66\u8f86\u200b\u4e0e\u200b\u9053\u8def\u200b\u8fb9\u7f18\u200b\u7684\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u7b2c\u4e8c\u5c42\u200b\u7528\u4e8e\u200b\u8bc6\u522b\u200b\u9053\u8def\u200b\u6807\u7ebf\u200b\uff0c\u200b\u7b2c\u4e09\u5c42\u200b\u7528\u4e8e\u200b\u8bc6\u522b\u200b\u8def\u4e0a\u200b\u7684\u200b\u5176\u4ed6\u200b\u8f66\u8f86\u200b\u7b49\u7b49\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u4ee5\u4e0a\u200b\u51e0\u6bb5\u200b\u8bdd\u200b\u7684\u200b\u7b80\u5355\u200b\u63cf\u8ff0\u200b\uff0cDL,ML\u200b\u548c\u200bAI\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u4e5f\u200b\u5c31\u200b\u660e\u786e\u200b\u4e86\u200b\u3002\u200b\u5b83\u4eec\u200b\u4e09\u8005\u200b\u7684\u200b\u5173\u7cfb\u200b\u5c31\u200b\u50cf\u662f\u200b\u4fc4\u7f57\u65af\u200b\u5957\u5a03\u200b\uff1aAI\u200b\u6700\u5927\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u901a\u8fc7\u200b\u8ba9\u200b\u673a\u5668\u200b\u6a21\u4eff\u200b\u4eba\u7c7b\u200b\u8fdb\u800c\u200b\u8d85\u8d8a\u200b\u4eba\u7c7b\u200b\uff1bML\u200b\u6b21\u4e4b\u200b\uff0c\u200b\u5b83\u200b\u662f\u200bAI\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5206\u652f\u200b\uff08\u200b\u4e5f\u200b\u662f\u200b\u6700\u200b\u91cd\u8981\u200b\u5206\u652f\u200b\uff09\uff0c\u200b\u662f\u200b\u8ba9\u200b\u673a\u5668\u200b\u6a21\u4eff\u200b\u4eba\u7c7b\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\uff1bDL\u200b\u66f4\u200b\u6b21\u4e4b\u200b\uff0c\u200b\u5b83\u200b\u662f\u200bML\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5206\u652f\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u8ba9\u200b\u673a\u5668\u200b\u4e0d\u200b\u501f\u52a9\u200b\u4eba\u5de5\u200b\u6807\u6ce8\u200b\uff0c\u200b\u4e5f\u200b\u80fd\u200b\u81ea\u4e3b\u200b\u63d0\u53d6\u200b\u76ee\u6807\u200b\u7279\u5f81\u200b\u8fdb\u800c\u200b\u89e3\u51b3\u95ee\u9898\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u501f\u7528\u200b\u4e00\u5f20\u200b\u7ecf\u5178\u200b\u7684\u200b\u5173\u7cfb\u200b\u56fe\u200b\u4f5c\u4e3a\u200b\u7ed3\u5c3e\u200b\uff1a</p> <p></p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/","title":"\u6a21\u578b\u200b\u8bc4\u4ef7\u200b\u6307\u6807","text":"<p>\u200b\u5728\u200b\u6211\u4eec\u200b\u5b66\u4e60\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ee5\u53ca\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff0c\u200b\u751a\u81f3\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u53ef\u907f\u514d\u200b\u7684\u200b\u8981\u200b\u5229\u7528\u200b\u4e00\u4e9b\u200b\u6307\u6807\u200b\u8bc4\u4ef7\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u672c\u200b\u5c0f\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u6a21\u578b\u200b\u76f8\u5173\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\uff0c</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b \uff0c\u200b\u6211\u4eec\u200b\u5c06\u4f1a\u200b\u5b66\u4e60\u200b\u5230\u200b\uff1a</p> <ul> <li>\u200b\u6a21\u578b\u200b\u5e38\u7528\u200b\u7684\u200b\u76f8\u5173\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b</li> <li>\u200b\u4e0d\u540c\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\u7684\u200b\u9002\u5e94\u6027\u200b\u4ee5\u53ca\u200b\u4f18\u7f3a\u70b9\u200b</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#_2","title":"\u6df7\u6dc6\u200b\u77e9\u9635","text":"<p>\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff08\u200b\u4e5f\u200b\u79f0\u200b\u8bef\u5dee\u200b\u77e9\u9635\u200b\uff09\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u8868\u793a\u200b\u7cbe\u5ea6\u200b\u8bc4\u4ef7\u200b\u7684\u200b\u4e00\u79cd\u200b\u6807\u51c6\u200b\u683c\u5f0f\u200b\uff0c\u200b\u5e38\u7528\u200bn\u200b\u884c\u200bn\u200b\u5217\u200b\u7684\u200b\u77e9\u9635\u200b\u5f62\u5f0f\u200b\u6765\u200b\u8868\u793a\u200b\u3002\u200b\u5176\u5217\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u9884\u6d4b\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u884c\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5b9e\u9645\u200b\u7684\u200b\u7c7b\u6807\u200b\uff0c\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u4e3a\u4f8b\u200b\u3002\u200b\u6211\u4eec\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u5305\u62ec\u200bTP, FP, FN, TN\uff0c\u200b\u5176\u4e2d\u200bTP\u200b\u4e3a\u200bTrue Positive\uff0cTrue\u200b\u4ee3\u8868\u200b\u5b9e\u9645\u200b\u548c\u200b\u9884\u6d4b\u200b\u76f8\u540c\u200b\uff0cPositive\u200b\u4ee3\u8868\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\u3002\u200b\u540c\u7406\u200b\u53ef\u200b\u5f97\u200b\uff0cFalse Positive (FP)\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u548c\u200b\u9884\u6d4b\u200b\u7c7b\u6807\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\uff1bFalse Negative (FN)\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u548c\u200b\u9884\u6d4b\u200b\u7c7b\u6807\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\uff0c\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff1bTrue Negative (TP)\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u548c\u200b\u9884\u6d4b\u200b\u7c7b\u6807\u200b\u76f8\u540c\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u548c\u200b\u5b9e\u9645\u200b\u7c7b\u522b\u200b\u5747\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\u3002</p> <p></p> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u4f8b\u5b50\u200b\u6765\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u7406\u89e3\u200b\u3002\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u662f\u200b\u67d0\u200b\u6838\u9178\u200b\u68c0\u6d4b\u200b\u673a\u6784\u200b\uff0c\u200b\u5c06\u200b\u5bf9\u200b100\u200b\u4e2a\u4eba\u200b\u8fdb\u884c\u200b\u6838\u9178\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u5b9e\u9645\u200b\u7ed3\u679c\u200b\u4e3a\u200b98\u200b\u4e2a\u200b\u9634\u6027\u200b\uff0c2\u200b\u4e2a\u200b\u9633\u6027\u200b\uff0c\u200b\u4f46\u662f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6838\u9178\u200b\u68c0\u6d4b\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e3a\u200b94\u200b\u4e2a\u200b\u9634\u6027\u200b\uff0c6\u200b\u4e2a\u200b\u9633\u6027\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u6838\u9178\u200b\u7ed3\u679c\u200b\u9634\u6027\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u6838\u9178\u200b\u7ed3\u679c\u200b\u9633\u6027\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0cTP\u200b\u4ee3\u8868\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9634\u6027\u200b\u4e14\u200b\u88ab\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u9634\u6027\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u5171\u6709\u200b94\u200b\u4eba\u200b\uff1bFP\u200b\u4ee3\u8868\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9633\u6027\u200b\uff0c\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u9634\u6027\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u5171\u6709\u200b0\u200b\u4eba\u200b\uff1bFN\u200b\u4ee3\u8868\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9634\u6027\u200b\u88ab\u200b\u6a21\u578b\u200b\u5224\u65ad\u200b\u4e3a\u200b\u9633\u6027\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u5171\u6709\u200b4\u200b\u4eba\u200b\uff1bTN\u200b\u4ee3\u8868\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9633\u6027\u200b\uff0c\u200b\u88ab\u200b\u6a21\u578b\u200b\u8bc6\u522b\u200b\u4e3a\u200b\u9633\u6027\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u5171\u6709\u200b2\u200b\u4eba\u200b\u3002 \u200b\u4e8e\u662f\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u4e0b\u9762\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</p> \\[ \\begin{bmatrix}     94 &amp; 0 \\\\     4 &amp; 2   \\end{bmatrix} \\tag{2} \\] <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u6b64\u200b\u4e5f\u200b\u5c55\u793a\u200b\u4e00\u4e2a\u200b10\u200b\u7c7b\u6807\u200b+1\u200b\u4e2a\u200b\u80cc\u666f\u200b\u6240\u200b\u4ea7\u751f\u200b\u7684\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u62e5\u6709\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200bAccuracy\uff0cPrecision\uff0cRecall\uff0cF1 Score\u200b\u7b49\u200b\u8861\u91cf\u200b\u6a21\u578b\u200b\u7684\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\u3002\u200b\u5173\u4e8e\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>sklearn.metrics.confusion_matrix()</code>\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b</p> <pre><code>from sklearn.metrics import confusion_matrix\ndef compute_confusion_matrix(labels,pred_labels_list,gt_labels_list):\n    pred_labels_list = np.asarray(pred_labels_list)\n    gt_labels_list = np.assarray(gt_labels_list)\n    matrix = confusion_matrix(test_label_list,\n                              pred_label_list,\n                              labels=labels)\n    return matrix\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#overall-accuracy","title":"Overall Accuracy","text":"<p>Overall Accuracy\u200b\u4ee3\u8868\u200b\u4e86\u200b\u6240\u6709\u200b\u9884\u6d4b\u200b\u6b63\u786e\u200b\u7684\u200b\u6837\u672c\u200b\u5360\u200b\u6240\u6709\u200b\u9884\u6d4b\u200b\u6837\u672c\u200b\u603b\u6570\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u7ed3\u5408\u200b\u4e0a\u8ff0\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e0b\u8ff0\u200b\u516c\u5f0f\u200b\u8868\u793a\u200b\uff1a $$ \\rm{OA} =\\frac{\\rm{TP+TN}}{\\rm{TP+TN+FP+FN}} = \\frac{N_{correct}}{N_{total}} $$ \u200b\u8fd9\u91cc\u200b\u7684\u200b\u5206\u7c7b\u200b\u6b63\u786e\u200b\u4ee3\u8868\u200b\u4e86\u200b\u6b63\u200b\u6837\u672c\u200b\u88ab\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u8d1f\u200b\u6837\u672c\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\u3002\u200b\u51c6\u786e\u7387\u200b\u662f\u200b\u63cf\u8ff0\u200b\u6a21\u578b\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u6307\u6807\u200b\uff0cAcc\u200b\u7684\u200b\u7f3a\u70b9\u200b\u4e3b\u8981\u200b\u5728\u4e8e\u200b\u5047\u5982\u200b\u8bf4\u200b\u6211\u4eec\u200b\u4e3e\u5f97\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u5728\u200b\u6837\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6838\u9178\u200b\u7ed3\u679c\u200b\u6210\u200b\u9634\u6027\u200b\u5360\u636e\u200b\u4e86\u200b\u6a21\u578b\u200b98%\uff0c \u200b\u5047\u5982\u200b\u8bf4\u200b\u6a21\u578b\u200b\u4e0d\u200b\u8c03\u8bd5\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u200b\u53ea\u8981\u200b\u4e00\u76f4\u200b\u5224\u65ad\u200b\u6838\u9178\u200b\u7ed3\u679c\u200b\u5448\u200b\u9634\u6027\u200b\uff0c\u200b\u90a3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6700\u7ec8\u200b\u6548\u679c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u66f4\u597d\u200b\uff0c\u200b\u4f46\u662f\u200b\u5b9e\u9645\u4e0a\u200b\u6a21\u578b\u200b\u7684\u200b\u6548\u7387\u200b\u4ee5\u53ca\u200b\u51c6\u786e\u7387\u200b\u90fd\u200b\u662f\u200b\u975e\u5e38\u200b\u4f4e\u4e0b\u200b\u7684\u200b\u3002\u200b\u5f53\u200b\u6211\u4eec\u200b\u7684\u200b\u6837\u672c\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u6781\u7aef\u200b\u4e0d\u200b\u5e73\u8861\u200b\u65f6\u200bAcc\u200b\u8fd9\u4e2a\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\u5176\u5b9e\u200b\u662f\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u518d\u200b\u56de\u5230\u200b\u4e0a\u9762\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u5728\u200b\u8be5\u200b\u4f8b\u5b50\u200b\u4e2d\u200bAcc = 0.96. $$ OA=\\frac{\\rm{TP+TN}}{100}= \\frac{96}{100}= 0.96 $$</p> <pre><code>def compute_oa(matrix):\n    \"\"\"\n    \u200b\u8ba1\u7b97\u200b\u603b\u4f53\u200b\u51c6\u786e\u7387\u200b,OA=(TP+TN)/(TP+TN+FP+FN)\n    :param matrix:\n    :return:\n    \"\"\"\n    return np.trace(matrix) / np.sum(matrix)\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#average-accuracy","title":"Average accuracy","text":"<p>Average accuracy( AA) \u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5e73\u5747\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5e73\u5747\u200b\u7cbe\u5ea6\u200b\u8ba1\u7b97\u200b\u7684\u200b\u662f\u200b\u6bcf\u200b\u4e00\u7c7b\u200b\u9884\u6d4b\u200b\u6b63\u786e\u200b\u7684\u200b\u6837\u672c\u200b\u4e0e\u200b\u8be5\u7c7b\u200b\u603b\u4f53\u200b\u6570\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6bd4\u503c\u200b\uff0c\u200b\u6700\u7ec8\u200b\u518d\u53d6\u200b\u6bcf\u200b\u4e00\u7c7b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u3002\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bnumpy\u200b\u7684\u200bdiag\u200b\u5c06\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u7684\u200b\u5bf9\u89d2\u7ebf\u200b\u5143\u7d20\u200b\u53d6\u51fa\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5bf9\u4e8e\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u8fdb\u884c\u200b\u5217\u200b\u6c42\u548c\u200b\uff0c\u200b\u7528\u200b\u5bf9\u89d2\u7ebf\u200b\u5143\u7d20\u200b\u9664\u200b\u4ee5\u6c42\u200b\u548c\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6700\u540e\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u8ba1\u7b97\u200b\u6c42\u200b\u51fa\u200b\u5e73\u5747\u503c\u200b\u3002</p> <pre><code>def compute_aa(matrix):\n    \"\"\"\n    \u200b\u8ba1\u7b97\u200b\u6bcf\u200b\u4e00\u7c7b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b,AA=(TP/(TP+FN)+TN/(FP+TN))/2\n    :param matrix:\n    :return:\n    \"\"\"\n    return np.mean(np.diag(matrix) / np.sum(matrix, axis=1))\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#kappa","title":"Kappa\u200b\u7cfb\u6570","text":"<p>Kappa\u200b\u7cfb\u6570\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u4e00\u81f4\u6027\u200b\u68c0\u9a8c\u200b\u7684\u200b\u6307\u6807\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u7528\u4e8e\u200b\u8861\u91cf\u200b\u5206\u7c7b\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u800c\u8a00\u200b\uff0c\u200b\u4e00\u81f4\u6027\u200b\u5c31\u662f\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u548c\u200b\u5b9e\u9645\u200b\u5206\u7c7b\u200b\u7ed3\u679c\u200b\u662f\u5426\u200b\u4e00\u81f4\u200b\u3002kappa\u200b\u7cfb\u6570\u200b\u7684\u200b\u8ba1\u7b97\u200b\u540c\u6837\u200b\u4e5f\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u53d6\u503c\u200b\u4e3a\u200b-1\u200b\u5230\u200b1\u200b\u4e4b\u95f4\u200b,\u200b\u901a\u5e38\u200b\u5927\u4e8e\u200b0\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u8ff0\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff1a $$ kappa = \\frac{p_o-p_e}{1-p_e}\\ p_o = OA\\ p_e = \\frac{\\sum_{i} (x_i \\cdot x_j)}{(\\sum_{j=0}^n\\sum_{i=0}^{n} x_{ij})^2} $$</p> <pre><code>def compute_kappa(matrix):\n    \"\"\"\n    \u200b\u8ba1\u7b97\u200bkappa\u200b\u7cfb\u6570\u200b\n    :param matrix:\n    :return:\n    \"\"\"\n    oa = self.compute_oa(matrix)\n    pe = 0\n    for i in range(len(matrix)):\n        pe += np.sum(matrix[i]) * np.sum(matrix[:, i])\n    pe = pe / np.sum(matrix) ** 2\n    return (oa - pe) / (1 - pe)\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#recall","title":"Recall","text":"<p>Recall\u200b\u4e5f\u200b\u79f0\u200b\u53ec\u56de\u200b\u7387\u200b\uff0c\u200b\u4ee3\u8868\u200b\u4e86\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\u5e76\u4e14\u200b\u4e5f\u200b\u88ab\u200b\u6b63\u786e\u200b\u8bc6\u522b\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\u7684\u200b\u6570\u91cf\u200b\u5360\u200b\u6837\u672c\u200b\u4e2d\u200b\u6240\u6709\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e0b\u8ff0\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u8868\u793a\u200b $$ \\rm{Recall} = \\frac{\\rm{TP}}{\\rm{TP + FN}} $$</p> <p>Recall\u200b\u662f\u200b\u5224\u65ad\u200b\u6a21\u578b\u200b\u6b63\u786e\u200b\u8bc6\u522b\u200b\u6240\u6709\u200b\u6b63\u200b\u6837\u672c\u200b\u7684\u200b\u80fd\u529b\u200b\u3002\u200b\u7ed3\u5408\u200b\u6211\u4eec\u200b\u6240\u4e3e\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u4ee3\u8868\u200b\u4e86\u200b\u6a21\u578b\u200b\u5bf9\u4e8e\u200b\u6b63\u200b\u6837\u672c\u200b\u7684\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\uff0c\u200b\u4e5f\u200b\u80fd\u200b\u8f83\u200b\u597d\u200b\u7684\u200b\u53cd\u5e94\u200b\u6a21\u578b\u200b\u7684\u200b\u4f18\u52a3\u200b\u3002\u200b\u4e0e\u200bPrecision\u200b\u4e0d\u540c\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u662f\u200b\u53cd\u6620\u200b\u4e86\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u65f6\u5019\u200b\u6709\u200b\u591a\u5c11\u200b\u9634\u6027\u200b\uff08\u200b\u6b63\u200b\u6837\u672c\u200b\uff09\u200b\u88ab\u200b\u68c0\u6d4b\u200b\u51fa\u6765\u200b\u3002\u200b\u5728\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\u4ee3\u8868\u200b\u4e86\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9634\u6027\u200b\u88ab\u200b\u6a21\u578b\u200b\u6b63\u786e\u200b\u5224\u65ad\u200b\u4e3a\u200b\u9634\u6027\u200b\u7684\u200b\u6570\u91cf\u200b\u5360\u200b\u5b9e\u9645\u200b\u4e3a\u200b\u9634\u6027\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u5373\u200b $$ \\rm{Recall} = \\frac{94}{98}=0.959 $$</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#precision","title":"Precision","text":"<p>Precision\u200b\u4e5f\u200b\u79f0\u200b\u7cbe\u51c6\u200b\u7387\u200b\uff0c\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5728\u200b\u5168\u90e8\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u6b63\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e2d\u200b\uff0c\u200b\u88ab\u200b\u9884\u6d4b\u200b\u6b63\u786e\u200b\u7684\u200b\u6b63\u200b\u6837\u672c\u200b\u6240\u200b\u5360\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e0b\u8ff0\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u8868\u793a\u200b $$ \\rm{Precision} = \\frac{\\rm{TP}}{\\rm{TP + FP}} $$ FP\u200b\u4ee3\u8868\u200b\u4e86\u200b\u9633\u6027\u200b\u60a3\u8005\u200b\u88ab\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u9634\u6027\u200b\uff08\u200b\u6b63\u200b\u6837\u672c\u200b\uff09\uff0cTP\u200b\u4ee3\u8868\u200b\u4e86\u200b\u9634\u6027\u200b\u88ab\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u9634\u6027\u200b\u3002\u200b\u76f8\u8f83\u200b\u4e8e\u200bAcc\uff0cPrecision\u200b\u66f4\u80fd\u200b\u8f83\u200b\u597d\u200b\u7684\u200b\u53cd\u5e94\u200b\u51fa\u200b\u6a21\u578b\u200b\u5bf9\u4e8e\u200b\u6b63\u200b\u6837\u672c\u200b\uff08\u200b\u9634\u6027\u200b\uff09\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u548c\u200bRecall\u200b\u4e0d\u540c\u200b\u7684\u200b\u662f\u200b\uff0cPrecision\u200b\u4ee3\u8868\u200b\u4e86\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e2d\u6709\u200b\u591a\u5c11\u200b\u6837\u672c\u200b\u662f\u200b\u5206\u7c7b\u200b\u6b63\u786e\u200b\u7684\u200b\u3002 \u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u7ed3\u5408\u200b\u6838\u9178\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u5728\u200b\u8fdb\u884c\u200b\u5168\u9762\u200b\u6838\u9178\u200b\u68c0\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u5e0c\u671b\u200b\u5728\u4e8e\u200b\u6a21\u578b\u200b\u5c3d\u53ef\u80fd\u5c11\u200b\u7684\u200b\u6f0f\u6389\u200b\u9633\u6027\u200b\u60a3\u8005\u200b\uff0c\u200b\u6b64\u65f6\u200b\u8ba4\u4e3a\u200b\u6a21\u578b\u200bPrecision\u200b\u663e\u5f97\u200b\u66f4\u4e3a\u91cd\u8981\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#f1","title":"F1","text":"\\[ \\rm{F_1} = 2\\cdot\\frac{P\\times R}{P+R} = \\frac{2TP}{FP+FN+2TP} \\] <p>F1\u200b\u5728\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u4e2d\u200b\u4e5f\u200b\u662f\u200b\u4e00\u79cd\u200b\u91cd\u8981\u200b\u7684\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\uff0cF1\u200b\u53ef\u4ee5\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u53ec\u56de\u200b\u7387\u200b\uff08Recall\uff09\u200b\u548c\u200bP\uff08\u200b\u7cbe\u786e\u200b\u7387\u200b\uff09\u200b\u7684\u200b\u52a0\u6743\u200b\u5e73\u5747\u200b\uff0cF1\u200b\u8d8a\u9ad8\u200b\uff0c\u200b\u8bf4\u660e\u200b\u6a21\u578b\u200b\u9c81\u68d2\u6027\u200b\u8d8a\u200b\u597d\u200b\u3002\u200b\u4eba\u4eec\u200b\u5e0c\u671b\u200b\u6709\u200b\u4e00\u79cd\u200b\u66f4\u52a0\u200b\u5e7f\u4e49\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5b9a\u4e49\u200bF-score\uff0c\u200b\u5e0c\u671b\u200b\u53ef\u4ee5\u200b\u6539\u53d8\u200bP\u200b\u548c\u200bR\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u4e8e\u662f\u200b\u4eba\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\\(F_{\\beta}\\)\uff0c\u200b\u5176\u200b\u5b9a\u4e49\u200b\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $$ \\rm{F_{\\beta}}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R} $$</p> <ul> <li>\u200b\u5f53\u200b \\(\\beta\\) &gt; 1 \u200b\u65f6\u200b\uff0c\u200b\u66f4\u200b\u504f\u597d\u200b\u53ec\u56de\u200b(Recall)</li> <li>\u200b\u5f53\u200b \\(\\beta\\) &lt; 1 \u200b\u65f6\u200b\uff0c\u200b\u66f4\u200b\u504f\u597d\u200b\u7cbe\u51c6\u200b(Precision)</li> <li>\u200b\u5f53\u200b \\(\\beta\\) = 1 \u200b\u65f6\u200b\uff0c\u200b\u5e73\u8861\u200b\u7cbe\u51c6\u200b\u548c\u200b\u53ec\u56de\u200b\uff0c\u200b\u5373\u200b\u4e3a\u200b F1</li> </ul> <p>\u200b\u5f53\u6709\u200b\u591a\u4e2a\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff08\u200b\u591a\u6b21\u200b\u8bad\u7ec3\u200b\u3001\u200b\u591a\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u591a\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\uff09\u200b\u65f6\u200b\uff0c\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u4f30\u7b97\u200b \u201c\u200b\u5168\u5c40\u200b\u201d \u200b\u6027\u80fd\u200b\uff1a</p> <ul> <li>macro \u200b\u65b9\u6cd5\u200b\uff1a\u200b\u5148\u200b\u8ba1\u7b97\u200b\u6bcf\u4e2a\u200b PR\uff0c\u200b\u53d6\u200b\u5e73\u5747\u200b\u540e\u200b\uff0c\u200b\u518d\u200b\u8ba1\u7b97\u200b F1</li> <li>micro \u200b\u65b9\u6cd5\u200b\uff1a\u200b\u5148\u200b\u8ba1\u7b97\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u5143\u7d20\u200b\u7684\u200b\u5e73\u5747\u200b\uff0c\u200b\u518d\u200b\u8ba1\u7b97\u200b PR \u200b\u548c\u200b F1</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#pr","title":"PR\u200b\u66f2\u7ebf","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e38\u7528\u200b\u6307\u6807\u200b\u4e2d\u200b\uff0cPR\u200b\u66f2\u7ebf\u200b\u4e5f\u200b\u80fd\u200b\u5f88\u200b\u76f4\u89c2\u200b\u7684\u200b\u53cd\u5e94\u200b\u6a21\u578b\u200b\u597d\u574f\u200b\u3002\u200b\u6211\u4eec\u200b\u7528\u200b\u4e00\u526f\u200b\u7ecf\u5178\u200b\u56fe\u6765\u200b\u8bf4\u660e\u200b\u95ee\u9898\u200b\uff1a</p> <p></p> <p>\u200b\u6a2a\u8f74\u200b\u662f\u200b\u53ec\u56de\u200b\u7387\u200b\uff0c\u200b\u7eb5\u8f74\u200b\u4ee3\u8868\u200b\u4e86\u200bP\uff08\u200b\u7cbe\u786e\u200b\u7387\u200b)\uff0cP-R\u200b\u66f2\u7ebf\u200b\u4e0a\u200b\u7684\u200b\u4e00\u4e2a\u70b9\u200b\u4ee3\u8868\u200b\u7740\u200b\uff0c\u200b\u5728\u200b\u67d0\u200b\u4e00\u200b\u9608\u503c\u200b\u4e0b\u200b\uff0c\u200b\u6a21\u578b\u200b\u5c06\u200b\u5927\u4e8e\u200b\u8be5\u200b\u9608\u503c\u200b\u7684\u200b\u7ed3\u679c\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u5c0f\u4e8e\u200b\u8be5\u200b\u9608\u503c\u200b\u7684\u200b\u7ed3\u679c\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\uff0c\u200b\u6b64\u65f6\u200b\u8fd4\u56de\u200b\u7ed3\u679c\u200b\u5bf9\u5e94\u200b\u7684\u200b\u53ec\u56de\u200b\u7387\u200b\u548c\u200b\u7cbe\u786e\u200b\u7387\u200b\u3002\u200b\u6574\u6761\u200bP-R\u200b\u66f2\u7ebf\u200b\u662f\u200b\u901a\u8fc7\u200b\u5c06\u200b\u9608\u503c\u200b\u4ece\u200b\u9ad8\u5230\u200b\u4f4e\u200b\u79fb\u52a8\u200b\u800c\u200b\u751f\u6210\u200b\u7684\u200b\u3002\u200b\u539f\u70b9\u200b\u9644\u8fd1\u200b\u4ee3\u8868\u200b\u5f53\u200b\u9608\u503c\u200b\u6700\u5927\u200b\u65f6\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u786e\u200b\u7387\u200b\u548c\u200b\u53ec\u56de\u200b\u7387\u200b\uff0c\u200b\u5728\u200bPR\u200b\u66f2\u7ebf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u56fe\u200b\u4e2d\u200b\u76f4\u89c2\u200b\u7684\u200b\u770b\u5230\u200b\u67d0\u200b\u4e00\u4e2a\u200b\u66f2\u7ebf\u200b\u88ab\u200b\u53e6\u5916\u200b\u4e00\u6761\u200b\u66f2\u7ebf\u200b\u5b8c\u5168\u200b\u5305\u88f9\u200b\uff0c\u200b\u6240\u200b\u5305\u56f4\u200b\u7684\u200b\u9762\u79ef\u200b\u5927\u4e8e\u200b\u53e6\u200b\u4e00\u6761\u200b\u66f2\u7ebf\u200b\u5305\u56f4\u200b\u7684\u200b\u9762\u79ef\u200b\uff0c\u200b\u4e3e\u4f8b\u200b\u56fe\u4e2d\u200b\u53ef\u4ee5\u200b\u8bf4\u660e\u200bA\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\u4f18\u4e8e\u200bB\u200b\u548c\u200bC\u3002 \u200b\u53c2\u8003\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>from typing import List, Tuple\nimport matplotlib.pyplot as plt\n\ndef get_confusion_matrix(\n    y_pred: List[int], \n    y_true: List[int]\n    ) -&gt; Tuple[int, int, int, int]:\n\n    length = len(y_pred)\n    assert length == len(y_true)\n    tp, fp, fn, tn = 94, 0, 4, 2\n    for i in range(length):\n        if y_pred[i] == y_true[i] and y_pred[i] == 1:\n            tp += 1\n        elif y_pred[i] == y_true[i] and y_pred[i] == 0:\n            tn += 1\n        elif y_pred[i] == 1 and y_true[i] == 0:\n            fp += 1\n        elif y_pred[i] == 0 and y_true[i] == 1:\n            fn += 1\n    return (tp, fp, tn, fn)\ndef calc_p(tp: int, fp: int) -&gt; float:\n    return tp / (tp + fp)\ndef calc_r(tp: int, fn: int) -&gt; float:\n    return tp / (tp + fn)\ndef get_pr_pairs(\n    y_pred_prob: List[float], \n    y_true: List[int]\n    ) -&gt; Tuple[List[int], List[int]]:\n    ps = [1]\n    rs = [0]\n    for prob1 in y_pred_prob:\n        y_pred_i = []\n        for prob2 in y_pred_prob:\n            if prob2 &lt; prob1:\n                y_pred_i.append(0)\n            else:\n                y_pred_i.append(1)\n        tp, fp, tn, fn = get_confusion_matrix(y_pred_i, y_true)\n        p = calc_p(tp, fp)\n        r = calc_r(tp, fn)\n        ps.append(p)\n        rs.append(r)\n    ps.append(0)\n    rs.append(1)\n    return ps, rs\n\ny_pred_prob = [0.9, 0.8, 0.7, 0.6, 0.55, 0.54, 0.53, 0.52, 0.51, 0.505,\n               0.4, 0.39, 0.38, 0.37, 0.36, 0.35, 0.34, 0.33, 0.3, 0.1]\ny_true = [1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]\ny_pred = [1] * 10 + [0] * 10\nps, rs = get_pr_pairs(y_pred_prob, y_true)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 5))\nax.plot(rs, ps);\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#_3","title":"\u7f6e\u4fe1\u5ea6","text":"<p>\u200b\u5728\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u5c06\u200b\u8fb9\u754c\u200b\u6846\u5185\u200b\u7269\u4f53\u200b\u5212\u5206\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\u548c\u200b\u8d1f\u200b\u6837\u672c\u200b\u3002\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7f6e\u4fe1\u5ea6\u200b\u8fd9\u4e2a\u200b\u6307\u6807\u200b\u6765\u200b\u8fdb\u884c\u200b\u5212\u5206\u200b\uff0c\u200b\u5f53\u200b\u5c0f\u4e8e\u200b\u7f6e\u4fe1\u5ea6\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u9608\u503c\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\uff08\u200b\u80cc\u666f\u200b\uff09\uff0c\u200b\u5927\u4e8e\u200b\u7f6e\u4fe1\u5ea6\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u9608\u503c\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b.</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#iou","title":"IOU","text":"<p>\u200b\u96c5\u200b\u5361\u5c14\u200b\u6307\u6570\u200b( Jaccard index)\uff0c\u200b\u4ea4\u5e76\u200b\u6bd4\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bed\u4e49\u200b\u5206\u5272\u200b\u9886\u57df\u200b\u7ecf\u5e38\u200b\u89c1\u5230\u200b\u7684\u200bIOU\u3002 IOU\u200b\u53ef\u4ee5\u200b\u53bb\u9664\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7684\u200b\u5197\u4f59\u200b\u6846\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b\u4f18\u5316\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200bLoss\u200b\u8fbe\u5230\u200b\u4f18\u5316\u200b\u6a21\u578b\u200b\u7684\u200b\u6548\u679c\u200b\u3002 $$ \\rm{IOU} = \\frac{A\\cap B}{A\\cup B}=\\frac{S_{\u200b\u4ea4\u96c6\u200b}}{S_{\u200b\u5e76\u96c6\u200b}} $$ \u200b\u5206\u5b50\u200b\u90e8\u5206\u200b\u662f\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u6846\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u6ce8\u200b\u6846\u200b\u4e4b\u95f4\u200b\u7684\u200b\u91cd\u53e0\u200b\u533a\u57df\u200b\uff0c\u200b\u5206\u6bcd\u200b\u90e8\u5206\u200b\u662f\u200b\u4e24\u8005\u200b\u7684\u200b\u5e76\u200b\u96c6\u200b\uff0c\u200b\u9884\u6d4b\u200b\u6846\u200b\u548c\u200b\u5b9e\u9645\u200b\u6846\u6240\u200b\u5360\u6709\u200b\u7684\u200b\u603b\u200b\u533a\u57df\u200b\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u6a21\u578b\u200b\u8bc6\u522b\u200b\u65f6\u4f1a\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u8bbe\u5b9a\u200b\u5408\u9002\u200b\u7684\u200b\u9608\u503c\u200b\u6765\u200b\u5224\u5b9a\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u3002</p> <p></p> <p>\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5728\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u4eba\u4eec\u200b\u5c06\u200bIOU \\(\\geq\\)0.7\u200b\u65f6\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u5176\u4f59\u200b\u60c5\u51b5\u200b\u5224\u5b9a\u200b\u4e3a\u200b\u8d1f\u200b\u6837\u672c\u200b\u3002</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#ap","title":"AP","text":"<p>\u200b\u7ed3\u5408\u200b\u6211\u200b\u6700\u8fd1\u200b\u505a\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u5408\u200b\u7406\u89e3\u200b\u4e0b\u200b\uff0cAP\u200b\u4e8b\u5b9e\u4e0a\u200b\u662f\u200bPR\u200b\u66f2\u7ebf\u200b\u6240\u200b\u5305\u542b\u200b\u7684\u200b\u9762\u79ef\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u53d6\u200b\u4e0d\u540c\u200b\u7684\u200b\u7f6e\u4fe1\u5ea6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u4e0d\u540c\u200b\u7684\u200bP\u200b\u548c\u200bRecall\uff0cPR\u200b\u66f2\u7ebf\u200b\u4e0b\u9762\u200b\u6240\u200b\u5305\u542b\u200b\u7684\u200b\u9762\u79ef\u200b\u5c31\u662f\u200b\u6a21\u578b\u200b\u68c0\u6d4b\u200b\u67d0\u4e2a\u200b\u7c7b\u200b\u7684\u200bAP\u200b\u503c\u200b\u3002\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u4ee5\u200b\u4e0a\u8ff0\u200b\u6838\u9178\u200b\u68c0\u6d4b\u200b\u4e3a\u4f8b\u200b\uff0c</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.2%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/#map","title":"mAP","text":"<p>\u200b\u6211\u4eec\u200b\u719f\u6089\u200b\u4e86\u200b\u4e0a\u9762\u200b\u7684\u200b\u57fa\u7840\u200b\u6982\u5ff5\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u7684\u200bmAP\u200b\u4e86\u200b\uff0c\u200b\u5168\u79f0\u200bmean Average Precision\uff0cmAP\u200b\u5219\u200b\u662f\u200b\u6240\u6709\u200b\u7c7b\u200b\u7684\u200bAP\u200b\u503c\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u3002\u200b\u901a\u8fc7\u200b\u4e0a\u8ff0\u200b\u7684\u200bPR\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u5bf9\u5e94\u200b\u7684\u200bAP\u200b\u503c\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4e86\u89e3\u200b\uff0c (1)\u200b\u5728\u200b2010\u200b\u5e74\u200b\u4ee5\u524d\u200b\uff0cVOC\u200b\u7ade\u8d5b\u200bAP\u200b\u503c\u200b\u7684\u200b\u5b9a\u4e49\u200b\u4e3a\u200b\uff1a\u200b\u5bf9\u4e8e\u200b\u4e00\u6761\u200bAP\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u6211\u4eec\u200b\u628a\u200bRecall\u200b\u7684\u200b\u503c\u200b\u4ece\u200b0-1\u200b\u5212\u5206\u200b\u4e3a\u200b11\u200b\u5206\u200b\uff0c\u200b\u5373\u200b\uff1a0\u30010.1\u30010.2\u30010.3\u30010.4\u30010.5\u30010.6\u30010.7\u30010.8\u30010.9\u30011.0\uff0c\u200b\u7136\u540e\u200b\u57fa\u4e8e\u200b\u6bcf\u4e2a\u200b\u5c0f\u200b\u533a\u95f4\u200b\u8ba1\u7b97\u200bPrecision\u200b\u7684\u200b\u6700\u5927\u503c\u200b\uff0c\u200b\u7136\u540e\u200b\u8ba1\u7b97\u200b\u4ed6\u4eec\u200b\u7684\u200b\u603b\u548c\u200b\u6c42\u200b\u5e73\u5747\u200b\uff0c\u200b\u5c31\u662f\u200bAP\u200b\u503c\u200b\uff0c\u200b\u88ab\u79f0\u4f5c\u200b11-poinst-interpolation. (2) 2010\u200b\u5e74\u200b\u540e\u200b\uff0c\u200b\u91cd\u65b0\u200b\u5b9a\u4e49\u200b\u4e3a\u200bPR\u200b\u66f2\u7ebf\u200b\u4e0a\u200b\u6240\u6709\u200b\u7684\u200bRecall\uff0c\u200b\u9009\u53d6\u200b\u5927\u4e8e\u200b\u7b49\u4e8e\u200b\u8fd9\u4e9b\u200bRecall\u200b\u503c\u65f6\u200b\u7684\u200b\u6700\u5927\u503c\u200bPrecision,\u200b\u8ba1\u7b97\u200bPR\u200b\u66f2\u7ebf\u200b\u4e0b\u9762\u200b\u9762\u79ef\u200b\u4f5c\u4e3a\u200bAP\u200b\u503c\u200b.\u200b\u88ab\u79f0\u4f5c\u200ball-poinst-interpolation.</p> <ul> <li>\u200b\u897f\u74dc\u200b\u4e66\u200b\u7b14\u8bb0\u200b</li> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b (\u200b\u8c46\u74e3\u200b)</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.3%20%E5%B8%B8%E7%94%A8%E5%8C%85%E7%9A%84%E5%AD%A6%E4%B9%A0/","title":"\u5e38\u7528\u200b\u5305\u200b\u7684\u200b\u5b66\u4e60","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.3%20%E5%B8%B8%E7%94%A8%E5%8C%85%E7%9A%84%E5%AD%A6%E4%B9%A0/#numpy","title":"Numpy","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.3%20%E5%B8%B8%E7%94%A8%E5%8C%85%E7%9A%84%E5%AD%A6%E4%B9%A0/#pandas","title":"pandas","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.3%20%E5%B8%B8%E7%94%A8%E5%8C%85%E7%9A%84%E5%AD%A6%E4%B9%A0/#matplotlib","title":"matplotlib","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/","title":"Jupyter notebook/Lab \u200b\u7b80\u8ff0","text":"<p>\u200b\u5728\u200b\u6570\u636e\u200b\u79d1\u5b66\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5373\u65f6\u200b\u8fdb\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u548c\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u8fd9\u79cd\u200b\u9700\u6c42\u200b\uff0c\u200b\u4eba\u4eec\u200b\u5f00\u53d1\u200b\u51fa\u200b\u4e86\u200b\u57fa\u4e8e\u200b\u7f51\u9875\u200b\u7684\u200b\u7528\u4e8e\u200b\u4ea4\u4e92\u200b\u8ba1\u7b97\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200bJupyter Notebook\u3002\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6559\u7a0b\u200b\u4ee3\u7801\u200b\u4e5f\u200b\u662f\u200b\u57fa\u4e8e\u200bJupyter notebook\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5b83\u4eec\u200b\u53ef\u200b\u88ab\u200b\u5e94\u7528\u200b\u4e8e\u200b\u5168\u8fc7\u7a0b\u200b\u8ba1\u7b97\u200b\uff1a\u200b\u5f00\u53d1\u200b\u3001\u200b\u6587\u6863\u200b\u7f16\u5199\u200b\u3001\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b\u548c\u200b\u5c55\u793a\u200b\u7ed3\u679c\u200b\u3002\u200b\u5728\u200bJupyter Notebook\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b\u6587\u6863\u200b\u4fdd\u5b58\u200b\u4e3a\u200b<code>.ipynb</code>\u200b\u7684\u200b<code>JSON</code>\u200b\u683c\u5f0f\u6587\u4ef6\u200b\uff0c\u200b\u6587\u6863\u200b\u53ef\u4ee5\u200b\u5bfc\u51fa\u200b\u4e3a\u200bHTML\u3001LaTeX\u3001markdown\u3001PDF\u200b\u7b49\u200b\u683c\u5f0f\u200b\u3002Jupyter Notebook\u200b\u7684\u200b\u4e3b\u8981\u200b\u7279\u70b9\u200b\u6709\u200b\uff1a</p> <ul> <li> <p>\u200b\u7f16\u7a0b\u200b\u65f6\u200b\u5177\u6709\u200b\u8bed\u6cd5\u200b\u9ad8\u4eae\u200b\u3001\u200b\u7f29\u8fdb\u200b\u3001tab\u200b\u8865\u5168\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> </li> <li> <p>\u200b\u53ef\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b\u6d4f\u89c8\u5668\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b\uff0c\u200b\u540c\u65f6\u200b\u5728\u200b\u4ee3\u7801\u200b\u5757\u200b\u4e0b\u65b9\u200b\u5c55\u793a\u200b\u8fd0\u884c\u200b\u7ed3\u679c\u200b\u3002</p> </li> <li> <p>\u200b\u4ee5\u5bcc\u200b\u5a92\u4f53\u683c\u5f0f\u200b\u5c55\u793a\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u3002\u200b\u5bcc\u200b\u5a92\u4f53\u683c\u5f0f\u200b\u5305\u62ec\u200b\uff1aHTML\uff0cLaTeX\uff0cPNG\uff0cSVG\u200b\u7b49\u200b\u3002</p> </li> <li> <p>\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u7f16\u5199\u200b\u8bf4\u660e\u200b\u6587\u6863\u200b\u6216\u200b\u8bed\u53e5\u200b\u65f6\u200b\uff0c\u200b\u652f\u6301\u200bMarkdown\u200b\u8bed\u6cd5\u200b\u3002</p> </li> <li> <p>\u200b\u652f\u6301\u200b\u4f7f\u7528\u200bLaTeX\u200b\u7f16\u5199\u200b\u6570\u5b66\u200b\u6027\u200b\u8bf4\u660e\u200b\u3002</p> </li> </ul> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u597d\u7528\u200b\u7684\u200bJupyter Notebook\u200b\u8fd8\u6709\u200b\u4e2a\u200b\u201c\u200b\u53cc\u80de\u80ce\u200b\u201d\u2014\u2014Jupyter Lab\u3002Jupyter Lab\u200b\u662f\u200b\u57fa\u4e8e\u200bWeb\u200b\u7684\u200b\u96c6\u6210\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u628a\u200b\u5b83\u200b\u5f53\u4f5c\u200b\u8fdb\u5316\u7248\u200b\u7684\u200bJupyter Notebook\u3002\u200b\u4f7f\u7528\u200bJupyter Lab\u200b\u53ef\u4ee5\u200b\u540c\u65f6\u200b\u5728\u200b\u4e00\u4e2a\u200b\u6d4f\u89c8\u5668\u200b\u9875\u9762\u200b\u6253\u5f00\u200b\u7f16\u8f91\u200b\u591a\u4e2a\u200bNotebook\u3001Ipython console\u200b\u548c\u200bterminal\u200b\u7ec8\u7aef\u200b\uff0c\u200b\u751a\u81f3\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bJupyter Lab\u200b\u8fde\u63a5\u200bGoogle Drive\u200b\u7b49\u200b\u670d\u52a1\u200b\u3002\u200b\u7531\u4e8e\u200bJupyter Lab\u200b\u62e5\u6709\u200b\u6a21\u5757\u5316\u200b\u7ed3\u6784\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u66f4\u200b\u591a\u200b\u7c7b\u4f3c\u200bIDE\u200b\u7684\u200b\u4f53\u9a8c\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u6709\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u4eba\u200b\u4ece\u200b\u4f7f\u7528\u200bJupyter Notebook\u200b\u8f6c\u5411\u200b\u4f7f\u7528\u200bJupyter Lab\u3002</p> <p>[* ]\u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5b89\u88c5\u200b\u548c\u200b\u914d\u7f6e\u200bJupyter Notebook\u200b\u548c\u200bJupyter Lab\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> <li>Jupyter Notebook\u200b\u7684\u200b\u57fa\u672c\u64cd\u4f5c\u200b\u548c\u200b\u5feb\u6377\u952e\u200b</li> <li>\u200b\u4f7f\u7528\u200bJupyter Notebook\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u7684\u200b\u65b9\u5f0f\u200b</li> <li>\u200b\u4e86\u89e3\u200bJupyter Notebook\u200b\u7684\u200bBash\u200b\u547d\u4ee4\u200b\u548c\u200b\u9b54\u672f\u200b\u547d\u4ee4\u200b</li> <li>\u200b\u4e3a\u200bJupyter Notebook\u200b\u5b89\u88c5\u200b\u62d3\u5c55\u200b\u63d2\u4ef6\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#1-jupyter-notebooklab","title":"1 Jupyter Notebook/Lab\u200b\u5b89\u88c5","text":"<ol> <li> <p>\u200b\u5b89\u88c5\u200bJupyter Notebook\uff1a\u200b\u6fc0\u6d3b\u200b\u865a\u62df\u73af\u5883\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b\u6307\u4ee4\u200b</p> <pre><code>conda install jupyter notebook\n# pip install jupyter notebook\n</code></pre> </li> <li> <p>\u200b\u6ce8\u200b\uff1a\u200b\u5982\u679c\u200bpip\u200b\u7248\u672c\u200b\u8fc7\u4f4e\u200b\uff0c\u200b\u8fd8\u200b\u9700\u200b\u63d0\u524d\u200b\u8fd0\u884c\u200b\u66f4\u65b0\u200bpip\u200b\u7684\u200b\u6307\u4ee4\u200b</p> </li> </ol> <pre><code>pip install --upgrade pip\n</code></pre> <ol> <li>\u200b\u5b89\u88c5\u200bJupyter Lab\uff1a\u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u540c\u6837\u200b\u4e5f\u200b\u53ea\u200b\u9700\u8981\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b\u6307\u4ee4\u200b</li> </ol> <pre><code>conda install -c conda-forge jupyterlab\n# pip install jupyterlab\n</code></pre> <ol> <li>\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b\u6307\u4ee4\u200b\u6253\u5f00\u200bJupyter Notebook</li> </ol> <pre><code>jupyter notebook  # \u200b\u6253\u5f00\u200bJupyter Notebook\njupyter lab  # \u200b\u6253\u5f00\u200bJupyter Lab\n</code></pre> <ul> <li>\u200b\u5982\u679c\u200b\u6d4f\u89c8\u5668\u200b\u6ca1\u6709\u200b\u81ea\u52a8\u200b\u6253\u5f00\u200bJupyter Notebook\u200b\u6216\u8005\u200bJupyter Lab\uff0c\u200b\u590d\u5236\u200b\u7aef\u53e3\u200b\u4fe1\u606f\u200b\u7c98\u8d34\u200b\u81f3\u200b\u6d4f\u89c8\u5668\u200b\u6253\u5f00\u200b</li> </ul> <p></p> <ul> <li>\u200b\u5982\u679c\u200b\u60f3\u8981\u200b\u81ea\u5b9a\u4e49\u200b\u7aef\u53e3\u200b\uff0c\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b\u5982\u4e0b\u200b\u6307\u4ee4\u200b\u4fee\u6539\u200b</li> </ul> <pre><code>jupyter notebook --port &lt;port_number&gt;\n</code></pre> <ul> <li>\u200b\u5982\u679c\u200b\u60f3\u200b\u542f\u52a8\u200b\u670d\u52a1\u5668\u200b\u4f46\u200b\u4e0d\u200b\u6253\u5f00\u200b\u6d4f\u89c8\u5668\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b</li> </ul> <pre><code>jupyter notebook --no-browser\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#2-jupyter-notebooklab","title":"2 Jupyter Notebook/Lab\u200b\u914d\u7f6e","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#21","title":"2.1 \u200b\u8bbe\u7f6e\u200b\u6587\u4ef6\u200b\u5b58\u653e\u200b\u4f4d\u7f6e","text":"<p>\u200b   \u200b\u5728\u200b\u4f7f\u7528\u200bJupyter Notebook/Jupyter Lab\u200b\u65f6\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u66f4\u6539\u200b\u9ed8\u8ba4\u200b\u6587\u4ef6\u200b\u5b58\u653e\u200b\u8def\u5f84\u200b\uff0c\u200b\u8be5\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <ul> <li> <p>Jupyter Notebook</p> </li> <li> <p>\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u67e5\u770b\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b</p> </li> </ul> <pre><code>jupyter notebook --generate-config\n</code></pre> <ol> <li>\u200b\u6211\u4eec\u200b\u8bb0\u4f4f\u200b\u51fa\u73b0\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u8def\u5f84\u200b\uff0c\u200b\u590d\u5236\u5230\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u6253\u5f00\u200b\uff08\u200b\u7ec8\u7aef\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u5199\u200bN\uff09</li> </ol> <p></p> <ol> <li>\u200b\u5728\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u53cc\u51fb\u200b\u6253\u5f00\u200b\u914d\u7f6e\u6587\u4ef6\u200b</li> </ol> <p></p> <ol> <li> <p>\u200b\u6253\u5f00\u200bPython\u200b\u6587\u4ef6\u200b\u540e\u200b\uff0c\u200b\u7528\u200b<code>Ctrl+F</code>\u200b\u5feb\u6377\u952e\u200b\u67e5\u627e\u200b\uff0c\u200b\u8f93\u5165\u200b\u5173\u952e\u8bcd\u200b\uff0c\u200b\u627e\u5230\u200b<code># c.NotebookApp.notebook_dir = ''</code></p> </li> <li> <p>\u200b\u53bb\u6389\u200b\u6ce8\u91ca\u200b\uff0c\u200b\u5e76\u200b\u586b\u5145\u200b\u8def\u5f84\u200b<code>c.NotebookApp.notebook_dir = 'D:\\\\Adatascience'</code></p> </li> </ol> <p></p> <ol> <li>\u200b\u6b64\u65f6\u200b\u6211\u4eec\u200b\u5728\u200b\u7ec8\u7aef\u200b\u4e2d\u200b\u8f93\u5165\u200b<code>jupyter notebook</code>\uff0c\u200b\u6253\u5f00\u200b\u9875\u9762\u200b\u540e\u200b\u53d1\u73b0\u200b\u6587\u4ef6\u200b\u9ed8\u8ba4\u200b\u8def\u5f84\u200b\u5df2\u7ecf\u200b\u88ab\u200b\u66f4\u6539\u200b\u3002\u200b\u4f46\u662f\u200b\u70b9\u51fb\u200b\u83dc\u5355\u680f\u200b\u4e2d\u200b\u7684\u200b\u5e94\u7528\u200b\u5feb\u6377\u65b9\u5f0f\u200b\u6253\u5f00\u200bJupyter Notebook\uff0c\u200b\u6253\u5f00\u200b\u9875\u9762\u200b\u53d1\u73b0\u200b\u6587\u4ef6\u200b\u4f4d\u7f6e\u200b\u4ecd\u7136\u200b\u662f\u200b\u9ed8\u8ba4\u200b\u8def\u5f84\u200b</li> </ol> <p></p> <ol> <li>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u66f4\u6539\u200b\u5e94\u7528\u200b\u5feb\u6377\u65b9\u5f0f\u200bJupyter Notebook\u200b\u7684\u200b\u6587\u4ef6\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u6b64\u65f6\u200b\u9700\u8981\u200b\u53f3\u952e\u200b\u9009\u4e2d\u200b\u5feb\u6377\u65b9\u5f0f\u200b\uff0c\u200b\u6253\u5f00\u200b\u6587\u4ef6\u200b\u6240\u5728\u4f4d\u7f6e\u200b\u3002\u200b\u518d\u200b\u53f3\u952e\u200b\u70b9\u51fb\u200b\u5feb\u6377\u65b9\u5f0f\u200b\uff0c\u200b\u67e5\u770b\u200b\u5c5e\u6027\u200b\uff0c\u200b\u518d\u200b\u70b9\u51fb\u200b\u5feb\u6377\u65b9\u5f0f\u200b</li> </ol> <p></p> <p></p> <ol> <li>\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5728\u200b\u201c\u200b\u76ee\u6807\u200b\u201d\u200b\u4e2d\u200b\u5220\u9664\u200b\u7ea2\u6846\u200b\u6807\u8bb0\u200b\u90e8\u5206\u200b\uff0c\u200b\u70b9\u51fb\u200b\u786e\u5b9a\u200b</li> </ol> <p></p> <ol> <li> <p>\u200b\u6b64\u65f6\u200b\u518d\u200b\u6253\u5f00\u200b\u83dc\u5355\u680f\u200b\u4e2d\u200bJupyter Notebook\u200b\u7684\u200b\u5feb\u6377\u65b9\u5f0f\u200b\uff0c\u200b\u53d1\u73b0\u200b\u9875\u9762\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u5df2\u7ecf\u200b\u53d8\u4e3a\u200b\u4e4b\u524d\u200b\u81ea\u4e3b\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u8def\u5f84\u200b\u5566\u200b\uff01</p> </li> <li> <p>Jupyter Lab\u200b\u7684\u200b\u4fee\u6539\u200b\u64cd\u4f5c\u200b\u548c\u200bJupyter Notebook\u200b\u6d41\u7a0b\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u7ec6\u8282\u200b\u4e0a\u200b\u6709\u4e9b\u200b\u4e0d\u540c\u200b</p> </li> <li> <p>\u200b\u540c\u6837\u200b\u6211\u4eec\u200b\u8fd8\u662f\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u67e5\u770b\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u5728\u200b\u7ec8\u7aef\u200b\u8f93\u5165\u200b</p> </li> </ol> <pre><code>jupyter lab --generate-config\n</code></pre> <ol> <li> <p>\u200b\u627e\u5230\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u6240\u5728\u200b\u7684\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u6253\u5f00\u200b\u914d\u7f6e\u6587\u4ef6\u200b</p> </li> <li> <p>\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u65f6\u200b\uff0c\u200b\u7528\u200b<code>Ctrl+F</code>\u200b\u5feb\u6377\u952e\u200b\u67e5\u627e\u200b\uff0c\u200b\u8f93\u5165\u200b\u5173\u952e\u8bcd\u200b\uff0c\u200b\u627e\u5230\u200b<code># c.ServerApp.notebook_dir</code>\uff0c\u200b\u53bb\u6389\u200b\u6ce8\u91ca\u200b\u3002\u200b\u6539\u4e3a\u200b<code>c.ServerApp.notebook_dir = 'D:\\\\Adatascience\uff08\u200b\u8fd9\u91cc\u200b\u586b\u200b\u81ea\u5df1\u200b\u60f3\u200b\u6539\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff09'</code></p> </li> <li> <p>\u200b\u4e4b\u540e\u200b\u7684\u200b\u6b65\u9aa4\u200b\u548c\u200bJupyter Notebook\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u7684\u200b\u7b2c\u4e03\u200b\u81f3\u200b\u7b2c\u200b\u5341\u6b65\u200b\u76f8\u540c\u200b</p> </li> </ol>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#22","title":"2.2 \u200b\u4f7f\u7528\u200b\u865a\u62df\u73af\u5883","text":"<p>\u200b       \u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0cAnaconda\u200b\u5b89\u88c5\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u548c\u200bJupyter Notebook\u200b\u8fd0\u884c\u200b\u9700\u8981\u200b\u7684\u200bKernel\u200b\u5e76\u200b\u4e0d\u200b\u4e92\u901a\u200b\u3002\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u5207\u6362\u200b\u5185\u6838\u200b\uff08Change Kernel\uff09\uff0c\u200b\u8be5\u200b\u5982\u4f55\u200b\u64cd\u4f5c\u200b\u5462\u200b\uff1f</p> <ol> <li>\u200b\u5c06\u200b\u5728\u200bAnaconda\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u6dfb\u52a0\u200b<code>ipykernel</code></li> </ol> <pre><code># \u200b\u5982\u679c\u200b\u8fd8\u200b\u6ca1\u200b\u521b\u5efa\u200b\u73af\u5883\u200b\uff0c\u200b\u5728\u200b\u521b\u5efa\u200b\u65f6\u8981\u200b\u52a0\u4e0a\u200bipykernel\nconda create -n env_name python=3.8 ipykernel\n# \u200b\u5982\u679c\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u73af\u5883\u200b\uff0c\u200b\u5728\u200b\u73af\u5883\u200b\u4e2d\u200b\u5b89\u88c5\u200bipykernel\npip install ipykernel\n</code></pre> <ol> <li>\u200b\u5c06\u200b\u865a\u62df\u73af\u5883\u200b\u5199\u200b\u8fdb\u200bJupyter</li> </ol> <pre><code>python -m ipykernel install --user --name env_name --display-name \"env_name\"\n</code></pre> <ol> <li>\u200b\u5728\u200b<code>Kernel</code>\u200b\u4e2d\u200b\u66f4\u6362\u200b\u6dfb\u52a0\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u5373\u53ef\u200b</li> </ol> <p></p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#3-jupyter-notebooklab","title":"3 Jupyter Notebook\\Lab\u200b\u57fa\u672c\u64cd\u4f5c","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#31","title":"3.1 \u200b\u57fa\u672c\u200b\u4f7f\u7528","text":""},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#311-jupyter-notebook","title":"3.1.1 Jupyter Notebook","text":"<ol> <li>\u200b\u521b\u5efa\u200b\u6587\u4ef6\u200b\uff1a\u200b\u70b9\u51fb\u200b\u53f3\u4e0a\u89d2\u200bNew\uff0c\u200b\u9009\u62e9\u200bNotebook\uff1b\u200b\u521b\u5efa\u200b\u6587\u4ef6\u5939\u200b\uff1a\u200b\u70b9\u51fb\u200b\u53f3\u4e0a\u89d2\u200bNew\uff0c\u200b\u9009\u62e9\u200bFolder</li> </ol> <ol> <li>\u200b\u5220\u9664\u200b\u6587\u4ef6\u200b\uff1a\u200b\u70b9\u51fb\u200b\u6587\u4ef6\u200b\u524d\u200b\u7684\u200b\u65b9\u6846\u200b\uff0c\u200b\u518d\u200b\u70b9\u51fb\u200b\u5220\u9664\u200b\u56fe\u6807\u200b</li> </ol> <ol> <li>\u200b\u91cd\u547d\u540d\u200b\u6587\u4ef6\u200b\uff1a\u200b\u5f53\u200b\u6587\u4ef6\u200b\u5728\u200b\u8fd0\u884c\u200b\u65f6\u200b\uff08\u200b\u5373\u200b\u6587\u4ef6\u200b\u524d\u200b\u56fe\u6807\u200b\u4e3a\u200b\u7eff\u8272\u200b\uff09\uff0c\u200b\u9700\u8981\u200b\u5148\u200b\u70b9\u51fb\u200b\u201cShutdown\u201d\uff08\u200b\u5173\u95ed\u200b\u7ec8\u7aef\u200b\uff09\uff0c\u200b\u518d\u200b\u70b9\u51fb\u200b\u201cRename\u201d</li> </ol> <ol> <li>\u200b\u91cd\u547d\u540d\u200b\u6587\u4ef6\u5939\u200b\uff1a\u200b\u70b9\u51fb\u200b\u6587\u4ef6\u5939\u200b\u524d\u200b\u7684\u200b\u65b9\u6846\u200b\uff0c\u200b\u518d\u200b\u70b9\u51fb\u200b\u201cRename\u201d</li> <li>\u200b\u590d\u5236\u200b\uff08Duplicate\uff09\u3001\u200b\u79fb\u52a8\u200b\uff08Move\uff09\u3001\u200b\u4e0b\u8f7d\u200b\uff08Download\uff09\u3001\u200b\u67e5\u770b\u200b\uff08View\uff09\u200b\u7b49\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u540c\u6837\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u6587\u4ef6\u200b\u524d\u200b\u7684\u200b\u65b9\u6846\u200b\uff0c\u200b\u518d\u70b9\u200b\u4e0a\u65b9\u200b\u7684\u200b\u56fe\u6807\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b</li> </ol>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#312-jupyter-lab","title":"3.1.2 Jupyter Lab","text":"<ol> <li>\u200b\u7ea2\u6846\u200b\u5185\u200b\u6309\u94ae\u200b\u4ece\u5de6\u5230\u53f3\u200b\u5206\u522b\u200b\u662f\u200b\u65b0\u5efa\u200b\u6587\u4ef6\u200b\u3001\u200b\u65b0\u5efa\u200b\u6587\u4ef6\u5939\u200b\u3001\u200b\u4e0a\u4f20\u200b\u6587\u4ef6\u200b\u548c\u200b\u5237\u65b0\u200b</li> </ol> <ol> <li> <p>\u200b\u4e0a\u4f20\u200b\u591a\u4e2a\u200b\u6587\u4ef6\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1a</p> </li> <li> <p>\u200b\u5c06\u200b\u6587\u4ef6\u200b\u6253\u5305\u200b\u6210\u200b\u4e00\u4e2a\u200bzip\u200b\u538b\u7f29\u5305\u200b</p> </li> <li>\u200b\u4e0a\u4f20\u200b\u8be5\u200b\u538b\u7f29\u5305\u200b</li> <li>\u200b\u89e3\u538b\u200b\u6587\u4ef6\u200b<code>!unzip (\u200b\u538b\u7f29\u5305\u200b\u6240\u5728\u200b\u8def\u5f84\u200b) -d (\u200b\u89e3\u538b\u200b\u8def\u5f84\u200b)</code>\uff0c\u200b\u4f8b\u5982\u200b\uff1a<code>!unzip coco.zip -d data/coco</code></li> <li>\u200b\u5220\u9664\u200b\u8be5\u200b\u538b\u7f29\u5305\u200b</li> </ol>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#32","title":"3.2 \u200b\u5e38\u7528\u200b\u5feb\u6377\u952e","text":"<ol> <li>\u200b\u5165\u95e8\u200b\u64cd\u4f5c\u200b</li> </ol> <pre><code># \u200b\u589e\u52a0\u200b\uff0c\u200b\u51cf\u5c11\u200b\uff0c\u200b\u526a\u5207\u200b\uff0c\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5220\u9664\u200b\u7b49\u200b\n# a, b, x, s, dd\n\n# \u200b\u5408\u5e76\u200b\uff0c\u200b\u6267\u884c\u200b\u672c\u200b\u5355\u5143\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5e76\u200b\u8df3\u8f6c\u200b\u5230\u200b\u4e0b\u200b\u4e00\u200b\u5355\u5143\u200b\uff0c\u200b\u6267\u884c\u200b\u672c\u200b\u5355\u5143\u200b\u4ee3\u7801\u200b\uff0c\u200b\u7559\u5728\u200b\u672c\u200b\u5355\u5143\u200b\n# Shift+M Shift+Enter Ctrl+Enter\n\n# \u200b\u663e\u793a\u200b\u884c\u200b\u6570\u200b\uff0c\u200b\u5207\u6362\u200bmarkdown/code\n# l, m/y\n</code></pre> <ol> <li> <p>Jupyter Notebook\u200b\u4e2d\u200b\u6309\u200b\u4e0b\u200bEnter\u200b\u8fdb\u5165\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6309\u4e0b\u200bEsc\u200b\u8fdb\u5165\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b</p> </li> <li> <p>\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff08\u200b\u7eff\u8272\u200b\uff09</p> </li> </ol> <p></p> <ul> <li>\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b\uff08\u200b\u84dd\u8272\u200b\uff09</li> </ul> <p></p> <ul> <li>\u200b\u5728\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b\u4e0b\u200b\uff0c\u200b\u70b9\u51fb\u200bh\uff0c\u200b\u4f1a\u5f39\u200b\u51fa\u200b\u5feb\u6377\u952e\u200b\u7a97\u53e3\u200b</li> </ul> <p></p> <ol> <li> <p>Jupyter Lab\u200b\u540c\u6837\u200b\u6709\u200b\u4e24\u79cd\u200b\u6a21\u5f0f\u200b\u3002\u200b\u6309\u4e0b\u200bEnter\u200b\u8fdb\u5165\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6309\u4e0b\u200bEsc\u200b\u8fdb\u5165\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b</p> </li> <li> <p>\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff08\u200b\u6709\u200b\u6846\u7ebf\u200b\u65e0\u200b\u5149\u6807\u200b\uff09</p> </li> </ol> <p></p> <ul> <li>\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b\uff08\u200b\u65e0\u6846\u7ebf\u200b\u65e0\u200b\u5149\u6807\u200b\uff09</li> </ul> <p></p> <ul> <li> <p>\u200b\u5feb\u6377\u952e\u200b\u64cd\u4f5c\u200b\u4e0e\u200bJupyter Notebook\u200b\u57fa\u672c\u76f8\u540c\u200b\uff0c\u200b\u53ef\u200b\u53c2\u8003\u200b\u4e0a\u200b\u4e00\u90e8\u5206\u200b</p> </li> <li> <p>\u200b\u5feb\u6377\u952e\u200b\u6c47\u603b\u200b</p> </li> </ul> <p>\u200b           \u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b\uff08\u200b\u6309\u200b<code>Esc</code>\uff09</p> <ul> <li>Enter : \u200b\u8f6c\u5165\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b</li> <li>Shift-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b\uff0c\u200b\u9009\u4e2d\u200b\u4e0b\u4e2a\u200b\u5355\u5143\u200b</li> <li>Ctrl-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b</li> <li>Alt-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b\uff0c\u200b\u5728\u200b\u5176\u4e0b\u200b\u63d2\u5165\u200b\u65b0\u200b\u5355\u5143\u200b</li> <li>Y : \u200b\u5355\u5143\u200b\u8f6c\u5165\u200b\u4ee3\u7801\u200b\u72b6\u6001\u200b</li> <li>M :\u200b\u5355\u5143\u200b\u8f6c\u5165\u200bmarkdown\u200b\u72b6\u6001\u200b</li> <li>R : \u200b\u5355\u5143\u200b\u8f6c\u5165\u200braw\u200b\u72b6\u6001\u200b</li> <li>1 : \u200b\u8bbe\u5b9a\u200b 1 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>2 : \u200b\u8bbe\u5b9a\u200b 2 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>3 : \u200b\u8bbe\u5b9a\u200b 3 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>4 : \u200b\u8bbe\u5b9a\u200b 4 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>5 : \u200b\u8bbe\u5b9a\u200b 5 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>6 : \u200b\u8bbe\u5b9a\u200b 6 \u200b\u7ea7\u200b\u6807\u9898\u200b</li> <li>Up : \u200b\u9009\u4e2d\u200b\u4e0a\u65b9\u200b\u5355\u5143\u200b</li> <li>K : \u200b\u9009\u4e2d\u200b\u4e0a\u65b9\u200b\u5355\u5143\u200b</li> <li>Down : \u200b\u9009\u4e2d\u200b\u4e0b\u65b9\u200b\u5355\u5143\u200b</li> <li>J : \u200b\u9009\u4e2d\u200b\u4e0b\u65b9\u200b\u5355\u5143\u200b</li> <li>Shift-K : \u200b\u6269\u5927\u200b\u9009\u4e2d\u200b\u4e0a\u65b9\u200b\u5355\u5143\u200b</li> <li>Shift-J : \u200b\u6269\u5927\u200b\u9009\u4e2d\u200b\u4e0b\u65b9\u200b\u5355\u5143\u200b</li> <li>A : \u200b\u5728\u200b\u4e0a\u65b9\u200b\u63d2\u5165\u200b\u65b0\u200b\u5355\u5143\u200b</li> <li>B : \u200b\u5728\u200b\u4e0b\u65b9\u200b\u63d2\u5165\u200b\u65b0\u200b\u5355\u5143\u200b</li> <li>X : \u200b\u526a\u5207\u200b\u9009\u4e2d\u200b\u7684\u200b\u5355\u5143\u200b</li> <li>C : \u200b\u590d\u5236\u200b\u9009\u4e2d\u200b\u7684\u200b\u5355\u5143\u200b</li> <li>Shift-V : \u200b\u7c98\u8d34\u200b\u5230\u200b\u4e0a\u65b9\u200b\u5355\u5143\u200b</li> <li>V : \u200b\u7c98\u8d34\u200b\u5230\u200b\u4e0b\u65b9\u200b\u5355\u5143\u200b</li> <li>Z : \u200b\u6062\u590d\u200b\u5220\u9664\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5355\u5143\u200b</li> <li>D,D : \u200b\u5220\u9664\u200b\u9009\u4e2d\u200b\u7684\u200b\u5355\u5143\u200b</li> <li>Shift-M : \u200b\u5408\u5e76\u200b\u9009\u4e2d\u200b\u7684\u200b\u5355\u5143\u200b</li> <li>Ctrl-S : \u200b\u6587\u4ef6\u200b\u5b58\u76d8\u200b</li> <li>S : \u200b\u6587\u4ef6\u200b\u5b58\u76d8\u200b</li> <li>L : \u200b\u8f6c\u6362\u200b\u884c\u53f7\u200b</li> <li>O : \u200b\u8f6c\u6362\u200b\u8f93\u51fa\u200b</li> <li>Shift-O : \u200b\u8f6c\u6362\u200b\u8f93\u51fa\u200b\u6eda\u52a8\u200b</li> <li>Esc : \u200b\u5173\u95ed\u200b\u9875\u9762\u200b</li> <li>Q : \u200b\u5173\u95ed\u200b\u9875\u9762\u200b</li> <li>H : \u200b\u663e\u793a\u200b\u5feb\u6377\u952e\u200b\u5e2e\u52a9\u200b</li> <li>I,I : \u200b\u4e2d\u65ad\u200bNotebook\u200b\u5185\u6838\u200b</li> <li>0,0 : \u200b\u91cd\u542f\u200bNotebook\u200b\u5185\u6838\u200b</li> <li>Shift : \u200b\u5ffd\u7565\u200b</li> <li>Shift-Space : \u200b\u5411\u4e0a\u200b\u6eda\u52a8\u200b</li> <li>Space : \u200b\u5411\u4e0b\u200b\u6eda\u52a8\u200b</li> </ul> <p>\u200b       \u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff08\u200b\u6309\u200b<code>Enter</code>\uff09</p> <ul> <li>Tab : \u200b\u4ee3\u7801\u200b\u8865\u5168\u200b\u6216\u200b\u7f29\u8fdb\u200b</li> <li>Shift-Tab : \u200b\u63d0\u793a\u200b</li> <li>Ctrl-] : \u200b\u7f29\u8fdb\u200b</li> <li>Ctrl-[ : \u200b\u89e3\u9664\u200b\u7f29\u8fdb\u200b</li> <li>Ctrl-A : \u200b\u5168\u9009\u200b</li> <li>Ctrl-Z : \u200b\u590d\u539f\u200b</li> <li>Ctrl-Shift-Z : \u200b\u518d\u200b\u505a\u200b</li> <li>Ctrl-Y : \u200b\u518d\u200b\u505a\u200b</li> <li>Ctrl-Home : \u200b\u8df3\u200b\u5230\u200b\u5355\u5143\u200b\u5f00\u5934\u200b</li> <li>Ctrl-Up : \u200b\u8df3\u200b\u5230\u200b\u5355\u5143\u200b\u5f00\u5934\u200b</li> <li>Ctrl-End : \u200b\u8df3\u200b\u5230\u200b\u5355\u5143\u200b\u672b\u5c3e\u200b</li> <li>Ctrl-Down : \u200b\u8df3\u200b\u5230\u200b\u5355\u5143\u200b\u672b\u5c3e\u200b</li> <li>Ctrl-Left : \u200b\u8df3\u200b\u5230\u200b\u5de6\u8fb9\u200b\u4e00\u4e2a\u200b\u5b57\u9996\u200b</li> <li>Ctrl-Right : \u200b\u8df3\u200b\u5230\u200b\u53f3\u8fb9\u200b\u4e00\u4e2a\u200b\u5b57\u9996\u200b</li> <li>Ctrl-Backspace : \u200b\u5220\u9664\u200b\u524d\u9762\u200b\u4e00\u4e2a\u200b\u5b57\u200b</li> <li>Ctrl-Delete : \u200b\u5220\u9664\u200b\u540e\u9762\u200b\u4e00\u4e2a\u200b\u5b57\u200b</li> <li>Esc : \u200b\u8fdb\u5165\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b</li> <li>Ctrl-M : \u200b\u8fdb\u5165\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b</li> <li>Shift-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b\uff0c\u200b\u9009\u4e2d\u200b\u4e0b\u200b\u4e00\u200b\u5355\u5143\u200b</li> <li>Ctrl-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b</li> <li>Alt-Enter : \u200b\u8fd0\u884c\u200b\u672c\u200b\u5355\u5143\u200b\uff0c\u200b\u5728\u200b\u4e0b\u9762\u200b\u63d2\u5165\u200b\u4e00\u200b\u5355\u5143\u200b</li> <li>Ctrl-Shift-- : \u200b\u5206\u5272\u200b\u5355\u5143\u200b</li> <li>Ctrl-Shift-Subtract : \u200b\u5206\u5272\u200b\u5355\u5143\u200b</li> <li>Ctrl-S : \u200b\u6587\u4ef6\u200b\u5b58\u76d8\u200b</li> <li>Shift : \u200b\u5ffd\u7565\u200b</li> <li>Up : \u200b\u5149\u6807\u200b\u4e0a\u79fb\u200b\u6216\u200b\u8f6c\u5165\u200b\u4e0a\u200b\u4e00\u200b\u5355\u5143\u200b</li> <li>Down :\u200b\u5149\u6807\u200b\u4e0b\u79fb\u200b\u6216\u200b\u8f6c\u5165\u200b\u4e0b\u200b\u4e00\u200b\u5355\u5143\u200b</li> </ul>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#33","title":"3.3 \u200b\u5b89\u88c5\u200b\u63d2\u4ef6","text":"<ul> <li> <p>Jupyter Notebook\u200b\u5b89\u88c5\u200b\u63d2\u4ef6\u200b\u7684\u200b\u65b9\u6cd5\u200b</p> </li> <li> <p>\u200b\u5728\u200bAnaconda Powershell Prompt\u200b\u4e2d\u200b\u8f93\u5165\u200b</p> </li> </ul> <pre><code>pip install jupyter_contrib_nbextensions\n</code></pre> <ol> <li>\u200b\u518d\u6b21\u200b\u8f93\u5165\u200b\u4ee5\u4e0b\u200b\u6307\u4ee4\u200b\uff0c\u200b\u5c06\u200b\u63d2\u4ef6\u200b\u6dfb\u52a0\u200b\u5230\u200b\u5de5\u5177\u680f\u200b</li> </ol> <pre><code>jupyter contrib nbextension install\n</code></pre> <ol> <li>\u200b\u6253\u5f00\u200bJupyter Notebook\uff0c\u200b\u70b9\u51fb\u200bNbextensions\uff0c\u200b\u53d6\u6d88\u200b\u52fe\u9009\u200b<code>disable configuration for nbextensions without explicit compatibility</code>\uff0c\u200b\u6b64\u65f6\u200b\u53ef\u4ee5\u200b\u6dfb\u52a0\u200b\u81ea\u5df1\u200b\u559c\u6b22\u200b\u7684\u200b\u63d2\u4ef6\u200b\u5566\u200b\uff01</li> </ol> <p></p> <ol> <li> <p>\u200b\u63a8\u8350\u200b\u4ee5\u4e0b\u200b\u4e24\u4e2a\u200b\u57fa\u7840\u200b\u63d2\u4ef6\u200b</p> </li> <li> <p>Execute Time\uff1a\u200b\u53ef\u4ee5\u200b\u663e\u793a\u200b\u6267\u884c\u200b\u4e00\u4e2a\u200bCell\u200b\u8981\u82b1\u8d39\u200b\u591a\u5c11\u200b\u65f6\u95f4\u200b</p> </li> <li> <p>Hinterland\uff1a\u200b\u63d0\u4f9b\u200b\u4ee3\u7801\u200b\u8865\u5168\u200b\u529f\u80fd\u200b</p> </li> <li> <p>Jupyter Lab\u200b\u5b89\u88c5\u200b\u63d2\u4ef6\u200b\u7684\u200b\u65b9\u6cd5\u200b</p> </li> <li> <p>Jupyter Lab\u200b\u5b89\u88c5\u200b\u63d2\u4ef6\u200b\u70b9\u51fb\u200b\u5de6\u4fa7\u200b\u7684\u200b\u7b2c\u56db\u4e2a\u200b\u6807\u5fd7\u200b\uff0c\u200b\u70b9\u51fb\u200b\u201cEnable\u201d\u200b\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u641c\u7d22\u200b\u680f\u4e2d\u200b\u641c\u7d22\u200b\u60f3\u8981\u200b\u7684\u200b\u63d2\u4ef6\u200b</p> </li> </ol> <p></p> <ol> <li>\u200b\u4f8b\u5982\u200b\u641c\u7d22\u200b<code>jupyterlab-execute-time</code>\u200b\u540e\u200b\uff0c\u200b\u5728\u200bSearch Results\u200b\u4e2d\u200b\u67e5\u770b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u70b9\u51fb\u200bInstall\u200b\u4fbf\u200b\u53ef\u200b\u5b89\u88c5\u200b\u63d2\u4ef6\u200b</li> </ol> <p></p> <ol> <li>\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5728\u200bAnaconda Powershell Prompt\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u6307\u4ee4\u200b\u6765\u200b\u5b89\u88c5\u200b\u63d2\u4ef6\u200b</li> </ol> <pre><code>jupyter labextension install jupyterlab-execute-time  # \u200b\u5b89\u88c5\u200bjupyterlab-execute-time\n</code></pre>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#4","title":"4 \u200b\u8fdb\u9636\u200b\u64cd\u4f5c","text":"<p>\u200b       \u200b\u9664\u4e86\u200b\u4ee5\u4e0a\u200b\u64cd\u4f5c\u200b\uff0cJupyter Notebook\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u4e30\u5bcc\u200b\u7684\u200b\u5185\u5bb9\u200b\u7b49\u5f85\u200b\u5927\u5bb6\u200b\u63a2\u7d22\u200b\u3002\u200b\u5982\u200bBash\u200b\u547d\u4ee4\u200b\u3001\u200b\u9b54\u672f\u200b\u547d\u4ee4\u200b\u7b49\u200b\u3002\u200b\u6211\u4eec\u200b\u4e3a\u200b\u5927\u5bb6\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4efd\u200b\u5728\u200bNotebook\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b\u8fdb\u9636\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5feb\u200b\u6765\u200b\u8bd5\u8bd5\u770b\u200b\u5427\u200b~</p> <p>\u200b       \u200b\u70b9\u51fb\u200b\u67e5\u770b\u200b\u8fdb\u9636\u200b\u6559\u7a0b\u200b</p>"},{"location":"00-%E7%AC%AC%E9%9B%B6%E7%AB%A0%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/0.4%20Jupyter%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/#5","title":"5 \u200b\u53c2\u8003\u8d44\u6599","text":"<p>\u30101\u3011Jupyter Notebook\u200b\u4ecb\u7ecd\u200b\u3001\u200b\u5b89\u88c5\u200b\u53ca\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b - \u200b\u77e5\u4e4e\u200b (zhihu.com)</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.1%20PyTorch%E7%AE%80%E4%BB%8B/","title":"1.1 PyTorch\u200b\u7b80\u4ecb","text":"<p>PyTorch\u200b\u662f\u200b\u7531\u200bMeta AI(Facebook)\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7814\u7a76\u200b\u5c0f\u7ec4\u200b\u5f00\u53d1\u200b\u7684\u200b\u4e00\u79cd\u200b\u57fa\u4e8e\u200bLua\u200b\u7f16\u5199\u200b\u7684\u200bTorch\u200b\u5e93\u200b\u7684\u200bPython\u200b\u5b9e\u73b0\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200b\uff0c\u200b\u76ee\u524d\u200b\u88ab\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u4e8e\u200b\u5b66\u672f\u754c\u200b\u548c\u200b\u5de5\u4e1a\u754c\u200b\uff0c\u200b\u76f8\u8f83\u200b\u4e8e\u200bTensorflow2.x\uff0cPyTorch\u200b\u5728\u200bAPI\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u4e0a\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\u3001\u200b\u4f18\u96c5\u200b\u548c\u200b\u6613\u61c2\u200b\u3002\u200b\u56e0\u6b64\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u6211\u4eec\u200b\u9009\u62e9\u200bPyTorch\u200b\u6765\u200b\u8fdb\u884c\u200b\u5f00\u6e90\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e86\u89e3\u200bPyTorch\u200b\u7684\u200b\u53d1\u5c55\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u4e86\u89e3\u200bPyTorch\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u5176\u4ed6\u200b\u6846\u67b6\u200b\u7684\u200b\u4f18\u52bf\u200b</li> </ul>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.1%20PyTorch%E7%AE%80%E4%BB%8B/#111-pytorch","title":"1.1.1 PyTorch\u200b\u7684\u200b\u53d1\u5c55","text":"<p>\u201cAll in PyTorch\u201d\uff0c\u200b\u5bf9\u4e8e\u200bPyTorch\u200b\u7684\u200b\u53d1\u5c55\u200b\u6211\u4eec\u200b\u53ea\u80fd\u200b\u7528\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u6765\u200b\u6982\u51b5\u200b\u4e86\u200b\uff0cPyTorch\u200b\u81ea\u4ece\u200b\u63a8\u51fa\u200b\u5c31\u200b\u83b7\u5f97\u200b\u5de8\u5927\u200b\u7684\u200b\u5173\u6ce8\u200b\u5e76\u200b\u53d7\u5230\u200b\u4e86\u200b\u5f88\u591a\u200b\u4eba\u200b\u7684\u200b\u559c\u6b22\u200b\uff0c\u200b\u800c\u200b\u6700\u200b\u76f4\u89c2\u200b\u7684\u200b\u83ab\u8fc7\u4e8e\u200b\u4e0b\u9762\u200b\u6570\u636e\u200b\u6240\u200b\u8868\u73b0\u200b\u7684\u200b\u7b80\u660e\u200b\u76f4\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e0b\u56fe\u200b\u6765\u81ea\u200bPaper with code\u200b\u7f51\u7ad9\u200b\uff0c\u200b\u989c\u8272\u200b\u9762\u79ef\u200b\u4ee3\u8868\u200b\u4f7f\u7528\u200b\u8be5\u200b\u6846\u67b6\u200b\u7684\u200b\u8bba\u6587\u200b\u516c\u5f00\u200b\u4ee3\u7801\u200b\u5e93\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u622a\u81f3\u200b2021\u200b\u5e74\u200b6\u200b\u6708\u200b\uff0cPyTorch\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u5df2\u7ecf\u200b\u662f\u200bTensorFlow\u200b\u5b9e\u73b0\u200b\u7684\u200b4\u200b\u500d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u770b\u200b\u7ea2\u8272\u200b\u90e8\u5206\u200b\u7684\u200bPyTorch\u200b\u6b63\u5728\u200b\u53d6\u4ee3\u200b\u4ed6\u200b\u7684\u200b\u8001\u5927\u54e5\u200b\u79f0\u9738\u200b\u5b66\u672f\u200b\u5708\u200b\uff0cPyTorch\u200b\u4f1a\u200b\u501f\u52a9\u200bONNX\u200b\u6240\u200b\u5e26\u6765\u200b\u7684\u200b\u843d\u5730\u200b\u80fd\u529b\u200b\u5728\u200b\u5de5\u4e1a\u754c\u200b\u9010\u6e10\u200b\u8d70\u5411\u200b\u4e3b\u5bfc\u5730\u4f4d\u200b\u3002</p> <p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u627f\u8ba4\u200b\u5230\u200b\u73b0\u5728\u200b\u4e3a\u6b62\u200bPyTorch 1.x\u200b\u8fd8\u662f\u200b\u6709\u200b\u4e0d\u5982\u200b\u522b\u7684\u200b\u6846\u67b6\u200b\u7684\u200b\u5730\u65b9\u200b\uff0c\u200b\u4f46\u662f\u200b\u6211\u4eec\u200b\u76f8\u4fe1\u200bPyTorch 2.x\u200b\u7248\u672c\u200b\u4f1a\u200b\u7ed9\u200b\u6211\u4eec\u200b\u5e26\u6765\u200b\u66f4\u5927\u200b\u7684\u200b\u60ca\u559c\u200b\u3002</p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.1%20PyTorch%E7%AE%80%E4%BB%8B/#112-pytorch","title":"1.1.2 PyTorch\u200b\u7684\u200b\u4f18\u52bf","text":"<ul> <li>\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200b\u5176\u4ed6\u200b\u7684\u200b\u6846\u67b6\u200b\uff0cPyTorch\u200b\u7684\u200b\u6846\u67b6\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\uff0c\u200b\u6613\u4e8e\u200b\u7406\u89e3\u200b\u3002PyTorch\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u8ffd\u6c42\u200b\u6700\u5c11\u200b\u7684\u200b\u5c01\u88c5\u200b\uff0c\u200b\u907f\u514d\u200b\u91cd\u590d\u200b\u9020\u200b\u8f6e\u5b50\u200b\u3002</li> <li>\u200b\u4e0a\u200b\u624b\u5feb\u200b\uff0c\u200b\u638c\u63e1\u200bnumpy\u200b\u548c\u200b\u57fa\u672c\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u77e5\u8bc6\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e0a\u200b\u624b\u200b\u3002</li> <li>PyTorch\u200b\u6709\u7740\u200b\u826f\u597d\u200b\u7684\u200b\u6587\u6863\u200b\u548c\u200b\u793e\u533a\u200b\u652f\u6301\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4eb2\u81ea\u200b\u7ef4\u62a4\u200b\u7684\u200b\u8bba\u575b\u200b\u4f9b\u200b\u7528\u6237\u200b\u4ea4\u6d41\u200b\u548c\u200b\u6c42\u6559\u200b\u95ee\u9898\u200b\u3002Meta AI(Facebook AI)\u200b\u5bf9\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5f3a\u529b\u200b\u652f\u6301\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u5f53\u4eca\u200b\u6392\u540d\u200b\u524d\u200b\u4e09\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u673a\u6784\u200b\uff0cMAIR\u200b\u7684\u200b\u652f\u6301\u200b\u8db3\u4ee5\u200b\u786e\u4fdd\u200bPyTorch\u200b\u83b7\u5f97\u200b\u6301\u7eed\u200b\u7684\u200b\u5f00\u53d1\u200b\u66f4\u65b0\u200b\u3002</li> <li>\u200b\u9879\u76ee\u200b\u5f00\u6e90\u200b\uff0c\u200b\u5728\u200bGithub\u200b\u4e0a\u200b\u6709\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u5f00\u200b\u6e90\u4ee3\u7801\u200b\u662f\u200b\u4f7f\u7528\u200bPyTorch\u200b\u8fdb\u884c\u200b\u5f00\u53d1\u200b\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u66f4\u597d\u200b\u7684\u200b\u8c03\u8bd5\u200b\u4ee3\u7801\u200b\uff0cPyTorch\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9010\u884c\u200b\u6267\u884c\u200b\u6211\u4eec\u200b\u7684\u200b\u811a\u672c\u200b\u3002\u200b\u8fd9\u200b\u5c31\u200b\u50cf\u200b\u8c03\u8bd5\u200bNumPy\u200b\u4e00\u6837\u200b \u2013 \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u8bbf\u95ee\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6253\u5370\u200b\u8bed\u53e5\u200b\uff08\u200b\u6216\u200b\u5176\u4ed6\u200b\u6807\u51c6\u200b\u7684\u200bPython\u200b\u8c03\u8bd5\u200b\uff09\u200b\u6765\u200b\u67e5\u770b\u200b\u65b9\u6cd5\u200b\u5931\u8d25\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u3002</li> <li>\u200b\u8d8a\u6765\u8d8a\u200b\u5b8c\u5584\u200b\u7684\u200b\u6269\u5c55\u200b\u5e93\u200b\uff0c\u200b\u6d3b\u529b\u200b\u65fa\u76db\u200b\uff0c\u200b\u6b63\u200b\u5904\u5728\u200b\u5f53\u6253\u200b\u4e4b\u200b\u5e74\u200b\u3002</li> </ul>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/","title":"1.2 PyTorch\u200b\u7684\u200b\u5b89\u88c5","text":"<p>PyTorch\u200b\u7684\u200b\u5b89\u88c5\u200b\u662f\u200b\u6211\u4eec\u200b\u5b66\u4e60\u200bPyTorch\u200b\u7684\u200b\u7b2c\u4e00\u6b65\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u7ecf\u5e38\u200b\u51fa\u9519\u200b\u7684\u200b\u4e00\u6b65\u200b\u3002\u200b\u5728\u200b\u5b89\u88c5\u200bPyTorch\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bAnaconda/miniconda+Pytorch+ IDE \u200b\u7684\u200b\u6d41\u7a0b\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>Anaconda/miniconda\u200b\u7684\u200b\u5b89\u88c5\u200b\u53ca\u5176\u200b\u5e38\u89c1\u200b\u547d\u4ee4\u200b</li> <li>PyTorch\u200b\u7684\u200b\u5b89\u88c5\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u5982\u4f55\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u9002\u5408\u200b\u81ea\u5df1\u200b\u7684\u200bPyTorch\u200b\u7248\u672c\u200b</li> </ul>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#121-anaconda","title":"1.2.1 Anaconda\u200b\u7684\u200b\u5b89\u88c5","text":"<p>\u200b\u5728\u200b\u6570\u636e\u200b\u79d1\u5b66\u200b\u548c\u200b\u6700\u8fd1\u200b\u5f88\u706b\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u8981\u200b\u7528\u5230\u200b\u5927\u91cf\u200b\u6210\u719f\u200b\u7684\u200bpackage\u3002\u200b\u6211\u4eec\u200b\u4e00\u4e2a\u4e2a\u200b\u5b89\u88c5\u200b package \u200b\u5f88\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u800c\u4e14\u200b\u5f88\u200b\u5bb9\u6613\u200b\u51fa\u73b0\u200b\u5305\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4f9d\u8d56\u200b\u4e0d\u200b\u9002\u914d\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u800c\u200b Anaconda/miniconda\u200b\u7684\u200b\u51fa\u73b0\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u89e3\u51b3\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5b83\u200b\u96c6\u6210\u200b\u4e86\u200b\u5e38\u7528\u200b\u4e8e\u200b\u79d1\u5b66\u5206\u6790\u200b\uff08\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff0c \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff09\u200b\u7684\u200b\u5927\u91cf\u200bpackage\uff0c\u200b\u5e76\u4e14\u200b\u501f\u52a9\u4e8e\u200bconda\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u5bf9\u200b\u865a\u62df\u200bPython\u200b\u73af\u5883\u200b\u7684\u200b\u7ba1\u7406\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-1anacondaminiconda","title":"Step 1\uff1a\u200b\u5b89\u88c5\u200bAnaconda/miniconda","text":"<p>\u200b\u767b\u9646\u200bAnaconda | Individual Edition\uff0c\u200b\u9009\u62e9\u200b\u76f8\u5e94\u200b\u7cfb\u7edf\u200bDownLoad\uff0c\u200b\u6b64\u5904\u200b\u4ee5\u200bWindows\u200b\u4e3a\u4f8b\u200b\uff08Linux\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u94fe\u63a5\u200b\u9009\u62e9\u200b\u5408\u9002\u200b\u7684\u200b\u7248\u672c\u200b\u8fdb\u884c\u200b\u4e0b\u8f7d\u200b\u6216\u8005\u200b\u901a\u8fc7\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200bshell\u200b\u811a\u672c\u200b\u8fdb\u884c\u200b\u4e0b\u8f7d\u200b\uff09\uff1a</p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-2","title":"Step 2\uff1a\u200b\u68c0\u9a8c\u200b\u662f\u5426\u200b\u5b89\u88c5\u200b\u6210\u529f","text":"<p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u9875\u200b\u627e\u5230\u200bAnaconda Prompt\uff0c\u200b\u4e00\u822c\u200b\u5728\u200bAnaconda3\u200b\u7684\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b,( Linux\u200b\u5728\u200b\u7ec8\u7aef\u200b\u4e0b\u200b\u5c31\u884c\u4e86\u200b\uff09</p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-3","title":"Step 3\uff1a\u200b\u521b\u5efa\u200b\u865a\u62df\u73af\u5883","text":"<p>Linux\u200b\u5728\u200b\u7ec8\u7aef\u200b(<code>Ctrl</code>+<code>Alt</code>+<code>T</code>)\u200b\u8fdb\u884c\u200b\uff0cWindows\u200b\u5728\u200b<code>Anaconda Prompt</code>\u200b\u8fdb\u884c\u200b</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_1","title":"\u67e5\u770b\u200b\u73b0\u5b58\u200b\u865a\u62df\u73af\u5883","text":"<p>\u200b\u67e5\u770b\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u597d\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e24\u4e2a\u200b\u73af\u5883\u200b\u5b58\u5728\u200b\u4e86\u200b</p> <pre><code>conda env list \n</code></pre> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_2","title":"\u521b\u5efa\u200b\u865a\u62df\u73af\u5883","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u521b\u5efa\u200b\u4e0d\u540c\u200b\u7248\u672c\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u6765\u200b\u6ee1\u8db3\u200b\u6211\u4eec\u200b\u7684\u200b\u4e00\u4e9b\u200b\u9700\u6c42\u200b\u3002\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u521b\u5efa\u200b\u865a\u62df\u73af\u5883\u200b\u7684\u200b\u547d\u4ee4\u200b\u3002 <pre><code>conda create -n env_name python==version \n# \u200b\u6ce8\u200b\uff1a\u200b\u5c06\u200benv_name \u200b\u66ff\u6362\u6210\u200b\u4f60\u200b\u7684\u200b\u73af\u5883\u200b\u7684\u200b\u540d\u79f0\u200b\uff0cversion\u200b\u66ff\u6362\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7248\u672c\u53f7\u200b\uff0ceg\uff1a3.8\n</code></pre></p> <p></p> <p>\u200b\u6ce8\u200b\uff1a 1. \u200b\u8fd9\u91cc\u200b\u5ffd\u7565\u200b\u6211\u4eec\u200b\u7684\u200bwarning\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u65f6\u5019\u200b\u5df2\u7ecf\u200b\u5b89\u200b\u4e86\u200b\u53c8\u200b\u5378\u8f7d\u200b\u4e00\u904d\u200b\u4e86\u200b\uff0c\u200b\u6b63\u5e38\u200b\u65f6\u200b\u662f\u200b\u4e0d\u4f1a\u200b\u6709\u200bwarning\u200b\u7684\u200b\u3002 2. \u200b\u5728\u200b\u9009\u62e9\u200bPython\u200b\u7248\u672c\u200b\u65f6\u200b\uff0c\u200b\u4e0d\u8981\u200b\u9009\u62e9\u200b\u592a\u9ad8\u200b\uff0c\u200b\u5efa\u8bae\u200b\u9009\u62e9\u200b3.6-3.8\uff0c\u200b\u7248\u672c\u200b\u8fc7\u9ad8\u4f1a\u200b\u5bfc\u81f4\u200b\u76f8\u5173\u200b\u5e93\u200b\u4e0d\u200b\u9002\u914d\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_3","title":"\u5b89\u88c5\u5305","text":"<pre><code>conda install package_name \n# \u200b\u6ce8\u200b\uff1apackage_name \u200b\u66ff\u6362\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5305\u200b\u7684\u200b\u540d\u79f0\u200b\uff0ceg: pandas\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_4","title":"\u5378\u8f7d\u200b\u5305","text":"<pre><code>conda remove package_name\n# \u200b\u6ce8\u200b\uff1apackage_name \u200b\u66ff\u6362\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5305\u200b\u7684\u200b\u540d\u79f0\u200b\uff0ceg: pandas\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_5","title":"\u663e\u793a\u200b\u6240\u6709\u200b\u5b89\u88c5\u200b\u7684\u200b\u5305","text":"<pre><code>conda list\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_6","title":"\u5220\u9664\u200b\u865a\u62df\u73af\u5883\u200b\u547d\u4ee4","text":"<pre><code>conda remove -n env_name --all \n# \u200b\u6ce8\u200b\uff1aenv_name \u200b\u66ff\u6362\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u73af\u5883\u200b\u7684\u200b\u540d\u79f0\u200b\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_7","title":"\u6fc0\u6d3b\u200b\u73af\u5883\u200b\u547d\u4ee4","text":"<pre><code>conda activate env_name\n# \u200b\u6ce8\u200b\uff1aenv_name \u200b\u66ff\u6362\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u73af\u5883\u200b\u7684\u200b\u540d\u79f0\u200b\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#_8","title":"\u9000\u51fa\u200b\u5f53\u524d\u200b\u73af\u5883","text":"<p><pre><code>conda deactivate\n</code></pre> \u200b\u5173\u4e8e\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u547d\u4ee4\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200bAnaconda/miniconda\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5b98\u7f51\u200b\u94fe\u63a5\u200b\uff1a\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-4","title":"Step 4\uff1a\u200b\u6362\u6e90","text":"<p>\u200b\u5728\u200b\u5b89\u88c5\u200bpackage\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u4f7f\u7528\u200b<code>pip install package_name</code>\u200b\u548c\u200b<code>conda install package_name</code> \u200b\u7684\u200b\u547d\u4ee4\u200b\uff0c\u200b\u4f46\u662f\u200b\u4e00\u4e9b\u200bpackage\u200b\u4e0b\u8f7d\u901f\u5ea6\u200b\u4f1a\u200b\u5f88\u6162\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u6362\u6e90\u200b\uff0c\u200b\u6362\u6210\u200b\u56fd\u5185\u200b\u6e90\u200b\uff0c\u200b\u52a0\u5feb\u200b\u6211\u4eec\u200b\u7684\u200b\u4e0b\u8f7d\u901f\u5ea6\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u4fbf\u662f\u200b\u4e24\u79cd\u200b\u5bf9\u5e94\u200b\u65b9\u5f0f\u200b\u7684\u200b\u6c38\u4e45\u200b\u6362\u6e90\u200b\u3002\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4ec5\u4ec5\u200b\u60f3\u4e3a\u200b\u5355\u6b21\u200b\u4e0b\u8f7d\u200b\u6362\u6e90\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>pip install package_name -i https://pypi.tuna.tsinghua.edu.cn/simple</code>\u200b\u8fdb\u884c\u200b\u4e0b\u8f7d\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#pip","title":"pip\u200b\u6362\u6e90","text":""},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#linux","title":"Linux\uff1a","text":"<p>Linux\u200b\u4e0b\u200b\u7684\u200b\u6362\u6e90\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u5728\u200b\u7528\u6237\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u65b0\u5efa\u200b\u6587\u4ef6\u5939\u200b<code>.pip</code>\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u6587\u4ef6\u5939\u200b\u5185\u200b\u65b0\u5efa\u200b\u6587\u4ef6\u200b<code>pip.conf</code>\uff0c\u200b\u5177\u4f53\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b <pre><code>cd ~\nmkdir .pip/\nvi pip.conf\n</code></pre> \u200b\u968f\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b<code>pip.conf</code>\u200b\u6dfb\u52a0\u200b\u4e0b\u65b9\u200b\u7684\u200b\u5185\u5bb9\u200b: <pre><code>[global]\nindex-url = http://pypi.douban.com/simple\n[install]\nuse-mirrors =true\nmirrors =http://pypi.douban.com/simple/\ntrusted-host =pypi.douban.com\n</code></pre></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#windows","title":"Windows\uff1a","text":"<p>1\u3001\u200b\u6587\u4ef6\u200b\u7ba1\u7406\u5668\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u5730\u5740\u680f\u200b\u6572\u200b\uff1a<code>%APPDATA%</code> \u200b\u56de\u8f66\u200b\uff0c\u200b\u5feb\u901f\u200b\u8fdb\u5165\u200b <code>C:\\Users\\\u200b\u7535\u8111\u200b\u7528\u6237\u200b\\AppData\\Roaming</code> \u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b 2\u3001\u200b\u65b0\u5efa\u200b pip \u200b\u6587\u4ef6\u5939\u200b\u5e76\u200b\u5728\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u65b0\u5efa\u200b <code>pip.ini</code> \u200b\u914d\u7f6e\u6587\u4ef6\u200b 3\u3001\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b<code>pip.ini</code> \u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4f7f\u7528\u200b\u8bb0\u4e8b\u672c\u200b\u6253\u5f00\u200b\uff0c\u200b\u8f93\u5165\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff0c\u200b\u5e76\u200b\u6309\u200b\u4e0b\u200bctrl+s\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u8c46\u74e3\u200b\u6e90\u4e3a\u200b\u4f8b\u5b50\u200b\u3002</p> <pre><code>[global]\nindex-url = http://pypi.douban.com/simple\n[install]\nuse-mirrors =true\nmirrors =http://pypi.douban.com/simple/\ntrusted-host =pypi.douban.com\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#conda","title":"conda\u200b\u6362\u6e90\u200b\uff08\u200b\u6e05\u534e\u200b\u6e90\u200b\uff09\u200b\u5b98\u65b9\u200b\u6362\u6e90\u200b\u5e2e\u52a9","text":""},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#windows_1","title":"Windows\u200b\u7cfb\u7edf\u200b\uff1a","text":"<p>TUNA \u200b\u63d0\u4f9b\u200b\u4e86\u200b Anaconda \u200b\u4ed3\u5e93\u200b\u4e0e\u200b\u7b2c\u4e09\u65b9\u200b\u6e90\u200b\u7684\u200b\u955c\u50cf\u200b\uff0c\u200b\u5404\u200b\u7cfb\u7edf\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u7528\u6237\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u7684\u200b <code>.condarc</code> \u200b\u6587\u4ef6\u200b\u3002Windows \u200b\u7528\u6237\u200b\u65e0\u6cd5\u200b\u76f4\u63a5\u200b\u521b\u5efa\u200b\u540d\u4e3a\u200b <code>.condarc</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u53ef\u200b\u5148\u200b\u6267\u884c\u200b<code>conda config --set show_channel_urls yes</code>\u200b\u751f\u6210\u200b\u8be5\u200b\u6587\u4ef6\u200b\u4e4b\u540e\u200b\u518d\u200b\u4fee\u6539\u200b\u3002</p> <p>\u200b\u5b8c\u6210\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4fee\u6539\u200b<code>C:\\Users\\User_name\\.condarc</code>\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6253\u5f00\u200b\u540e\u200b\u5c06\u200b\u6587\u4ef6\u200b\u91cc\u200b\u539f\u59cb\u200b\u5185\u5bb9\u200b\u5220\u9664\u200b\uff0c\u200b\u5c06\u200b\u4e0b\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\u590d\u5236\u200b\u8fdb\u53bb\u200b\u5e76\u200b\u4fdd\u5b58\u200b\u3002</p> <pre><code>channels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n</code></pre> <p>\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u6253\u5f00\u200b<code>Anaconda Prompt</code> \u200b\u8fd0\u884c\u200b <code>conda clean -i</code> \u200b\u6e05\u9664\u200b\u7d22\u5f15\u200b\u7f13\u5b58\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u7528\u200b\u7684\u200b\u662f\u200b\u955c\u50cf\u200b\u7ad9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#linux_1","title":"Linux\u200b\u7cfb\u7edf\u200b\uff1a","text":"<p>\u200b\u5728\u200bLinux\u200b\u7cfb\u7edf\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u662f\u200b\u9700\u8981\u200b\u4fee\u6539\u200b<code>.condarc</code>\u200b\u6765\u200b\u8fdb\u884c\u200b\u6362\u6e90\u200b <pre><code>cd ~\nvi .condarc\n</code></pre> \u200b\u5728\u200b<code>vim</code>\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8f93\u5165\u200b<code>i</code>\u200b\u8fdb\u5165\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5c06\u200b\u4e0a\u65b9\u200b\u5185\u5bb9\u200b\u7c98\u8d34\u200b\u8fdb\u53bb\u200b\uff0c\u200b\u6309\u200b<code>ESC</code>\u200b\u9000\u51fa\u200b\u7f16\u8f91\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8f93\u5165\u200b<code>:wq</code>\u200b\u4fdd\u5b58\u200b\u5e76\u200b\u9000\u51fa\u200b</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>conda config --show default_channels</code>\u200b\u68c0\u67e5\u200b\u4e0b\u200b\u662f\u5426\u200b\u6362\u6e90\u200b\u6210\u529f\u200b\uff0c\u200b\u5982\u679c\u200b\u51fa\u73b0\u200b\u4e0b\u56fe\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5373\u200b\u4ee3\u8868\u200b\u6211\u4eec\u200b\u6362\u6e90\u200b\u6210\u529f\u200b\u3002</p> <p></p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u9700\u8981\u200b<code>conda clean -i</code> \u200b\u6e05\u9664\u200b\u7d22\u5f15\u200b\u7f13\u5b58\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u7528\u200b\u7684\u200b\u662f\u200b\u955c\u50cf\u200b\u7ad9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#122","title":"1.2.2 \u200b\u67e5\u770b\u200b\u663e\u5361","text":"<p>\u200b\u8be5\u200b\u90e8\u5206\u200b\u5982\u679c\u200b\u4ec5\u4ec5\u200b\u53ea\u6709\u200bCPU\u200b\u6216\u8005\u200b\u96c6\u663e\u200b\u7684\u200b\u5c0f\u4f19\u4f34\u200b\u4eec\u200b\u53ef\u4ee5\u200b\u8df3\u8fc7\u200b\u8be5\u200b\u90e8\u5206\u200b</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#windows_2","title":"windows\uff1a","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b<code>cmd/terminal\u200b\u4e2d\u200b</code>\u200b\u8f93\u5165\u200b<code>nvidia-smi</code>\uff08Linux\u200b\u548c\u200bWin\u200b\u547d\u4ee4\u200b\u4e00\u6837\u200b\uff09\u3001\u200b\u4f7f\u7528\u200bNVIDIA\u200b\u63a7\u5236\u9762\u677f\u200b\u548c\u200b\u4f7f\u7528\u200b\u4efb\u52a1\u200b\u7ba1\u7406\u5668\u200b\u67e5\u770b\u200b\u81ea\u5df1\u200b\u662f\u5426\u200b\u6709\u200bNVIDIA\u200b\u7684\u200b\u72ec\u7acb\u200b\u663e\u5361\u200b\u53ca\u5176\u200b\u578b\u53f7\u200b</p> <p></p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#linux_2","title":"linux\uff1a","text":"<p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u770b\u200b\u4e0b\u200b\u7248\u672c\u53f7\u200b\uff0c\u200b\u770b\u200b\u81ea\u5df1\u200b\u53ef\u4ee5\u200b\u517c\u5bb9\u200b\u7684\u200bCUDA\u200b\u7248\u672c\u200b\uff0c\u200b\u7b49\u200b\u4f1a\u200b\u5b89\u88c5\u200bPyTorch\u200b\u65f6\u200b\u662f\u200b\u53ef\u4ee5\u200b\u5411\u4e0b\u517c\u5bb9\u200b\u7684\u200b\u3002\u200b\u5177\u4f53\u200b\u9002\u914d\u200b\u8868\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\u3002</p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#123-pytorch","title":"1.2.3 \u200b\u5b89\u88c5\u200bPyTorch","text":""},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-1pytorch","title":"Step 1\uff1a\u200b\u767b\u5f55\u200bPyTorch\u200b\u5b98\u7f51","text":""},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-2install","title":"Step 2\uff1aInstall","text":"<p>\u200b\u8fd9\u4e2a\u200b\u754c\u9762\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u672c\u5730\u200b\u5f00\u59cb\u200b\uff08Start Locally\uff09\uff0c\u200b\u4e91\u200b\u5f00\u53d1\u200b\uff08Cloud Partners)\uff0c\u200b\u4ee5\u524d\u200b\u7684\u200bPytorch\u200b\u7248\u672c\u200b\uff08Previous PyTorch Versions\uff09\uff0c\u200b\u79fb\u52a8\u200b\u7aef\u200b\u5f00\u53d1\u200b\uff08Mobile\uff09\uff0c\u200b\u5728\u200b\u6b64\u5904\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u672c\u5730\u200b\u5b89\u88c5\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-3_1","title":"Step 3\uff1a\u200b\u9009\u62e9\u200b\u547d\u4ee4","text":"<p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7ed3\u5408\u200b\u81ea\u5df1\u200b\u60c5\u51b5\u200b\u9009\u62e9\u200b\u547d\u4ee4\u200b\u5e76\u200b\u590d\u5236\u200b\u4e0b\u6765\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200bconda\u200b\u4e0b\u8f7d\u200b\u6216\u8005\u200bpip\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5efa\u8bae\u200bconda\u200b\u5b89\u88c5\u200b\uff09</p> <p>\u200b\u6253\u5f00\u200b<code>Terminal</code>\uff0c\u200b\u8f93\u5165\u200b<code>conda activate env_name</code>(env_name \u200b\u4e3a\u200b\u4f60\u200b\u5bf9\u5e94\u200b\u7684\u200b\u73af\u5883\u200b\u540d\u79f0\u200b)\uff0c\u200b\u5207\u6362\u200b\u5230\u200b\u5bf9\u5e94\u200b\u7684\u200b\u73af\u5883\u200b\u4e0b\u9762\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200bPyTorch\u200b\u7684\u200b\u5b89\u88c5\u200b\u4e86\u200b\u3002</p> <p></p> <p>\u200b\u6ce8\u200b\uff1a 1. Stable\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u7a33\u5b9a\u200b\u7248\u672c\u200b\uff0cPreview\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5148\u884c\u200b\u7248\u672c\u200b 2. \u200b\u53ef\u4ee5\u200b\u7ed3\u5408\u200b\u7535\u8111\u200b\u662f\u5426\u200b\u6709\u200b\u663e\u5361\u200b\uff0c\u200b\u9009\u62e9\u200bCPU\u200b\u7248\u672c\u200b\u8fd8\u662f\u200bCUDA\u200b\u7248\u672c\u200b\uff0cCUDA\u200b\u7248\u672c\u200b\u9700\u8981\u200b\u62e5\u6709\u200b\u72ec\u663e\u4e14\u200b\u662f\u200bNVIDIA\u200b\u7684\u200bGPU 3. \u200b\u5b98\u65b9\u200b\u5efa\u8bae\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bAnaconda/miniconda\u200b\u6765\u200b\u8fdb\u884c\u200b\u7ba1\u7406\u200b 4. \u200b\u5173\u4e8e\u200b\u5b89\u88c5\u200b\u7684\u200b\u7cfb\u7edf\u200b\u8981\u6c42\u200b</p> <ol> <li>Windows\uff1a<ol> <li>Windows 7\u200b\u53ca\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\uff1b\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200bWindows 10\u200b\u6216\u8005\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u7248\u672c\u200b</li> <li>Windows Server 2008 r2 \u200b\u53ca\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b</li> </ol> </li> <li>Linux\uff1a\u200b\u4ee5\u200b\u5e38\u89c1\u200b\u7684\u200bCentOS\u200b\u548c\u200bUbuntu\u200b\u4e3a\u4f8b\u200b<ol> <li>CentOS, \u200b\u6700\u4f4e\u200b\u7248\u672c\u200b7.3-1611</li> <li>Ubuntu, \u200b\u6700\u4f4e\u200b\u7248\u672c\u200b 13.04\uff0c\u200b\u8fd9\u91cc\u200b\u4f1a\u200b\u5bfc\u81f4\u200bcuda\u200b\u5b89\u88c5\u200b\u7684\u200b\u6700\u5927\u200b\u7248\u672c\u200b\u4e0d\u540c\u200b</li> </ol> </li> <li> <p>macOS\uff1a</p> <ol> <li>macOS 10.10\u200b\u53ca\u5176\u200b\u4ee5\u4e0a\u200b</li> </ol> </li> <li> <p>\u200b\u6709\u4e9b\u200b\u7535\u8111\u200b\u6240\u200b\u652f\u6301\u200b\u7684\u200bcuda\u200b\u7248\u672c\u200b&lt;10.2\uff0c\u200b\u6b64\u65f6\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u624b\u52a8\u200b\u964d\u7ea7\u200b\uff0c\u200b\u5373\u200b\u5c31\u662f\u200bcudatoolkit = \u200b\u4f60\u200b\u6240\u200b\u9002\u5408\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u4f46\u662f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u4e0b\u200b\u4e00\u5b9a\u200b\u8981\u200b\u4fdd\u6301\u200bPyTorch\u200b\u548c\u200bcudatoolkit\u200b\u7684\u200b\u7248\u672c\u200b\u9002\u914d\u200b\u3002\u200b\u67e5\u770b\u200bPrevious PyTorch Versions | PyTorch</p> </li> </ol>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-4_1","title":"Step 4\uff1a\u200b\u5728\u7ebf\u200b\u4e0b\u8f7d","text":"<p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b<code>Anaconda Prompt</code>\u200b\u8fdb\u884c\u200b\u4e0b\u8f7d\u200b\u7684\u8bdd\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u901a\u8fc7\u200b<code>conda activate env_name</code>\uff0c\u200b\u6fc0\u6d3b\u200b\u6211\u4eec\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u4e2d\u200b\u53bb\u200b\uff0c\u200b\u518d\u200b\u8f93\u5165\u200b\u547d\u4ee4\u200b\u3002</p> <p>\u200b\u6ce8\u200b: \u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8981\u200b\u628a\u200b\u4e0b\u8f7d\u200b\u6307\u4ee4\u200b\u540e\u9762\u200b\u7684\u200b -c pytorch \u200b\u53bb\u6389\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u4f7f\u7528\u200b\u6e05\u534e\u200b\u6e90\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u5426\u5219\u200b\u8fd8\u662f\u200b\u9ed8\u8ba4\u200b\u4ece\u200b\u5b98\u7f51\u200b\u4e0b\u8f7d\u200b\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-5","title":"Step 5\uff1a\u200b\u79bb\u7ebf\u200b\u4e0b\u8f7d","text":""},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#windows_3","title":"Windows\uff1a","text":"<p>\u200b\u5728\u200b\u5b89\u88c5\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e00\u4e9b\u200b\u5947\u5947\u602a\u602a\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u5728\u7ebf\u200b\u4e0b\u8f7d\u200b\u4e0d\u200b\u6210\u529f\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u79bb\u7ebf\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\uff1ahttps://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</p> <p>\u200b\u901a\u8fc7\u200b\u4e0a\u9762\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e0b\u8f7d\u200b\u597d\u200b\u5bf9\u5e94\u200b\u7248\u672c\u200b\u7684\u200bpytorch\u200b\u548c\u200b torchvision \u200b\u5305\u200b\uff0c\u200b\u7136\u540e\u200b\u6253\u5f00\u200b<code>Anaconda Prompt</code>/<code>Terminal</code>\u200b\u4e2d\u200b\uff0c\u200b\u8fdb\u5165\u200b\u6211\u4eec\u200b\u5b89\u88c5\u200b\u7684\u200b\u8def\u5f84\u200b\u4e0b\u200b\u3002</p> <p><pre><code>cd package_location\nconda activate env_name\n</code></pre> \u200b\u63a5\u4e0b\u6765\u200b\u8f93\u5165\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5b89\u88c5\u200b\u4e24\u4e2a\u200b\u5305\u200b</p> <pre><code>conda install --offline pytorch\u200b\u538b\u7f29\u5305\u200b\u7684\u200b\u5168\u79f0\u200b\uff08\u200b\u540e\u7f00\u200b\u90fd\u200b\u4e0d\u80fd\u200b\u5fd8\u8bb0\u200b\uff09\nconda install --offline torchvision\u200b\u538b\u7f29\u5305\u200b\u7684\u200b\u5168\u79f0\u200b\uff08\u200b\u540e\u7f00\u200b\u90fd\u200b\u4e0d\u80fd\u200b\u5fd8\u8bb0\u200b\uff09\n</code></pre>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-6","title":"Step 6\uff1a\u200b\u68c0\u9a8c\u200b\u662f\u5426\u200b\u5b89\u88c5\u200b\u6210\u529f","text":"<p>\u200b\u8fdb\u5165\u200b\u6240\u5728\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff0c\u200b\u7d27\u63a5\u7740\u200b\u8f93\u5165\u200b<code>python</code>\uff0c\u200b\u5728\u200b\u8f93\u5165\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p><pre><code>import torch\n\ntorch.cuda.is_available()\n</code></pre> <pre><code>False\n</code></pre> \u200b\u8fd9\u200b\u6761\u200b\u547d\u4ee4\u200b\u610f\u601d\u200b\u662f\u200b\u68c0\u9a8c\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200bcuda\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5b89\u88c5\u200b\u7684\u200b\u662f\u200bCPU\u200b\u7248\u672c\u200b\u7684\u8bdd\u200b\u4f1a\u200b\u8fd4\u56de\u200bFalse\uff0c\u200b\u80fd\u591f\u200b\u8c03\u7528\u200bGPU\u200b\u7684\u200b\u4f1a\u200b\u8fd4\u56de\u200bTrue\u3002\u200b\u4e00\u822c\u200b\u8fd9\u4e2a\u200b\u547d\u4ee4\u200b\u4e0d\u200b\u62a5\u9519\u200b\u7684\u8bdd\u200b\u5c31\u200b\u8bc1\u660e\u200b\u5b89\u88c5\u200b\u6210\u529f\u200b\u3002</p> <ul> <li>Windows\u200b\u7cfb\u7edf\u200b</li> </ul> <p></p> <ul> <li>Linux\u200b\u7cfb\u7edf\u200b</li> </ul> <p></p> <p>PyTorch\u200b\u7684\u200b\u5b89\u88c5\u200b\u7edd\u5bf9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5bb9\u6613\u200b\u4e0a\u706b\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u800c\u4e14\u200b\u7f51\u7edc\u200b\u4e0a\u200b\u7684\u200b\u6559\u7a0b\u200b\u5f88\u200b\u53ef\u80fd\u200b\u5bf9\u5e94\u200b\u65e9\u671f\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u6216\u662f\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e00\u4e9b\u200b\u5947\u5947\u602a\u602a\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f46\u662f\u200b\u522b\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u591a\u88c5\u200b\u51e0\u6b21\u200b\u591a\u200b\u9047\u5230\u200b\u70b9\u200b\u5947\u5947\u602a\u602a\u200b\u7684\u200b\u95ee\u9898\u200b\u5c31\u200b\u597d\u200b\u4e86\u200b\uff01</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#124-pycharm","title":"1.2.4 PyCharm\u200b\u5b89\u88c5\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\u64cd\u4f5c\u200b\uff09","text":"<p>VSCode\u200b\u8fd9\u4e9b\u200b\u4e5f\u200b\u662f\u200bok\u200b\u7684\u200b\uff0c\u200b\u5b89\u88c5\u200bPyCharm\u200b\u975e\u200b\u5fc5\u987b\u200b\u64cd\u4f5c\u200b</p> <p>Linux\uff0cWindows\u200b\u6b64\u5904\u200b\u64cd\u4f5c\u200b\u76f8\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5efa\u8bae\u200bWindows\u200b\u7684\u200b\u540c\u5b66\u200b\u5b89\u88c5\u200bPycharm\u200b\u5373\u53ef\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200bLinux\u200b\u4e0a\u200bpycharm\u200b\u5e76\u200b\u4e0d\u662f\u200b\u4e3b\u6d41\u200b\u7684\u200bIDE\u3002</p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-1","title":"Step 1\uff1a\u200b\u8fdb\u5165\u200b\u5b98\u7f51\u200b\u4e0b\u8f7d","text":"<p>\u200b\u5982\u679c\u200b\u662f\u200b\u5b66\u751f\u200b\u7684\u8bdd\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b66\u751f\u200b\u90ae\u7bb1\u200b\u6ce8\u518c\u200b\u5e76\u200b\u4e0b\u8f7d\u200bProfessional\u200b\u7248\u672c\u200b\uff0cCommunity\u200b\u7248\u672c\u200b\u4e5f\u200b\u57fa\u672c\u200b\u80fd\u200b\u6ee1\u8db3\u200b\u6211\u4eec\u200b\u7684\u200b\u65e5\u5e38\u200b\u9700\u6c42\u200b\u3002</p> <p></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.2%20PyTorch%E7%9A%84%E5%AE%89%E8%A3%85/#step-2_1","title":"Step 2\uff1a\u200b\u914d\u7f6e\u200b\u73af\u5883","text":"<p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u865a\u62df\u73af\u5883\u200b\u8bbe\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u7f16\u8bd1\u5668\u200b\uff0c\u200b\u5177\u4f53\u64cd\u4f5c\u200b\uff1aFile --&gt; Settings --&gt; Project:\u200b\u4f60\u200b\u7684\u200b\u9879\u76ee\u540d\u79f0\u200b--&gt; Python Interpreter</p> <p>\u200b\u8fdb\u53bb\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u89c1\u200b\u4ed6\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u9ed8\u8ba4\u200b\u7684\u200bbase\u200b\u73af\u5883\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u6210\u200b\u6211\u4eec\u200b\u7684\u200b<code>test</code>\u200b\u73af\u5883\u200b,\u200b\u70b9\u51fb\u200b<code>\u200b\u9f7f\u8f6e\u200b</code>\uff0c\u200b\u9009\u62e9\u200b<code>Add</code></p> <p></p> <p>\u200b\u70b9\u51fb\u200b<code>Conda Environment</code> \uff0c\u200b\u9009\u62e9\u200b<code>Existing environment</code>\uff0c\u200b\u5c06\u200b<code>Interpreter</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200btest\u200b\u73af\u5883\u200b\u4e0b\u200b\u7684\u200b<code>python.exe</code></p> <p></p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u5982\u679c\u200b\u5728\u200bpycharm\u200b\u7684\u200b\u73af\u5883\u200b\u65f6\u200b\uff0c\u200b\u60f3\u200b\u8fdb\u5165\u200b\u6211\u4eec\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff0c\u200b\u8981\u200b\u4f7f\u7528\u200b<code>conda activate \u200b\u540d\u79f0\u200b</code></p>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.3%20PyTorch%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90/","title":"1.3 PyTorch\u200b\u76f8\u5173\u200b\u8d44\u6e90","text":"<p>PyTorch\u200b\u4e4b\u6240\u4ee5\u200b\u88ab\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u4eba\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4e0d\u4ec5\u200b\u5728\u4e8e\u200b\u5176\u200b\u5b8c\u5907\u200b\u7684\u200b\u6559\u7a0b\u200b\uff0c\u200b\u8fd8\u200b\u53d7\u76ca\u200b\u4e8e\u200b\u8bb8\u591a\u200b\u76f8\u5173\u200b\u7684\u200b\u8d44\u6e90\u200b\u548c\u200b\u5b8c\u5584\u200b\u7684\u200b\u8bba\u575b\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u7684\u200b\u4f18\u8d28\u200b\u5b66\u4e60\u200b\u8d44\u6e90\u200b</li> </ul>"},{"location":"01-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/1.3%20PyTorch%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90/#131-pytorch","title":"1.3.1 PyTorch\u200b\u5b66\u4e60\u200b\u8d44\u6e90","text":"<ol> <li>Awesome-pytorch-list\uff1a\u200b\u76ee\u524d\u200b\u5df2\u83b7\u200b12K Star\uff0c\u200b\u5305\u542b\u200b\u4e86\u200bNLP,CV,\u200b\u5e38\u89c1\u200b\u5e93\u200b\uff0c\u200b\u8bba\u6587\u200b\u5b9e\u73b0\u200b\u4ee5\u53ca\u200bPytorch\u200b\u7684\u200b\u5176\u4ed6\u200b\u9879\u76ee\u200b\u3002</li> <li>PyTorch\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\uff1a\u200b\u5b98\u65b9\u200b\u53d1\u5e03\u200b\u7684\u200b\u6587\u6863\u200b\uff0c\u200b\u5341\u5206\u200b\u4e30\u5bcc\u200b\u3002</li> <li>Pytorch-handbook\uff1aGitHub\u200b\u4e0a\u200b\u5df2\u7ecf\u200b\u6536\u83b7\u200b14.8K\uff0cpytorch\u200b\u624b\u4e2d\u200b\u4e66\u200b\u3002</li> <li>PyTorch\u200b\u5b98\u65b9\u200b\u793e\u533a\u200b\uff1aPyTorch\u200b\u62e5\u6709\u200b\u4e00\u4e2a\u200b\u6d3b\u8dc3\u200b\u7684\u200b\u793e\u533a\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u548c\u200b\u5f00\u53d1\u200bpytorch\u200b\u7684\u200b\u4eba\u4eec\u200b\u8fdb\u884c\u200b\u4ea4\u6d41\u200b\u3002</li> <li>PyTorch\u200b\u5b98\u65b9\u200btutorials\uff1a\u200b\u5b98\u65b9\u200b\u7f16\u5199\u200b\u7684\u200btutorials\uff0c\u200b\u53ef\u4ee5\u200b\u7ed3\u5408\u200bcolab\u200b\u8fb9\u200b\u52a8\u624b\u200b\u8fb9\u200b\u5b66\u4e60\u200b</li> <li>\u200b\u52a8\u624b\u200b\u5b66\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff1a\u200b\u52a8\u624b\u200b\u5b66\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u662f\u200b\u7531\u200b\u674e\u200b\u6c90\u200b\u8001\u5e08\u200b\u4e3b\u8bb2\u200b\u7684\u200b\u4e00\u95e8\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5165\u95e8\u200b\u8bfe\u200b\uff0c\u200b\u62e5\u6709\u200b\u6210\u719f\u200b\u7684\u200b\u4e66\u7c4d\u200b\u8d44\u6e90\u200b\u548c\u200b\u8bfe\u7a0b\u200b\u8d44\u6e90\u200b\uff0c\u200b\u5728\u200bB\u200b\u7ad9\u200b\uff0cYoutube\u200b\u5747\u200b\u6709\u200b\u56de\u653e\u200b\u3002</li> <li>Awesome-PyTorch-Chinese\uff1a\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e2d\u6587\u200b\u4f18\u8d28\u200bPyTorch\u200b\u8d44\u6e90\u200b</li> <li>labml.ai Deep Learning Paper Implementations\uff1a\u200b\u624b\u628a\u624b\u200b\u5b9e\u73b0\u200b\u7ecf\u5178\u200b\u7f51\u7edc\u200b\u4ee3\u7801\u200b</li> <li>YSDA course in Natural Language Processing:YSDA course in Natural Language Processing</li> <li>huggingface:hugging face</li> <li>ModelScope: \u200b\u9b54\u200b\u642d\u200b\u793e\u533a\u200b</li> </ol> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u5f88\u591a\u200b\u5b66\u4e60\u200bpytorch\u200b\u7684\u200b\u8d44\u6e90\u200b\u5728\u200bb\u200b\u7ad9\u200b\uff0cstackoverflow\uff0c\u200b\u77e5\u4e4e\u200b......\u200b\u672a\u6765\u200b\u5927\u5bb6\u200b\u8fd8\u200b\u9700\u8981\u200b\u591a\u591a\u200b\u63a2\u7d22\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u5728\u200b\u5b9e\u6218\u200b\u4e2d\u200b\u4e0d\u65ad\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4e0d\u65ad\u200b\u7ed9\u4e88\u200b\u6211\u4eec\u200b\u8bfe\u7a0b\u200b\u53cd\u9988\u200b\u3002</p> <p>\u200b\u4ee5\u4e0a\u200b\u4fbf\u662f\u200b\u7b2c\u4e00\u7ae0\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e86\u200b\uff0c\u200b\u80fd\u529b\u200b\u6709\u9650\u200b\uff0c\u200b\u5e0c\u671b\u200b\u5404\u4f4d\u200b\u4e00\u5b9a\u200b\u8981\u200b\u5e26\u200b\u7740\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\u53bb\u200b\u601d\u8003\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u5e0c\u671b\u200b\u5404\u4f4d\u200b\u80fd\u200b\u6307\u51fa\u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u4e0d\u65ad\u6539\u8fdb\u200b\u5185\u5bb9\u200b\uff0c\u200b\u7ed9\u200b\u5927\u5bb6\u200b\u5448\u73b0\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u6559\u7a0b\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.1%20%E5%BC%A0%E9%87%8F/","title":"2.1 \u200b\u5f20\u91cf","text":"<p>\u200b\u4ece\u200b\u672c\u7ae0\u200b\u5f00\u59cb\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f00\u59cb\u200b\u4ecb\u7ecd\u200bPyTorch\u200b\u57fa\u7840\u77e5\u8bc6\u200b\uff0c\u200b\u672c\u7ae0\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u5efa\u7acb\u200b\u8d77\u200b\u5bf9\u200b\u6570\u636e\u200b\u7684\u200b\u63cf\u8ff0\u200b\uff0c\u200b\u968f\u540e\u200b\u6211\u4eec\u200b\u518d\u200b\u4ecb\u7ecd\u200b\u5f20\u91cf\u200b\u7684\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u6700\u540e\u200b\u518d\u200b\u8bb2\u200bPyTorch\u200b\u4e2d\u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6838\u5fc3\u200b\u5305\u200b <code>autograd</code>\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u81ea\u52a8\u200b\u5fae\u5206\u200b\uff0c\u200b\u4e86\u89e3\u200b\u5b8c\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8f83\u200b\u597d\u200b\u5730\u200b\u7406\u89e3\u200bPyTorch\u200b\u4ee3\u7801\u200b\u4e86\u200b\u3002\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u5c06\u200b\u6570\u636e\u200b\u4ee5\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u5f0f\u200b\u8fdb\u884c\u200b\u8868\u793a\u200b\uff0c\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u7528\u200b\u4e09\u7ef4\u200b\u5f20\u91cf\u200b\u8868\u793a\u200b\u4e00\u4e2a\u200bRGB\u200b\u56fe\u50cf\u200b\uff0c\u200b\u56db\u7ef4\u5f20\u91cf\u200b\u8868\u793a\u200b\u89c6\u9891\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5f20\u91cf\u200b\u7684\u200b\u7b80\u4ecb\u200b</li> <li>PyTorch\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b</li> <li>PyTorch\u200b\u4e2d\u200b\u5f20\u91cf\u200b\u7684\u200b\u64cd\u4f5c\u200b</li> <li>PyTorch\u200b\u4e2d\u200b\u5f20\u91cf\u200b\u7684\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b</li> </ul>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.1%20%E5%BC%A0%E9%87%8F/#211","title":"2.1.1 \u200b\u7b80\u4ecb","text":"<p>\u200b\u51e0\u4f55\u200b\u4ee3\u6570\u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u7684\u200b\u5f20\u91cf\u200b\u662f\u200b\u57fa\u4e8e\u200b\u5411\u91cf\u200b\u548c\u200b\u77e9\u9635\u200b\u7684\u200b\u63a8\u5e7f\u200b\uff0c\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6807\u91cf\u200b\u89c6\u4e3a\u200b\u96f6\u9636\u200b\u5f20\u91cf\u200b\uff0c\u200b\u77e2\u91cf\u200b\u53ef\u4ee5\u200b\u89c6\u4e3a\u200b\u4e00\u9636\u200b\u5f20\u91cf\u200b\uff0c\u200b\u77e9\u9635\u200b\u5c31\u662f\u200b\u4e8c\u9636\u200b\u5f20\u91cf\u200b\u3002 |\u200b\u5f20\u91cf\u200b\u7ef4\u5ea6\u200b|\u200b\u4ee3\u8868\u200b\u542b\u4e49\u200b| |---|---| | 0\u200b\u7ef4\u200b\u5f20\u91cf\u200b | \u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u6807\u91cf\u200b\uff08\u200b\u6570\u5b57\u200b\uff09 | | 1\u200b\u7ef4\u200b\u5f20\u91cf\u200b |  \u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u5411\u91cf\u200b| | 2\u200b\u7ef4\u200b\u5f20\u91cf\u200b |\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u77e9\u9635\u200b| |3\u200b\u7ef4\u200b\u5f20\u91cf\u200b |\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u6570\u636e\u200b \u200b\u80a1\u4ef7\u200b \u200b\u6587\u672c\u200b\u6570\u636e\u200b \u200b\u5355\u5f20\u200b\u5f69\u8272\u56fe\u7247\u200b(RGB)|</p> <p>\u200b\u5f20\u91cf\u200b\u662f\u200b\u73b0\u4ee3\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u7840\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u6838\u5fc3\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u5bb9\u5668\u200b\uff0c\u200b\u591a\u6570\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5b83\u200b\u5305\u542b\u200b\u6570\u5b57\u200b\uff0c\u200b\u6709\u65f6\u5019\u200b\u5b83\u200b\u4e5f\u200b\u5305\u542b\u200b\u5b57\u7b26\u4e32\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u6bd4\u8f83\u200b\u5c11\u200b\u3002\u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u628a\u200b\u5b83\u200b\u60f3\u8c61\u200b\u6210\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u7684\u200b\u6c34\u6876\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e00\u4e9b\u200b\u5b58\u50a8\u200b\u5728\u200b\u5404\u79cd\u7c7b\u578b\u200b\u5f20\u91cf\u200b\u7684\u200b\u516c\u7528\u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u578b\u200b\uff1a</p> <ul> <li>3\u200b\u7ef4\u200b = \u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b</li> <li>4\u200b\u7ef4\u200b = \u200b\u56fe\u50cf\u200b</li> <li>5\u200b\u7ef4\u200b = \u200b\u89c6\u9891\u200b</li> </ul> <p>\u200b\u4f8b\u5b50\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e09\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\u8868\u793a\u200b\uff1a</p> <pre><code>(width, height, channel) = 3D\n</code></pre> <p>\u200b\u4f46\u662f\u200b\uff0c\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u8981\u200b\u5904\u7406\u200b\u4e0d\u6b62\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u6216\u200b\u4e00\u7bc7\u200b\u6587\u6863\u200b\u2014\u2014\u200b\u6211\u4eec\u200b\u8981\u200b\u5904\u7406\u200b\u4e00\u4e2a\u200b\u96c6\u5408\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u6709\u200b10,000\u200b\u5f20\u200b\u90c1\u91d1\u9999\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7528\u5230\u200b4D\u200b\u5f20\u91cf\u200b\uff1a</p> <pre><code>(batch_size, width, height, channel) = 4D\n</code></pre> <p>\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\uff0c <code>torch.Tensor</code> \u200b\u662f\u200b\u5b58\u50a8\u200b\u548c\u200b\u53d8\u6362\u200b\u6570\u636e\u200b\u7684\u200b\u4e3b\u8981\u200b\u5de5\u5177\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u4e4b\u524d\u200b\u7528\u8fc7\u200b<code>NumPy</code>\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b <code>Tensor</code> \u200b\u548c\u200bNumPy\u200b\u7684\u200b\u591a\u7ef4\u200b\u6570\u7ec4\u200b\u975e\u5e38\u200b\u7c7b\u4f3c\u200b\u3002\u200b\u7136\u800c\u200b\uff0c<code>Tensor</code> \u200b\u63d0\u4f9b\u200bGPU\u200b\u8ba1\u7b97\u200b\u548c\u200b\u81ea\u52a8\u200b\u6c42\u200b\u68af\u5ea6\u200b\u7b49\u200b\uf901\u200b\u591a\u529f\u80fd\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u4f7f\u200b <code>Tensor</code> \u200b\u8fd9\u4e00\u200b\u6570\u636e\u7c7b\u578b\u200b\u66f4\u52a0\u200b\u9002\u5408\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.1%20%E5%BC%A0%E9%87%8F/#212-tensor","title":"2.1.2 \u200b\u521b\u5efa\u200btensor","text":"<p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u51e0\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u521b\u5efa\u200b<code>tensor</code>\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <ol> <li>\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u77e9\u9635\u200b \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>torch.rand()</code>\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6784\u9020\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u77e9\u9635\u200b\uff1a</li> </ol> <p><pre><code>import torch\nx = torch.rand(4, 3) \nprint(x)\n</code></pre> <pre><code>tensor([[0.7569, 0.4281, 0.4722],\n        [0.9513, 0.5168, 0.1659],\n        [0.4493, 0.2846, 0.4363],\n        [0.5043, 0.9637, 0.1469]])\n</code></pre> 2. \u200b\u5168\u200b0\u200b\u77e9\u9635\u200b\u7684\u200b\u6784\u5efa\u200b \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>torch.zeros()</code>\u200b\u6784\u9020\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b\u5168\u4e3a\u200b 0\uff0c\u200b\u5e76\u4e14\u200b\u901a\u8fc7\u200b<code>dtype</code>\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e3a\u200b long\u3002\u200b\u9664\u6b64\u4ee5\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200btorch.zero_()\u200b\u548c\u200btorch.zeros_like()\u200b\u5c06\u200b\u73b0\u6709\u200b\u77e9\u9635\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5168\u200b0\u200b\u77e9\u9635\u200b.</p> <p><pre><code>import torch\nx = torch.zeros(4, 3, dtype=torch.long)\nprint(x)\n</code></pre> <pre><code>tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n</code></pre> 3. \u200b\u5f20\u91cf\u200b\u7684\u200b\u6784\u5efa\u200b  \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>torch.tensor()</code>\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\uff0c\u200b\u6784\u9020\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\uff1a <pre><code>import torch\nx = torch.tensor([5.5, 3]) \nprint(x)\n</code></pre> <pre><code>tensor([5.5000, 3.0000])\n</code></pre> 4. \u200b\u57fa\u4e8e\u200b\u5df2\u7ecf\u200b\u5b58\u5728\u200b\u7684\u200b tensor\uff0c\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b tensor \uff1a</p> <p><pre><code>x = x.new_ones(4, 3, dtype=torch.double) \n# \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u5168\u200b1\u200b\u77e9\u9635\u200btensor\uff0c\u200b\u8fd4\u56de\u200b\u7684\u200btensor\u200b\u9ed8\u8ba4\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200btorch.dtype\u200b\u548c\u200btorch.device\n# \u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u4e4b\u524d\u200b\u7684\u200b\u5199\u6cd5\u200b x = torch.ones(4, 3, dtype=torch.double)\nprint(x)\nx = torch.randn_like(x, dtype=torch.float)\n# \u200b\u91cd\u7f6e\u200b\u6570\u636e\u7c7b\u578b\u200b\nprint(x)\n# \u200b\u7ed3\u679c\u200b\u4f1a\u200b\u6709\u200b\u4e00\u6837\u200b\u7684\u200bsize\n# \u200b\u83b7\u53d6\u200b\u5b83\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4fe1\u606f\u200b\nprint(x.size())\nprint(x.shape)\n</code></pre> <pre><code>tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[ 2.7311, -0.0720,  0.2497],\n        [-2.3141,  0.0666, -0.5934],\n        [ 1.5253,  1.0336,  1.3859],\n        [ 1.3806, -0.6965, -1.2255]])\ntorch.Size([4, 3])\ntorch.Size([4, 3])\n</code></pre> \u200b\u8fd4\u56de\u200b\u7684\u200btorch.Size\u200b\u5176\u5b9e\u200b\u662f\u200b\u4e00\u4e2a\u200btuple\uff0c\u200b\u2f40\u200b\u6301\u200b\u6240\u6709\u200btuple\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u7d22\u5f15\u200b\u64cd\u4f5c\u200b\u53d6\u5f97\u200b\u5f20\u91cf\u200b\u7684\u200b\u957f\u200b\u3001\u200b\u5bbd\u200b\u7b49\u200b\u6570\u636e\u200b\u7ef4\u5ea6\u200b\u3002</p> <ol> <li>\u200b\u5e38\u89c1\u200b\u7684\u200b\u6784\u9020\u200bTensor\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1a</li> </ol> \u200b\u51fd\u6570\u200b \u200b\u529f\u80fd\u200b Tensor(sizes) \u200b\u57fa\u7840\u200b\u6784\u9020\u51fd\u6570\u200b tensor(data) \u200b\u7c7b\u4f3c\u200b\u4e8e\u200bnp.array ones(sizes) \u200b\u5168\u200b1 zeros(sizes) \u200b\u5168\u200b0 eye(sizes) \u200b\u5bf9\u89d2\u200b\u4e3a\u200b1\uff0c\u200b\u5176\u4f59\u200b\u4e3a\u200b0 arange(s,e,step) \u200b\u4ece\u200bs\u200b\u5230\u200be\uff0c\u200b\u6b65\u957f\u200b\u4e3a\u200bstep linspace(s,e,steps) \u200b\u4ece\u200bs\u200b\u5230\u200be\uff0c\u200b\u5747\u5300\u200b\u5206\u6210\u200bstep\u200b\u4efd\u200b rand/randn(sizes) rand\u200b\u662f\u200b[0,1)\u200b\u5747\u5300\u5206\u5e03\u200b\uff1brandn\u200b\u662f\u200b\u670d\u4ece\u200bN(0\uff0c1)\u200b\u7684\u200b\u6b63\u6001\u5206\u5e03\u200b normal(mean,std) \u200b\u6b63\u6001\u5206\u5e03\u200b(\u200b\u5747\u503c\u200b\u4e3a\u200bmean\uff0c\u200b\u6807\u51c6\u5dee\u200b\u662f\u200bstd) randperm(m) \u200b\u968f\u673a\u200b\u6392\u5217"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.1%20%E5%BC%A0%E9%87%8F/#213","title":"2.1.3 \u200b\u5f20\u91cf\u200b\u7684\u200b\u64cd\u4f5c","text":"<p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u51e0\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u5f20\u91cf\u200b\u7684\u200b\u64cd\u4f5c\u65b9\u6cd5\u200b\uff1a 1. \u200b\u52a0\u6cd5\u200b\u64cd\u4f5c\u200b\uff1a <pre><code>import torch\n# \u200b\u65b9\u5f0f\u200b1\ny = torch.rand(4, 3) \nprint(x + y)\n\n# \u200b\u65b9\u5f0f\u200b2\nprint(torch.add(x, y))\n\n# \u200b\u65b9\u5f0f\u200b3 in-place\uff0c\u200b\u539f\u503c\u200b\u4fee\u6539\u200b\ny.add_(x) \nprint(y)\n</code></pre> <pre><code>tensor([[ 2.8977,  0.6581,  0.5856],\n        [-1.3604,  0.1656, -0.0823],\n        [ 2.1387,  1.7959,  1.5275],\n        [ 2.2427, -0.3100, -0.4826]])\ntensor([[ 2.8977,  0.6581,  0.5856],\n        [-1.3604,  0.1656, -0.0823],\n        [ 2.1387,  1.7959,  1.5275],\n        [ 2.2427, -0.3100, -0.4826]])\ntensor([[ 2.8977,  0.6581,  0.5856],\n        [-1.3604,  0.1656, -0.0823],\n        [ 2.1387,  1.7959,  1.5275],\n        [ 2.2427, -0.3100, -0.4826]])\n</code></pre> 2. \u200b\u7d22\u5f15\u200b\u64cd\u4f5c\u200b\uff1a(\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bnumpy)</p> <p>\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff1a\u200b\u7d22\u5f15\u200b\u51fa\u6765\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u539f\u200b\u6570\u636e\u200b\u5171\u4eab\u5185\u5b58\u200b\uff0c\u200b\u4fee\u6539\u200b\u4e00\u4e2a\u200b\uff0c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u4f1a\u200b\u8ddf\u7740\u200b\u4fee\u6539\u200b\u3002\u200b\u5982\u679c\u200b\u4e0d\u60f3\u200b\u4fee\u6539\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u4f7f\u7528\u200bcopy()\u200b\u7b49\u200b\u65b9\u6cd5\u200b</p> <p><pre><code>import torch\nx = torch.rand(4,3)\n# \u200b\u53d6\u200b\u7b2c\u4e8c\u5217\u200b\nprint(x[:, 1]) \n</code></pre> <pre><code>tensor([-0.0720,  0.0666,  1.0336, -0.6965])\n</code></pre></p> <p><pre><code>y = x[0,:]\ny += 1\nprint(y)\nprint(x[0, :]) # \u200b\u6e90\u200btensor\u200b\u4e5f\u200b\u88ab\u200b\u6539\u200b\u4e86\u200b\u4e86\u200b\n</code></pre> <pre><code>tensor([3.7311, 0.9280, 1.2497])\ntensor([3.7311, 0.9280, 1.2497])\n</code></pre> 3. \u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b \u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b\u5e38\u89c1\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6709\u200b<code>torch.view()</code>\u200b\u548c\u200b<code>torch.reshape()</code>\uff0c\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u7b2c\u4e00\u200b\u4e2d\u200b\u65b9\u6cd5\u200b<code>torch.view()</code>\uff1a</p> <p><pre><code>x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8) # -1\u200b\u662f\u200b\u6307\u200b\u8fd9\u200b\u4e00\u7ef4\u200b\u7684\u200b\u7ef4\u6570\u200b\u7531\u200b\u5176\u4ed6\u200b\u7ef4\u5ea6\u200b\u51b3\u5b9a\u200b\nprint(x.size(), y.size(), z.size())\n</code></pre> <pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n</code></pre> \u200b\u6ce8\u200b: <code>torch.view()</code> \u200b\u8fd4\u56de\u200b\u7684\u200b\u65b0\u200b<code>tensor</code>\u200b\u4e0e\u6e90\u200b<code>tensor</code>\u200b\u5171\u4eab\u5185\u5b58\u200b(\u200b\u5176\u5b9e\u200b\u662f\u200b\u540c\u4e00\u4e2a\u200b<code>tensor</code>)\uff0c\u200b\uf901\u200b\u6539\u200b\u5176\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\uff0c\u200b\u53e6\u5916\u200b\u4e00\u4e2a\u200b\u4e5f\u200b\u4f1a\u200b\u8ddf\u7740\u200b\u6539\u53d8\u200b\u3002(\u200b\u987e\u540d\u601d\u4e49\u200b\uff0cview()\u200b\u4ec5\u4ec5\u200b\u662f\u200b\u6539\u53d8\u200b\uf9ba\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u5f20\u200b\uf97e\u200b\u7684\u200b\u89c2\u5bdf\u200b\u89d2\u5ea6\u200b)</p> <p><pre><code>x += 1\nprint(x)\nprint(y) # \u200b\u4e5f\u200b\u52a0\u200b\u4e86\u200b\u4e86\u200b1\n</code></pre> <pre><code>tensor([[ 1.3019,  0.3762,  1.2397,  1.3998],\n        [ 0.6891,  1.3651,  1.1891, -0.6744],\n        [ 0.3490,  1.8377,  1.6456,  0.8403],\n        [-0.8259,  2.5454,  1.2474,  0.7884]])\ntensor([ 1.3019,  0.3762,  1.2397,  1.3998,  0.6891,  1.3651,  1.1891, -0.6744,\n         0.3490,  1.8377,  1.6456,  0.8403, -0.8259,  2.5454,  1.2474,  0.7884])\n</code></pre> \u200b\u4e0a\u9762\u200b\u6211\u4eec\u200b\u8bf4\u200b\u8fc7\u200btorch.view()\u200b\u4f1a\u200b\u6539\u53d8\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4f46\u662f\u200b\u5f88\u591a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u548c\u200b\u53d8\u6362\u200b\u540e\u200b\u7684\u200b\u5f20\u91cf\u200b\u4e92\u76f8\u200b\u4e0d\u200b\u5f71\u54cd\u200b\u3002\u200b\u4e3a\u200b\u4e3a\u4e86\u200b\u4f7f\u200b\u521b\u5efa\u200b\u7684\u200b\u5f20\u91cf\u200b\u548c\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\uf967\u200b\u5171\u4eab\u5185\u5b58\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u7b2c\u4e8c\u79cd\u200b\u65b9\u6cd5\u200b<code>torch.reshape()</code>\uff0c \u200b\u540c\u6837\u200b\u53ef\u4ee5\u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4f46\u662f\u200b\u6b64\u200b\u51fd\u6570\u200b\u5e76\u200b\u4e0d\u80fd\u200b\u4fdd\u8bc1\u200b\u8fd4\u56de\u200b\u7684\u200b\u662f\u200b\u5176\u200b\u62f7\u8d1d\u200b\u503c\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5b98\u65b9\u200b\uf967\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u3002\u200b\u63a8\u8350\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u6211\u4eec\u200b\u5148\u200b\u7528\u200b <code>clone()</code> \u200b\u521b\u9020\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u526f\u672c\u200b\u7136\u540e\u200b\u518d\u200b\u4f7f\u7528\u200b <code>torch.view()</code>\u200b\u8fdb\u884c\u200b\u51fd\u6570\u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b \u3002</p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u4f7f\u7528\u200b <code>clone()</code> \u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u597d\u5904\u200b\u662f\u200b\u4f1a\u200b\u88ab\u200b\u8bb0\u5f55\u200b\u5728\u200b\u8ba1\u7b97\u200b\u56fe\u4e2d\u200b\uff0c\u200b\u5373\u200b\u68af\u5ea6\u200b\u56de\u200b\u4f20\u5230\u200b\u526f\u672c\u200b\u65f6\u200b\u4e5f\u200b\u4f1a\u200b\u4f20\u5230\u200b\u6e90\u200b Tensor \u3002 3. \u200b\u53d6\u503c\u200b\u64cd\u4f5c\u200b \u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5143\u7d20\u200b <code>tensor</code> \uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>.item()</code> \u200b\u6765\u200b\u83b7\u5f97\u200b\u8fd9\u4e2a\u200b <code>value</code>\uff0c\u200b\u800c\u200b\u4e0d\u200b\u83b7\u5f97\u200b\u5176\u4ed6\u200b\u6027\u8d28\u200b\uff1a</p> <p><pre><code>import torch\nx = torch.randn(1) \nprint(type(x)) \nprint(type(x.item()))\n</code></pre> <pre><code>&lt;class 'torch.Tensor'&gt;\n&lt;class 'float'&gt;\n</code></pre> PyTorch\u200b\u4e2d\u200b\u7684\u200b Tensor \u200b\u652f\u6301\u200b\u8d85\u8fc7\u200b\u4e00\u767e\u79cd\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5305\u62ec\u200b\u8f6c\u7f6e\u200b\u3001\u200b\u7d22\u5f15\u200b\u3001\u200b\u5207\u7247\u200b\u3001\u200b\u6570\u5b66\u200b\u8fd0\u7b97\u200b\u3001\u200b\u7ebf\u6027\u4ee3\u6570\u200b\u3001\u200b\u968f\u673a\u6570\u200b\u7b49\u7b49\u200b\uff0c\u200b\u5177\u4f53\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u53ef\u200b\u53c2\u8003\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.1%20%E5%BC%A0%E9%87%8F/#214","title":"2.1.4 \u200b\u5e7f\u64ad\u200b\u673a\u5236","text":"<p>\u200b\u5f53\u5bf9\u200b\u4e24\u4e2a\u200b\u5f62\u72b6\u200b\uf967\u200b\u540c\u200b\u7684\u200b Tensor \u200b\u6309\u200b\u5143\u7d20\u200b\u8fd0\u7b97\u200b\u65f6\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u89e6\u53d1\u200b\u5e7f\u64ad\u200b(broadcasting)\u200b\u673a\u5236\u200b\uff1a\u200b\u5148\u200b\u9002\u5f53\u200b\u590d\u5236\u200b\u5143\u7d20\u200b\u4f7f\u200b\u8fd9\u200b\u4e24\u4e2a\u200b Tensor \u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\u540e\u200b\u518d\u200b\u6309\u200b\u5143\u7d20\u200b\u8fd0\u7b97\u200b\u3002</p> <p><pre><code>x = torch.arange(1, 3).view(1, 2)\nprint(x)\ny = torch.arange(1, 4).view(3, 1)\nprint(y)\nprint(x + y)\n</code></pre> <pre><code>tensor([[1, 2]])\ntensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\n</code></pre></p> <p>\u200b\u7531\u4e8e\u200bx\u200b\u548c\u200by\u200b\u5206\u522b\u200b\u662f\u200b1\u200b\ufa08\u200b2\u200b\u5217\u200b\u548c\u200b3\u200b\ufa08\u200b1\u200b\u5217\u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u5982\u679c\u200b\u8981\u200b\u8ba1\u7b97\u200bx+y\uff0c\u200b\u90a3\u4e48\u200bx\u200b\u4e2d\u200b\u7b2c\u4e00\u200b\ufa08\u200b\u7684\u200b2\u200b\u4e2a\u200b\u5143\u7d20\u200b\u88ab\u200b\u5e7f\u64ad\u200b (\u200b\u590d\u5236\u200b)\u200b\u5230\u200b\uf9ba\u200b\u7b2c\u4e8c\u200b\ufa08\u200b\u548c\u200b\u7b2c\u4e09\u200b\ufa08\u200b\uff0c\u200b\u2f7d\u200by\u200b\u4e2d\u200b\u7b2c\u200b\u2f00\u200b\uf99c\u200b\u7684\u200b3\u200b\u4e2a\u200b\u5143\u7d20\u200b\u88ab\u200b\u5e7f\u64ad\u200b(\u200b\u590d\u5236\u200b)\u200b\u5230\u200b\u4e86\u200b\u7b2c\u4e8c\u200b\uf99c\u200b\u3002\u200b\u5982\u6b64\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5bf9\u200b2\u200b\u4e2a\u200b3\u200b\ufa08\u200b2\u200b\uf99c\u200b\u7684\u200b\u77e9\u9635\u200b\u6309\u200b\u5143\u7d20\u200b\u76f8\u52a0\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.2%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/","title":"2.2 \u200b\u81ea\u52a8\u200b\u6c42\u5bfc","text":"<p>PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6838\u5fc3\u200b\u662f\u200b <code>autograd</code>\u200b\u5305\u200b\u3002autograd\u200b\u5305\u4e3a\u200b\u5f20\u91cf\u200b\u4e0a\u200b\u7684\u200b\u6240\u6709\u200b\u64cd\u4f5c\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u673a\u5236\u200b\u3002\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5728\u200b\u8fd0\u884c\u200b\u65f6\u200b\u5b9a\u4e49\u200b ( define-by-run \uff09\u200b\u7684\u200b\u6846\u67b6\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u662f\u200b\u6839\u636e\u200b\u4ee3\u7801\u200b\u5982\u4f55\u200b\u8fd0\u884c\u200b\u6765\u200b\u51b3\u5b9a\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6bcf\u6b21\u200b\u8fed\u4ee3\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>autograd\u200b\u7684\u200b\u6c42\u5bfc\u200b\u673a\u5236\u200b</li> <li>\u200b\u68af\u5ea6\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b</li> </ul>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.2%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/#autograd","title":"Autograd\u200b\u7b80\u4ecb","text":"<p><code>torch.Tensor</code>\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u5305\u200b\u7684\u200b\u6838\u5fc3\u200b\u7c7b\u200b\u3002\u200b\u5982\u679c\u200b\u8bbe\u7f6e\u200b\u5b83\u200b\u7684\u200b\u5c5e\u6027\u200b<code>.requires_grad</code> \u200b\u4e3a\u200b <code>True</code>\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u5c06\u200b\u4f1a\u200b\u8ffd\u8e2a\u200b\u5bf9\u4e8e\u200b\u8be5\u200b\u5f20\u91cf\u200b\u7684\u200b\u6240\u6709\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5f53\u200b\u5b8c\u6210\u200b\u8ba1\u7b97\u200b\u540e\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u7528\u200b<code>.backward()</code>\uff0c\u200b\u6765\u200b\u81ea\u52a8\u200b\u8ba1\u7b97\u200b\u6240\u6709\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u7684\u200b\u6240\u6709\u200b\u68af\u5ea6\u200b\u5c06\u4f1a\u200b\u81ea\u52a8\u200b\u7d2f\u52a0\u200b\u5230\u200b<code>.grad</code>\u200b\u5c5e\u6027\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5728\u200b y.backward() \u200b\u65f6\u200b\uff0c\u200b\u5982\u679c\u200b y \u200b\u662f\u200b\u6807\u200b\uf97e\u200b\uff0c\u200b\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\u4e3a\u200b backward() \u200b\u4f20\u5165\u200b\u4efb\u4f55\u200b\u53c2\u6570\u200b\uff1b\u200b\u5426\u5219\u200b\uff0c\u200b\u9700\u8981\u200b\u4f20\u5165\u200b\u4e00\u4e2a\u200b\u4e0e\u200b y \u200b\u540c\u5f62\u200b\u7684\u200bTensor\u3002</p> <p>\u200b\u8981\u200b\u963b\u6b62\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u88ab\u200b\u8ddf\u8e2a\u200b\u5386\u53f2\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b<code>.detach()</code>\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u8ba1\u7b97\u200b\u5386\u53f2\u200b\u5206\u79bb\u200b\uff0c\u200b\u5e76\u200b\u963b\u6b62\u200b\u5b83\u200b\u672a\u6765\u200b\u7684\u200b\u8ba1\u7b97\u200b\u8bb0\u5f55\u200b\u88ab\u200b\u8ddf\u8e2a\u200b\u3002\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u8ddf\u8e2a\u200b\u5386\u53f2\u8bb0\u5f55\u200b(\u200b\u548c\u200b\u4f7f\u7528\u200b\u5185\u5b58\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4ee3\u7801\u200b\u5757\u200b\u5305\u88c5\u200b\u5728\u200b <code>with torch.no_grad():</code>\u200b\u4e2d\u200b\u3002\u200b\u5728\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u65f6\u200b\u7279\u522b\u200b\u6709\u7528\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u5177\u6709\u200b <code>requires_grad = True</code> \u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u662f\u200b\u6211\u4eec\u200b\u4e0d\u200b\u9700\u8981\u200b\u5728\u200b\u6b64\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u5bf9\u200b\u4ed6\u4eec\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u5bf9\u4e8e\u200b<code>autograd</code>\u200b\u7684\u200b\u5b9e\u73b0\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\uff1a<code>Function</code>\u3002<code>Tensor</code>\u200b\u548c\u200b<code>Function</code> \u200b\u4e92\u76f8\u200b\u8fde\u63a5\u200b\u751f\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65e0\u73af\u56fe\u200b (acyclic graph)\uff0c\u200b\u5b83\u200b\u7f16\u7801\u200b\u4e86\u200b\u5b8c\u6574\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5386\u53f2\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u5f20\u91cf\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b<code>.grad_fn</code>\u200b\u5c5e\u6027\u200b\uff0c\u200b\u8be5\u200b\u5c5e\u6027\u200b\u5f15\u7528\u200b\u4e86\u200b\u521b\u5efa\u200b <code>Tensor</code>\u200b\u81ea\u8eab\u200b\u7684\u200b<code>Function</code>(\u200b\u9664\u975e\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u662f\u200b\u7528\u6237\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\uff0c\u200b\u5373\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u7684\u200b<code>grad_fn</code>\u200b\u662f\u200b <code>None</code> )\u3002\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5f20\u91cf\u200b\u7531\u200b\u7528\u6237\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\uff0c\u200b\u56e0\u6b64\u200bgrad_fn\u200b\u8fd4\u56de\u200b\u7ed3\u679c\u200b\u662f\u200bNone\u3002</p> <p><pre><code>from __future__ import print_function\nimport torch\nx = torch.randn(3,3,requires_grad=True)\nprint(x.grad_fn)\n</code></pre> <pre><code>None\n</code></pre> \u200b\u5982\u679c\u200b\u9700\u8981\u200b\u8ba1\u7b97\u200b\u5bfc\u6570\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>Tensor</code> \u200b\u4e0a\u200b\u8c03\u7528\u200b <code>.backward()</code>\u3002\u200b\u5982\u679c\u200b<code>Tensor</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u6807\u91cf\u200b(\u200b\u5373\u200b\u5b83\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u5143\u7d20\u200b\u7684\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\u4e3a\u200b <code>backward()</code>\u200b\u6307\u5b9a\u200b\u4efb\u4f55\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u662f\u200b\u5982\u679c\u200b\u5b83\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5143\u7d20\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u6307\u5b9a\u200b\u4e00\u4e2a\u200b<code>gradient</code>\u200b\u53c2\u6570\u200b\uff0c\u200b\u8be5\u200b\u53c2\u6570\u200b\u662f\u200b\u5f62\u72b6\u200b\u5339\u914d\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u5e76\u200b\u8bbe\u7f6e\u200b<code>requires_grad=True</code>\u200b\u7528\u6765\u200b\u8ffd\u8e2a\u200b\u5176\u200b\u8ba1\u7b97\u200b\u5386\u53f2\u200b</p> <p><pre><code>x = torch.ones(2, 2, requires_grad=True)\nprint(x)\n</code></pre> <pre><code>tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n</code></pre></p> <p>\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u505a\u200b\u4e00\u6b21\u200b\u8fd0\u7b97\u200b\uff1a</p> <p><pre><code>y = x**2\nprint(y)\n</code></pre> <pre><code>tensor([[1., 1.],\n        [1., 1.]], grad_fn=&lt;PowBackward0&gt;)\n</code></pre></p> <p><code>y</code>\u200b\u662f\u200b\u8ba1\u7b97\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5b83\u200b\u6709\u200b<code>grad_fn</code>\u200b\u5c5e\u6027\u200b\u3002</p> <p><pre><code>print(y.grad_fn)\n</code></pre> <pre><code>&lt;PowBackward0 object at 0x000001CB45988C70&gt;\n</code></pre></p> <p>\u200b\u5bf9\u200b y \u200b\u8fdb\u884c\u200b\u66f4\u200b\u591a\u200b\u64cd\u4f5c\u200b</p> <p><pre><code>z = y * y * 3\nout = z.mean()\n\nprint(z, out)\n</code></pre> <pre><code>tensor([[3., 3.],\n        [3., 3.]], grad_fn=&lt;MulBackward0&gt;) tensor(3., grad_fn=&lt;MeanBackward0&gt;)\n</code></pre> <code>.requires_grad_(...)</code>\u200b\u539f\u5730\u200b\u6539\u53d8\u200b\u4e86\u200b\u73b0\u6709\u200b\u5f20\u91cf\u200b\u7684\u200b<code>requires_grad</code>\u200b\u6807\u5fd7\u200b\u3002\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\u6307\u5b9a\u200b\u7684\u8bdd\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u8f93\u5165\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u6807\u5fd7\u200b\u662f\u200b<code>False</code>\u3002</p> <p><pre><code>a = torch.randn(2, 2) # \u200b\u7f3a\u5931\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u9ed8\u8ba4\u200b requires_grad = False\na = ((a * 3) / (a - 1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb = (a * a).sum()\nprint(b.grad_fn)\n</code></pre> <pre><code>False\nTrue\n&lt;SumBackward0 object at 0x000001CB4A19FB50&gt;\n</code></pre></p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.2%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/#221","title":"2.2.1 \u200b\u68af\u5ea6","text":"<p>\u200b\u73b0\u5728\u200b\u5f00\u59cb\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u4e3a\u200b<code>out</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u6807\u91cf\u200b\uff0c\u200b\u56e0\u6b64\u200b<code>out.backward()</code>\u200b\u548c\u200b<code>out.backward(torch.tensor(1.))</code> \u200b\u7b49\u4ef7\u200b\u3002</p> <pre><code>out.backward()\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\u5bfc\u6570\u200b<code>d(out)/dx</code></p> <p><pre><code>print(x.grad)\n</code></pre> <pre><code>tensor([[3., 3.],\n        [3., 3.]])\n</code></pre> \u200b\u6570\u5b66\u200b\u4e0a\u200b\uff0c\u200b\u82e5\u6709\u200b\u5411\u91cf\u200b\u51fd\u6570\u200b\\(\\vec{y}=f(\\vec{x})\\)\uff0c\u200b\u90a3\u4e48\u200b \\(\\vec{y}\\) \u200b\u5173\u4e8e\u200b \\(\\vec{x}\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u96c5\u200b\u53ef\u6bd4\u200b\u77e9\u9635\u200b\uff1a $ J=\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{1}}{\\partial x_{n}} \\ \\vdots &amp; \\ddots &amp; \\vdots \\ \\frac{\\partial y_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right) $ \u200b\u800c\u200b <code>torch.autograd</code> \u200b\u8fd9\u4e2a\u200b\u5305\u200b\u5c31\u662f\u200b\u7528\u6765\u200b\u8ba1\u7b97\u200b\u4e00\u4e9b\u200b\u96c5\u200b\u53ef\u6bd4\u200b\u77e9\u9635\u200b\u7684\u200b\u4e58\u79ef\u200b\u7684\u200b\u3002\u200b\uf9b5\u200b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b \\(v\\) \u200b\u662f\u200b\u4e00\u4e2a\u200b\u6807\u200b\uf97e\u200b\u51fd\u6570\u200b \\(l = g(\\vec{y})\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\uff1a $ v=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} &amp; \\cdots &amp; \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) $ \u200b\u7531\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\uff1a $ v J=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial y_{1}} &amp; \\cdots &amp; \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{1}}{\\partial x_{n}} \\ \\vdots &amp; \\ddots &amp; \\vdots \\ \\frac{\\partial y_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)=\\left(\\begin{array}{lll}\\frac{\\partial l}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right) $</p> <p>\u200b\u6ce8\u610f\u200b\uff1agrad\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u662f\u200b\u7d2f\u52a0\u200b\u7684\u200b(accumulated)\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6bcf\u200b\u4e00\u6b21\u200b\u8fd0\u200b\ufa08\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u68af\u5ea6\u200b\u90fd\u200b\u4f1a\u200b\u7d2f\u52a0\u200b\u4e4b\u524d\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e00\u822c\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e4b\u524d\u200b\u9700\u200b\u628a\u200b\u68af\u5ea6\u200b\u6e05\u96f6\u200b\u3002</p> <p><pre><code># \u200b\u518d\u200b\u6765\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u2f00\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6ce8\u610f\u200bgrad\u200b\u662f\u200b\u7d2f\u52a0\u200b\u7684\u200b\nout2 = x.sum()\nout2.backward()\nprint(x.grad)\n\nout3 = x.sum()\nx.grad.data.zero_()\nout3.backward()\nprint(x.grad)\n</code></pre> <pre><code>tensor([[4., 4.],\n        [4., 4.]])\ntensor([[1., 1.],\n        [1., 1.]])\n</code></pre> \u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6765\u770b\u200b\u4e00\u4e2a\u200b\u96c5\u200b\u53ef\u6bd4\u200b\u5411\u91cf\u200b\u79ef\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff1a</p> <p><pre><code>x = torch.randn(3, requires_grad=True)\nprint(x)\n\ny = x * 2\ni = 0\nwhile y.data.norm() &lt; 1000:\n    y = y * 2\n    i = i + 1\nprint(y)\nprint(i)\n</code></pre> <pre><code>tensor([-0.9332,  1.9616,  0.1739], requires_grad=True)\ntensor([-477.7843, 1004.3264,   89.0424], grad_fn=&lt;MulBackward0&gt;)\n8\n</code></pre></p> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c<code>y</code>\u200b\u4e0d\u518d\u200b\u662f\u200b\u6807\u91cf\u200b\u3002<code>torch.autograd</code> \u200b\u4e0d\u80fd\u200b\u76f4\u63a5\u200b\u8ba1\u7b97\u200b\u5b8c\u6574\u200b\u7684\u200b\u96c5\u200b\u53ef\u6bd4\u200b\u77e9\u9635\u200b\uff0c\u200b\u4f46\u662f\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u53ea\u200b\u60f3\u8981\u200b\u96c5\u200b\u53ef\u6bd4\u200b\u5411\u91cf\u200b\u79ef\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u5411\u91cf\u200b\u4f5c\u4e3a\u200b\u53c2\u6570\u200b\u4f20\u7ed9\u200b <code>backward\uff1a</code></p> <p><pre><code>v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\ny.backward(v)\n\nprint(x.grad)\n</code></pre> <pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n</code></pre></p> <p>\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u4ee3\u7801\u200b\u5757\u200b\u5305\u88c5\u200b\u5728\u200b<code>with torch.no_grad():</code> \u200b\u4e2d\u200b\uff0c\u200b\u6765\u200b\u963b\u6b62\u200b autograd \u200b\u8ddf\u8e2a\u200b\u8bbe\u7f6e\u200b\u4e86\u200b<code>.requires_grad=True</code>\u200b\u7684\u200b\u5f20\u91cf\u200b\u7684\u200b\u5386\u53f2\u8bb0\u5f55\u200b\u3002</p> <p><pre><code>print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n    print((x ** 2).requires_grad)\n</code></pre> <pre><code>True\nTrue\nFalse\n</code></pre></p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4fee\u6539\u200b tensor \u200b\u7684\u200b\u6570\u503c\u200b\uff0c\u200b\u4f46\u662f\u200b\u53c8\u200b\uf967\u200b\u5e0c\u671b\u200b\u88ab\u200b autograd \u200b\u8bb0\u5f55\u200b(\u200b\u5373\u200b\uf967\u200b\u4f1a\u200b\u5f71\u54cd\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b)\uff0c \u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b tensor.data \u200b\u8fdb\u200b\ufa08\u200b\u64cd\u4f5c\u200b\u3002</p> <p><pre><code>x = torch.ones(1,requires_grad=True)\n\nprint(x.data) # \u200b\u8fd8\u662f\u200b\u4e00\u4e2a\u200btensor\nprint(x.data.requires_grad) # \u200b\u4f46\u662f\u200b\u5df2\u7ecf\u200b\u662f\u200b\u72ec\u7acb\u200b\u4e8e\u200b\u8ba1\u7b97\u200b\u56fe\u200b\u4e4b\u5916\u200b\n\ny = 2 * x\nx.data *= 100 # \u200b\u53ea\u200b\u6539\u53d8\u200b\u4e86\u200b\u503c\u200b\uff0c\u200b\u4e0d\u4f1a\u200b\u8bb0\u5f55\u200b\u5728\u200b\u8ba1\u7b97\u200b\u56fe\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b\u68af\u5ea6\u200b\u4f20\u64ad\u200b\n\ny.backward()\nprint(x) # \u200b\u66f4\u6539\u200bdata\u200b\u7684\u200b\u503c\u200b\u4e5f\u200b\u4f1a\u200b\u5f71\u54cd\u200btensor\u200b\u7684\u200b\u503c\u200b \nprint(x.grad)\n</code></pre> <pre><code>tensor([1.])\nFalse\ntensor([100.], requires_grad=True)\ntensor([2.])\n</code></pre></p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/","title":"2.3 \u200b\u5e76\u884c\u8ba1\u7b97\u200b\u7b80\u4ecb","text":"<p>\u200b\u5728\u200b\u5229\u7528\u200bPyTorch\u200b\u505a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u6570\u636e\u91cf\u200b\u8f83\u5927\u200b\u65e0\u6cd5\u200b\u5728\u200b\u5355\u5757\u200bGPU\u200b\u4e0a\u200b\u5b8c\u6210\u200b\uff0c\u200b\u6216\u8005\u200b\u9700\u8981\u200b\u63d0\u5347\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u7684\u200b\u573a\u666f\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u5c31\u200b\u9700\u8981\u200b\u7528\u5230\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u3002\u200b\u5b8c\u6210\u200b\u672c\u200b\u8282\u200b\u5185\u5bb9\u200b\u65f6\u200b\uff0c\u200b\u8bf7\u200b\u4f60\u200b\u786e\u4fdd\u200b\u81f3\u5c11\u200b\u5b89\u88c5\u200b\u4e86\u200b\u4e00\u4e2a\u200bNVIDIA GPU\u200b\u5e76\u200b\u5b89\u88c5\u200b\u4e86\u200b\u76f8\u5173\u200b\u7684\u200b\u9a71\u52a8\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u7684\u200b\u7b80\u4ecb\u200b</li> <li>CUDA\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u7684\u200b\u4e09\u79cd\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b</li> <li>\u200b\u4f7f\u7528\u200bCUDA\u200b\u52a0\u901f\u200b\u8bad\u7ec3\u200b</li> </ul>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#231","title":"2.3.1  \u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u505a\u200b\u5e76\u884c\u8ba1\u7b97","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\u79bb\u4e0d\u5f00\u200b\u7b97\u529b\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0cGPU\u200b\u7684\u200b\u51fa\u73b0\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u8bad\u7ec3\u200b\u7684\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u66f4\u597d\u200b\u3002\u200b\u6240\u4ee5\u200b\uff0c\u200b\u5982\u4f55\u200b\u5145\u5206\u5229\u7528\u200bGPU\u200b\u7684\u200b\u6027\u80fd\u200b\u6765\u200b\u63d0\u9ad8\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u6280\u80fd\u200b\u662f\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u8981\u200b\u5b66\u4e60\u200b\u7684\u200b\u3002\u200b\u8fd9\u200b\u4e00\u8282\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u8bb2\u200b\u7684\u200b\u5c31\u662f\u200bPyTorch\u200b\u7684\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u3002PyTorch\u200b\u53ef\u4ee5\u200b\u5728\u200b\u7f16\u5199\u200b\u5b8c\u200b\u6a21\u578b\u200b\u4e4b\u540e\u200b\uff0c\u200b\u8ba9\u200b\u591a\u4e2a\u200bGPU\u200b\u6765\u200b\u53c2\u4e0e\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u51cf\u5c11\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4f7f\u7528\u200b<code>nvidia-smi</code>\u200b\u547d\u4ee4\u200b\u6765\u200b\u67e5\u770b\u200b\u4f60\u200b\u7684\u200bGPU\u200b\u4fe1\u606f\u200b\u548c\u200b\u4f7f\u7528\u200b\u60c5\u51b5\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#232-cuda","title":"2.3.2  \u200b\u4e3a\u4ec0\u4e48\u200b\u9700\u8981\u200bCUDA","text":"<p><code>CUDA</code>\u200b\u662f\u200bNVIDIA\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e00\u79cd\u200bGPU\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u6846\u67b6\u200b\u3002\u200b\u5bf9\u4e8e\u200bGPU\u200b\u672c\u8eab\u200b\u7684\u200b\u7f16\u7a0b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b<code>CUDA</code>\u200b\u8bed\u8a00\u200b\u6765\u200b\u5b9e\u73b0\u200b\u7684\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPyTorch\u200b\u7f16\u5199\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b<code>CUDA</code>\u200b\u53c8\u200b\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u610f\u601d\u200b\u3002\u200b\u5728\u200bPyTorch\u200b\u4f7f\u7528\u200b <code>CUDA</code>\u200b\u8868\u793a\u200b\u8981\u200b\u5f00\u59cb\u200b\u8981\u6c42\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6216\u8005\u200b\u6570\u636e\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200bGPU\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5728\u200b\u7f16\u5199\u7a0b\u5e8f\u200b\u4e2d\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>.cuda()</code> \u200b\u65f6\u200b\uff0c\u200b\u5176\u200b\u529f\u80fd\u200b\u662f\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6216\u8005\u200b\u6570\u636e\u200b\u4ece\u200bCPU\u200b\u8fc1\u79fb\u200b\u5230\u200bGPU\u200b\u4e0a\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u662f\u200b0\u200b\u53f7\u200bGPU\uff09\u200b\u5f53\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200bGPU\u200b\u5f00\u59cb\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u6ce8\u200b\uff1a 1. \u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bGPU\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b<code>.cuda()</code>\u200b\u800c\u200b\u4e0d\u662f\u200b\u4f7f\u7528\u200b<code>.gpu()</code>\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5f53\u524d\u200bGPU\u200b\u7684\u200b\u7f16\u7a0b\u200b\u63a5\u53e3\u200b\u91c7\u7528\u200bCUDA\uff0c\u200b\u4f46\u662f\u200b\u5e02\u9762\u4e0a\u200b\u7684\u200bGPU\u200b\u5e76\u200b\u4e0d\u662f\u200b\u90fd\u200b\u652f\u6301\u200bCUDA\uff0c\u200b\u53ea\u6709\u200b\u90e8\u5206\u200bNVIDIA\u200b\u7684\u200bGPU\u200b\u624d\u200b\u652f\u6301\u200b\uff0cAMD\u200b\u7684\u200bGPU\u200b\u7f16\u7a0b\u200b\u63a5\u53e3\u200b\u91c7\u7528\u200b\u7684\u200b\u662f\u200bOpenCL\uff0c\u200b\u5728\u200b\u73b0\u9636\u6bb5\u200bPyTorch\u200b\u5e76\u200b\u4e0d\u200b\u652f\u6301\u200b\u3002 2. \u200b\u6570\u636e\u200b\u5728\u200bGPU\u200b\u548c\u200bCPU\u200b\u4e4b\u95f4\u200b\u8fdb\u884c\u200b\u4f20\u9012\u200b\u65f6\u4f1a\u200b\u6bd4\u8f83\u200b\u8017\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u5f53\u200b\u5c3d\u91cf\u907f\u514d\u200b\u6570\u636e\u200b\u7684\u200b\u5207\u6362\u200b\u3002 3. GPU\u200b\u8fd0\u7b97\u200b\u5f88\u5feb\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u4f7f\u7528\u200b\u7b80\u5355\u200b\u7684\u200b\u64cd\u4f5c\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5c3d\u91cf\u200b\u4f7f\u7528\u200bCPU\u200b\u53bb\u200b\u5b8c\u6210\u200b\u3002 4. \u200b\u5f53\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u6709\u200b\u591a\u4e2a\u200bGPU\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u6307\u660e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u662f\u200b\u54ea\u4e00\u5757\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u8bbe\u7f6e\u200b\u7684\u8bdd\u200b\uff0ctensor.cuda()\u200b\u65b9\u6cd5\u200b\u4f1a\u200b\u9ed8\u8ba4\u200b\u5c06\u200btensor\u200b\u4fdd\u5b58\u200b\u5230\u200b\u7b2c\u4e00\u5757\u200bGPU\u200b\u4e0a\u200b\uff0c\u200b\u7b49\u4ef7\u200b\u4e8e\u200btensor.cuda(0)\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u6709\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u7206\u51fa\u200b<code>out of memory</code>\u200b\u7684\u200b\u9519\u8bef\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u7ee7\u7eed\u200b\u8bbe\u7f6e\u200b\u3002    1.  <code>python         #\u200b\u8bbe\u7f6e\u200b\u5728\u200b\u6587\u4ef6\u200b\u6700\u200b\u5f00\u59cb\u200b\u90e8\u5206\u200b        import os        os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # \u200b\u8bbe\u7f6e\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u663e\u5361\u200b</code>    2.  <code>bash         CUDA_VISBLE_DEVICE=0,1 python train.py # \u200b\u4f7f\u7528\u200b0\uff0c1\u200b\u4e24\u5757\u200bGPU</code></p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#233","title":"2.3.3  \u200b\u5e38\u89c1\u200b\u7684\u200b\u5e76\u884c\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1a","text":""},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#network-partitioning","title":"\u7f51\u7edc\u7ed3\u6784\u200b\u5206\u5e03\u200b\u5230\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e2d\u200b(Network partitioning)","text":"<p>\u200b\u5728\u200b\u521a\u200b\u5f00\u59cb\u200b\u505a\u200b\u6a21\u578b\u200b\u5e76\u884c\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u65b9\u6848\u200b\u4f7f\u7528\u200b\u7684\u200b\u6bd4\u8f83\u200b\u591a\u200b\u3002\u200b\u5176\u4e2d\u200b\u4e3b\u8981\u200b\u7684\u200b\u601d\u8def\u200b\u662f\u200b\uff0c\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u62c6\u5206\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u90e8\u5206\u200b\u653e\u5165\u200b\u5230\u200bGPU\u200b\u6765\u200b\u505a\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\u7684\u200b\u8ba1\u7b97\u200b\u3002\u200b\u5176\u200b\u67b6\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u8fd9\u91cc\u200b\u9047\u5230\u200b\u7684\u200b\u95ee\u9898\u200b\u5c31\u662f\u200b\uff0c\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u7ec4\u4ef6\u200b\u5728\u200b\u4e0d\u540c\u200b\u7684\u200bGPU\u200b\u4e0a\u65f6\u200b\uff0cGPU\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4f20\u8f93\u200b\u5c31\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u5bf9\u4e8e\u200bGPU\u200b\u4e4b\u95f4\u200b\u7684\u200b\u901a\u4fe1\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8003\u9a8c\u200b\u3002\u200b\u4f46\u662f\u200bGPU\u200b\u7684\u200b\u901a\u4fe1\u200b\u5728\u200b\u8fd9\u79cd\u200b\u5bc6\u96c6\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u5f88\u200b\u96be\u200b\u529e\u5230\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8fd9\u4e2a\u200b\u65b9\u5f0f\u200b\u6162\u6162\u200b\u6de1\u51fa\u200b\u4e86\u200b\u89c6\u91ce\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#layer-wise-partitioning","title":"\u540c\u200b\u4e00\u5c42\u200b\u7684\u200b\u4efb\u52a1\u200b\u5206\u5e03\u200b\u5230\u200b\u4e0d\u540c\u200b\u6570\u636e\u200b\u4e2d\u200b(Layer-wise partitioning)","text":"<p>\u200b\u7b2c\u4e8c\u79cd\u200b\u65b9\u5f0f\u200b\u5c31\u662f\u200b\uff0c\u200b\u540c\u200b\u4e00\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u505a\u200b\u4e00\u4e2a\u200b\u62c6\u5206\u200b\uff0c\u200b\u8ba9\u200b\u4e0d\u540c\u200b\u7684\u200bGPU\u200b\u53bb\u200b\u8bad\u7ec3\u200b\u540c\u200b\u4e00\u5c42\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u5206\u200b\u4efb\u52a1\u200b\u3002\u200b\u5176\u200b\u67b6\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u4fdd\u8bc1\u200b\u5728\u200b\u4e0d\u540c\u200b\u7ec4\u4ef6\u200b\u4e4b\u95f4\u200b\u4f20\u8f93\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5927\u91cf\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u540c\u6b65\u200b\u4efb\u52a1\u200b\u52a0\u91cd\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u548c\u200b\u7b2c\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u4e00\u6837\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#data-parallelism","title":"\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u5206\u5e03\u200b\u5230\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e2d\u200b\uff0c\u200b\u6267\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u4efb\u52a1\u200b(Data parallelism)","text":"<p>\u200b\u7b2c\u4e09\u79cd\u200b\u65b9\u5f0f\u200b\u6709\u70b9\u200b\u4e0d\u200b\u4e00\u6837\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u903b\u8f91\u200b\u662f\u200b\uff0c\u200b\u6211\u200b\u4e0d\u518d\u200b\u62c6\u5206\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\u6a21\u578b\u200b\u90fd\u200b\u662f\u200b\u4e00\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u3002\u200b\u4f46\u662f\u200b\u6211\u200b\u5c06\u200b\u8f93\u5165\u200b\u7684\u200b\u6570\u636e\u200b\u62c6\u5206\u200b\u3002\u200b\u6240\u8c13\u200b\u7684\u200b\u62c6\u5206\u200b\u6570\u636e\u200b\u5c31\u662f\u200b\uff0c\u200b\u540c\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b\u4e0d\u540c\u200bGPU\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\u4e00\u90e8\u5206\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u5206\u522b\u200b\u8ba1\u7b97\u200b\u4e00\u90e8\u5206\u200b\u6570\u636e\u200b\u4e4b\u540e\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u8f93\u51fa\u200b\u7684\u200b\u6570\u636e\u200b\u505a\u200b\u4e00\u4e2a\u200b\u6c47\u603b\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u53cd\u4f20\u200b\u3002\u200b\u5176\u200b\u67b6\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u53ef\u4ee5\u200b\u89e3\u51b3\u200b\u4e4b\u524d\u200b\u6a21\u5f0f\u200b\u9047\u5230\u200b\u7684\u200b\u901a\u8baf\u200b\u95ee\u9898\u200b\u3002\u200b\u73b0\u5728\u200b\u7684\u200b\u4e3b\u6d41\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6570\u636e\u200b\u5e76\u884c\u200b\u7684\u200b\u65b9\u5f0f\u200b(Data parallelism)</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#234-cuda","title":"2.3.4 \u200b\u4f7f\u7528\u200bCUDA\u200b\u52a0\u901f\u200b\u8bad\u7ec3","text":""},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#_1","title":"\u5355\u5361\u200b\u8bad\u7ec3","text":"<p>\u200b\u5728\u200bPyTorch\u200b\u6846\u67b6\u200b\u4e0b\u200b\uff0cCUDA\u200b\u7684\u200b\u4f7f\u7528\u200b\u53d8\u5f97\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u663e\u5f0f\u200b\u7684\u200b\u5c06\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u901a\u8fc7\u200b<code>.cuda()</code>\u200b\u65b9\u6cd5\u200b\u8f6c\u79fb\u200b\u5230\u200bGPU\u200b\u4e0a\u200b\u5c31\u200b\u53ef\u200b\u52a0\u901f\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>model = Net()\nmodel.cuda() # \u200b\u6a21\u578b\u200b\u663e\u793a\u200b\u8f6c\u79fb\u200b\u5230\u200bCUDA\u200b\u4e0a\u200b\n\nfor image,label in dataloader:\n    # \u200b\u56fe\u50cf\u200b\u548c\u200b\u6807\u7b7e\u200b\u663e\u793a\u200b\u8f6c\u79fb\u200b\u5230\u200bCUDA\u200b\u4e0a\u200b\n    image = image.cuda() \n    label = label.cuda()\n</code></pre>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#_2","title":"\u591a\u5361\u200b\u8bad\u7ec3","text":"<p>PyTorch\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e24\u79cd\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5206\u522b\u200b\u4e3a\u200b<code>DataParallel</code>\u200b\u548c\u200b<code>DistributedDataParallel</code>\uff08\u200b\u4ee5\u4e0b\u200b\u6211\u4eec\u200b\u5206\u522b\u200b\u7b80\u79f0\u200b\u4e3a\u200bDP\u200b\u548c\u200bDDP\uff09\u3002\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u5b98\u65b9\u200b\u66f4\u200b\u63a8\u8350\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b<code>DDP</code>\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u7684\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\u3002\u200b\u4f46\u662f\u200b<code>DDP</code>\u200b\u7684\u200b\u4f7f\u7528\u200b\u6bd4\u8f83\u590d\u6742\u200b\uff0c\u200b\u800c\u200b<code>DP</code>\u200b\u7ecf\u200b\u9700\u8981\u200b\u6539\u53d8\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u65e2\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u5148\u200b\u4ecb\u7ecd\u200b<code>DP</code>\uff0c\u200b\u518d\u200b\u4ecb\u7ecd\u200b<code>DDP</code>\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#dp","title":"\u5355\u673a\u200b\u591a\u5361\u200bDP","text":"<p>\u200b\u9996\u5148\u200b\u6211\u4eec\u200b\u6765\u770b\u200b\u5355\u673a\u200b\u591a\u5361\u200bDP\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u4e00\u79cd\u200b\u53eb\u505a\u200b\u6570\u636e\u200b\u5e76\u884c\u200b (Data parallelism) \u200b\u7684\u200b\u7b56\u7565\u200b\uff0c\u200b\u5373\u5c06\u200b\u8ba1\u7b97\u200b\u4efb\u52a1\u200b\u5212\u5206\u200b\u6210\u200b\u591a\u4e2a\u200b\u5b50\u200b\u4efb\u52a1\u200b\u5e76\u200b\u5728\u200b\u591a\u4e2a\u200bGPU\u200b\u5361\u4e0a\u200b\u540c\u65f6\u200b\u6267\u884c\u200b\u8fd9\u4e9b\u200b\u5b50\u200b\u4efb\u52a1\u200b\u3002\u200b\u4e3b\u8981\u200b\u4f7f\u7528\u200b\u5230\u200b\u4e86\u200b<code>nn.DataParallel</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u4f7f\u7528\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u4e00\u822c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u52a0\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u5b9e\u73b0\u200b</p> <pre><code>model = Net()\nmodel.cuda() # \u200b\u6a21\u578b\u200b\u663e\u793a\u200b\u8f6c\u79fb\u200b\u5230\u200bCUDA\u200b\u4e0a\u200b\n\nif torch.cuda.device_count() &gt; 1: # \u200b\u542b\u6709\u200b\u591a\u200b\u5f20\u200bGPU\u200b\u7684\u200b\u5361\u200b\n    model = nn.DataParallel(model) # \u200b\u5355\u673a\u200b\u591a\u5361\u200bDP\u200b\u8bad\u7ec3\u200b\n</code></pre> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200bGPU\u200b\u8fdb\u884c\u200b\u5e76\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4e00\u822c\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b</p> <ul> <li><code>nn.DataParallel</code>\u200b\u51fd\u6570\u200b\u4f20\u5165\u200b<code>device_ids</code>\u200b\u53c2\u6570\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200b\u4e86\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b</li> </ul> <pre><code>model = nn.DataParallel(model, device_ids=[0,1]) # \u200b\u4f7f\u7528\u200b\u7b2c\u200b0\u200b\u548c\u200b\u7b2c\u200b1\u200b\u5f20\u5361\u200b\u8fdb\u884c\u200b\u5e76\u884c\u200b\u8bad\u7ec3\u200b\n</code></pre> <ul> <li>\u200b\u8981\u200b\u624b\u52a8\u200b\u6307\u5b9a\u200b\u5bf9\u200b\u7a0b\u5e8f\u200b\u53ef\u89c1\u200b\u7684\u200bGPU\u200b\u8bbe\u5907\u200b</li> </ul> <pre><code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n</code></pre>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#ddp","title":"\u591a\u673a\u200b\u591a\u5361\u200bDDP","text":"<p>\u200b\u4e0d\u8fc7\u200b\u901a\u8fc7\u200bDP\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5bb9\u6613\u200b\u9020\u6210\u200b\u8d1f\u8f7d\u200b\u4e0d\u200b\u5747\u8861\u200b\uff0c\u200b\u6709\u200b\u53ef\u80fd\u200b\u7b2c\u4e00\u5757\u200bGPU\u200b\u663e\u5b58\u200b\u5360\u7528\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8f93\u51fa\u200b\u9ed8\u8ba4\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u200bgather\u200b\u5230\u200b\u7b2c\u4e00\u5757\u200bGPU\u200b\u4e0a\u200b\u3002\u200b\u4e3a\u6b64\u200bPytorch\u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200b<code>torch.nn.parallel.DistributedDataParallel</code>\uff08DDP\uff09\u200b\u65b9\u6cd5\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u9488\u5bf9\u200b\u6bcf\u4e2a\u200bGPU\uff0c\u200b\u542f\u52a8\u200b\u4e00\u4e2a\u200b\u8fdb\u7a0b\u200b\uff0c\u200b\u7136\u540e\u200b\u8fd9\u4e9b\u200b\u8fdb\u7a0b\u200b\u5728\u200b\u6700\u200b\u5f00\u59cb\u200b\u7684\u200b\u65f6\u5019\u200b\u4f1a\u200b\u4fdd\u6301\u4e00\u81f4\u200b\uff08\u200b\u6a21\u578b\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u53c2\u6570\u200b\u4e5f\u200b\u4e00\u81f4\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8fdb\u7a0b\u200b\u62e5\u6709\u200b\u81ea\u5df1\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\uff09\uff0c\u200b\u540c\u65f6\u200b\u5728\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u68af\u5ea6\u200b\u4f20\u64ad\u200b\u4e5f\u200b\u662f\u200b\u5b8c\u5168\u4e00\u81f4\u200b\u7684\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4fdd\u8bc1\u200b\u4efb\u4f55\u200b\u4e00\u4e2a\u200bGPU\u200b\u4e0a\u9762\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5c31\u662f\u200b\u5b8c\u5168\u4e00\u81f4\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b<code>DataParallel</code>\u200b\u90a3\u6837\u200b\u663e\u5b58\u200b\u4e0d\u200b\u5747\u8861\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u4e0d\u8fc7\u200b\u76f8\u5bf9\u200b\u5e94\u200b\u7684\u200b\uff0c\u200b\u4f1a\u200b\u6bd4\u8f83\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u4ecb\u7ecd\u200b\u4e00\u4e0b\u200b\u591a\u673a\u200b\u591a\u5361\u200bDDP\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u5f00\u59cb\u200b\u4e4b\u524d\u200b\u9700\u8981\u200b\u5148\u200b\u719f\u6089\u200b\u51e0\u4e2a\u200b\u6982\u5ff5\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u8fd8\u662f\u200b\u6709\u200b\u5fc5\u8981\u200b\u63d0\u200b\u4e00\u4e0b\u200b\u7684\u200b</p> <p>\u200b\u8fdb\u7a0b\u200b\u7ec4\u200b\u7684\u200b\u76f8\u5173\u200b\u6982\u5ff5\u200b</p> <ul> <li>GROUP\uff1a\u200b\u8fdb\u7a0b\u200b\u7ec4\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ea\u6709\u200b\u4e00\u4e2a\u7ec4\u200b\uff0c\u200b\u4e00\u4e2a\u200b job \u200b\u5373\u200b\u4e3a\u200b\u4e00\u4e2a\u7ec4\u200b\uff0c\u200b\u4e5f\u200b\u5373\u200b\u4e00\u4e2a\u200b world\u3002\uff08\u200b\u5f53\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u66f4\u52a0\u200b\u7cbe\u7ec6\u200b\u7684\u200b\u901a\u4fe1\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b new_group \u200b\u63a5\u53e3\u200b\uff0c\u200b\u4f7f\u7528\u200b world \u200b\u7684\u200b\u5b50\u96c6\u200b\uff0c\u200b\u521b\u5efa\u200b\u65b0\u7ec4\u200b\uff0c\u200b\u7528\u4e8e\u200b\u96c6\u4f53\u200b\u901a\u4fe1\u200b\u7b49\u200b\u3002\uff09</li> <li>WORLD_SIZE\uff1a\u200b\u8868\u793a\u200b\u5168\u5c40\u200b\u8fdb\u7a0b\u200b\u4e2a\u6570\u200b\u3002\u200b\u5982\u679c\u200b\u662f\u200b\u591a\u200b\u673a\u591a\u5361\u200b\u5c31\u200b\u8868\u793a\u200b\u673a\u5668\u200b\u6570\u91cf\u200b\uff0c\u200b\u5982\u679c\u200b\u662f\u200b\u5355\u673a\u200b\u591a\u5361\u200b\u5c31\u200b\u8868\u793a\u200b GPU \u200b\u6570\u91cf\u200b\u3002</li> <li>RANK\uff1a\u200b\u8868\u793a\u200b\u8fdb\u7a0b\u200b\u5e8f\u53f7\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8fdb\u7a0b\u200b\u95f4\u200b\u901a\u8baf\u200b\uff0c\u200b\u8868\u5f81\u200b\u8fdb\u7a0b\u200b\u4f18\u5148\u7ea7\u200b\u3002rank = 0 \u200b\u7684\u200b\u4e3b\u673a\u200b\u4e3a\u200b master \u200b\u8282\u70b9\u200b\u3002 \u200b\u5982\u679c\u200b\u662f\u200b\u591a\u200b\u673a\u591a\u5361\u200b\u5c31\u200b\u8868\u793a\u200b\u5bf9\u5e94\u200b\u7b2c\u200b\u51e0\u53f0\u200b\u673a\u5668\u200b\uff0c\u200b\u5982\u679c\u200b\u662f\u200b\u5355\u673a\u200b\u591a\u5361\u200b\uff0c\u200b\u7531\u4e8e\u200b\u4e00\u4e2a\u200b\u8fdb\u7a0b\u200b\u5185\u200b\u5c31\u200b\u53ea\u6709\u200b\u4e00\u4e2a\u200b GPU\uff0c\u200b\u6240\u4ee5\u200b rank \u200b\u4e5f\u200b\u5c31\u200b\u8868\u793a\u200b\u7b2c\u200b\u51e0\u5757\u200b GPU\u3002</li> <li>LOCAL_RANK\uff1a\u200b\u8868\u793a\u200b\u8fdb\u7a0b\u200b\u5185\u200b\uff0cGPU \u200b\u7f16\u53f7\u200b\uff0c\u200b\u975e\u663e\u5f0f\u200b\u53c2\u6570\u200b\uff0c\u200b\u7531\u200b torch.distributed.launch \u200b\u5185\u90e8\u200b\u6307\u5b9a\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u591a\u673a\u200b\u591a\u5361\u4e2d\u200b rank = 3\uff0clocal_rank = 0 \u200b\u8868\u793a\u200b\u7b2c\u200b 3 \u200b\u4e2a\u200b\u8fdb\u7a0b\u200b\u5185\u200b\u7684\u200b\u7b2c\u200b 1 \u200b\u5757\u200b GPU\u3002</li> </ul> <p>DDP\u200b\u7684\u200b\u57fa\u672c\u200b\u7528\u6cd5\u200b (\u200b\u4ee3\u7801\u200b\u7f16\u5199\u200b\u6d41\u7a0b\u200b)</p> <ul> <li>\u200b\u5728\u200b\u4f7f\u7528\u200b <code>distributed</code> \u200b\u5305\u200b\u7684\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u51fd\u6570\u200b\u4e4b\u524d\u200b\uff0c\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b <code>init_process_group</code> \u200b\u521d\u59cb\u5316\u200b\u8fdb\u7a0b\u200b\u7ec4\u200b\uff0c\u200b\u540c\u65f6\u200b\u521d\u59cb\u5316\u200b <code>distributed</code> \u200b\u5305\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.nn.parallel.DistributedDataParallel</code> \u200b\u521b\u5efa\u200b \u200b\u5206\u5e03\u5f0f\u200b\u6a21\u578b\u200b <code>DDP(model, device_ids=device_ids)</code></li> <li>\u200b\u4f7f\u7528\u200b <code>torch.utils.data.distributed.DistributedSampler</code> \u200b\u521b\u5efa\u200b DataLoader</li> <li>\u200b\u4f7f\u7528\u200b\u542f\u52a8\u200b\u5de5\u5177\u200b <code>torch.distributed.launch</code> \u200b\u5728\u200b\u6bcf\u4e2a\u200b\u4e3b\u673a\u200b\u4e0a\u200b\u6267\u884c\u200b\u4e00\u6b21\u200b\u811a\u672c\u200b\uff0c\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b</li> </ul> <p>\u200b\u9996\u5148\u200b\u662f\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b  --local_rank</p> <pre><code>import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--local_rank\", type=int) # \u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u5f88\u200b\u91cd\u8981\u200b\nargs = parser.parse_args()\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200blocal_rank\u200b\u53c2\u6570\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b<code>torch.distributed.launch</code>\u200b\u5728\u200b\u7ed9\u200b\u4e00\u4e2a\u200bGPU\u200b\u521b\u5efa\u200b\u8fdb\u7a0b\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u7ed9\u200b\u8fd9\u4e2a\u200b\u8fdb\u7a0b\u200b\u63d0\u4f9b\u200b\u7684\u200bGPU\u200b\u53f7\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u662f\u200b\u7a0b\u5e8f\u200b\u81ea\u52a8\u200b\u7ed9\u200b\u7684\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u624b\u52a8\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u6307\u5b9a\u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u3002</p> <p><pre><code>local_rank = int(os.environ[\"LOCAL_RANK\"]) #\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u81ea\u52a8\u200b\u83b7\u53d6\u200b\n</code></pre> \u200b\u7136\u540e\u200b\u5728\u200b\u6240\u6709\u200b\u548c\u200bGPU\u200b\u76f8\u5173\u200b\u4ee3\u7801\u200b\u7684\u200b\u524d\u9762\u200b\u6dfb\u52a0\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u5199\u200b\u8fd9\u53e5\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6240\u6709\u200b\u7684\u200b\u8fdb\u7a0b\u200b\u90fd\u200b\u9ed8\u8ba4\u200b\u5728\u200b\u4f60\u200b\u4f7f\u7528\u200b<code>CUDA_VISIBLE_DEVICES</code>\u200b\u53c2\u6570\u200b\u8bbe\u5b9a\u200b\u7684\u200b0\u200b\u53f7\u200bGPU\u200b\u4e0a\u9762\u200b\u542f\u52a8\u200b</p> <pre><code>torch.cuda.set_device(args.local_rank) # \u200b\u8c03\u6574\u200b\u8ba1\u7b97\u200b\u7684\u200b\u4f4d\u7f6e\u200b\n</code></pre> <p>\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u5f97\u200b\u521d\u59cb\u5316\u200b<code>backend</code>\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u4fd7\u79f0\u200b\u7684\u200b\u540e\u200b\u7aef\u200b\uff0cpytorch\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u540e\u200b\u7aef\u200b\uff1a</p> <p></p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u4e86\u200b<code>gloo</code>\uff0c<code>nccl</code>\uff0c<code>mpi</code>\uff0c\u200b\u90a3\u4e48\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u9009\u62e9\u200b\u5462\u200b\uff0c\u200b\u5b98\u7f51\u200b\u4e2d\u200b\u4e5f\u200b\u7ed9\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u5efa\u8bae\u200b</p> <ul> <li>\u200b\u7ecf\u9a8c\u4e4b\u8c08\u200b</li> <li>\u200b\u5982\u679c\u200b\u662f\u200b\u4f7f\u7528\u200b<code>cpu</code>\u200b\u7684\u200b\u5206\u5e03\u5f0f\u8ba1\u7b97\u200b, \u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b<code>gloo</code>\uff0c\u200b\u56e0\u4e3a\u200b\u8868\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b <code>gloo</code>\u200b\u5bf9\u200b<code>cpu</code>\u200b\u7684\u200b\u652f\u6301\u200b\u662f\u200b\u6700\u597d\u200b\u7684\u200b</li> <li> <p>\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b<code>gpu</code>\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u8ba1\u7b97\u200b, \u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b<code>nccl</code>\u3002</p> </li> <li> <p>GPU\u200b\u4e3b\u673a\u200b</p> </li> <li>InfiniBand\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b<code>nccl</code>\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u662f\u200b\u76ee\u524d\u200b\u552f\u4e00\u200b\u652f\u6301\u200b InfiniBand \u200b\u548c\u200b GPUDirect \u200b\u7684\u200b\u540e\u200b\u7aef\u200b\u3002</li> <li>Ethernet\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b<code>nccl</code>\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u7684\u200b\u5206\u5e03\u5f0f\u200bGPU\u200b\u8bad\u7ec3\u200b\u6027\u80fd\u200b\u76ee\u524d\u200b\u662f\u200b\u6700\u597d\u200b\u7684\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u5bf9\u4e8e\u200b\u591a\u200b\u8fdb\u7a0b\u200b\u5355\u200b\u8282\u70b9\u200b\u6216\u200b\u591a\u200b\u8282\u70b9\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u3002 \u200b\u5982\u679c\u200b\u5728\u200b\u4f7f\u7528\u200b <code>nccl</code>\u200b\u65f6\u200b\u9047\u5230\u200b\u4efb\u4f55\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>gloo</code> \u200b\u4f5c\u4e3a\u200b\u540e\u5907\u200b\u9009\u9879\u200b\u3002 \uff08\u200b\u4e0d\u8fc7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5bf9\u4e8e\u200b GPU\uff0c<code>gloo</code> \u200b\u76ee\u524d\u200b\u7684\u200b\u8fd0\u884c\u200b\u901f\u5ea6\u200b\u6bd4\u200b <code>nccl</code> \u200b\u6162\u200b\u3002\uff09</li> <li>CPU\u200b\u4e3b\u673a\u200b</li> <li>InfiniBand\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5982\u679c\u200b\u542f\u7528\u200b\u4e86\u200bIP over IB\uff0c\u200b\u90a3\u200b\u5c31\u200b\u4f7f\u7528\u200b<code>gloo</code>\uff0c\u200b\u5426\u5219\u200b\u4f7f\u7528\u200b<code>mpi</code></li> <li>Ethernet\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b<code>gloo</code>\uff0c\u200b\u9664\u975e\u200b\u6709\u200b\u4e0d\u5f97\u5df2\u200b\u7684\u200b\u7406\u7531\u200b\u4f7f\u7528\u200b<code>mpi</code>\u3002</li> </ul> <p>\u200b\u5f53\u540e\u200b\u7aef\u200b\u9009\u62e9\u200b\u597d\u200b\u4e86\u200b\u4e4b\u540e\u200b, \u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u4e00\u4e0b\u200b\u7f51\u7edc\u63a5\u53e3\u200b, \u200b\u56e0\u4e3a\u200b\u591a\u4e2a\u200b\u4e3b\u673a\u200b\u4e4b\u95f4\u200b\u80af\u5b9a\u200b\u662f\u200b\u4f7f\u7528\u200b\u7f51\u7edc\u200b\u8fdb\u884c\u200b\u4ea4\u6362\u200b, \u200b\u90a3\u200b\u80af\u5b9a\u200b\u5c31\u200b\u6d89\u53ca\u200b\u5230\u200bIP\u200b\u4e4b\u7c7b\u200b\u7684\u200b, \u200b\u5bf9\u4e8e\u200b<code>nccl</code>\u200b\u548c\u200b<code>gloo</code>\u200b\u4e00\u822c\u200b\u4f1a\u200b\u81ea\u5df1\u200b\u5bfb\u627e\u200b\u7f51\u7edc\u63a5\u53e3\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u6709\u65f6\u5019\u200b\u5982\u679c\u200b\u7f51\u5361\u200b\u6bd4\u8f83\u200b\u591a\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5c31\u200b\u9700\u8981\u200b\u81ea\u5df1\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b</p> <pre><code>import os\n# \u200b\u4ee5\u4e0b\u200b\u4e8c\u9009\u200b\u4e00\u200b, \u200b\u7b2c\u4e00\u4e2a\u200b\u662f\u200b\u4f7f\u7528\u200bgloo\u200b\u540e\u200b\u7aef\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u7684\u200b, \u200b\u7b2c\u4e8c\u4e2a\u200b\u662f\u200b\u4f7f\u7528\u200bnccl\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u7684\u200b\nos.environ['GLOO_SOCKET_IFNAME'] = 'eth0'\nos.environ['NCCL_SOCKET_IFNAME'] = 'eth0'\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u64cd\u4f5c\u200b\u77e5\u9053\u200b\u81ea\u5df1\u200b\u7684\u200b\u7f51\u7edc\u63a5\u53e3\u200b\uff0c\u200b\u8f93\u5165\u200b<code>ifconfig</code>, \u200b\u7136\u540e\u200b\u627e\u5230\u200b\u81ea\u5df1\u200bIP\u200b\u5730\u5740\u200b\u7684\u200b\u5c31\u662f\u200b, \u200b\u4e00\u822c\u200b\u5c31\u662f\u200b<code>em0</code>, <code>eth0</code>, <code>esp2s0</code>\u200b\u4e4b\u7c7b\u200b\u7684\u200b, </p> <p>\u200b\u4ece\u200b\u4ee5\u4e0a\u200b\u4ecb\u7ecd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c \u200b\u5f53\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u65f6\u5019\u200b, <code>nccl</code>\u200b\u7684\u200b\u6548\u7387\u200b\u662f\u200b\u9ad8\u4e8e\u200b<code>gloo</code>\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u822c\u200b\u8fd8\u662f\u200b\u4f1a\u200b\u9009\u62e9\u200b<code>nccl</code>\u200b\u540e\u200b\u7aef\u200b\uff0c\u200b\u8bbe\u7f6e\u200bGPU\u200b\u4e4b\u95f4\u200b\u901a\u4fe1\u200b\u4f7f\u7528\u200b\u7684\u200b\u540e\u200b\u7aef\u200b\u548c\u200b\u7aef\u53e3\u200b\uff1a</p> <pre><code># ps \u200b\u68c0\u67e5\u200bnccl\u200b\u662f\u5426\u200b\u53ef\u7528\u200b\n# torch.distributed.is_nccl_available ()\ntorch.distributed.init_process_group(backend='nccl') # \u200b\u9009\u62e9\u200bnccl\u200b\u540e\u200b\u7aef\u200b\uff0c\u200b\u521d\u59cb\u5316\u200b\u8fdb\u7a0b\u200b\u7ec4\u200b\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>DistributedSampler</code> \u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u5212\u5206\u200b\u3002\u200b\u5b83\u200b\u80fd\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bcf\u4e2a\u200b batch \u200b\u5212\u5206\u200b\u6210\u200b\u51e0\u4e2a\u200b partition\uff0c\u200b\u5728\u200b\u5f53\u524d\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u53ea\u200b\u9700\u8981\u200b\u83b7\u53d6\u200b\u548c\u200b rank \u200b\u5bf9\u5e94\u200b\u7684\u200b\u90a3\u4e2a\u200b partition \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code># \u200b\u521b\u5efa\u200bDataloader\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=train_sampler)\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a testset\u200b\u4e0d\u7528\u200bsampler</p> <p>\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b<code>torch.nn.parallel.DistributedDataParallel</code>\u200b\u5305\u88c5\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># DDP\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])\n</code></pre> <p>\u200b\u5982\u4f55\u200b\u542f\u52a8\u200bDDP</p> <p>\u200b\u90a3\u4e48\u200b\u5982\u4f55\u200b\u542f\u52a8\u200bDDP\uff0c\u200b\u8fd9\u200b\u4e0d\u540c\u4e8e\u200bDP\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u9700\u8981\u200b\u4f7f\u7528\u200btorch.distributed.launch\u200b\u542f\u52a8\u5668\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5355\u673a\u200b\u591a\u5361\u200b\u7684\u200b\u60c5\u51b5\u200b\uff1a</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py\n# nproc_per_node: \u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u662f\u200b\u6307\u200b\u4f60\u200b\u4f7f\u7528\u200b\u8fd9\u53f0\u200b\u670d\u52a1\u5668\u200b\u4e0a\u9762\u200b\u7684\u200b\u51e0\u5f20\u200b\u663e\u5361\u200b\n</code></pre> <p>\u200b\u6709\u65f6\u5019\u200b\u867d\u7136\u200b\u8bf4\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7b80\u5355\u200b\u4f7f\u7528\u200bDP\uff0c\u200b\u4f46\u662f\u200bDDP\u200b\u7684\u200b\u6548\u7387\u200b\u662f\u200b\u6bd4\u200bDP\u200b\u9ad8\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5f88\u591a\u200b\u65f6\u5019\u200b\u5355\u673a\u200b\u591a\u5361\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u662f\u200b\u4f1a\u200b\u53bb\u200b\u4f7f\u7528\u200bDDP</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#dp-ddp","title":"DP \u200b\u4e0e\u200b DDP \u200b\u7684\u200b\u4f18\u7f3a\u70b9","text":""},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#dp_1","title":"DP \u200b\u7684\u200b\u4f18\u52bf","text":"<p><code>nn.DataParallel</code>\u200b\u6ca1\u6709\u200b\u6539\u53d8\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u8f93\u51fa\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e0d\u200b\u9700\u8981\u200b\u505a\u200b\u4efb\u4f55\u200b\u66f4\u6539\u200b\uff0c\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u641e\u5b9a\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#dp_2","title":"DP \u200b\u7684\u200b\u7f3a\u70b9","text":"<p><code>DP</code>\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5bb9\u6613\u200b\u9020\u6210\u200b\u8d1f\u8f7d\u200b\u4e0d\u200b\u5747\u8861\u200b\uff0c\u200b\u7b2c\u4e00\u5757\u200bGPU\u200b\u663e\u5b58\u200b\u5360\u7528\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8f93\u51fa\u200b\u9ed8\u8ba4\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u200bgather\u200b\u5230\u200b\u7b2c\u4e00\u5757\u200bGPU\u200b\u4e0a\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u540e\u7eed\u200b\u7684\u200bloss\u200b\u8ba1\u7b97\u200b\u53ea\u4f1a\u200b\u5728\u200b<code>cuda:0</code>\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff0c\u200b\u6ca1\u6cd5\u200b\u5e76\u884c\u200b\u3002</p> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b<code>DP</code>\u200b\u53ea\u80fd\u200b\u5728\u200b\u5355\u673a\u200b\u4e0a\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4e14\u200b<code>DP</code>\u200b\u662f\u200b\u5355\u200b\u8fdb\u7a0b\u200b\u591a\u7ebf\u7a0b\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u6bd4\u200b<code>DDP</code>\u200b\u591a\u200b\u8fdb\u7a0b\u200b\u591a\u7ebf\u7a0b\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4f1a\u200b\u6548\u7387\u200b\u4f4e\u200b\u4e00\u4e9b\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#ddp_1","title":"DDP\u200b\u7684\u200b\u4f18\u52bf","text":"<p>1. \u200b\u6bcf\u4e2a\u200b\u8fdb\u7a0b\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u4e14\u200b\u53ea\u200b\u5bf9\u200b\u68af\u5ea6\u200b\u7b49\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u4fe1\u606f\u200b\u4ea4\u6362\u200b\u3002</p> <p><code>DDP</code> \u200b\u5728\u200b\u6bcf\u6b21\u200b\u8fed\u4ee3\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8fdb\u7a0b\u200b\u5177\u6709\u200b\u81ea\u5df1\u200b\u7684\u200b <code>optimizer</code> \uff0c\u200b\u5e76\u200b\u72ec\u7acb\u200b\u5b8c\u6210\u200b\u6240\u6709\u200b\u7684\u200b\u4f18\u5316\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u8fdb\u7a0b\u200b\u5185\u200b\u4e0e\u200b\u4e00\u822c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65e0\u5f02\u200b\u3002</p> <p>\u200b\u5728\u200b\u5404\u200b\u8fdb\u7a0b\u200b\u68af\u5ea6\u200b\u8ba1\u7b97\u200b\u5b8c\u6210\u200b\u4e4b\u540e\u200b\uff0c\u200b\u5404\u200b\u8fdb\u7a0b\u200b\u9700\u8981\u200b\u5c06\u200b\u68af\u5ea6\u200b\u8fdb\u884c\u200b\u6c47\u603b\u200b\u5e73\u5747\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u7531\u200b <code>rank=0</code> \u200b\u7684\u200b\u8fdb\u7a0b\u200b\uff0c\u200b\u5c06\u200b\u5176\u200b <code>broadcast</code> \u200b\u5230\u200b\u6240\u6709\u200b\u8fdb\u7a0b\u200b\u3002\u200b\u4e4b\u540e\u200b\uff0c\u200b\u5404\u200b\u8fdb\u7a0b\u200b\u7528\u8be5\u200b\u68af\u5ea6\u200b\u6765\u200b\u72ec\u7acb\u200b\u7684\u200b\u66f4\u65b0\u200b\u53c2\u6570\u200b\u3002\u200b\u800c\u200b <code>DP</code>\u200b\u662f\u200b\u68af\u5ea6\u200b\u6c47\u603b\u200b\u5230\u200b\u4e3b\u200b <code>GPU</code>\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u66f4\u65b0\u200b\u53c2\u6570\u200b\uff0c\u200b\u518d\u200b\u5e7f\u64ad\u200b\u53c2\u6570\u200b\u7ed9\u200b\u5176\u4ed6\u200b\u7684\u200b GPU\u3002</p> <p><code>DDP</code> \u200b\u4e2d\u200b\u7531\u4e8e\u200b\u5404\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u521d\u59cb\u200b\u53c2\u6570\u200b\u4e00\u81f4\u200b (\u200b\u521d\u59cb\u200b\u65f6\u523b\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b <code>broadcast</code>)\uff0c\u200b\u800c\u200b\u6bcf\u6b21\u200b\u7528\u4e8e\u200b\u66f4\u65b0\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e5f\u200b\u4e00\u81f4\u200b\uff0c\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5404\u200b\u8fdb\u7a0b\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u59cb\u7ec8\u4fdd\u6301\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u800c\u200b\u5728\u200b<code>DP</code> \u200b\u4e2d\u200b\uff0c\u200b\u5168\u7a0b\u200b\u7ef4\u62a4\u200b\u4e00\u4e2a\u200b <code>optimizer</code>\uff0c\u200b\u5bf9\u200b\u5404\u200b <code>GPU</code> \u200b\u4e0a\u200b\u68af\u5ea6\u200b\u8fdb\u884c\u200b\u6c42\u548c\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u4e3b\u200b <code>GPU</code> \u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\uff0c\u200b\u4e4b\u540e\u200b\u518d\u200b\u5c06\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b <code>broadcast</code> \u200b\u5230\u200b\u5176\u4ed6\u200b <code>GPU</code>\u3002</p> <p>\u200b\u76f8\u8f83\u200b\u4e8e\u200b<code>DP</code>\uff0c<code>DDP</code>\u200b\u4f20\u8f93\u200b\u7684\u200b\u6570\u636e\u91cf\u200b\u66f4\u200b\u5c11\u200b\uff0c\u200b\u56e0\u6b64\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u6548\u7387\u200b\u66f4\u9ad8\u200b\u3002</p> <p>2. \u200b\u6bcf\u4e2a\u200b\u8fdb\u7a0b\u200b\u5305\u542b\u200b\u72ec\u7acb\u200b\u7684\u200b\u89e3\u91ca\u5668\u200b\u548c\u200b GIL\u3002</p> <p>\u200b\u4e00\u822c\u200b\u4f7f\u7528\u200b\u7684\u200b <code>Python</code> \u200b\u89e3\u91ca\u5668\u200b <code>CPython</code>\uff1a\u200b\u662f\u200b\u7528\u200b <code>C</code> \u200b\u8bed\u8a00\u200b\u5b9e\u73b0\u200b <code>Pyhon</code>\uff0c\u200b\u662f\u200b\u76ee\u524d\u200b\u5e94\u7528\u200b\u6700\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u89e3\u91ca\u5668\u200b\u3002\u200b\u5168\u5c40\u200b\u9501\u200b\u4f7f\u200b <code>Python</code> \u200b\u5728\u200b\u591a\u7ebf\u7a0b\u200b\u6548\u80fd\u200b\u4e0a\u200b\u8868\u73b0\u200b\u4e0d\u4f73\u200b\uff0c\u200b\u5168\u5c40\u200b\u89e3\u91ca\u5668\u200b\u9501\u200b\uff08<code>Global Interpreter Lock</code>\uff09\u200b\u662f\u200b <code>Python</code> \u200b\u7528\u4e8e\u200b\u540c\u6b65\u200b\u7ebf\u7a0b\u200b\u7684\u200b\u5de5\u5177\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u4efb\u4f55\u65f6\u523b\u200b\u4ec5\u200b\u6709\u200b\u4e00\u4e2a\u200b\u7ebf\u7a0b\u200b\u5728\u200b\u6267\u884c\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6bcf\u4e2a\u200b\u8fdb\u7a0b\u200b\u62e5\u6709\u200b\u72ec\u7acb\u200b\u7684\u200b\u89e3\u91ca\u5668\u200b\u548c\u200b <code>GIL</code>\uff0c\u200b\u6d88\u9664\u200b\u4e86\u200b\u6765\u81ea\u200b\u5355\u4e2a\u200b <code>Python</code> \u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u591a\u4e2a\u200b\u6267\u884c\u200b\u7ebf\u7a0b\u200b\uff0c\u200b\u6a21\u578b\u200b\u526f\u672c\u200b\u6216\u200b <code>GPU</code> \u200b\u7684\u200b\u989d\u5916\u200b\u89e3\u91ca\u5668\u200b\u5f00\u9500\u200b\u548c\u200b <code>GIL-thrashing</code> \uff0c\u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u51cf\u5c11\u200b\u89e3\u91ca\u5668\u200b\u548c\u200b <code>GIL</code> \u200b\u4f7f\u7528\u200b\u51b2\u7a81\u200b\u3002\u200b\u8fd9\u200b\u5bf9\u4e8e\u200b\u4e25\u91cd\u200b\u4f9d\u8d56\u200b <code>Python runtime</code> \u200b\u7684\u200b <code>models</code> \u200b\u800c\u8a00\u200b\uff0c\u200b\u6bd4\u5982\u8bf4\u200b\u5305\u542b\u200b <code>RNN</code> \u200b\u5c42\u200b\u6216\u200b\u5927\u91cf\u200b\u5c0f\u7ec4\u200b\u4ef6\u200b\u7684\u200b <code>models</code> \u200b\u800c\u8a00\u200b\uff0c\u200b\u8fd9\u200b\u5c24\u4e3a\u91cd\u8981\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#ddp_2","title":"DDP \u200b\u7684\u200b\u7f3a\u70b9","text":"<p>\u200b\u6682\u65f6\u200b\u6765\u8bf4\u200b\uff0c<code>DDP</code>\u200b\u662f\u200b\u91c7\u7528\u200b\u591a\u200b\u8fdb\u7a0b\u200b\u591a\u7ebf\u7a0b\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u8f83\u200b\u9ad8\u200b\uff0c\u200b\u4ed6\u200b\u7684\u200b\u7f3a\u70b9\u200b\u4e3b\u8981\u200b\u5c31\u662f\u200b\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u6bd4\u8f83\u200b\u591a\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6bd4\u200b<code>DP</code>\u200b\u7684\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u8f83\u4e3a\u200b\u7e41\u7410\u200b\u8bb8\u591a\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B/#_3","title":"\u53c2\u8003\u8d44\u6599\u200b\uff1a","text":"<ol> <li>Pytorch \u200b\u5e76\u884c\u200b\u8bad\u7ec3\u200b\uff08DP\uff0c DDP\uff09\u200b\u7684\u200b\u539f\u7406\u200b\u548c\u200b\u5e94\u7528\u200b</li> <li>Pytorch\u200b\u4e2d\u200b\u5355\u673a\u200b\u591a\u5361\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b</li> <li>Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</li> <li>DISTRIBUTEDDATAPARALLEL</li> <li>Pytorch \u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\uff08DP/DDP\uff09</li> <li>DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED</li> <li>pytorch\u200b\u591a\u200bgpu\u200b\u5e76\u884c\u200b\u8bad\u7ec3\u200b</li> </ol>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/","title":"AI\u200b\u786c\u4ef6\u52a0\u901f\u200b\u8bbe\u5907","text":"<p>\u200b\u5728\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u548c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u65f6\u200b\u4f1a\u200b\u53d7\u9650\u4e8e\u200bCPU\u200b\u548c\u200bGPU\u200b\u7684\u200b\u6027\u80fd\u200b\u3002\u200b\u8fd9\u65f6\u200b\uff0c\u200b\u4e13\u7528\u200b\u7684\u200bAI\u200b\u82af\u7247\u200b\u5c31\u200b\u663e\u5f97\u200b\u5c24\u4e3a\u91cd\u8981\u200b\u3002\u200b\u5728\u200b\u6b63\u5f0f\u200b\u5f00\u59cb\u200b\u672c\u200b\u8282\u200b\u5185\u5bb9\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u4e86\u89e3\u200b\u4e00\u4e0b\u200b\u4ec0\u4e48\u200b\u662f\u200bCPU\u200b\u548c\u200bGPU\u3002</p> <p>CPU\u200b\u5373\u200bCentral Processing Unit\uff0c\u200b\u4e2d\u6587\u540d\u200b\u4e3a\u200b\u4e2d\u592e\u5904\u7406\u5668\u200b\uff0c\u200b\u662f\u200b\u6211\u4eec\u200b\u7535\u8111\u200b\u4e2d\u200b\u7684\u200b\u6838\u5fc3\u200b\u914d\u4ef6\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u529f\u80fd\u200b\u4e3b\u8981\u200b\u662f\u200b\u5904\u7406\u200b\u6307\u4ee4\u200b\u3001\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u3001\u200b\u63a7\u5236\u200b\u65f6\u95f4\u200b\u3001\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5728\u200b\u73b0\u4ee3\u200b\u8ba1\u7b97\u673a\u200b\u4f53\u7cfb\u7ed3\u6784\u200b\u4e2d\u200b\uff0cCPU \u200b\u5bf9\u200b\u8ba1\u7b97\u673a\u200b\u7684\u200b\u6240\u6709\u200b\u786c\u4ef6\u8d44\u6e90\u200b\uff08\u200b\u5982\u200b\u5b58\u50a8\u5668\u200b\u3001\u200b\u8f93\u5165\u8f93\u51fa\u200b\u5355\u5143\u200b\uff09 \u200b\u8fdb\u884c\u200b\u63a7\u5236\u200b\u8c03\u914d\u200b\u3001\u200b\u6267\u884c\u200b\u901a\u7528\u200b\u8fd0\u7b97\u200b\u7684\u200b\u6838\u5fc3\u200b\u786c\u4ef6\u200b\u5355\u5143\u200b\u3002CPU \u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u7684\u200b\u8fd0\u7b97\u200b\u548c\u200b\u63a7\u5236\u200b\u6838\u5fc3\u200b\u3002\u200b\u8ba1\u7b97\u673a\u7cfb\u7edf\u200b\u4e2d\u200b\u6240\u6709\u200b\u8f6f\u4ef6\u200b\u5c42\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6700\u7ec8\u200b\u90fd\u200b\u5c06\u200b\u901a\u8fc7\u200b\u6307\u4ee4\u96c6\u200b\u6620\u5c04\u200b\u4e3a\u200bCPU\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>GPU\u200b\u5373\u200bGraphics Processing Unit\uff0c\u200b\u4e2d\u6587\u540d\u200b\u4e3a\u200b\u56fe\u5f62\u200b\u5904\u7406\u5355\u5143\u200b\u3002\u200b\u5728\u200b\u4f20\u7edf\u200b\u7684\u200b\u51af\u200b\u00b7\u200b\u8bfa\u200b\u4f9d\u66fc\u7ed3\u6784\u200b\u4e2d\u200b\uff0cCPU \u200b\u6bcf\u200b\u6267\u884c\u200b\u4e00\u6761\u200b\u6307\u4ee4\u200b\u90fd\u200b\u9700\u8981\u200b\u4ece\u200b\u5b58\u50a8\u5668\u200b\u4e2d\u200b\u8bfb\u53d6\u6570\u636e\u200b\uff0c\u200b\u6839\u636e\u200b\u6307\u4ee4\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u76f8\u5e94\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002\u200b\u4ece\u200b\u8fd9\u4e2a\u200b\u7279\u70b9\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0cCPU \u200b\u7684\u200b\u4e3b\u8981\u804c\u8d23\u200b\u5e76\u200b\u4e0d\u200b\u53ea\u662f\u200b\u6570\u636e\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u6267\u884c\u200b\u5b58\u50a8\u200b\u8bfb\u53d6\u200b\u3001\u200b\u6307\u4ee4\u200b\u5206\u6790\u200b\u3001\u200b\u5206\u652f\u200b\u8df3\u8f6c\u200b\u7b49\u200b\u547d\u4ee4\u200b\u3002\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u6d77\u91cf\u200b\u7684\u200b\u6570\u636e\u5904\u7406\u200b\uff0c\u200b\u7528\u200b CPU \u200b\u6267\u884c\u200b\u7b97\u6cd5\u200b\u65f6\u200b\uff0cCPU \u200b\u5c06\u200b\u82b1\u8d39\u200b\u5927\u91cf\u200b\u7684\u200b\u65f6\u95f4\u200b\u5728\u200b\u6570\u636e\u200b/\u200b\u6307\u4ee4\u200b\u7684\u200b\u8bfb\u53d6\u200b\u5206\u6790\u200b\u4e0a\u200b\uff0c\u200b\u800c\u200b CPU\u200b\u7684\u200b\u9891\u7387\u200b\u3001\u200b\u5185\u5b58\u200b\u7684\u200b\u5e26\u5bbd\u200b\u7b49\u200b\u6761\u4ef6\u200b\u53c8\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u65e0\u200b\u9650\u5236\u200b\u63d0\u9ad8\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9650\u5236\u200b\u4e86\u200b\u5904\u7406\u5668\u200b\u7684\u200b\u6027\u80fd\u200b\u3002\u200b\u800c\u200b GPU \u200b\u7684\u200b\u63a7\u5236\u200b\u76f8\u5bf9\u200b\u7b80\u5355\u200b\uff0c\u200b\u5927\u90e8\u5206\u200b\u7684\u200b\u6676\u4f53\u7ba1\u200b\u53ef\u4ee5\u200b\u7ec4\u6210\u200b\u5404\u7c7b\u200b\u4e13\u7528\u200b\u7535\u8def\u200b\u3001\u200b\u591a\u6761\u200b\u6d41\u6c34\u7ebf\u200b\uff0c\u200b\u4f7f\u5f97\u200b GPU \u200b\u7684\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u8fdc\u9ad8\u4e8e\u200bCPU\uff1b\u200b\u540c\u65f6\u200b GPU \u200b\u62e5\u6709\u200b\u4e86\u200b\u66f4\u52a0\u200b\u5f3a\u5927\u200b\u7684\u200b\u6d6e\u70b9\u8fd0\u7b97\u200b\u80fd\u529b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7f13\u89e3\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96be\u9898\u200b\uff0c\u200b\u91ca\u653e\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u6f5c\u80fd\u200b\u3002</p> <p>\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0cGPU\u200b\u6ca1\u6709\u200b\u72ec\u7acb\u200b\u5de5\u4f5c\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u5fc5\u987b\u200b\u7531\u200bCPU\u200b\u8fdb\u884c\u200b\u63a7\u5236\u200b\u8c03\u7528\u200b\u624d\u80fd\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u4e14\u200bGPU\u200b\u7684\u200b\u529f\u8017\u200b\u4e00\u822c\u200b\u6bd4\u8f83\u200b\u9ad8\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u968f\u7740\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u4e0d\u65ad\u200b\u53d1\u5c55\u200b\uff0c\u200b\u9ad8\u529f\u8017\u200b\u4f4e\u6548\u7387\u200b\u7684\u200bGPU\u200b\u4e0d\u518d\u200b\u80fd\u200b\u6ee1\u8db3\u200bAI\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8981\u6c42\u200b\uff0c\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4e00\u5927\u6279\u200b\u529f\u80fd\u200b\u76f8\u5bf9\u200b\u5355\u4e00\u200b\uff0c\u200b\u4f46\u200b\u901f\u5ea6\u200b\u66f4\u5feb\u200b\u7684\u200b\u4e13\u7528\u200b\u96c6\u6210\u7535\u8def\u200b\u76f8\u7ee7\u95ee\u4e16\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u4e86\u89e3\u200b\u4e00\u4e0b\u200b\u4ec0\u4e48\u200b\u662f\u200b\u4e13\u7528\u200b\u96c6\u6210\u7535\u8def\u200b\uff1a</p> <p>\u200b\u4e13\u7528\u200b\u96c6\u6210\u7535\u8def\u200b\uff08Application-Specific Integrated Circuit\uff0cASIC\uff09\u200b\u662f\u200b\u4e13\u7528\u200b\u5b9a\u5236\u200b\u82af\u7247\u200b\uff0c\u200b\u5373\u200b\u4e3a\u200b\u5b9e\u73b0\u200b\u7279\u5b9a\u200b\u8981\u6c42\u200b\u800c\u200b\u5b9a\u5236\u200b\u7684\u200b\u82af\u7247\u200b\u3002\u200b\u5b9a\u5236\u200b\u7684\u200b\u7279\u6027\u200b\u6709\u52a9\u4e8e\u200b\u63d0\u9ad8\u200b ASIC \u200b\u7684\u200b\u6027\u80fd\u200b\u529f\u8017\u200b\u6bd4\u200b\u3002ASIC\u200b\u7684\u200b\u7f3a\u70b9\u200b\u662f\u200b\u7535\u8def\u8bbe\u8ba1\u200b\u9700\u8981\u200b\u5b9a\u5236\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u5f00\u53d1\u5468\u671f\u200b\u957f\u200b\uff0c\u200b\u529f\u80fd\u200b\u96be\u4ee5\u200b\u6269\u5c55\u200b\u3002\u200b\u4f46\u200b\u5728\u200b\u529f\u8017\u200b\u3001\u200b\u53ef\u9760\u6027\u200b\u3001\u200b\u96c6\u6210\u5ea6\u200b\u7b49\u200b\u65b9\u9762\u200b\u90fd\u200b\u6709\u200b\u4f18\u52bf\u200b\uff0c\u200b\u5c24\u5176\u200b\u5728\u200b\u8981\u6c42\u200b\u9ad8\u6027\u80fd\u200b\u3001\u200b\u4f4e\u529f\u8017\u200b\u7684\u200b\u79fb\u52a8\u200b\u5e94\u7528\u200b\u7aef\u200b\u4f53\u73b0\u200b\u660e\u663e\u200b\u3002\u200b\u4e0b\u6587\u200b\u63d0\u5230\u200b\u7684\u200b\u8c37\u6b4c\u200b\u7684\u200bTPU\uff0c\u200b\u5bd2\u6b66\u7eaa\u200b\u7684\u200bNPU\u200b\u90fd\u200b\u5c5e\u4e8e\u200bASIC\u200b\u7684\u200b\u8303\u7574\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u5165\u200b\u672c\u8282\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4ec0\u4e48\u200b\u662f\u200bTPU</li> <li>\u200b\u4ec0\u4e48\u200b\u662f\u200bNPU</li> </ul>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#241-tpu","title":"2.4.1 TPU","text":"<p>TPU\u200b\u5373\u200bTensor Processing Unit\uff0c\u200b\u4e2d\u6587\u540d\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u5904\u7406\u5668\u200b\u30022006\u200b\u5e74\u200b\uff0c\u200b\u8c37\u6b4c\u200b\u5f00\u59cb\u200b\u8ba1\u5212\u200b\u4e3a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u4e13\u7528\u200b\u7684\u200b\u96c6\u6210\u7535\u8def\u200b\uff08ASIC\uff09\u3002\u200b\u968f\u7740\u200b\u8ba1\u7b97\u200b\u9700\u6c42\u200b\u548c\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u4e0d\u65ad\u200b\u4e0a\u6da8\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u9700\u6c42\u200b\u5728\u200b2013\u200b\u5e74\u200b\u5f00\u59cb\u200b\u53d8\u5f97\u200b\u5c24\u4e3a\u200b\u7d27\u8feb\u200b\u3002\u200b\u4e8e\u662f\u200b\uff0c\u200b\u8c37\u6b4c\u200b\u5728\u200b2015\u200b\u5e74\u200b6\u200b\u6708\u200b\u7684\u200bIO\u200b\u5f00\u53d1\u8005\u200b\u5927\u4f1a\u200b\u4e0a\u200b\u63a8\u51fa\u200b\u4e86\u200b\u4e3a\u200b\u4f18\u5316\u200b\u81ea\u8eab\u200b\u7684\u200bTensorFlow\u200b\u6846\u67b6\u200b\u800c\u200b\u8bbe\u8ba1\u200b\u6253\u9020\u200b\u7684\u200b\u4e00\u6b3e\u200b\u8ba1\u7b97\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e13\u7528\u200b\u82af\u7247\u200b\u3002\u200b\u5b83\u200b\u4e3b\u8981\u200b\u7528\u4e8e\u200b\u8fdb\u884c\u200b\u641c\u7d22\u200b\uff0c\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8bed\u97f3\u200b\u7b49\u200b\u6a21\u578b\u200b\u548c\u200b\u6280\u672f\u200b\u7684\u200b\u5904\u7406\u200b\u3002</p> <p>\u200b\u622a\u81f3\u200b\u76ee\u524d\u200b\uff0c\u200b\u8c37\u6b4c\u200b\u5df2\u7ecf\u200b\u53d1\u884c\u200b\u4e86\u200b\u56db\u4ee3\u200bTPU\u200b\u82af\u7247\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#_1","title":"\u82af\u7247\u200b\u67b6\u6784\u8bbe\u8ba1","text":"<p>TPU\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u67b6\u6784\u200b\u5982\u4e0b\u200b\u56fe\u200b</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aIn-datacenter performance analysis of a tensor processing unit\uff0cfigure 1</p> <p>\u200b\u7531\u200b\u4e0a\u200b\u56fe\u200b\u53ef\u89c1\u200b\uff0c\u200b\u6574\u4e2a\u200bTPU\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5355\u5143\u200b\u662f\u200b\u53f3\u4e0a\u89d2\u200b\u9ec4\u8272\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u200b\u5355\u5143\u200b\u201cMatrix Multiply Unit\u201d\uff0c\u200b\u5b83\u200b\u5305\u542b\u200b256x256\u200b\u4e2a\u200bMAC\u200b\u90e8\u4ef6\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u6267\u884c\u200b\u6709\u200b\u7b26\u53f7\u200b\u6216\u8005\u200b\u65e0\u200b\u7b26\u53f7\u200b\u7684\u200b8\u200b\u4f4d\u4e58\u52a0\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u8f93\u5165\u200b\u4e3a\u200b\u6743\u91cd\u200b\u6570\u636e\u200b\u961f\u5217\u200bFIFO\u200b\u548c\u200b\u7edf\u4e00\u200b\u7f13\u51b2\u200bUnified Buffer\uff0c\u200b\u5373\u56fe\u200b\u4e2d\u200b\u6307\u5411\u200b\u5b83\u200b\u7684\u200b\u4e24\u4e2a\u200b\u84dd\u8272\u200b\u90e8\u5206\u200b\u3002\u200b\u5728\u200b\u8ba1\u7b97\u200b\u7ed3\u675f\u200b\u540e\u200b\uff0c16\u200b\u4f4d\u200b\u7ed3\u679c\u200b\u88ab\u200b\u6536\u96c6\u200b\u5e76\u200b\u4f20\u9012\u200b\u5230\u200b\u4f4d\u4e8e\u200b\u77e9\u9635\u200b\u5355\u5143\u200b\u4e0b\u65b9\u200b\u7684\u200b4MiB 32\u200b\u4f4d\u200b\u84dd\u8272\u200b\u7d2f\u52a0\u5668\u200bAccumulators\u200b\u4e2d\u200b\uff0c\u200b\u4e4b\u540e\u200b\u7531\u200b\u9ec4\u8272\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u5355\u5143\u200b\u5728\u200b\u7d2f\u52a0\u200b\u540e\u200b\u6267\u884c\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u6700\u7ec8\u200b\u5c06\u200b\u6570\u636e\u200b\u8fd4\u56de\u200b\u7ed9\u200b\u7edf\u4e00\u200b\u7f13\u51b2\u200b\u3002</p> <p>Matrix Multiply Unit\u200b\u77e9\u9635\u200b\u5904\u7406\u5668\u200b\u4f5c\u4e3a\u200bTPU\u200b\u7684\u200b\u6838\u5fc3\u200b\u90e8\u5206\u200b\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5728\u200b\u5355\u4e2a\u200b\u65f6\u949f\u200b\u5468\u671f\u200b\u5185\u200b\u5904\u7406\u200b\u6570\u5341\u4e07\u200b\u6b21\u200b\u77e9\u9635\u200b\uff08Matrix\uff09\u200b\u8fd0\u7b97\u200b\u3002MMU\u200b\u6709\u7740\u200b\u4e0e\u200b\u4f20\u7edf\u200bCPU\u3001GPU\u200b\u622a\u7136\u4e0d\u540c\u200b\u7684\u200b\u67b6\u6784\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u8109\u52a8\u9635\u5217\u200b\uff08systolic array\uff09\u3002\u200b\u4e4b\u6240\u4ee5\u200b\u53eb\u200b\u201c\u200b\u8109\u52a8\u200b\u201d\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u5728\u200b\u8fd9\u79cd\u200b\u7ed3\u6784\u200b\u4e2d\u200b\uff0c\u200b\u6570\u636e\u200b\u4e00\u200b\u6ce2\u4e00\u6ce2\u200b\u5730\u200b\u6d41\u8fc7\u200b\u82af\u7247\u200b\uff0c\u200b\u4e0e\u200b\u5fc3\u810f\u200b\u8df3\u52a8\u200b\u4f9b\u8840\u200b\u7684\u200b\u65b9\u5f0f\u200b\u7c7b\u4f3c\u200b\u3002\u200b\u800c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cCPU\u200b\u548c\u200bGPU\u200b\u5728\u200b\u6bcf\u6b21\u200b\u8fd0\u7b97\u200b\u4e2d\u200b\u90fd\u200b\u9700\u8981\u200b\u4ece\u200b\u591a\u4e2a\u200b\u5bc4\u5b58\u5668\u200b\uff08register\uff09\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u5b58\u53d6\u200b\uff0c\u200b\u800c\u200bTPU\u200b\u7684\u200b\u8109\u52a8\u9635\u5217\u200b\u5c06\u200b\u591a\u4e2a\u200b\u8fd0\u7b97\u200b\u903b\u8f91\u200b\u5355\u5143\u200b\uff08ALU\uff09\u200b\u4e32\u8054\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u590d\u7528\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u5bc4\u5b58\u5668\u200b\u4e2d\u200b\u8bfb\u53d6\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u6bcf\u4e2a\u200bALU\u200b\u5355\u5143\u200b\u7ed3\u6784\u200b\u7b80\u5355\u200b\uff0c\u200b\u4e00\u822c\u200b\u53ea\u200b\u5305\u542b\u200b\u4e58\u6cd5\u5668\u200b\u3001\u200b\u52a0\u6cd5\u5668\u200b\u4ee5\u53ca\u200b\u5bc4\u5b58\u5668\u200b\u4e09\u200b\u90e8\u5206\u200b\uff0c\u200b\u9002\u5408\u200b\u5927\u91cf\u200b\u5806\u780c\u200b\u3002</p> <p></p> <p>\u200b\u4f46\u662f\u200b\uff0c\u200b\u5728\u200b\u6781\u5927\u200b\u589e\u52a0\u200b\u6570\u636e\u200b\u590d\u7528\u200b\u3001\u200b\u964d\u4f4e\u200b\u5185\u5b58\u200b\u5e26\u5bbd\u200b\u538b\u529b\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u8109\u52a8\u9635\u5217\u200b\u4e5f\u200b\u6709\u200b\u4e24\u4e2a\u200b\u7f3a\u70b9\u200b\uff0c\u200b\u5373\u200b\u6570\u636e\u200b\u91cd\u6392\u200b\u548c\u200b\u89c4\u6a21\u200b\u9002\u914d\u200b\u3002\u200b\u7b2c\u4e00\u200b\uff0c\u200b\u8109\u52a8\u200b\u77e9\u9635\u200b\u4e3b\u8981\u200b\u5b9e\u73b0\u200b\u5411\u91cf\u200b/\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002\u200b\u4ee5\u200bCNN\u200b\u8ba1\u7b97\u200b\u4e3a\u4f8b\u200b\uff0cCNN\u200b\u6570\u636e\u200b\u8fdb\u5165\u200b\u8109\u52a8\u9635\u5217\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u597d\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4e25\u683c\u200b\u9075\u5faa\u200b\u65f6\u949f\u200b\u8282\u62cd\u200b\u548c\u200b\u7a7a\u95f4\u200b\u987a\u5e8f\u200b\u8f93\u5165\u200b\u3002\u200b\u6570\u636e\u200b\u91cd\u6392\u200b\u7684\u200b\u989d\u5916\u200b\u64cd\u4f5c\u200b\u589e\u52a0\u200b\u4e86\u200b\u590d\u6742\u6027\u200b\u3002\u200b\u7b2c\u4e8c\u200b\uff0c\u200b\u5728\u200b\u6570\u636e\u200b\u6d41\u7ecf\u200b\u6574\u4e2a\u200b\u9635\u5217\u200b\u540e\u200b\uff0c\u200b\u624d\u80fd\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u3002\u200b\u5f53\u200b\u8ba1\u7b97\u200b\u7684\u200b\u5411\u91cf\u200b\u4e2d\u200b\u5143\u7d20\u200b\u8fc7\u5c11\u200b\uff0c\u200b\u8109\u52a8\u9635\u5217\u200b\u89c4\u6a21\u200b\u8fc7\u5927\u65f6\u200b\uff0c\u200b\u4e0d\u4ec5\u200b\u96be\u4ee5\u200b\u5c06\u200b\u9635\u5217\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5355\u5143\u200b\u90fd\u200b\u5229\u7528\u200b\u8d77\u6765\u200b\uff0c\u200b\u6570\u636e\u200b\u7684\u200b\u5bfc\u5165\u200b\u548c\u200b\u5bfc\u51fa\u200b\u5ef6\u65f6\u200b\u4e5f\u200b\u968f\u7740\u200b\u5c3a\u5bf8\u200b\u6269\u5927\u200b\u800c\u200b\u589e\u52a0\u200b\uff0c\u200b\u964d\u4f4e\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u6548\u7387\u200b\u3002\u200b\u56e0\u6b64\u200b\u5728\u200b\u786e\u5b9a\u200b\u8109\u52a8\u9635\u5217\u200b\u7684\u200b\u89c4\u6a21\u200b\u65f6\u200b\uff0c\u200b\u5728\u200b\u8003\u8651\u200b\u9762\u79ef\u200b\u3001\u200b\u80fd\u8017\u200b\u3001\u200b\u5cf0\u503c\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u8fd8\u8981\u200b\u8003\u8651\u200b\u5178\u578b\u200b\u5e94\u7528\u200b\u4e0b\u200b\u7684\u200b\u6548\u7387\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#_2","title":"\u6280\u672f\u200b\u7279\u70b9","text":""},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#ai_1","title":"AI\u200b\u52a0\u901f\u200b\u4e13\u7528","text":"<p>TPU\u200b\u7684\u200b\u67b6\u6784\u200b\u5c5e\u4e8e\u200bDomain-specific Architecture\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u7279\u5b9a\u200b\u9886\u57df\u200b\u67b6\u6784\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u5b9a\u4f4d\u200b\u51c6\u786e\u200b\uff0c\u200b\u67b6\u6784\u200b\u7b80\u5355\u200b\uff0c\u200b\u5355\u7ebf\u7a0b\u200b\u63a7\u5236\u200b\uff0c\u200b\u5b9a\u5236\u200b\u6307\u4ee4\u96c6\u200b\u4f7f\u5f97\u200b\u5b83\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u8fd0\u7b97\u200b\u65b9\u9762\u200b\u6548\u7387\u200b\u6781\u9ad8\u200b\uff0c\u200b\u4e14\u200b\u5bb9\u6613\u200b\u6269\u5c55\u200b\u3002\u200b\u76f8\u6bd4\u4e4b\u4e0b\u200b\uff0c\u200b\u4f20\u7edf\u200b\u8bf8\u5982\u200bCPU\u3001GPU\u200b\u7b49\u200b\u901a\u7528\u200b\u5904\u7406\u5668\u200b\u5fc5\u987b\u200b\u8003\u8651\u200b\u7075\u6d3b\u6027\u200b\u548c\u200b\u517c\u5bb9\u6027\u200b\uff0c\u200b\u6709\u200b\u592a\u91cd\u200b\u7684\u200b\u5305\u88b1\u200b\u3002\u200b\u4f46\u200bTPU\u200b\u8fd9\u79cd\u200b\u7279\u70b9\u200b\u4e5f\u200b\u51b3\u5b9a\u200b\u5b83\u200b\u53ea\u80fd\u200b\u88ab\u200b\u9650\u5236\u200b\u7528\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u52a0\u901f\u200b\u573a\u666f\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#_3","title":"\u8109\u52a8\u9635\u5217\u200b\u8bbe\u8ba1","text":"<p>TPU\u200b\u91c7\u7528\u200b\u4e86\u200b\u4e0e\u200b\u4f20\u7edf\u200bCPU\u200b\u548c\u200bGPU\u200b\u622a\u7136\u4e0d\u540c\u200b\u7684\u200b\u8109\u52a8\u9635\u5217\u200b\uff08systolic array\uff09\u200b\u7ed3\u6784\u200b\u6765\u200b\u52a0\u901f\u200bAI\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u8109\u52a8\u9635\u5217\u200b\u80fd\u591f\u200b\u5728\u200b\u4e00\u4e2a\u200b\u65f6\u949f\u200b\u5468\u671f\u200b\u5185\u200b\u5904\u7406\u200b\u6570\u5341\u4e07\u200b\u6b21\u200b\u77e9\u9635\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u5728\u200b\u6bcf\u6b21\u200b\u8fd0\u7b97\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0cTPU\u200b\u80fd\u591f\u200b\u5c06\u200b\u591a\u4e2a\u200b\u8fd0\u7b97\u200b\u903b\u8f91\u200b\u5355\u5143\u200b\uff08ALU\uff09\u200b\u4e32\u8054\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u5e76\u200b\u590d\u7528\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u5bc4\u5b58\u5668\u200b\u4e2d\u200b\u53d6\u5f97\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u8fd9\u79cd\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u4e0d\u4ec5\u200b\u80fd\u591f\u200b\u5c06\u200b\u6570\u636e\u200b\u590d\u7528\u200b\u5b9e\u73b0\u200b\u6700\u5927\u5316\u200b\uff0c\u200b\u51cf\u5c11\u200b\u82af\u7247\u200b\u5728\u200b\u8fd0\u7b97\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u5185\u5b58\u200b\u8bbf\u95ee\u200b\u6b21\u6570\u200b\uff0c\u200b\u63d0\u9ad8\u200bAI\u200b\u8ba1\u7b97\u200b\u6548\u7387\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u964d\u4f4e\u200b\u4e86\u200b\u5185\u5b58\u200b\u5e26\u5bbd\u200b\u538b\u529b\u200b\uff0c\u200b\u8fdb\u800c\u200b\u964d\u4f4e\u200b\u5185\u5b58\u200b\u8bbf\u95ee\u200b\u7684\u200b\u80fd\u8017\u200b\u3002</p> <p>MMU\u200b\u7684\u200b\u8109\u52a8\u9635\u5217\u200b\u5305\u542b\u200b256 \u00d7 256 = 65,536\u200b\u4e2a\u200bALU\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200bTPU\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u53ef\u4ee5\u200b\u5904\u7406\u200b65,536\u200b\u6b21\u200b8\u200b\u4f4d\u200b\u6574\u6570\u200b\u7684\u200b\u4e58\u6cd5\u200b\u548c\u200b\u52a0\u6cd5\u200b\u3002</p> <p>TPU\u200b\u4ee5\u200b700\u200b\u5146\u8d6b\u5179\u200b\u7684\u200b\u529f\u7387\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u6bcf\u79d2\u200b\u53ef\u4ee5\u200b\u8fd0\u884c\u200b65,536 \u00d7 700,000,000 = 46 \u00d7 1012\u200b\u6b21\u200b\u4e58\u6cd5\u200b\u548c\u200b\u52a0\u6cd5\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u6216\u200b\u6bcf\u79d2\u200b92\u200b\u4e07\u4ebf\u200b\uff0892 \u00d7 1012\uff09\u200b\u6b21\u200b\u77e9\u9635\u200b\u5355\u5143\u200b\u4e2d\u200b\u7684\u200b\u8fd0\u7b97\u200b\u3002</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aIn-datacenter performance analysis of a tensor processing unit\uff0cfigure 4</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#_4","title":"\u786e\u5b9a\u200b\u6027\u529f\u80fd\u200b\u548c\u200b\u5927\u89c4\u6a21\u200b\u7247\u4e0a\u200b\u5185\u5b58","text":"<p>\u200b\u5982\u56fe\u200b\u662f\u200bTPU\u200b\u7684\u200b\u5e73\u9762\u8bbe\u8ba1\u200b\u7b80\u56fe\u200b\uff0c\u200b\u9ec4\u8272\u200b\u4e3a\u200bMMU\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\uff0c\u200b\u84dd\u8272\u200b\u662f\u200b\u7edf\u4e00\u200b\u7f13\u5b58\u200b\u548c\u200b\u7d2f\u52a0\u5668\u200b\u7b49\u200b\u6570\u636e\u200b\u5355\u5143\u200b\uff0c\u200b\u7eff\u8272\u200b\u662f\u200bI/O\uff0c\u200b\u7ea2\u8272\u200b\u662f\u200b\u903b\u8f91\u200b\u63a7\u5236\u200b\u5355\u5143\u200b\u3002</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aIn-datacenter performance analysis of a tensor processing unit\uff0cfigure 2</p> <p>\u200b\u4f20\u7edf\u200bGPU\u200b\u7531\u4e8e\u200b\u7247\u4e0a\u200b\u5185\u5b58\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u8fd0\u884c\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9700\u8981\u200b\u4e0d\u65ad\u200b\u5730\u53bb\u200b\u8bbf\u95ee\u200b\u7247\u5916\u200b\u52a8\u6001\u200b\u968f\u673a\u5b58\u53d6\u200b\u5b58\u50a8\u5668\u200b\uff08DRAM\uff09\uff0c\u200b\u4ece\u800c\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u6d6a\u8d39\u200b\u4e86\u200b\u4e0d\u5fc5\u8981\u200b\u7684\u200b\u80fd\u8017\u200b\u3002\u200b\u4e0e\u200bCPU\u200b\u548c\u200bGPU\u200b\u76f8\u6bd4\u200b\uff0cTPU\u200b\u7684\u200b\u63a7\u5236\u200b\u5355\u5143\u200b\u66f4\u200b\u5c0f\u200b\uff0c\u200b\u66f4\u200b\u5bb9\u6613\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u9762\u79ef\u200b\u53ea\u200b\u5360\u200b\u4e86\u200b\u6574\u4e2a\u200b\u51b2\u6a21\u200b\u7684\u200b2%\uff0c\u200b\u7ed9\u7247\u200b\u4e0a\u200b\u5b58\u50a8\u5668\u200b\u548c\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\u7559\u4e0b\u200b\u4e86\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u7a7a\u95f4\u200b\u3002\u200b\u5982\u4e0a\u56fe\u200b\u6240\u793a\u200b\u7684\u200bTPU\u200b\u4e00\u4ee3\u200b\u67b6\u6784\u200b\u4e2d\u200b\uff0c\u200b\u603b\u5171\u200b\u8bbe\u8ba1\u200b\u4e86\u200b\u5360\u200b\u603b\u200b\u82af\u7247\u200b\u9762\u79ef\u200b35%\u200b\u7684\u200b\u5185\u5b58\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u62ec\u200b24MB\u200b\u7684\u200b\u5c40\u90e8\u200b\u5185\u5b58\u200b\u30014MB\u200b\u7684\u200b\u7d2f\u52a0\u5668\u200b\u5185\u5b58\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u7528\u4e8e\u200b\u4e0e\u200b\u4e3b\u63a7\u200b\u5904\u7406\u5668\u200b\u5bf9\u63a5\u200b\u7684\u200b\u5185\u5b58\u200b\u3002\u200b\u8fd9\u4e00\u200b\u6bd4\u4f8b\u200b\u5927\u5927\u200b\u8d85\u51fa\u200b\u4e86\u200bGPU\u200b\u7b49\u200b\u901a\u7528\u200b\u5904\u7406\u5668\u200b\uff0c\u200b\u8282\u7ea6\u200b\u4e86\u200b\u5927\u91cf\u200b\u7247\u5916\u200b\u6570\u636e\u200b\u8bbf\u5b58\u200b\u80fd\u8017\u200b\uff0c\u200b\u4f7f\u5f97\u200bTPU\u200b\u8ba1\u7b97\u200b\u7684\u200b\u80fd\u6548\u200b\u6bd4\u200b\u5927\u5927\u63d0\u9ad8\u200b\u3002\u200b\u4ece\u200bTPU\u200b\u4e8c\u4ee3\u200b\u5f00\u59cb\u200b\u91c7\u7528\u200bHBM\u200b\u7247\u4e0a\u200b\u9ad8\u5e26\u5bbd\u200b\u5185\u5b58\u200b\uff0c\u200b\u867d\u7136\u200b\u548c\u200b\u6700\u65b0\u200b\u4e00\u4ee3\u200bGPU\u200b\u7247\u4e0a\u200b\u5185\u5b58\u200b\u6280\u672f\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u662f\u200bTPU\u200b\u82af\u7247\u200b\u7684\u200b\u9762\u79ef\u200b\u8981\u200b\u8fdc\u8fdc\u200b\u5c0f\u4e8e\u200bGPU\u3002\u200b\u7845\u7247\u200b\u8d8a\u5c0f\u200b\uff0c\u200b\u6210\u672c\u200b\u8d8a\u4f4e\u200b\uff0c\u200b\u826f\u54c1\u7387\u200b\u4e5f\u200b\u8d8a\u200b\u9ad8\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u7531\u4e8e\u200bTPU\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u200b\u7528\u9014\u200b\u82af\u7247\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u7f13\u5b58\u200b\u3001\u200b\u5206\u652f\u200b\u9884\u6d4b\u200b\u3001\u200b\u591a\u9053\u200b\u5904\u7406\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002\u200b\u8fd9\u200b\u5c31\u200b\u610f\u5473\u7740\u200bTPU\u200b\u7684\u200b\u529f\u80fd\u200b\u662f\u200b\u5355\u4e00\u200b\u4e14\u200b\u786e\u5b9a\u200b\u7684\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bTPU\u200b\u8f7b\u6613\u200b\u7684\u200b\u9884\u6d4b\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u80fd\u200b\u8ba9\u200b\u82af\u7247\u200b\u4ee5\u200b\u541e\u5410\u91cf\u200b\u63a5\u8fd1\u200b\u5cf0\u503c\u200b\u7684\u200b\u72b6\u6001\u200b\u8fd0\u884c\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e25\u683c\u63a7\u5236\u200b\u5ef6\u8fdf\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#242-npu","title":"2.4.2 NPU","text":"<p>NPU\u200b\u5373\u200bNeural-network Processing Unit\uff0c\u200b\u4e2d\u6587\u540d\u200b\u4e3a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5904\u7406\u5668\u200b\uff0c\u200b\u5b83\u200b\u91c7\u7528\u200b\u201c\u200b\u6570\u636e\u200b\u9a71\u52a8\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u201d\u200b\u7684\u200b\u67b6\u6784\u200b\uff0c\u200b\u7279\u522b\u200b\u64c5\u957f\u200b\u5904\u7406\u200b\u89c6\u9891\u200b\u3001\u200b\u56fe\u50cf\u200b\u7c7b\u200b\u7684\u200b\u6d77\u91cf\u200b\u591a\u5a92\u4f53\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u957f\u671f\u4ee5\u6765\u200b\uff0c\u200b\u5e94\u7528\u200b\u9700\u6c42\u200b\u4e00\u76f4\u200b\u7275\u52a8\u200b\u7740\u200b\u5d4c\u5165\u5f0f\u200b\u6280\u672f\u200b\u7684\u200b\u53d1\u5c55\u200b\u65b9\u5411\u200b\u3002\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5174\u8d77\u200b\uff0c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u3001\u200b\u5927\u200b\u6570\u636e\u200b\u65f6\u4ee3\u200b\u7684\u200b\u6765\u4e34\u200b\uff0cCPU\u200b\u548c\u200bGPU\u200b\u6e10\u6e10\u200b\u96be\u4ee5\u200b\u6ee1\u8db3\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u9700\u8981\u200b\uff0c\u200b\u9762\u5bf9\u200b\u65e5\u6e10\u200b\u65fa\u76db\u200b\u7684\u200b\u9700\u6c42\u200b\u548c\u200b\u5e7f\u5927\u200b\u7684\u200b\u9884\u671f\u200b\u5e02\u573a\u200b\uff0c\u200b\u8bbe\u8ba1\u200b\u4e00\u6b3e\u200b\u4e13\u95e8\u200b\u7528\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u9ad8\u6548\u200b\u667a\u80fd\u200b\u5904\u7406\u5668\u200b\u663e\u5f97\u200b\u5341\u5206\u5fc5\u8981\u200b\uff0c\u200b\u56e0\u6b64\u200bNPU\u200b\u5e94\u8fd0\u800c\u751f\u200b\u3002</p> <p>\u200b\u4ece\u200b\u6280\u672f\u200b\u89d2\u5ea6\u770b\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u4e00\u7c7b\u200b\u591a\u5c42\u200b\u5927\u89c4\u6a21\u200b\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u5b83\u200b\u6a21\u4eff\u200b\u751f\u7269\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u800c\u200b\u6784\u5efa\u200b\uff0c\u200b\u7531\u200b\u82e5\u5e72\u200b\u4eba\u5de5\u200b\u795e\u7ecf\u5143\u200b\u7ed3\u70b9\u200b\u4e92\u8054\u200b\u800c\u6210\u200b\u3002\u200b\u795e\u7ecf\u5143\u200b\u4e4b\u95f4\u200b\u901a\u8fc7\u200b\u7a81\u89e6\u200b\u4e24\u200b\u4e24\u200b\u8fde\u63a5\u200b\uff0c\u200b\u7a81\u89e6\u200b\u8bb0\u5f55\u200b\u4e86\u200b\u795e\u7ecf\u5143\u200b\u95f4\u200b\u8054\u7cfb\u200b\u7684\u200b\u6743\u503c\u200b\u5f3a\u5f31\u200b\u3002\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u672c\u64cd\u4f5c\u200b\u662f\u200b\u795e\u7ecf\u5143\u200b\u548c\u200b\u7a81\u89e6\u200b\u7684\u200b\u5904\u7406\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5b58\u50a8\u200b\u548c\u200b\u5904\u7406\u200b\u662f\u200b\u4e00\u4f53\u5316\u200b\u7684\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u901a\u8fc7\u200b\u7a81\u89e6\u200b\u6743\u91cd\u200b\u6765\u200b\u4f53\u73b0\u200b\uff0c\u200b\u800c\u200b\u51af\u200b\u00b7\u200b\u8bfa\u4f0a\u66fc\u200b\u7ed3\u6784\u200b\u4e2d\u200b\uff0c\u200b\u5b58\u50a8\u200b\u548c\u200b\u5904\u7406\u200b\u662f\u200b\u5206\u79bb\u200b\u7684\u200b\uff0c\u200b\u5206\u522b\u200b\u7531\u200b\u5b58\u50a8\u5668\u200b\u548c\u200b\u8fd0\u7b97\u5668\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4e8c\u8005\u4e4b\u95f4\u200b\u5b58\u5728\u200b\u5de8\u5927\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002\u200b\u5f53\u200b\u7528\u200b\u73b0\u6709\u200b\u7684\u200b\u57fa\u4e8e\u200b\u51af\u200b\u00b7\u200b\u8bfa\u4f0a\u66fc\u200b\u7ed3\u6784\u200b\u7684\u200b\u7ecf\u5178\u200b\u8ba1\u7b97\u673a\u200b(\u200b\u5982\u200bX86\u200b\u5904\u7406\u5668\u200b\u548c\u200b\u82f1\u200b\u4f1f\u8fbe\u200bGPU)\u200b\u8fd0\u884c\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5e94\u7528\u200b\u65f6\u200b\uff0c\u200b\u5c31\u200b\u4e0d\u53ef\u907f\u514d\u200b\u5730\u200b\u53d7\u5230\u200b\u5b58\u50a8\u200b\u548c\u200b\u5904\u7406\u200b\u5206\u79bb\u5f0f\u200b\u7ed3\u6784\u200b\u7684\u200b\u5236\u7ea6\u200b\uff0c\u200b\u56e0\u800c\u200b\u5f71\u54cd\u200b\u6548\u7387\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e13\u95e8\u200b\u9488\u5bf9\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u4e13\u4e1a\u200b\u82af\u7247\u200bNPU\u200b\u66f4\u200b\u6709\u200b\u7814\u53d1\u200b\u7684\u200b\u5fc5\u8981\u200b\u548c\u200b\u9700\u6c42\u200b\u3002</p> <p>\u200b\u5728\u200bNPU\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u4e0a\u200b\uff0c\u200b\u4e2d\u56fd\u200b\u8d70\u200b\u5728\u200b\u4e86\u200b\u4e16\u754c\u200b\u524d\u5217\u200b\u3002\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u5bd2\u6b66\u7eaa\u200b\u7684\u200bDianNao\u200b\u7cfb\u5217\u200b\u67b6\u6784\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u6765\u200b\u7b80\u8981\u200b\u4ecb\u7ecd\u200bNPU\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#diannao","title":"DianNao","text":"<p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aDianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning\uff0cfigure 9</p> <p>\u200b\u57fa\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u662f\u200b\u6a21\u62df\u200b\u4eba\u7c7b\u200b\u5927\u8111\u200b\u5185\u90e8\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u7684\u200bneuron\u200b\u4ee3\u8868\u200b\u7684\u200b\u5c31\u662f\u200b\u5355\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff0csynapse\u200b\u4ee3\u8868\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u7a81\u89e6\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5c31\u8981\u200b\u7ed3\u5408\u200b\u9ad8\u4e2d\u200b\u751f\u7269\u8bfe\u200b\u7684\u200b\u77e5\u8bc6\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u6709\u200b\u8bb8\u591a\u200b\u7a81\u89e6\u200b\uff0c\u200b\u7ed9\u200b\u522b\u7684\u200b\u795e\u7ecf\u5143\u200b\u4f20\u9012\u4fe1\u606f\u200b\u3002\u200b\u540c\u6837\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u4e5f\u200b\u4f1a\u200b\u63a5\u6536\u200b\u6765\u81ea\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u6240\u6709\u200b\u63a5\u53d7\u200b\u5230\u200b\u7684\u200b\u4fe1\u606f\u200b\u7d2f\u52a0\u200b\uff0c\u200b\u4f1a\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f3a\u70c8\u200b\u7a0b\u5ea6\u200b\uff0c\u200b\u5728\u200b\u751f\u7269\u200b\u4e0a\u200b\u662f\u200b\u4ee5\u200b\u5316\u5b66\u6210\u5206\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5b58\u5728\u200b\uff0c\u200b\u5f53\u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u8fbe\u5230\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5f3a\u70c8\u200b\u7a0b\u5ea6\u200b\uff0c\u200b\u5c31\u200b\u4f1a\u200b\u4f7f\u200b\u6574\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u5904\u4e8e\u200b\u5174\u594b\u200b\u72b6\u6001\u200b\uff08\u200b\u6fc0\u6d3b\u200b\uff09\uff0c\u200b\u5426\u5219\u200b\u5c31\u662f\u200b\u4e0d\u200b\u5174\u594b\u200b\uff08\u200b\u4e0d\u200b\u6fc0\u6d3b\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u5174\u594b\u200b\u4e86\u200b\uff0c\u200b\u5c31\u200b\u7ed9\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u5143\u200b\u4f20\u9012\u4fe1\u606f\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u200b\u5174\u594b\u200b\uff0c\u200b\u5c31\u200b\u4e0d\u200b\u4f20\u9012\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u5355\u72ec\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6a21\u5f0f\u200b\u3002\u200b\u90a3\u4e48\u200b\u6709\u200b\u6210\u5343\u4e0a\u4e07\u200b\u4e2a\u200b\u8fd9\u6837\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200bDianNao\u200b\u662f\u200b\u5982\u4f55\u200b\u6a21\u62df\u200b\u795e\u7ecf\u5143\u200b\u8fdb\u884c\u200b\u5de5\u4f5c\u200b\u7684\u200b\u5462\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u770b\u200b\u5b83\u200b\u7684\u200b\u5185\u90e8\u200b\u7ed3\u6784\u56fe\u200b\uff1a</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aDianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning\uff0cfigure 11</p> <p>\u200b\u5982\u56fe\u6240\u793a\u200b\uff0c\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u6d45\u84dd\u8272\u200b\u7684\u200b\u90e8\u5206\u200b\u5c31\u662f\u200b\u7528\u200b\u786c\u4ef6\u200b\u903b\u8f91\u6a21\u62df\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\uff0c\u200b\u79f0\u4e3a\u200bNFU\uff08Neural Functional Units\uff09\u3002\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u7ec6\u5206\u200b\u4e3a\u200b\u4e09\u4e2a\u200b\u90e8\u5206\u200b\uff0c\u200b\u5373\u200b\u9014\u4e2d\u200b\u7684\u200bNFU-1\uff0cNFU-2\uff0c\u200b\u548c\u200bNFU-3\u3002</p> <p>NFU-1\u200b\u662f\u200b\u4e58\u6cd5\u200b\u5355\u5143\u200b\uff0c\u200b\u5b83\u200b\u91c7\u7528\u200b16bit\u200b\u5b9a\u70b9\u6570\u200b\u4e58\u6cd5\u5668\u200b\uff0c1\u200b\u4f4d\u200b\u7b26\u53f7\u200b\u4f4d\u200b\uff0c5\u200b\u4f4d\u200b\u6574\u6570\u200b\u4f4d\u200b\uff0c10\u200b\u4f4d\u200b\u5c0f\u6570\u4f4d\u200b\u3002\u200b\u8be5\u200b\u90e8\u5206\u200b\u603b\u8ba1\u6709\u200b256\u200b\u4e2a\u200b\u4e58\u6cd5\u5668\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u4e58\u6cd5\u5668\u200b\u7684\u200b\u8ba1\u7b97\u200b\u662f\u200b\u540c\u65f6\u200b\u7684\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5468\u671f\u200b\u5185\u200b\u53ef\u4ee5\u200b\u6267\u884c\u200b256\u200b\u6b21\u200b\u4e58\u6cd5\u200b\u3002</p> <p>NFU-2\u200b\u662f\u200b\u52a0\u6cd5\u200b\u6811\u200b\uff0c\u200b\u603b\u8ba1\u200b16\u200b\u4e2a\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u52a0\u6cd5\u200b\u6811\u200b\u90fd\u200b\u662f\u200b8-4-2-1\u200b\u8fd9\u6837\u200b\u7684\u200b\u7ec4\u6210\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5373\u200b\u5c31\u662f\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u52a0\u6cd5\u200b\u6811\u4e2d\u200b\u90fd\u200b\u6709\u200b15\u200b\u4e2a\u200b\u52a0\u6cd5\u5668\u200b\u3002</p> <p>NFU-3\u200b\u662f\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u90e8\u5206\u200b\u7531\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u8fd1\u4f3c\u200b\u5b9e\u73b0\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u6839\u636e\u200b\u524d\u9762\u200b\u4e24\u4e2a\u200b\u5355\u5143\u200b\u8ba1\u7b97\u200b\u5f97\u5230\u200b\u7684\u200b\u523a\u6fc0\u200b\u91cf\u200b\uff0c\u200b\u4ece\u800c\u200b\u5224\u65ad\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u6fc0\u6d3b\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u5f53\u200b\u9700\u8981\u200b\u5b9e\u73b0\u200b\u5411\u91cf\u200b\u76f8\u4e58\u200b\u548c\u200b\u5377\u79ef\u200b\u8fd0\u7b97\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200bNFU-1\u200b\u5b8c\u6210\u200b\u5bf9\u5e94\u200b\u4f4d\u7f6e\u200b\u5143\u7d20\u200b\u76f8\u4e58\u200b\uff0cNFU-2\u200b\u5b8c\u6210\u200b\u76f8\u4e58\u200b\u7ed3\u679c\u200b\u76f8\u52a0\u200b\uff0c\u200b\u6700\u540e\u200b\u7531\u200bNFU-3\u200b\u5b8c\u6210\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u6620\u5c04\u200b\u3002\u200b\u5b8c\u6210\u200b\u6c60\u5316\u200b\u8fd0\u7b97\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200bNFU-2\u200b\u5b8c\u6210\u200b\u591a\u4e2a\u200b\u5143\u7d20\u200b\u53d6\u200b\u6700\u5927\u503c\u200b\u6216\u53d6\u200b\u5e73\u5747\u503c\u200b\u8fd0\u7b97\u200b\u3002\u200b\u7531\u6b64\u200b\u5206\u6790\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u8be5\u200b\u8fd0\u7b97\u200b\u6a21\u5757\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u4e5f\u200b\u8986\u76d6\u200b\u4e86\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u8fd0\u7b97\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#dadiannao","title":"DaDianNao","text":"<p>\u200b\u4f5c\u4e3a\u200bDianNao\u200b\u7684\u200b\u591a\u200b\u6838\u200b\u5347\u7ea7\u200b\u7248\u672c\u200b\uff0cDaDianNao\u200b\u7684\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200bNFU\u200b\u4e0e\u200bDianNao\u200b\u57fa\u672c\u76f8\u540c\u200b\uff0c\u200b\u6700\u5927\u200b\u7684\u200b\u533a\u522b\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5b8c\u6210\u200b\u8bad\u7ec3\u4efb\u52a1\u200b\u591a\u52a0\u200b\u4e86\u200b\u51e0\u6761\u200b\u6570\u636e\u901a\u8def\u200b\uff0c\u200b\u4e14\u200b\u914d\u7f6e\u200b\u66f4\u52a0\u200b\u7075\u6d3b\u200b\u3002NFU\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u4e3a\u200b16x16\uff0c\u200b\u5373\u200b16\u200b\u4e2a\u200b\u8f93\u51fa\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8f93\u51fa\u200b\u795e\u7ecf\u5143\u200b\u6709\u200b16\u200b\u4e2a\u200b\u8f93\u5165\u200b\uff08\u200b\u8f93\u5165\u200b\u7aef\u200b\u9700\u8981\u200b\u4e00\u6b21\u200b\u63d0\u4f9b\u200b256\u200b\u4e2a\u200b\u6570\u636e\u200b\uff09\u3002\u200b\u540c\u65f6\u200b\uff0cNFU\u200b\u53ef\u4ee5\u200b\u53ef\u9009\u200b\u7684\u200b\u8df3\u200b\u8fc7\u200b\u4e00\u4e9b\u200b\u6b65\u9aa4\u200b\u4ee5\u200b\u8fbe\u5230\u200b\u7075\u6d3b\u200b\u53ef\u200b\u914d\u7f6e\u200b\u7684\u200b\u529f\u80fd\u200b\u3002DaDianNao\u200b\u7684\u200bNFU\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aDaDianNao: A Machine-Learning Supercomputer\uff0cfigure 6</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#shidiannao","title":"ShiDianNao","text":"<p>ShiDianNao\u200b\u662f\u200b\u673a\u5668\u200b\u89c6\u89c9\u200b\u4e13\u7528\u200b\u52a0\u901f\u5668\u200b\uff0c\u200b\u96c6\u6210\u200b\u4e86\u200b\u89c6\u9891\u200b\u5904\u7406\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u5b83\u200b\u4e5f\u200b\u662f\u200bDianNao\u200b\u7cfb\u5217\u200b\u4e2d\u200b\u552f\u4e00\u200b\u4e00\u4e2a\u200b\u8003\u8651\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\u7ea7\u200b\u6570\u636e\u200b\u91cd\u7528\u200b\u7684\u200b\u52a0\u901f\u5668\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u552f\u4e00\u200b\u4f7f\u7528\u200b\u4e8c\u7ef4\u200b\u8fd0\u7b97\u200b\u9635\u5217\u200b\u7684\u200b\u52a0\u901f\u5668\u200b\uff0c\u200b\u5176\u200b\u52a0\u901f\u5668\u200b\u7684\u200b\u8fd0\u7b97\u200b\u9635\u5217\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aShiDianNao: Shifting vision processing closer to the sensor\uff0cfigure 5</p> <p>ShiDianNao\u200b\u7684\u200b\u8fd0\u7b97\u200b\u9635\u5217\u200b\u4e3a\u200b2D\u200b\u683c\u70b9\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\uff08\u200b\u8282\u70b9\u200b\uff09\u200b\u800c\u8a00\u200b\uff0c\u200b\u8fd0\u7b97\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u53c2\u6570\u200b\u7edf\u4e00\u200b\u6765\u6e90\u4e8e\u200bKernel\uff0c\u200b\u800c\u200b\u53c2\u4e0e\u200b\u8fd0\u7b97\u200b\u7684\u200b\u6570\u636e\u200b\u5219\u200b\u53ef\u80fd\u200b\u6765\u81ea\u200b\u4e8e\u200b\uff1a\u200b\u6570\u636e\u200b\u7f13\u5b58\u200bNBin\uff0c\u200b\u4e0b\u65b9\u200b\u7684\u200b\u8282\u70b9\u200b\uff0c\u200b\u53f3\u4fa7\u200b\u7684\u200b\u8282\u70b9\u200b\u3002</p> <p>\u200b\u4e0b\u56fe\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\u7684\u200b\u7ed3\u6784\u200b\uff1a</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aShiDianNao: Shifting vision processing closer to the sensor\uff0cfigure 6</p> <p>\u200b\u8be5\u200b\u8ba1\u7b97\u200b\u8282\u70b9\u200b\u7684\u200b\u529f\u80fd\u200b\u5305\u62ec\u200b\u8f6c\u53d1\u200b\u6570\u636e\u200b\u548c\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff1a</p> <p>\u200b\u8f6c\u53d1\u200b\u6570\u636e\u200b\uff1a\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u53ef\u200b\u6765\u6e90\u4e8e\u200b\u53f3\u4fa7\u200b\u8282\u70b9\u200b\uff0c\u200b\u4e0b\u65b9\u200b\u8282\u70b9\u200b\u548c\u200bNBin\uff0c\u200b\u6839\u636e\u200b\u63a7\u5236\u200b\u4fe1\u53f7\u200b\u9009\u62e9\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u5b58\u50a8\u200b\u5230\u200b\u8f93\u5165\u200b\u5bc4\u5b58\u5668\u200b\u4e2d\u200b\uff0c\u200b\u4e14\u200b\u6839\u636e\u200b\u63a7\u5236\u200b\u4fe1\u53f7\u200b\u53ef\u9009\u200b\u7684\u200b\u5c06\u200b\u5176\u200b\u5b58\u50a8\u200b\u5230\u200bFIFO-H\u200b\u548c\u200bFIFO-V\u200b\u4e2d\u200b\u3002\u200b\u540c\u65f6\u200b\u6839\u636e\u200b\u63a7\u5236\u200b\u4fe1\u53f7\u200b\u9009\u62e9\u200bFIFO-H\u200b\u548c\u200bFIFO-V\u200b\u4e2d\u200b\u7684\u200b\u4fe1\u53f7\u200b\u4ece\u200bFIFO output\u200b\u7aef\u53e3\u200b\u8f93\u51fa\u200b</p> <p>\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff1a\u200b\u6839\u636e\u200b\u63a7\u5236\u200b\u4fe1\u53f7\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5305\u62ec\u200b\u76f8\u52a0\u200b\uff0c\u200b\u7d2f\u52a0\u200b\uff0c\u200b\u4e58\u52a0\u200b\u548c\u200b\u6bd4\u8f83\u200b\u7b49\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u7ed3\u679c\u200b\u5b58\u50a8\u200b\u5230\u200b\u8f93\u51fa\u200b\u5bc4\u5b58\u5668\u200b\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u63a7\u5236\u200b\u4fe1\u53f7\u200b\u9009\u62e9\u200b\u5bc4\u5b58\u5668\u200b\u6216\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u8f93\u51fa\u200b\u5230\u200bPE output\u200b\u7aef\u53e3\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u8ba1\u7b97\u200b\u529f\u80fd\u200b\uff0c\u200b\u6839\u636e\u200b\u4e0a\u6587\u200b\u7684\u200b\u7ed3\u6784\u56fe\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0cPE\u200b\u652f\u6301\u200b\u7684\u200b\u8fd0\u7b97\u200b\u6709\u200b\uff1akernel\u200b\u548c\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u76f8\u4e58\u200b\u5e76\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u5bc4\u5b58\u5668\u200b\u6570\u636e\u200b\u76f8\u52a0\u200b\uff08\u200b\u4e58\u52a0\u200b\uff09\uff0c\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u5bc4\u5b58\u5668\u200b\u6570\u636e\u200b\u53d6\u200b\u6700\u5927\u200b\u6216\u200b\u6700\u5c0f\u200b\uff08\u200b\u5e94\u7528\u200b\u4e8e\u200b\u6c60\u5316\u200b\uff09\uff0ckernel\u200b\u4e0e\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u76f8\u52a0\u200b\uff08\u200b\u5411\u91cf\u200b\u52a0\u6cd5\u200b\uff09\uff0c\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u5bc4\u5b58\u5668\u200b\u6570\u636e\u200b\u76f8\u52a0\u200b\uff08\u200b\u7d2f\u52a0\u200b\uff09\u200b\u7b49\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#pudiannao","title":"PuDianNao","text":"<p>\u200b\u4f5c\u4e3a\u200bDianNao\u200b\u7cfb\u5217\u200b\u7684\u200b\u6536\u5c71\u4e4b\u4f5c\u200b\uff0cPuDianNao\u200b\u7684\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\u662f\u200b\u7535\u8111\u200b\u7cfb\u5217\u200b\u4e2d\u200b\u552f\u4e00\u200b\u4e00\u4e2a\u200b\u5f02\u6784\u200b\u7684\u200b\uff0c\u200b\u9664\u4e86\u200b\u6709\u200bMLU\uff08\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5355\u5143\u200b\uff09\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200bALU\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u901a\u7528\u200b\u8fd0\u7b97\u200b\u548c\u200bMLU\u200b\u65e0\u6cd5\u200b\u5904\u7406\u200b\u7684\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u5176\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\uff08\u200b\u4e0a\u200b\uff09\u200b\u548c\u200bMLU\uff08\u200b\u4e0b\u200b\uff09\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u4e0a\u200b\u56fe\u200b\uff1aPuDianNao: A Polyvalent Machine Learning Accelerator\uff0cfigure 11&amp;12</p> <p>\u200b\u8be5\u200bMLU\u200b\u5171\u200b\u5206\u4e3a\u200b6\u200b\u5c42\u200b\uff1a</p> <p>\u200b\u8ba1\u6570\u200b\u5c42\u200b/\u200b\u6bd4\u8f83\u200b\u5c42\u200b\uff1a\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u7684\u200b\u5904\u7406\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u6570\u200b\u6309\u4f4d\u200b\u4e0e\u200b\u6216\u200b\u6bd4\u8f83\u200b\u5927\u5c0f\u200b\uff0c\u200b\u7ed3\u679c\u200b\u5c06\u200b\u88ab\u200b\u7d2f\u52a0\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u53ef\u4ee5\u200b\u5355\u72ec\u200b\u8f93\u51fa\u200b\u4e14\u200b\u53ef\u4ee5\u200b\u88ab\u200bbypass</p> <p>\u200b\u52a0\u6cd5\u200b\u5c42\u200b\uff1a\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\u5bf9\u5e94\u200b\u76f8\u52a0\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u53ef\u4ee5\u200b\u5355\u72ec\u200b\u8f93\u51fa\u200b\u4e14\u200b\u53ef\u4ee5\u200b\u88ab\u200bbypass</p> <p>\u200b\u4e58\u6cd5\u200b\u5c42\u200b\uff1a\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\u6216\u200b\u4e0a\u200b\u4e00\u5c42\u200b\uff08\u200b\u52a0\u6cd5\u200b\u5c42\u200b\uff09\u200b\u7ed3\u679c\u200b\u5bf9\u5e94\u200b\u4f4d\u7f6e\u200b\u76f8\u4e58\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5355\u72ec\u200b\u8f93\u51fa\u200b</p> <p>\u200b\u52a0\u6cd5\u200b\u6811\u5c42\u200b\uff1a\u200b\u5c06\u200b\u4e58\u6cd5\u200b\u5c42\u200b\u7684\u200b\u7ed3\u679c\u200b\u7d2f\u52a0\u200b</p> <p>\u200b\u7d2f\u52a0\u200b\u5c42\u200b\uff1a\u200b\u5c06\u200b\u4e0a\u200b\u4e00\u5c42\u200b\uff08\u200b\u52a0\u6cd5\u200b\u6811\u5c42\u200b\uff09\u200b\u7684\u200b\u7ed3\u679c\u200b\u7d2f\u52a0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5355\u72ec\u200b\u8f93\u51fa\u200b</p> <p>\u200b\u7279\u6b8a\u200b\u5904\u7406\u200b\u5c42\u200b\uff1a\u200b\u7531\u200b\u4e00\u4e2a\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u903c\u8fd1\u200b\u5b9e\u73b0\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u548c\u200bk\u200b\u6392\u5e8f\u200b\u5668\u200b\uff08\u200b\u8f93\u51fa\u200b\u4e0a\u200b\u4e00\u5c42\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u6700\u5c0f\u200b\u7684\u200b\u8f93\u51fa\u200b\uff09\u200b\u7ec4\u6210\u200b</p> <p>\u200b\u8be5\u200b\u8fd0\u7b97\u200b\u5355\u5143\u200b\u662f\u200bDianNao\u200b\u7cfb\u5217\u200b\u4e2d\u200b\u529f\u80fd\u200b\u6700\u591a\u200b\u7684\u200b\u5355\u5143\u200b\uff0c\u200b\u914d\u7f6e\u200b\u975e\u5e38\u7075\u6d3b\u200b\u3002\u200b\u4f8b\u5982\u200b\u5b9e\u73b0\u200b\u5411\u91cf\u200b\u76f8\u4e58\u200b\uff08\u200b\u5bf9\u5e94\u200b\u4f4d\u7f6e\u200b\u76f8\u4e58\u200b\u540e\u200b\u7d2f\u52a0\u200b\uff09\u200b\u65f6\u200b\uff0c\u200b\u5f03\u7528\u200b\u8ba1\u6570\u200b\u5c42\u200b\uff0c\u200b\u52a0\u6cd5\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b\u6570\u636e\u200b\u4ece\u200b\u4e58\u6cd5\u200b\u5c42\u200b\uff0c\u200b\u52a0\u6cd5\u200b\u6811\u5c42\u200b\u548c\u200b\u7d2f\u52a0\u200b\u5c42\u6d41\u200b\u8fc7\u200b\u5373\u53ef\u200b\u5b9e\u73b0\u200b\u3002</p> <p>PuDianNao\u200b\u652f\u6301\u200b7\u200b\u79cd\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\uff1a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u7ebf\u6027\u200b\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\uff0c\u200b\u51b3\u7b56\u6811\u200b\uff0c\u200b\u6734\u7d20\u200b\u8d1d\u53f6\u65af\u200b\uff0cK\u200b\u4e34\u8fd1\u200b\u548c\u200bK\u200b\u7c7b\u805a\u200b\uff0c\u200b\u6240\u200b\u9700\u8981\u200b\u652f\u6301\u200b\u7684\u200b\u8fd0\u7b97\u200b\u8f83\u200b\u591a\u200b\uff0c\u200b\u56e0\u6b64\u200bPuDianNao\u200b\u7684\u200b\u8fd0\u7b97\u200b\u5206\u6790\u200b\u4e3b\u8981\u200b\u96c6\u4e2d\u200b\u5728\u200b\u5b58\u50a8\u200b\u65b9\u9762\u200b\uff0c\u200b\u5176\u200b\u8fd0\u7b97\u200b\u6838\u5fc3\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u4e2d\u200b\u8bf4\u660e\u200bPuDianNao\u200b\u652f\u6301\u200b\u7684\u200b\u8fd0\u7b97\u200b\u4e3b\u8981\u200b\u6709\u200b\uff1a\u200b\u5411\u91cf\u200b\u70b9\u200b\u4e58\u200b\uff0c\u200b\u8ddd\u79bb\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u8ba1\u6570\u200b\uff0c\u200b\u6392\u5e8f\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u3002\u200b\u5176\u4ed6\u200b\u672a\u200b\u8986\u76d6\u200b\u7684\u200b\u8ba1\u7b97\u200b\u4f7f\u7528\u200bALU\u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"02-%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87/#_5","title":"\u53c2\u8003\u8d44\u6599\u200b\uff1a","text":"<p>In-datacenter performance analysis of a tensor processing unit</p> <p>Neural Network Processor</p> <p>DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning</p> <p>DaDianNao: A Machine-Learning Supercomputer</p> <p>ShiDianNao: Shifting vision processing closer to the sensor</p> <p>PuDianNao: A Polyvalent Machine Learning Accelerator</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.1%20%E6%80%9D%E8%80%83%EF%BC%9A%E5%AE%8C%E6%88%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BF%85%E8%A6%81%E9%83%A8%E5%88%86/","title":"3.1 \u200b\u601d\u8003\u200b\uff1a\u200b\u5b8c\u6210\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u5fc5\u8981\u200b\u90e8\u5206","text":"<p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e00\u9879\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b/\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u7684\u200b\u6574\u4f53\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u7684\u200b\u4f5c\u7528\u200b</li> </ul> <p>\u200b\u4e00\u9879\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u65f6\u200b\u5e38\u5e38\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u51e0\u4e2a\u200b\u91cd\u8981\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u9996\u5148\u200b\u662f\u200b\u6570\u636e\u200b\u7684\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u5176\u4e2d\u200b\u91cd\u8981\u200b\u7684\u200b\u6b65\u9aa4\u200b\u5305\u62ec\u200b\u6570\u636e\u683c\u5f0f\u200b\u7684\u200b\u7edf\u4e00\u200b\u3001\u200b\u5f02\u5e38\u200b\u6570\u636e\u200b\u7684\u200b\u6d88\u9664\u200b\u548c\u200b\u5fc5\u8981\u200b\u7684\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\uff0c\u200b\u540c\u65f6\u200b\u5212\u5206\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3001\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u3001\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5305\u62ec\u200b\uff1a\u200b\u6309\u200b\u6bd4\u4f8b\u200b\u968f\u673a\u200b\u9009\u53d6\u200b\uff0cKFold\u200b\u65b9\u6cd5\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bsklearn\u200b\u5e26\u200b\u7684\u200btest_train_split\u200b\u51fd\u6570\u200b\u3001kfold\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff09\u3002\u200b\u63a5\u4e0b\u6765\u200b\u9009\u62e9\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u8bbe\u5b9a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u5f53\u7136\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bsklearn\u200b\u8fd9\u6837\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e93\u4e2d\u200b\u6a21\u578b\u200b\u81ea\u5e26\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff09\u3002\u200b\u6700\u540e\u200b\u7528\u200b\u6a21\u578b\u200b\u53bb\u200b\u62df\u5408\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u200b/\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u3002</p> <p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5728\u200b\u6d41\u7a0b\u200b\u4e0a\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e0a\u200b\u6709\u200b\u8f83\u5927\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002\u200b\u9996\u5148\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6837\u672c\u91cf\u200b\u5f88\u5927\u200b\uff0c\u200b\u4e00\u6b21\u200b\u52a0\u8f7d\u200b\u5168\u90e8\u200b\u6570\u636e\u200b\u8fd0\u884c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8d85\u51fa\u200b\u5185\u5b58\u5bb9\u91cf\u200b\u800c\u200b\u65e0\u6cd5\u200b\u5b9e\u73b0\u200b\uff1b\u200b\u540c\u65f6\u200b\u8fd8\u6709\u200b\u6279\u200b\uff08batch\uff09\u200b\u8bad\u7ec3\u200b\u7b49\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u7684\u200b\u7b56\u7565\u200b\uff0c\u200b\u9700\u8981\u200b\u6bcf\u6b21\u200b\u8bad\u7ec3\u200b\u8bfb\u53d6\u200b\u56fa\u5b9a\u200b\u6570\u91cf\u200b\u7684\u200b\u6837\u672c\u200b\u9001\u5165\u200b\u6a21\u578b\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5728\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u4e0a\u200b\u9700\u8981\u200b\u6709\u200b\u4e13\u95e8\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u3002</p> <p>\u200b\u5728\u200b\u6a21\u578b\u200b\u5b9e\u73b0\u200b\u4e0a\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e5f\u200b\u6709\u200b\u5f88\u5927\u200b\u5dee\u5f02\u200b\u3002\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c42\u6570\u200b\u5f80\u5f80\u200b\u8f83\u200b\u591a\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f1a\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7528\u4e8e\u200b\u5b9e\u73b0\u200b\u7279\u5b9a\u200b\u529f\u80fd\u200b\u7684\u200b\u5c42\u200b\uff08\u200b\u5982\u200b\u5377\u79ef\u200b\u5c42\u200b\u3001\u200b\u6c60\u5316\u5c42\u200b\u3001\u200b\u6279\u200b\u6b63\u5219\u200b\u5316\u5c42\u200b\u3001LSTM\u200b\u5c42\u200b\u7b49\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5f80\u5f80\u200b\u9700\u8981\u200b\u201c\u200b\u9010\u5c42\u200b\u201d\u200b\u642d\u5efa\u200b\uff0c\u200b\u6216\u8005\u200b\u9884\u5148\u200b\u5b9a\u4e49\u200b\u597d\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u7279\u5b9a\u200b\u529f\u80fd\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u518d\u200b\u628a\u200b\u8fd9\u4e9b\u200b\u6a21\u5757\u200b\u7ec4\u88c5\u200b\u8d77\u6765\u200b\u3002\u200b\u8fd9\u79cd\u200b\u201c\u200b\u5b9a\u5236\u200b\u5316\u200b\u201d\u200b\u7684\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u65b9\u5f0f\u200b\u80fd\u591f\u200b\u5145\u5206\u4fdd\u8bc1\u200b\u6a21\u578b\u200b\u7684\u200b\u7075\u6d3b\u6027\u200b\uff0c\u200b\u4e5f\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u63d0\u51fa\u200b\u4e86\u200b\u65b0\u200b\u7684\u200b\u8981\u6c42\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\u662f\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u8bbe\u5b9a\u200b\u3002\u200b\u8fd9\u90e8\u5206\u200b\u548c\u200b\u7ecf\u5178\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u5b9e\u73b0\u200b\u662f\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u3002\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6a21\u578b\u200b\u8bbe\u5b9a\u200b\u7684\u200b\u7075\u6d3b\u6027\u200b\uff0c\u200b\u56e0\u6b64\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u8981\u200b\u80fd\u591f\u200b\u4fdd\u8bc1\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u80fd\u591f\u200b\u5728\u200b\u7528\u6237\u200b\u81ea\u884c\u200b\u5b9a\u4e49\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u4e0a\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u5b8c\u6210\u200b\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u3002\u200b\u6211\u4eec\u200b\u524d\u9762\u200b\u4ecb\u7ecd\u200b\u4e86\u200bGPU\u200b\u7684\u200b\u6982\u5ff5\u200b\u548c\u200bGPU\u200b\u7528\u4e8e\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u52a0\u901f\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u7a0b\u5e8f\u200b\u9ed8\u8ba4\u200b\u662f\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0c\u200b\u9700\u8981\u200b\u628a\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u201c\u200b\u653e\u5230\u200b\u201dGPU\u200b\u4e0a\u53bb\u200b\u505a\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u9700\u8981\u200b\u4fdd\u8bc1\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u80fd\u591f\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u5de5\u4f5c\u200b\u3002\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u591a\u200b\u5f20\u200bGPU\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u5206\u914d\u200b\u3001\u200b\u6574\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u540e\u7eed\u200b\u8ba1\u7b97\u200b\u4e00\u4e9b\u200b\u6307\u6807\u200b\u8fd8\u200b\u9700\u8981\u200b\u628a\u200b\u6570\u636e\u200b\u201c\u200b\u653e\u200b\u56de\u200b\u201dCPU\u3002\u200b\u8fd9\u91cc\u200b\u6d89\u53ca\u200b\u5230\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u6709\u200b\u5173\u4e8e\u200bGPU\u200b\u7684\u200b\u914d\u7f6e\u200b\u548c\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\u6700\u5927\u200b\u7684\u200b\u7279\u70b9\u200b\u5728\u4e8e\u200b\u8bfb\u5165\u200b\u6570\u636e\u200b\u662f\u200b\u6309\u6279\u200b\u7684\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u8bfb\u5165\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u653e\u5165\u200bGPU\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u56de\u200b\u7f51\u7edc\u200b\u6700\u200b\u524d\u9762\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f7f\u7528\u200b\u4f18\u5316\u200b\u5668\u200b\u8c03\u6574\u200b\u7f51\u7edc\u200b\u53c2\u6570\u200b\u3002\u200b\u8fd9\u91cc\u200b\u4f1a\u200b\u6d89\u53ca\u200b\u5230\u200b\u5404\u4e2a\u200b\u6a21\u5757\u200b\u914d\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b\u540e\u200b\u8fd8\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u8bbe\u5b9a\u200b\u597d\u200b\u7684\u200b\u6307\u6807\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u5c31\u200b\u5b8c\u6210\u200b\u4e86\u200b\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u8be6\u7ec6\u200b\u8bb2\u89e3\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u4e4b\u524d\u200b\uff0c\u200b\u5148\u200b\u68b3\u7406\u200b\u4e86\u200b\u5b8c\u6210\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c31\u200b\u53bb\u200b\u8fdb\u4e00\u6b65\u200b\u4e86\u89e3\u200b\u4e00\u4e0b\u200bPyTorch\u200b\u662f\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u7684\u200b\uff0c\u200b\u4ee5\u53ca\u200bPyTorch\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u62e5\u6709\u200b\u7684\u200b\u6a21\u5757\u5316\u200b\u7279\u70b9\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.2%20%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/","title":"3.2 \u200b\u57fa\u672c\u200b\u914d\u7f6e","text":"<p>\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u9879\u76ee\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bfc\u5165\u200b\u4e00\u4e9b\u200bPython\u200b\u5e38\u7528\u200b\u7684\u200b\u5305\u6765\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5feb\u901f\u200b\u5b9e\u73b0\u200b\u529f\u80fd\u200b\u3002\u200b\u5e38\u89c1\u200b\u7684\u200b\u5305\u6709\u200bos\u3001numpy\u200b\u7b49\u200b\uff0c\u200b\u6b64\u5916\u200b\u8fd8\u200b\u9700\u8981\u200b\u8c03\u7528\u200bPyTorch\u200b\u81ea\u8eab\u200b\u4e00\u4e9b\u200b\u6a21\u5757\u200b\u4fbf\u4e8e\u200b\u7075\u6d3b\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6bd4\u5982\u200btorch\u3001torch.nn\u3001torch.utils.data.Dataset\u3001torch.utils.data.DataLoader\u3001torch.optimizer\u200b\u7b49\u7b49\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b/\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u5e38\u7528\u200b\u5230\u200b\u7684\u200b\u5305\u200b</li> <li>GPU\u200b\u7684\u200b\u914d\u7f6e\u200b</li> </ul> <p>\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b\u5fc5\u987b\u200b\u7684\u200b\u5305\u200b\u3002\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u53ea\u662f\u200b\u5efa\u8bae\u200b\u5bfc\u5165\u200b\u7684\u200b\u5305\u200b\u5bfc\u5165\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u6848\u200b\uff0c\u200b\u6bd4\u5982\u200b\u6d89\u53ca\u200b\u5230\u200b\u8868\u683c\u200b\u4fe1\u606f\u200b\u7684\u200b\u8bfb\u5165\u200b\u5f88\u200b\u53ef\u80fd\u200b\u7528\u5230\u200bpandas\uff0c\u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u9879\u76ee\u200b\u53ef\u80fd\u200b\u8fd8\u200b\u9700\u8981\u200b\u5bfc\u5165\u200b\u4e00\u4e9b\u200b\u66f4\u200b\u4e0a\u5c42\u200b\u7684\u200b\u5305\u5982\u200bcv2\u200b\u7b49\u200b\u3002\u200b\u5982\u679c\u200b\u6d89\u53ca\u200b\u53ef\u89c6\u5316\u200b\u8fd8\u4f1a\u200b\u7528\u5230\u200bmatplotlib\u3001seaborn\u200b\u7b49\u200b\u3002\u200b\u6d89\u53ca\u200b\u5230\u200b\u4e0b\u6e38\u200b\u5206\u6790\u200b\u548c\u200b\u6307\u6807\u200b\u8ba1\u7b97\u200b\u4e5f\u200b\u5e38\u7528\u200b\u5230\u200bsklearn\u3002</p> <pre><code>import os \nimport numpy as np \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optimizer\n</code></pre> <p>\u200b\u6839\u636e\u200b\u524d\u9762\u200b\u6211\u4eec\u200b\u5bf9\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u7684\u200b\u68b3\u7406\u200b\uff0c\u200b\u6709\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u7edf\u4e00\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u8c03\u8bd5\u200b\u65f6\u200b\u4fee\u6539\u200b\uff1a</p> <ul> <li>batch size</li> <li>\u200b\u521d\u59cb\u200b\u5b66\u4e60\u200b\u7387\u200b\uff08\u200b\u521d\u59cb\u200b\uff09</li> <li>\u200b\u8bad\u7ec3\u200b\u6b21\u6570\u200b\uff08max_epochs\uff09</li> <li>GPU\u200b\u914d\u7f6e\u200b</li> </ul> <p><pre><code>batch_size = 16\n# \u200b\u6279\u6b21\u200b\u7684\u200b\u5927\u5c0f\u200b\nlr = 1e-4\n# \u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\nmax_epochs = 100\n</code></pre> \u200b\u9664\u4e86\u200b\u76f4\u63a5\u200b\u5c06\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u4ee3\u7801\u200b\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200byaml\u3001json\uff0cdict\u200b\u7b49\u200b\u6587\u4ef6\u200b\u6765\u200b\u5b58\u50a8\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u7684\u200b\u8c03\u8bd5\u200b\u548c\u200b\u4fee\u6539\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u4e5f\u200b\u662f\u200b\u5e38\u89c1\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200b\uff08mmdetection\uff0cPaddledetection\uff0cdetectron2\uff09\u200b\u548c\u200b\u4e00\u4e9b\u200bAI Lab\u200b\u91cc\u9762\u200b\u6bd4\u8f83\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e00\u79cd\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\u7ecf\u8fc7\u200b\u663e\u5f0f\u200b\u6307\u660e\u200b\u8bbe\u5907\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4f1a\u200b\u5b58\u50a8\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u52a0\u901f\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u663e\u5f0f\u200b\u8c03\u7528\u200bGPU\uff0c\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u200bGPU\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u6709\u200b\u4e24\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff1a</p> <pre><code># \u200b\u65b9\u6848\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200bos.environ\uff0c\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u5982\u679c\u200b\u4f7f\u7528\u200bGPU\u200b\u4e0d\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # \u200b\u6307\u660e\u200b\u8c03\u7528\u200b\u7684\u200bGPU\u200b\u4e3a\u200b0,1\u200b\u53f7\u200b\n\n# \u200b\u65b9\u6848\u200b\u4e8c\u200b\uff1a\u200b\u4f7f\u7528\u200b\u201cdevice\u201d\uff0c\u200b\u540e\u7eed\u200b\u5bf9\u8981\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u53d8\u91cf\u200b\u7528\u200b.to(device)\u200b\u5373\u53ef\u200b\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") # \u200b\u6307\u660e\u200b\u8c03\u7528\u200b\u7684\u200bGPU\u200b\u4e3a\u200b1\u200b\u53f7\u200b\n</code></pre> <p>\u200b\u5f53\u7136\u200b\u8fd8\u4f1a\u200b\u6709\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u6a21\u5757\u200b\u6216\u200b\u7528\u6237\u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u5757\u200b\u4f1a\u200b\u7528\u5230\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u6709\u200b\u9700\u8981\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e00\u200b\u5f00\u59cb\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.3%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5/","title":"3.3 \u200b\u6570\u636e\u200b\u8bfb\u5165","text":"<p>PyTorch\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u662f\u200b\u901a\u8fc7\u200bDataset+DataLoader\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b8c\u6210\u200b\u7684\u200b\uff0cDataset\u200b\u5b9a\u4e49\u200b\u597d\u200b\u6570\u636e\u200b\u7684\u200b\u683c\u5f0f\u200b\u548c\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\u5f62\u5f0f\u200b\uff0cDataLoader\u200b\u7528\u200biterative\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0d\u65ad\u200b\u8bfb\u5165\u200b\u6279\u6b21\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b</li> <li>\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u6d41\u7a0b\u200b</li> </ul> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5b9a\u4e49\u200b\u81ea\u5df1\u200b\u7684\u200bDataset\u200b\u7c7b\u6765\u200b\u5b9e\u73b0\u200b\u7075\u6d3b\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\uff0c\u200b\u5b9a\u4e49\u200b\u7684\u200b\u7c7b\u200b\u9700\u8981\u200b\u7ee7\u627f\u200bPyTorch\u200b\u81ea\u8eab\u200b\u7684\u200bDataset\u200b\u7c7b\u200b\u3002\u200b\u4e3b\u8981\u200b\u5305\u542b\u200b\u4e09\u4e2a\u200b\u51fd\u6570\u200b\uff1a</p> <ul> <li><code>__init__</code>: \u200b\u7528\u4e8e\u200b\u5411\u7c7b\u200b\u4e2d\u200b\u4f20\u5165\u200b\u5916\u90e8\u200b\u53c2\u6570\u200b\uff0c\u200b\u540c\u65f6\u200b\u5b9a\u4e49\u200b\u6837\u672c\u200b\u96c6\u200b</li> <li><code>__getitem__</code>: \u200b\u7528\u4e8e\u200b\u9010\u4e2a\u200b\u8bfb\u53d6\u200b\u6837\u672c\u200b\u96c6\u5408\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u4e00\u5b9a\u200b\u7684\u200b\u53d8\u6362\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u8fd4\u56de\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6570\u636e\u200b</li> <li><code>__len__</code>: \u200b\u7528\u4e8e\u200b\u8fd4\u56de\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6837\u672c\u6570\u200b</li> </ul> <p>\u200b\u4e0b\u9762\u200b\u4ee5\u200bcifar10\u200b\u6570\u636e\u200b\u96c6\u4e3a\u4f8b\u200b\u7ed9\u51fa\u200b\u6784\u5efa\u200bDataset\u200b\u7c7b\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff1a</p> <pre><code>import torch\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(train_path, transform=data_transform)\nval_data = datasets.ImageFolder(val_path, transform=data_transform)\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u4e86\u200bPyTorch\u200b\u81ea\u5e26\u200b\u7684\u200bImageFolder\u200b\u7c7b\u200b\u7684\u200b\u7528\u4e8e\u200b\u8bfb\u53d6\u200b\u6309\u200b\u4e00\u5b9a\u200b\u7ed3\u6784\u200b\u5b58\u50a8\u200b\u7684\u200b\u56fe\u7247\u200b\u6570\u636e\u200b\uff08path\u200b\u5bf9\u5e94\u200b\u56fe\u7247\u200b\u5b58\u653e\u200b\u7684\u200b\u76ee\u5f55\u200b\uff0c\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5305\u542b\u200b\u82e5\u5e72\u200b\u5b50\u76ee\u5f55\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5b50\u76ee\u5f55\u200b\u5bf9\u5e94\u200b\u5c5e\u4e8e\u200b\u540c\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u56fe\u7247\u200b\uff09\u3002</p> <p>\u200b\u5176\u4e2d\u200b<code>data_transform</code>\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u4e00\u5b9a\u200b\u7684\u200b\u53d8\u6362\u200b\uff0c\u200b\u5982\u200b\u7ffb\u8f6c\u200b\u3001\u200b\u88c1\u526a\u200b\u7b49\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u53ef\u200b\u81ea\u5df1\u200b\u5b9a\u4e49\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u4e0b\u200b\u4e00\u7ae0\u200b\u901a\u8fc7\u200b\u5b9e\u6218\u200b\u52a0\u4ee5\u200b\u4ecb\u7ecd\u200b\u5e76\u200b\u5728\u200bnotebook\u200b\u4e2d\u200b\u505a\u200b\u4e86\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u7ed9\u51fa\u200b\u4e00\u4e2a\u200b\u81ea\u5df1\u200b\u5b9a\u5236\u200bDataset\u200b\u7684\u200b\u4f8b\u5b50\u200b</p> <p><pre><code>import os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass MyDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        \"\"\"\n        Args:\n            annotations_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n            target_transform (callable, optional): Optional transform to be applied\n                on the target.\n        \"\"\"\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index\n        \"\"\"\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n</code></pre> \u200b\u5176\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6807\u7b7e\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u5f62\u5f0f\u200b\uff1a <pre><code>image1.jpg, 0\nimage2.jpg, 1\n......\nimage9.jpg, 9\n</code></pre> \u200b\u6784\u5efa\u200b\u597d\u200bDataset\u200b\u540e\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bDataLoader\u200b\u6765\u200b\u6309\u200b\u6279\u6b21\u200b\u8bfb\u5165\u200b\u6570\u636e\u200b\u4e86\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)\n</code></pre> <p>\u200b\u5176\u4e2d\u200b:</p> <ul> <li>batch_size\uff1a\u200b\u6837\u672c\u200b\u662f\u200b\u6309\u200b\u201c\u200b\u6279\u200b\u201d\u200b\u8bfb\u5165\u200b\u7684\u200b\uff0cbatch_size\u200b\u5c31\u662f\u200b\u6bcf\u6b21\u200b\u8bfb\u5165\u200b\u7684\u200b\u6837\u672c\u6570\u200b</li> <li>num_workers\uff1a\u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u200b\u8fdb\u7a0b\u200b\u7528\u4e8e\u200b\u8bfb\u53d6\u6570\u636e\u200b\uff0cWindows\u200b\u4e0b\u8be5\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0cLinux\u200b\u4e0b\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e3a\u200b4\u200b\u6216\u8005\u200b8\uff0c\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u7684\u200b\u7535\u8111\u914d\u7f6e\u200b\u6765\u200b\u8bbe\u7f6e\u200b</li> <li>shuffle\uff1a\u200b\u662f\u5426\u200b\u5c06\u200b\u8bfb\u5165\u200b\u7684\u200b\u6570\u636e\u200b\u6253\u4e71\u200b\uff0c\u200b\u4e00\u822c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bTrue\uff0c\u200b\u9a8c\u8bc1\u200b\u96c6\u4e2d\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bFalse</li> <li>drop_last\uff1a\u200b\u5bf9\u4e8e\u200b\u6837\u672c\u200b\u6700\u540e\u200b\u4e00\u90e8\u5206\u200b\u6ca1\u6709\u200b\u8fbe\u5230\u200b\u6279\u200b\u6b21\u6570\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u4e0d\u518d\u200b\u53c2\u4e0e\u200b\u8bad\u7ec3\u200b</li> </ul> <p>\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u770b\u200b\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u3002PyTorch\u200b\u4e2d\u200b\u7684\u200bDataLoader\u200b\u7684\u200b\u8bfb\u53d6\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bnext\u200b\u548c\u200biter\u200b\u6765\u200b\u5b8c\u6210\u200b</p> <pre><code>import matplotlib.pyplot as plt\nimages, labels = next(iter(val_loader))\nprint(images.shape)\nplt.imshow(images[0].transpose(1,2,0))\nplt.show()\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/","title":"3.4 \u200b\u6a21\u578b\u200b\u6784\u5efa","text":"<p>\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u7b2c\u4e09\u6b21\u200b\u6d6a\u6f6e\u200b\u53d7\u76ca\u200b\u4e8e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u51fa\u73b0\u200b\u548c\u200bBP\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7b97\u6cd5\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u7814\u7a76\u200b\u51fa\u200b\u4e86\u200b\u8bb8\u8bb8\u591a\u591a\u200b\u7684\u200b\u6a21\u578b\u200b\uff0cPyTorch\u200b\u4e2d\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6784\u9020\u200b\u4e00\u822c\u200b\u662f\u200b\u57fa\u4e8e\u200b<code>nn.Module</code>\u200b\u7c7b\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u200b\u5b8c\u6210\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u8ba9\u200b\u6a21\u578b\u200b\u6784\u9020\u200b\uf901\u200b\u52a0\u200b\u7075\u6d3b\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u4e2d\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6784\u9020\u65b9\u6cd5\u200b</li> <li>PyTorch\u200b\u4e2d\u200b\u7279\u6b8a\u200b\u5c42\u200b\u7684\u200b\u6784\u5efa\u200b</li> <li>LeNet\u200b\u7684\u200bPyTorch\u200b\u5b9e\u73b0\u200b</li> </ul>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/#341","title":"3.4.1 \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6784\u9020","text":"<p><code>Module</code> \u200b\u7c7b\u200b\u662f\u200b <code>torch.nn</code> \u200b\u6a21\u5757\u200b\uf9e9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6784\u9020\u200b\u7c7b\u200b\uff0c\u200b\u662f\u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u5757\u200b\u7684\u200b\u57fa\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ee7\u627f\u200b\u5b83\u200b\u6765\u200b\u5b9a\u4e49\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u4e0b\u9762\u200b\u7ee7\u627f\u200b Module \u200b\u7c7b\u200b\u6784\u9020\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u3002\u200b\u8fd9\u200b\uf9e9\u200b\u5b9a\u4e49\u200b\u7684\u200b MLP \u200b\u7c7b\u200b\u91cd\u8f7d\u200b\uf9ba\u200b <code>Module</code> \u200b\u7c7b\u200b\u7684\u200b <code>__init__</code> \u200b\u51fd\u6570\u200b\u548c\u200b <code>forward</code> \u200b\u51fd\u6570\u200b\u3002\u200b\u5b83\u4eec\u200b\u5206\u522b\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u548c\u200b\u5b9a\u4e49\u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\uff08\u200b\u6b63\u5411\u200b\u4f20\u64ad\u200b\uff09\u3002\u200b\u4e0b\u9762\u200b\u7684\u200b MLP \u200b\u7c7b\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u4e24\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u3002</p> <pre><code>import torch\nfrom torch import nn\n\nclass MLP(nn.Module):\n  # \u200b\u58f0\u660e\u200b\u5e26\u6709\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u58f0\u660e\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\n  def __init__(self, **kwargs):\n    # \u200b\u8c03\u7528\u200bMLP\u200b\u7236\u7c7b\u200bBlock\u200b\u7684\u200b\u6784\u9020\u51fd\u6570\u200b\u6765\u200b\u8fdb\u884c\u200b\u5fc5\u8981\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u8fd9\u6837\u200b\u5728\u200b\u6784\u9020\u200b\u5b9e\u4f8b\u200b\u65f6\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200b\u5176\u4ed6\u200b\u51fd\u6570\u200b\n    super(MLP, self).__init__(**kwargs)\n    self.hidden = nn.Linear(784, 256)\n    self.act = nn.ReLU()\n    self.output = nn.Linear(256,10)\n\n   # \u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5373\u200b\u5982\u4f55\u200b\u6839\u636e\u200b\u8f93\u5165\u200bx\u200b\u8ba1\u7b97\u200b\u8fd4\u56de\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\n  def forward(self, x):\n    o = self.act(self.hidden(x))\n    return self.output(o)   \n</code></pre> <p>\u200b\u4ee5\u4e0a\u200b\u7684\u200b MLP \u200b\u7c7b\u4e2d\u200b\u2f46\u200b\u987b\u200b\u5b9a\u4e49\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u51fd\u6570\u200b\u3002\u200b\u7cfb\u7edf\u200b\u5c06\u200b\u901a\u8fc7\u200b\u2f83\u200b\u52a8\u6c42\u200b\u68af\u5ea6\u200b\u2f7d\u200b\u81ea\u52a8\u200b\u2f63\u200b\u6210\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u6240\u200b\u9700\u200b\u7684\u200b <code>backward</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5b9e\u4f8b\u200b\u5316\u200b <code>MLP</code> \u200b\u7c7b\u200b\u5f97\u5230\u200b\u6a21\u578b\u200b\u53d8\u200b\uf97e\u200b <code>net</code> \u3002\u200b\u4e0b\u200b\u2faf\u200b\u7684\u200b\u4ee3\u7801\u200b\u521d\u59cb\u5316\u200b <code>net</code> \u200b\u5e76\u200b\u4f20\u5165\u200b\u8f93\u200b\u2f0a\u200b\u6570\u636e\u200b <code>X</code> \u200b\u505a\u200b\u4e00\u6b21\u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c <code>net(X)</code> \u200b\u4f1a\u200b\u8c03\u7528\u200b <code>MLP</code> \u200b\u7ee7\u627f\u200b\u2f83\u200b\u81ea\u200b <code>Module</code> \u200b\u7c7b\u200b\u7684\u200b <code>__call__</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5c06\u8c03\u200b\u2f64\u200b\u7528\u200b <code>MLP</code> \u200b\u7c7b\u200b\u5b9a\u4e49\u200b\u7684\u200b<code>forward</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b8c\u6210\u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\u3002\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u6784\u9020\u200b\u6a21\u578b\u200b\u65f6\u200b\u9700\u8981\u200b\u660e\u786e\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7684\u200b<code>forward</code>\u200b\u8fc7\u7a0b\u200b</p> <p><pre><code>X = torch.rand(2,784) # \u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b\nnet = MLP() # \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u6a21\u578b\u200b\nprint(net) # \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\nnet(X) # \u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\n</code></pre> <pre><code>MLP(\n  (hidden): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\ntensor([[ 0.0149, -0.2641, -0.0040,  0.0945, -0.1277, -0.0092,  0.0343,  0.0627,\n         -0.1742,  0.1866],\n        [ 0.0738, -0.1409,  0.0790,  0.0597, -0.1572,  0.0479, -0.0519,  0.0211,\n         -0.1435,  0.1958]], grad_fn=&lt;AddmmBackward&gt;)\n</code></pre></p> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u200b\uf9e9\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5c06\u200b <code>Module</code> \u200b\u7c7b\u200b\u547d\u540d\u200b\u4e3a\u200b <code>Layer</code> (\u200b\u5c42\u200b)\u200b\u6216\u8005\u200b <code>Model</code> (\u200b\u6a21\u578b\u200b)\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u540d\u5b57\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u8be5\u7c7b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u4f9b\u200b\u2f83\u200b\u7531\u200b\u7ec4\u5efa\u200b\u7684\u200b\u90e8\u4ef6\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u5b50\u7c7b\u200b\u65e2\u200b\u53ef\u4ee5\u200b\u662f\u200b\u2f00\u200b\u4e2a\u5c42\u200b(\u200b\u5982\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b Linear \u200b\u7c7b\u200b)\uff0c\u200b\u2f1c\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b(\u200b\u5982\u200b\u8fd9\u200b\uf9e9\u200b\u5b9a\u4e49\u200b\u7684\u200b MLP \u200b\u7c7b\u200b)\uff0c\u200b\u6216\u8005\u200b\u662f\u200b\u6a21\u578b\u200b\u7684\u200b\u2f00\u200b\u4e2a\u200b\u90e8\u5206\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/#342","title":"3.4.2 \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u5c42","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u4e2a\u200b\u9b45\u200b\uf98a\u200b\u5728\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5404\u5f0f\u5404\u6837\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u3001\u200b\u5377\u79ef\u200b\u5c42\u200b\u3001\u200b\u6c60\u5316\u5c42\u200b\u4e0e\u200b\u5faa\u73af\u200b\u5c42\u200b\u7b49\u7b49\u200b\u3002\u200b\u867d\u7136\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u2f24\u200b\uf97e\u200b\u5e38\u7528\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4f46\u200b\u6709\u65f6\u5019\u200b\u6211\u4eec\u200b\u4f9d\u7136\u200b\u5e0c\u671b\u200b\u2f83\u200b\u5b9a\u4e49\u200b\u5c42\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f1a\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b <code>Module</code> \u200b\u6765\u81ea\u200b\u5b9a\u4e49\u200b\u5c42\u200b\uff0c\u200b\u4ece\u800c\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u53cd\u590d\u200b\u8c03\u7528\u200b\u3002</p> <ul> <li>\u200b\u4e0d\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5c42\u200b</li> </ul> <p>\u200b\u6211\u4eec\u200b\u5148\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\uf967\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u5c42\u200b\u3002\u200b\u4e0b\u200b\u2faf\u200b\u6784\u9020\u200b\u7684\u200b <code>MyLayer</code> \u200b\u7c7b\u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200b <code>Module</code> \u200b\u7c7b\u200b\u81ea\u5b9a\u4e49\u200b\uf9ba\u200b\u4e00\u4e2a\u200b\u5c06\u200b\u8f93\u5165\u200b\u51cf\u6389\u200b\u5747\u503c\u200b\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5c42\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5b9a\u4e49\u200b\u5728\u200b\uf9ba\u200b forward \u200b\u51fd\u6570\u200b\uf9e9\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u5c42\u200b\uf9e9\u200b\uf967\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u3002</p> <pre><code>import torch\nfrom torch import nn\n\nclass MyLayer(nn.Module):\n    def __init__(self, **kwargs):\n        super(MyLayer, self).__init__(**kwargs)\n    def forward(self, x):\n        return x - x.mean()  \n</code></pre> <p>\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u5b9e\u4f8b\u200b\u5316\u8be5\u200b\u5c42\u200b\uff0c\u200b\u7136\u540e\u200b\u505a\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b</p> <p><pre><code>layer = MyLayer()\nlayer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))\n</code></pre> <pre><code>tensor([-2., -1.,  0.,  1.,  2.])\n</code></pre></p> <ul> <li>\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5c42\u200b</li> </ul> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u81ea\u5b9a\u4e49\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u5c42\u200b\u3002\u200b\u5176\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u5b66\u51fa\u200b\u3002</p> <p><code>Parameter</code> \u200b\u7c7b\u200b\u5176\u5b9e\u200b\u662f\u200b <code>Tensor</code> \u200b\u7684\u200b\u5b50\u7c7b\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b <code>Tensor</code> \u200b\u662f\u200b <code>Parameter</code> \uff0c\u200b\u90a3\u4e48\u200b\u5b83\u4f1a\u200b\u2f83\u200b\u52a8\u200b\u88ab\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uf99c\u200b\u8868\u200b\uf9e9\u200b\u3002\u200b\u6240\u4ee5\u200b\u5728\u200b\u2f83\u200b\u5b9a\u4e49\u200b\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5c42\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5c06\u200b\u53c2\u6570\u200b\u5b9a\u4e49\u200b\u6210\u200b <code>Parameter</code> \uff0c\u200b\u9664\u4e86\u200b\u76f4\u63a5\u200b\u5b9a\u4e49\u200b\u6210\u200b <code>Parameter</code> \u200b\u7c7b\u5916\u200b\uff0c\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u200b\u2f64\u200b <code>ParameterList</code> \u200b\u548c\u200b <code>ParameterDict</code> \u200b\u5206\u522b\u200b\u5b9a\u4e49\u200b\u53c2\u6570\u200b\u7684\u200b\uf99c\u200b\u8868\u200b\u548c\u200b\u5b57\u5178\u200b\u3002</p> <pre><code>class MyListDense(nn.Module):\n    def __init__(self):\n        super(MyListDense, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n        self.params.append(nn.Parameter(torch.randn(4, 1)))\n\n    def forward(self, x):\n        for i in range(len(self.params)):\n            x = torch.mm(x, self.params[i])\n        return x\nnet = MyListDense()\nprint(net)\n</code></pre> <pre><code>class MyDictDense(nn.Module):\n    def __init__(self):\n        super(MyDictDense, self).__init__()\n        self.params = nn.ParameterDict({\n                'linear1': nn.Parameter(torch.randn(4, 4)),\n                'linear2': nn.Parameter(torch.randn(4, 1))\n        })\n        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # \u200b\u65b0\u589e\u200b\n\n    def forward(self, x, choice='linear1'):\n        return torch.mm(x, self.params[choice])\n\nnet = MyDictDense()\nprint(net)\n</code></pre> <p>\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200b\u5e38\u89c1\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5c42\u200b\uff0c\u200b\u6bd4\u5982\u200b\u5377\u79ef\u200b\u5c42\u200b\u3001\u200b\u6c60\u5316\u5c42\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8f83\u4e3a\u200b\u57fa\u7840\u200b\u7684\u200bAlexNet\uff0cLeNet\u200b\u7b49\u200b\u3002</p> <ul> <li>\u200b\u4e8c\u7ef4\u200b\u5377\u79ef\u200b\u5c42\u200b</li> </ul> <p>\u200b\u4e8c\u7ef4\u200b\u5377\u79ef\u200b\u5c42\u200b\u5c06\u200b\u8f93\u5165\u200b\u548c\u200b\u5377\u79ef\u200b\u6838\u505a\u200b\u4e92\u76f8\u200b\u5173\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u5e76\u200b\u52a0\u4e0a\u200b\u4e00\u4e2a\u200b\u6807\u200b\uf97e\u200b\u504f\u5dee\u200b\u6765\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u3002\u200b\u5377\u79ef\u200b\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5305\u62ec\u200b\uf9ba\u200b\u5377\u79ef\u200b\u6838\u548c\u6807\u200b\uf97e\u200b\u504f\u5dee\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u901a\u5e38\u200b\u6211\u4eec\u200b\u5148\u5bf9\u200b\u5377\u79ef\u200b\u6838\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u7136\u540e\u200b\uf967\u200b\u65ad\u200b\u8fed\u4ee3\u200b\u5377\u79ef\u200b\u6838\u200b\u548c\u200b\u504f\u5dee\u200b\u3002</p> <pre><code>import torch\nfrom torch import nn\n\n# \u200b\u5377\u79ef\u200b\u8fd0\u7b97\u200b\uff08\u200b\u4e8c\u7ef4\u200b\u4e92\u76f8\u200b\u5173\u200b\uff09\ndef corr2d(X, K): \n    h, w = K.shape\n    X, K = X.float(), K.float()\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n    return Y\n\n# \u200b\u4e8c\u7ef4\u200b\u5377\u79ef\u200b\u5c42\u200b\nclass Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super(Conv2D, self).__init__()\n        self.weight = nn.Parameter(torch.randn(kernel_size))\n        self.bias = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n</code></pre> <p>\u200b\u5377\u79ef\u200b\u7a97\u53e3\u200b\u5f62\u72b6\u200b\u4e3a\u200b \\(p \\times q\\) \u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u79f0\u4e3a\u200b \\(p \\times q\\)  \u200b\u5377\u79ef\u200b\u5c42\u200b\u3002\u200b\u540c\u6837\u200b\uff0c  \\(p \\times q\\) \u200b\u5377\u79ef\u200b\u6216\u200b \\(p \\times q\\) \u200b\u5377\u79ef\u200b\u6838\u200b\u8bf4\u660e\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u5206\u522b\u200b\u4e3a\u200b \\(p\\) \u200b\u548c\u200b \\(q\\)\uff0c\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\\(p=q\\)\u3002</p> <p>\u200b\u586b\u5145\u200b(padding)\u200b\u662f\u200b\u6307\u200b\u5728\u200b\u8f93\u200b\u2f0a\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u7684\u200b\u4e24\u4fa7\u200b\u586b\u5145\u200b\u5143\u7d20\u200b(\u200b\u901a\u5e38\u200b\u662f\u200b0\u200b\u5143\u7d20\u200b)\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u7684\u200b\uf9b5\u200b\u5b50\u91cc\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u2fbc\u200b\u548c\u200b\u5bbd\u200b\u4e3a\u200b3\u200b\u7684\u200b\u4e8c\u7ef4\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u7136\u540e\u200b\u8bbe\u8f93\u200b\u2f0a\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4e24\u4fa7\u200b\u7684\u200b\u586b\u5145\u200b\u6570\u200b\u5206\u522b\u200b\u4e3a\u200b1\u3002\u200b\u7ed9\u5b9a\u200b\u4e00\u4e2a\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4e3a\u200b8\u200b\u7684\u200b\u8f93\u5165\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u8f93\u51fa\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4e5f\u200b\u662f\u200b8\u3002</p> <p><pre><code>import torch\nfrom torch import nn\n\n# \u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u8ba1\u7b97\u200b\u5377\u79ef\u200b\u5c42\u200b\u3002\u200b\u5b83\u200b\u5bf9\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u505a\u200b\u76f8\u5e94\u200b\u7684\u200b\u5347\u200b\u7ef4\u548c\u200b\u964d\u7ef4\u200b\ndef comp_conv2d(conv2d, X):\n    # (1, 1)\u200b\u4ee3\u8868\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u548c\u200b\u901a\u9053\u200b\u6570\u200b\n    X = X.view((1, 1) + X.shape)\n    Y = conv2d(X)\n    return Y.view(Y.shape[2:]) # \u200b\u6392\u9664\u200b\u4e0d\u200b\u5173\u5fc3\u200b\u7684\u200b\u524d\u200b\u4e24\u7ef4\u200b:\u200b\u6279\u91cf\u200b\u548c\u200b\u901a\u9053\u200b\n\n\n# \u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u662f\u200b\u4e24\u4fa7\u200b\u5206\u522b\u200b\u586b\u5145\u200b1\u200b\u2f8f\u200b\u6216\u5217\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b\u4e24\u4fa7\u200b\u4e00\u5171\u200b\u586b\u5145\u200b2\u200b\u2f8f\u200b\u6216\u5217\u200b\nconv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)\n\nX = torch.rand(8, 8)\ncomp_conv2d(conv2d, X).shape\n</code></pre> <pre><code>torch.Size([8, 8])\n</code></pre></p> <p>\u200b\u5f53\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\uf967\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4e0a\u200b\uf967\u200b\u540c\u200b\u7684\u200b\u586b\u5145\u200b\u6570\u4f7f\u200b\u8f93\u51fa\u200b\u548c\u200b\u8f93\u5165\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u3002</p> <p><pre><code># \u200b\u4f7f\u7528\u200b\u9ad8\u4e3a\u200b5\u3001\u200b\u5bbd\u4e3a\u200b3\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u200b\u3002\u200b\u5728\u200b\u2fbc\u200b\u548c\u200b\u5bbd\u200b\u4e24\u4fa7\u200b\u7684\u200b\u586b\u5145\u200b\u6570\u200b\u5206\u522b\u200b\u4e3a\u200b2\u200b\u548c\u200b1\nconv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\ncomp_conv2d(conv2d, X).shape\n</code></pre> <pre><code>torch.Size([8, 8])\n</code></pre> \u200b\u5728\u200b\u4e8c\u7ef4\u200b\u4e92\u76f8\u200b\u5173\u200b\u8fd0\u7b97\u200b\u4e2d\u200b\uff0c\u200b\u5377\u79ef\u200b\u7a97\u53e3\u200b\u4ece\u200b\u8f93\u5165\u200b\u6570\u7ec4\u200b\u7684\u200b\u6700\u200b\u5de6\u4e0a\u65b9\u200b\u5f00\u59cb\u200b\uff0c\u200b\u6309\u200b\u4ece\u5de6\u5f80\u53f3\u200b\u3001\u200b\u4ece\u200b\u4e0a\u200b\u5f80\u200b\u4e0b\u200b \u200b\u7684\u200b\u987a\u5e8f\u200b\uff0c\u200b\u4f9d\u6b21\u200b\u5728\u200b\u8f93\u200b\u2f0a\u200b\u6570\u7ec4\u200b\u4e0a\u200b\u6ed1\u52a8\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bcf\u6b21\u200b\u6ed1\u52a8\u200b\u7684\u200b\ufa08\u200b\u6570\u200b\u548c\u200b\uf99c\u200b\u6570\u200b\u79f0\u4e3a\u200b\u6b65\u5e45\u200b(stride)\u3002</p> <p><pre><code>conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\ncomp_conv2d(conv2d, X).shape\n</code></pre> <pre><code>torch.Size([2, 2])\n</code></pre></p> <p>\u200b\u586b\u5145\u200b\u53ef\u4ee5\u200b\u589e\u52a0\u200b\u8f93\u51fa\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u3002\u200b\u8fd9\u200b\u5e38\u7528\u200b\u6765\u200b\u4f7f\u200b\u8f93\u51fa\u200b\u4e0e\u200b\u8f93\u5165\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u3002</p> <p>\u200b\u6b65\u5e45\u200b\u53ef\u4ee5\u200b\u51cf\u5c0f\u200b\u8f93\u51fa\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\uff0c\u200b\uf9b5\u200b\u5982\u200b\u8f93\u51fa\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4ec5\u200b\u4e3a\u200b\u8f93\u5165\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u7684\u200b ( \u200b\u4e3a\u200b\u5927\u4e8e\u200b1\u200b\u7684\u200b\u6574\u6570\u200b)\u3002</p> <ul> <li>\u200b\u6c60\u5316\u5c42\u200b</li> </ul> <p>\u200b\u6c60\u5316\u5c42\u200b\u6bcf\u6b21\u200b\u5bf9\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u4e00\u4e2a\u200b\u56fa\u5b9a\u200b\u5f62\u72b6\u200b\u7a97\u53e3\u200b(\u200b\u2f1c\u200b\u79f0\u6c60\u5316\u200b\u7a97\u53e3\u200b)\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b\u3002\u200b\uf967\u200b\u540c\u200b\u4e8e\u200b\u5377\u79ef\u200b\u5c42\u200b\uf9e9\u200b\u8ba1\u7b97\u200b\u8f93\u200b\u2f0a\u200b\u548c\u200b\u6838\u200b\u7684\u200b\u4e92\u200b\u76f8\u5173\u6027\u200b\uff0c\u200b\u6c60\u5316\u5c42\u200b\u76f4\u63a5\u200b\u8ba1\u7b97\u200b\u6c60\u5316\u200b\u7a97\u53e3\u200b\u5185\u200b\u5143\u7d20\u200b\u7684\u200b\u5c5e\u6027\u200b\uff08\u200b\u5747\u503c\u200b\u3001\u200b\u6700\u5927\u503c\u200b\u7b49\u200b\uff09\u3002\u200b\u5e38\u89c1\u200b\u7684\u200b\u6c60\u5316\u200b\u5305\u62ec\u200b\u6700\u5927\u200b\u6c60\u5316\u200b\u6216\u200b\u5e73\u5747\u200b\u6c60\u5316\u200b\u3002\u200b\u5728\u200b\u4e8c\u7ef4\u200b\u6700\u200b\u2f24\u200b\u6c60\u5316\u200b\u4e2d\u200b\uff0c\u200b\u6c60\u5316\u200b\u7a97\u53e3\u200b\u4ece\u200b\u8f93\u5165\u200b\u6570\u7ec4\u200b\u7684\u200b\u6700\u200b\u5de6\u4e0a\u65b9\u200b\u5f00\u59cb\u200b\uff0c\u200b\u6309\u200b\u4ece\u5de6\u5f80\u53f3\u200b\u3001\u200b\u4ece\u200b\u4e0a\u200b\u5f80\u200b\u4e0b\u200b\u7684\u200b\u987a\u5e8f\u200b\uff0c\u200b\u4f9d\u6b21\u200b\u5728\u200b\u8f93\u200b\u2f0a\u200b\u6570\u7ec4\u200b\u4e0a\u200b\u6ed1\u52a8\u200b\u3002\u200b\u5f53\u6c60\u5316\u200b\u7a97\u53e3\u200b\u6ed1\u52a8\u200b\u5230\u200b\u67d0\u200b\u2f00\u200b\u4f4d\u7f6e\u200b\u65f6\u200b\uff0c\u200b\u7a97\u53e3\u200b\u4e2d\u200b\u7684\u200b\u8f93\u5165\u200b\u5b50\u200b\u6570\u7ec4\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u5373\u200b\u8f93\u51fa\u200b\u6570\u7ec4\u200b\u4e2d\u200b\u76f8\u5e94\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u5143\u7d20\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u628a\u200b\u6c60\u5316\u5c42\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u5b9e\u73b0\u200b\u5728\u200b<code>pool2d</code>\u200b\u51fd\u6570\u200b\u91cc\u200b\u3002</p> <pre><code>import torch\nfrom torch import nn\n\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n            elif mode == 'avg':\n                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n    return Y\n</code></pre> <p><pre><code>X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\npool2d(X, (2, 2))\n</code></pre> <pre><code>tensor([[4., 5.],\n    [7., 8.]])\n</code></pre> <pre><code>pool2d(X, (2, 2), 'avg')\n</code></pre> <pre><code>tensor([[2., 3.],\n    [5., 6.]])\n</code></pre></p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torch.nn</code>\u200b\u5305\u6765\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4ecb\u7ecd\u200b\u4e86\u200b<code>autograd</code>\u200b\u5305\u200b\uff0c<code>nn</code>\u200b\u5305\u5219\u200b\u4f9d\u8d56\u4e8e\u200b<code>autograd</code>\u200b\u5305\u6765\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u5e76\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u6c42\u5bfc\u200b\u3002\u200b\u4e00\u4e2a\u200b<code>nn.Module</code>\u200b\u5305\u542b\u200b\u5404\u4e2a\u200b\u5c42\u200b\u548c\u200b\u4e00\u4e2a\u200b<code>forward(input)</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u8fd4\u56de\u200b<code>output</code>\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.4%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/#343","title":"3.4.3 \u200b\u6a21\u578b\u200b\u793a\u4f8b","text":"<ul> <li>LeNet</li> </ul> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u524d\u9988\u200b\u795e\u7ecf\u7f51\u7edc\u200b (feed-forward network\uff09\uff08LeNet\uff09\u3002\u200b\u5b83\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5b83\u200b\u9001\u5165\u200b\u4e0b\u200b\u4e00\u5c42\u200b\uff0c\u200b\u4e00\u5c42\u200b\u63a5\u200b\u4e00\u5c42\u200b\u7684\u200b\u4f20\u9012\u200b\uff0c\u200b\u6700\u540e\u200b\u7ed9\u51fa\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5178\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u5b9a\u4e49\u200b\u5305\u542b\u200b\u4e00\u4e9b\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b(\u200b\u6216\u8005\u200b\u53eb\u200b\u6743\u91cd\u200b\uff09\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b</li> <li>\u200b\u5728\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fed\u4ee3\u200b</li> <li>\u200b\u901a\u8fc7\u200b\u7f51\u7edc\u200b\u5904\u7406\u200b\u8f93\u5165\u200b</li> <li>\u200b\u8ba1\u7b97\u200b loss (\u200b\u8f93\u51fa\u200b\u548c\u200b\u6b63\u786e\u200b\u7b54\u6848\u200b\u7684\u200b\u8ddd\u79bb\u200b\uff09</li> <li>\u200b\u5c06\u200b\u68af\u5ea6\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7ed9\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b</li> <li>\u200b\u66f4\u65b0\u200b\u7f51\u7edc\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u4e00\u822c\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u89c4\u5219\u200b\uff1a<code>weight = weight - learning_rate * gradient</code></li> </ol> <p><pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # \u200b\u8f93\u5165\u200b\u56fe\u50cf\u200bchannel\uff1a1\uff1b\u200b\u8f93\u51fa\u200bchannel\uff1a6\uff1b5x5\u200b\u5377\u79ef\u200b\u6838\u200b\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 2x2 Max pooling\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # \u200b\u5982\u679c\u200b\u662f\u200b\u65b9\u9635\u200b,\u200b\u5219\u200b\u53ef\u4ee5\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u8fdb\u884c\u200b\u5b9a\u4e49\u200b\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # \u200b\u9664\u53bb\u200b\u6279\u5904\u7406\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5176\u4ed6\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnet = Net()\nprint(net)\n</code></pre> <pre><code>Net(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n</code></pre></p> <p>\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5b9a\u4e49\u200b <code>forward</code> \u200b\u51fd\u6570\u200b\uff0c<code>backward</code>\u200b\u51fd\u6570\u200b\u4f1a\u200b\u5728\u200b\u4f7f\u7528\u200b<code>autograd</code>\u200b\u65f6\u200b\u81ea\u52a8\u200b\u5b9a\u4e49\u200b\uff0c<code>backward</code>\u200b\u51fd\u6570\u200b\u7528\u6765\u200b\u8ba1\u7b97\u200b\u5bfc\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>forward</code> \u200b\u51fd\u6570\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u4efb\u4f55\u200b\u9488\u5bf9\u200b\u5f20\u91cf\u200b\u7684\u200b\u64cd\u4f5c\u200b\u548c\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>net.parameters()</code>\u200b\u8fd4\u56de\u200b</p> <p><pre><code>params = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1\u200b\u7684\u200b\u6743\u91cd\u200b\n</code></pre> <pre><code>10\ntorch.Size([6, 1, 5, 5])\n</code></pre> \u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b 32x32 \u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u6ce8\u610f\u200b:\u200b\u8fd9\u4e2a\u200b\u7f51\u7edc\u200b (LeNet\uff09\u200b\u7684\u200b\u671f\u5f85\u200b\u8f93\u5165\u200b\u662f\u200b 32x32 \u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b MNIST \u200b\u6570\u636e\u200b\u96c6\u6765\u200b\u8bad\u7ec3\u200b\u8fd9\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u8981\u200b\u628a\u200b\u56fe\u7247\u5927\u5c0f\u200b\u91cd\u65b0\u200b\u8c03\u6574\u200b\u5230\u200b 32x32\u3002</p> <pre><code>input = torch.randn(1, 1, 32, 32)\nout = net(input)\nprint(out)\n</code></pre> <p>\u200b\u6e05\u96f6\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u7f13\u5b58\u200b\uff0c\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff1a</p> <pre><code>net.zero_grad()\nout.backward(torch.randn(1, 10))\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a<code>torch.nn</code>\u200b\u53ea\u200b\u652f\u6301\u200b\u5c0f\u6279\u91cf\u200b\u5904\u7406\u200b (mini-batches\uff09\u3002\u200b\u6574\u4e2a\u200b <code>torch.nn</code> \u200b\u5305\u53ea\u200b\u652f\u6301\u200b\u5c0f\u6279\u91cf\u200b\u6837\u672c\u200b\u7684\u200b\u8f93\u5165\u200b\uff0c\u200b\u4e0d\u200b\u652f\u6301\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c<code>nn.Conv2d</code> \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b4\u200b\u7ef4\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5373\u200b<code>nSamples x nChannels x Height x Width</code>\u200b\u5982\u679c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b<code>input.unsqueeze(0)</code> \u200b\u6765\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u201c\u200b\u5047\u200b\u7684\u200b\u201d\u200b\u6279\u200b\u5927\u5c0f\u200b\u7ef4\u5ea6\u200b\u3002</p> <ul> <li> <p><code>torch.Tensor</code> - \u200b\u4e00\u4e2a\u200b\u591a\u7ef4\u200b\u6570\u7ec4\u200b\uff0c\u200b\u652f\u6301\u200b\u8bf8\u5982\u200b<code>backward()</code>\u200b\u7b49\u200b\u7684\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u5f20\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002</p> </li> <li> <p><code>nn.Module</code>- \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u5757\u200b\u3002\u200b\u662f\u200b\u4e00\u79cd\u200b\u65b9\u4fbf\u200b\u5c01\u88c5\u200b\u53c2\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5177\u6709\u200b\u5c06\u200b\u53c2\u6570\u200b\u79fb\u52a8\u200b\u5230\u200bGPU\u3001\u200b\u5bfc\u51fa\u200b\u3001\u200b\u52a0\u8f7d\u200b\u7b49\u200b\u529f\u80fd\u200b\u3002</p> </li> <li> <p><code>nn.Parameter</code>- \u200b\u5f20\u91cf\u200b\u7684\u200b\u4e00\u79cd\u200b\uff0c\u200b\u5f53\u200b\u5b83\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u5c5e\u6027\u200b\u5206\u914d\u200b\u7ed9\u200b\u4e00\u4e2a\u200b<code>Module</code>\u200b\u65f6\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u88ab\u200b\u81ea\u52a8\u200b\u6ce8\u518c\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u3002</p> </li> <li> <p><code>autograd.Function</code> - \u200b\u5b9e\u73b0\u200b\u4e86\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u524d\u200b\u5411\u200b\u548c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u6bcf\u4e2a\u200b<code>Tensor</code>\u200b\u81f3\u5c11\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b<code>Function</code>\u200b\u8282\u70b9\u200b\uff0c\u200b\u8be5\u200b\u8282\u70b9\u200b\u8fde\u63a5\u200b\u5230\u200b\u521b\u5efa\u200b<code>Tensor</code>\u200b\u7684\u200b\u51fd\u6570\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u5386\u53f2\u200b\u8fdb\u884c\u200b\u7f16\u7801\u200b\u3002</p> </li> </ul> <p>\u200b\u4e0b\u9762\u200b\u518d\u200b\u4ecb\u7ecd\u200b\u4e00\u4e2a\u200b\u6bd4\u8f83\u200b\u57fa\u7840\u200b\u7684\u200b\u6848\u4f8b\u200bAlexNet</p> <ul> <li>AlexNet</li> </ul> <p></p> <pre><code>class AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2), # kernel_size, stride\n            # \u200b\u51cf\u5c0f\u200b\u5377\u79ef\u200b\u7a97\u53e3\u200b\uff0c\u200b\u4f7f\u7528\u200b\u586b\u5145\u200b\u4e3a\u200b2\u200b\u6765\u200b\u4f7f\u5f97\u200b\u8f93\u5165\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\u4e00\u81f4\u200b\uff0c\u200b\u4e14\u200b\u589e\u5927\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\n            nn.Conv2d(96, 256, 5, 1, 2),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2),\n            # \u200b\u8fde\u7eed\u200b3\u200b\u4e2a\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u4e14\u200b\u4f7f\u7528\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u5377\u79ef\u200b\u7a97\u53e3\u200b\u3002\u200b\u9664\u4e86\u200b\u6700\u540e\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u5916\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u589e\u5927\u200b\u4e86\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\u3002\n            # \u200b\u524d\u200b\u4e24\u4e2a\u200b\u5377\u79ef\u200b\u5c42\u540e\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u6c60\u5316\u5c42\u200b\u6765\u200b\u51cf\u5c0f\u200b\u8f93\u5165\u200b\u7684\u200b\u9ad8\u200b\u548c\u200b\u5bbd\u200b\n            nn.Conv2d(256, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2)\n        )\n         # \u200b\u8fd9\u91cc\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e2a\u6570\u200b\u6bd4\u200bLeNet\u200b\u4e2d\u200b\u7684\u200b\u5927\u200b\u6570\u500d\u200b\u3002\u200b\u4f7f\u7528\u200b\u4e22\u5f03\u200b\u5c42\u6765\u200b\u7f13\u89e3\u200b\u8fc7\u200b\u62df\u5408\u200b\n        self.fc = nn.Sequential(\n            nn.Linear(256*5*5, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            # \u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002\u200b\u7531\u4e8e\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200bFashion-MNIST\uff0c\u200b\u6240\u4ee5\u200b\u7528\u200b\u7c7b\u522b\u200b\u6570\u4e3a\u200b10\uff0c\u200b\u800c\u200b\u975e\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b1000\n            nn.Linear(4096, 10),\n        )\n\n    def forward(self, img):\n        feature = self.conv(img)\n        output = self.fc(feature.view(img.shape[0], -1))\n        return output\n</code></pre> <p><pre><code>net = AlexNet()\nprint(net)\n</code></pre> <pre><code>AlexNet(\n  (conv): Sequential(\n    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU()\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU()\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=6400, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n</code></pre></p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.5%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/","title":"3.5 \u200b\u6a21\u578b\u200b\u521d\u59cb\u5316","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0c\u200b\u6743\u91cd\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\u6781\u4e3a\u91cd\u8981\u200b\u3002\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\uff0c\u200b\u4f1a\u200b\u4f7f\u200b\u6a21\u578b\u200b\u6536\u655b\u200b\u901f\u5ea6\u200b\u63d0\u9ad8\u200b\uff0c\u200b\u4f7f\u200b\u6a21\u578b\u200b\u51c6\u786e\u7387\u200b\u66f4\u200b\u7cbe\u786e\u200b\u3002\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u5168\u200b0\u200b\u521d\u59cb\u503c\u200b\u8bad\u7ec3\u200b\u7f51\u7edc\u200b\u3002\u200b\u4e3a\u4e86\u200b\u5229\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u51cf\u5c11\u200b\u6536\u655b\u200b\u65f6\u95f4\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u5408\u7406\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u3002PyTorch\u200b\u4e5f\u200b\u5728\u200b<code>torch.nn.init</code>\u200b\u4e2d\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\u3002 \u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u5b66\u4e60\u200b\u5230\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a - \u200b\u5e38\u89c1\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u51fd\u6570\u200b - \u200b\u521d\u59cb\u5316\u200b\u51fd\u6570\u200b\u7684\u200b\u4f7f\u7528\u200b</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.5%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/#torchnninit","title":"torch.nn.init\u200b\u5185\u5bb9","text":"<p>\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200btorch.nn.init\u200b\u7684\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u94fe\u63a5\u200b \uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b<code>torch.nn.init</code>\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\uff1a 1 . <code>torch.nn.init.uniform_</code>(tensor, a=0.0, b=1.0) 2 . <code>torch.nn.init.normal_</code>(tensor, mean=0.0, std=1.0) 3 . <code>torch.nn.init.constant_</code>(tensor, val) 4 . <code>torch.nn.init.ones_</code>(tensor) 5 . <code>torch.nn.init.zeros_</code>(tensor) 6 . <code>torch.nn.init.eye_</code>(tensor) 7 . <code>torch.nn.init.dirac_</code>(tensor, groups=1) 8 . <code>torch.nn.init.xavier_uniform_</code>(tensor, gain=1.0) 9 . <code>torch.nn.init.xavier_normal_</code>(tensor, gain=1.0) 10 . <code>torch.nn.init.kaiming_uniform_</code>(tensor, a=0, mode='fan__in', nonlinearity='leaky_relu') 11 . <code>torch.nn.init.kaiming_normal_</code>(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') 12 . <code>torch.nn.init.orthogonal_</code>(tensor, gain=1) 13 . <code>torch.nn.init.sparse_</code>(tensor, sparsity, std=0.01) 14 .  <code>torch.nn.init.calculate_gain</code>(nonlinearity, param=None) \u200b\u5173\u4e8e\u200b\u8ba1\u7b97\u200b\u589e\u76ca\u200b\u5982\u4e0b\u200b\u8868\u200b\uff1a |nonlinearity|gain| | ---- | ---- | |Linear/Identity|1| |Conv{1,2,3}D|1| |Sigmod|1| |Tanh|5/3| |ReLU|sqrt(2)| |Leaky Relu|sqrt(2/1+neg_slop^2)|</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u9664\u4e86\u200b<code>calculate_gain</code>\uff0c\u200b\u6240\u6709\u200b\u51fd\u6570\u200b\u7684\u200b\u540e\u7f00\u200b\u90fd\u200b\u5e26\u6709\u200b\u4e0b\u5212\u7ebf\u200b\uff0c\u200b\u610f\u5473\u7740\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u5c06\u4f1a\u200b\u76f4\u63a5\u200b\u539f\u5730\u200b\u66f4\u6539\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b\u7684\u200b\u503c\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.5%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/#torchnninit_1","title":"torch.nn.init\u200b\u4f7f\u7528","text":"<p>\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u4f1a\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u6a21\u578b\u200b\u6765\u200b\u4f7f\u7528\u200b<code>torch.nn.init</code>\u200b\u8fdb\u884c\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b<code>isinstance()</code>\u200b\u6765\u200b\u8fdb\u884c\u200b\u5224\u65ad\u200b\u6a21\u5757\u200b\uff08\u200b\u56de\u987e\u200b3.4\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\uff09\u200b\u5c5e\u4e8e\u200b\u4ec0\u4e48\u200b\u7c7b\u578b\u200b\u3002 <pre><code>import torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(1,3,3)\nlinear = nn.Linear(10,1)\n\nisinstance(conv,nn.Conv2d) # \u200b\u5224\u65ad\u200bconv\u200b\u662f\u5426\u662f\u200bnn.Conv2d\u200b\u7c7b\u578b\u200b\nisinstance(linear,nn.Conv2d) # \u200b\u5224\u65ad\u200blinear\u200b\u662f\u5426\u662f\u200bnn.Conv2d\u200b\u7c7b\u578b\u200b\n</code></pre> <pre><code>True\nFalse\n</code></pre> \u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u578b\u200b\u5c42\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u4e0d\u540c\u200b\u7684\u200b\u6743\u503c\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002 <pre><code># \u200b\u67e5\u770b\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u7684\u200bconv\u200b\u53c2\u6570\u200b\nconv.weight.data\n# \u200b\u67e5\u770b\u200blinear\u200b\u7684\u200b\u53c2\u6570\u200b\nlinear.weight.data\n</code></pre> <pre><code>tensor([[[[ 0.1174,  0.1071,  0.2977],\n          [-0.2634, -0.0583, -0.2465],\n          [ 0.1726, -0.0452, -0.2354]]],\n        [[[ 0.1382,  0.1853, -0.1515],\n          [ 0.0561,  0.2798, -0.2488],\n          [-0.1288,  0.0031,  0.2826]]],\n        [[[ 0.2655,  0.2566, -0.1276],\n          [ 0.1905, -0.1308,  0.2933],\n          [ 0.0557, -0.1880,  0.0669]]]])\n\ntensor([[-0.0089,  0.1186,  0.1213, -0.2569,  0.1381,  0.3125,  0.1118, -0.0063, -0.2330,  0.1956]])\n</code></pre> <pre><code># \u200b\u5bf9\u200bconv\u200b\u8fdb\u884c\u200bkaiming\u200b\u521d\u59cb\u5316\u200b\ntorch.nn.init.kaiming_normal_(conv.weight.data)\nconv.weight.data\n# \u200b\u5bf9\u200blinear\u200b\u8fdb\u884c\u200b\u5e38\u6570\u200b\u521d\u59cb\u5316\u200b\ntorch.nn.init.constant_(linear.weight.data,0.3)\nlinear.weight.data\n</code></pre> <pre><code>tensor([[[[ 0.3249, -0.0500,  0.6703],\n          [-0.3561,  0.0946,  0.4380],\n          [-0.9426,  0.9116,  0.4374]]],\n        [[[ 0.6727,  0.9885,  0.1635],\n          [ 0.7218, -1.2841, -0.2970],\n          [-0.9128, -0.1134, -0.3846]]],\n        [[[ 0.2018,  0.4668, -0.0937],\n          [-0.2701, -0.3073,  0.6686],\n          [-0.3269, -0.0094,  0.3246]]]])\ntensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,0.3000]])\n</code></pre></p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.5%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/#_1","title":"\u521d\u59cb\u5316\u200b\u51fd\u6570\u200b\u7684\u200b\u5c01\u88c5","text":"<p>\u200b\u4eba\u4eec\u200b\u5e38\u5e38\u200b\u5c06\u200b\u5404\u79cd\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\u5b9a\u4e49\u200b\u4e3a\u200b\u4e00\u4e2a\u200b<code>initialize_weights()</code>\u200b\u7684\u200b\u51fd\u6570\u200b\u5e76\u200b\u5728\u200b\u6a21\u578b\u200b\u521d\u59cb\u200b\u540e\u200b\u8fdb\u884c\u200b\u4f7f\u7528\u200b\u3002 <pre><code>def initialize_weights(model):\n    for m in model.modules():\n        # \u200b\u5224\u65ad\u200b\u662f\u5426\u200b\u5c5e\u4e8e\u200bConv2d\n        if isinstance(m, nn.Conv2d):\n            torch.nn.init.zeros_(m.weight.data)\n            # \u200b\u5224\u65ad\u200b\u662f\u5426\u200b\u6709\u200b\u504f\u7f6e\u200b\n            if m.bias is not None:\n                torch.nn.init.constant_(m.bias.data,0.3)\n        elif isinstance(m, nn.Linear):\n            torch.nn.init.normal_(m.weight.data, 0.1)\n            if m.bias is not None:\n                torch.nn.init.zeros_(m.bias.data)\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)       \n            m.bias.data.zeros_()    \n</code></pre> \u200b\u8fd9\u200b\u6bb5\u200b\u4ee3\u7801\u200b\u6d41\u7a0b\u200b\u662f\u200b\u904d\u5386\u200b\u5f53\u524d\u200b\u6a21\u578b\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\uff0c\u200b\u7136\u540e\u200b\u5224\u65ad\u200b\u5404\u5c42\u200b\u5c5e\u4e8e\u200b\u4ec0\u4e48\u200b\u7c7b\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u6839\u636e\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u5c42\u200b\uff0c\u200b\u8bbe\u5b9a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6743\u503c\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e0b\u9762\u200b\u7684\u200b\u4f8b\u7a0b\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u7b80\u77ed\u200b\u7684\u200b\u6f14\u793a\u200b\uff1a <pre><code># \u200b\u6a21\u578b\u200b\u7684\u200b\u5b9a\u4e49\u200b\nclass MLP(nn.Module):\n  # \u200b\u58f0\u660e\u200b\u5e26\u6709\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u58f0\u660e\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\n  def __init__(self, **kwargs):\n    # \u200b\u8c03\u7528\u200bMLP\u200b\u7236\u7c7b\u200bBlock\u200b\u7684\u200b\u6784\u9020\u51fd\u6570\u200b\u6765\u200b\u8fdb\u884c\u200b\u5fc5\u8981\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u8fd9\u6837\u200b\u5728\u200b\u6784\u9020\u200b\u5b9e\u4f8b\u200b\u65f6\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200b\u5176\u4ed6\u200b\u51fd\u6570\u200b\n    super(MLP, self).__init__(**kwargs)\n    self.hidden = nn.Conv2d(1,1,3)\n    self.act = nn.ReLU()\n    self.output = nn.Linear(10,1)\n\n   # \u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5373\u200b\u5982\u4f55\u200b\u6839\u636e\u200b\u8f93\u5165\u200bx\u200b\u8ba1\u7b97\u200b\u8fd4\u56de\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\n  def forward(self, x):\n    o = self.act(self.hidden(x))\n    return self.output(o)\n\nmlp = MLP()\nprint(mlp.hidden.weight.data)\nprint(\"-------\u200b\u521d\u59cb\u5316\u200b-------\")\n\nmlp.apply(initialize_weights)\n# \u200b\u6216\u8005\u200binitialize_weights(mlp)\nprint(mlp.hidden.weight.data)\n</code></pre> <pre><code>tensor([[[[ 0.3069, -0.1865,  0.0182],\n          [ 0.2475,  0.3330,  0.1352],\n          [-0.0247, -0.0786,  0.1278]]]])\n\"-------\u200b\u521d\u59cb\u5316\u200b-------\"\ntensor([[[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]]])\n</code></pre> \u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5728\u200b\u521d\u59cb\u5316\u200b\u65f6\u200b\uff0c\u200b\u6700\u597d\u200b\u4e0d\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b0\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u6837\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\uff0c\u200b\u4ece\u800c\u200b\u5f71\u54cd\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u521d\u59cb\u5316\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200b\u521d\u59cb\u5316\u200b\u65b9\u6cd5\u200b\u6216\u8005\u200b\u5c06\u200b\u6a21\u578b\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5f88\u5c0f\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5982\u200b0.01\uff0c0.1\u200b\u7b49\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"3.6 \u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e7f\u4e3a\u200b\u4f7f\u7528\u200b\u7684\u200b\u4eca\u5929\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8111\u6d77\u200b\u91cc\u200b\u6e05\u6670\u200b\u7684\u200b\u77e5\u9053\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u60f3\u8981\u200b\u8fbe\u5230\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u6548\u679c\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u5e38\u8bf4\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u79bb\u4e0d\u5f00\u200b\u4f18\u8d28\u200b\u7684\u200b\u8d1f\u53cd\u9988\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5c31\u662f\u200b\u6a21\u578b\u200b\u7684\u200b\u8d1f\u53cd\u9988\u200b\u3002</p> <p></p> <p>\u200b\u6240\u4ee5\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u662f\u200b\u5fc5\u4e0d\u53ef\u5c11\u200b\u7684\u200b\u3002\u200b\u5b83\u200b\u662f\u200b\u6570\u636e\u200b\u8f93\u5165\u200b\u5230\u200b\u6a21\u578b\u200b\u5f53\u4e2d\u200b\uff0c\u200b\u4ea7\u751f\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u7684\u200b\u8bc4\u4ef7\u200b\u6307\u6807\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u6309\u7167\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u76ee\u6807\u200b\u6765\u200b\u505a\u51fa\u200b\u6539\u8fdb\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f00\u59cb\u200b\u63a2\u7d22\u200bPyTorch\u200b\u7684\u200b\u6240\u200b\u62e5\u6709\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u8fd9\u91cc\u200b\u5c06\u200b\u5217\u51fa\u200bPyTorch\u200b\u4e2d\u200b\u5e38\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u4e00\u822c\u200b\u901a\u8fc7\u200btorch.nn\u200b\u8c03\u7528\u200b\uff09\uff0c\u200b\u5e76\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u6bcf\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u529f\u80fd\u200b\u4ecb\u7ecd\u200b\u3001\u200b\u6570\u5b66\u516c\u5f0f\u200b\u548c\u200b\u8c03\u7528\u200b\u4ee3\u7801\u200b\u3002\u200b\u5f53\u7136\u200b\uff0cPyTorch\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8fd8\u200b\u8fdc\u4e0d\u6b62\u200b\u8fd9\u4e9b\u200b\uff0c\u200b\u5728\u200b\u89e3\u51b3\u200b\u5b9e\u9645\u200b\u95ee\u9898\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9700\u8981\u200b\u8fdb\u4e00\u6b65\u200b\u63a2\u7d22\u200b\u3001\u200b\u501f\u9274\u200b\u73b0\u6709\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u6216\u8005\u200b\u8bbe\u8ba1\u200b\u81ea\u5df1\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u53ca\u5176\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b</li> <li>PyTorch\u200b\u4e2d\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8c03\u7528\u200b</li> </ul>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#351","title":"3.5.1 \u200b\u4e8c\u200b\u5206\u7c7b\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<pre><code>torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u200b\u529f\u80fd\u200b\uff1a\u200b\u8ba1\u7b97\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u65f6\u200b\u7684\u200b\u4ea4\u53c9\u200b\u71b5\u200b\uff08Cross Entropy\uff09\u200b\u51fd\u6570\u200b\u3002\u200b\u5728\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u4e2d\u200b\uff0clabel\u200b\u662f\u200b{0,1}\u3002\u200b\u5bf9\u4e8e\u200b\u8fdb\u5165\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u51fd\u6570\u200b\u7684\u200binput\u200b\u4e3a\u200b\u6982\u7387\u5206\u5e03\u200b\u7684\u200b\u5f62\u5f0f\u200b\u3002\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0cinput\u200b\u4e3a\u200bsigmoid\u200b\u6fc0\u6d3b\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u6216\u8005\u200bsoftmax\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b\uff1a</p> <p><code>weight</code>:\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200bloss\u200b\u8bbe\u7f6e\u200b\u6743\u503c\u200b</p> <p><code>size_average</code>:\u200b\u6570\u636e\u200b\u4e3a\u200bbool\uff0c\u200b\u4e3a\u200bTrue\u200b\u65f6\u200b\uff0c\u200b\u8fd4\u56de\u200b\u7684\u200bloss\u200b\u4e3a\u200b\u5e73\u5747\u503c\u200b\uff1b\u200b\u4e3a\u200bFalse\u200b\u65f6\u200b\uff0c\u200b\u8fd4\u56de\u200b\u7684\u200b\u5404\u200b\u6837\u672c\u200b\u7684\u200bloss\u200b\u4e4b\u200b\u548c\u200b\u3002</p> <p><code>reduce</code>:\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e3a\u200bbool\uff0c\u200b\u4e3a\u200bTrue\u200b\u65f6\u200b\uff0closs\u200b\u7684\u200b\u8fd4\u56de\u200b\u662f\u200b\u6807\u91cf\u200b\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $ \\ell(x, y)=\\left{\\begin{array}{ll} \\operatorname{mean}(L), &amp; \\text { if reduction }=\\text { 'mean' } \\ \\operatorname{sum}(L), &amp; \\text { if reduction }=\\text { 'sum' } \\end{array}\\right. $</p> <pre><code>m = nn.Sigmoid()\nloss = nn.BCELoss()\ninput = torch.randn(3, requires_grad=True)\ntarget = torch.empty(3).random_(2)\noutput = loss(m(input), target)\noutput.backward()\n</code></pre> <pre><code>print('BCELoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>BCELoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.5732, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#352","title":"3.5.2 \u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<pre><code>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n</code></pre> <p>\u200b\u529f\u80fd\u200b\uff1a\u200b\u8ba1\u7b97\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u51fd\u6570\u200b</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b\uff1a  </p> <p><code>weight</code>:\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200bloss\u200b\u8bbe\u7f6e\u200b\u6743\u503c\u200b\u3002</p> <p><code>size_average</code>:\u200b\u6570\u636e\u200b\u4e3a\u200bbool\uff0c\u200b\u4e3a\u200bTrue\u200b\u65f6\u200b\uff0c\u200b\u8fd4\u56de\u200b\u7684\u200bloss\u200b\u4e3a\u200b\u5e73\u5747\u503c\u200b\uff1b\u200b\u4e3a\u200bFalse\u200b\u65f6\u200b\uff0c\u200b\u8fd4\u56de\u200b\u7684\u200b\u5404\u200b\u6837\u672c\u200b\u7684\u200bloss\u200b\u4e4b\u200b\u548c\u200b\u3002</p> <p><code>ignore_index</code>:\u200b\u5ffd\u7565\u200b\u67d0\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p><code>reduce</code>:\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e3a\u200bbool\uff0c\u200b\u4e3a\u200bTrue\u200b\u65f6\u200b\uff0closs\u200b\u7684\u200b\u8fd4\u56de\u200b\u662f\u200b\u6807\u91cf\u200b\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $ \\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right) $</p> <pre><code>loss = nn.CrossEntropyLoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\noutput = loss(input, target)\noutput.backward()\n</code></pre> <pre><code>print(output)\n</code></pre> <pre><code>tensor(2.0115, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#353-l1","title":"3.5.3 L1\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<pre><code>torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b<code>y</code>\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b<code>target</code>\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u503c\u200b\u7684\u200b\u7edd\u5bf9\u503c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u7684\u200b\u662f\u200b\uff0c<code>reduction</code>\u200b\u53c2\u6570\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u3002\u200b\u6709\u200b\u4e09\u79cd\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u53ef\u200b\u9009\u200b\uff1anone\uff1a\u200b\u9010\u4e2a\u200b\u5143\u7d20\u200b\u8ba1\u7b97\u200b\u3002 sum\uff1a\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002 mean\uff1a\u200b\u52a0\u6743\u200b\u5e73\u5747\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002  \u200b\u5982\u679c\u200b\u9009\u62e9\u200b<code>none</code>\uff0c\u200b\u90a3\u4e48\u200b\u8fd4\u56de\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u548c\u200b\u8f93\u5165\u200b\u5143\u7d20\u200b\u76f8\u540c\u200b\u5c3a\u5bf8\u200b\u7684\u200b\u3002\u200b\u9ed8\u8ba4\u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6c42\u200b\u5e73\u5747\u200b\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $ L_{n} = |x_{n}-y_{n}| $</p> <pre><code>loss = nn.L1Loss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\noutput = loss(input, target)\noutput.backward()\n</code></pre> <pre><code>print('L1\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>L1\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(1.5729, grad_fn=&lt;L1LossBackward&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#354-mse","title":"3.5.4 MSE\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p><pre><code>torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b<code>y</code>\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b<code>target</code>\u200b\u4e4b\u5dee\u200b\u7684\u200b\u5e73\u65b9\u200b\u3002</p> <p>\u200b\u548c\u200b<code>L1Loss</code>\u200b\u4e00\u6837\u200b\uff0c<code>MSELoss</code>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c<code>reduction</code>\u200b\u53c2\u6570\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u3002\u200b\u6709\u200b\u4e09\u79cd\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u53ef\u200b\u9009\u200b\uff1anone\uff1a\u200b\u9010\u4e2a\u200b\u5143\u7d20\u200b\u8ba1\u7b97\u200b\u3002 sum\uff1a\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002\u200b\u9ed8\u8ba4\u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6c42\u200b\u5e73\u5747\u200b\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <p>$ l_{n}=\\left(x_{n}-y_{n}\\right)^{2} $</p> <pre><code>loss = nn.MSELoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\noutput = loss(input, target)\noutput.backward()\n</code></pre> <pre><code>print('MSE\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>MSE\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(1.6968, grad_fn=&lt;MseLossBackward&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#355-l1-smooth-l1","title":"3.5.5 \u200b\u5e73\u6ed1\u200bL1 (Smooth L1)\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p><pre><code>torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a L1\u200b\u7684\u200b\u5e73\u6ed1\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5176\u200b\u529f\u80fd\u200b\u662f\u200b\u51cf\u8f7b\u200b\u79bb\u7fa4\u200b\u70b9\u200b\u5e26\u6765\u200b\u7684\u200b\u5f71\u54cd\u200b</p> <p><code>reduction</code>\u200b\u53c2\u6570\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u3002\u200b\u6709\u200b\u4e09\u79cd\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\u53ef\u200b\u9009\u200b\uff1anone\uff1a\u200b\u9010\u4e2a\u200b\u5143\u7d20\u200b\u8ba1\u7b97\u200b\u3002 sum\uff1a\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002\u200b\u9ed8\u8ba4\u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6c42\u200b\u5e73\u5747\u200b\u3002</p> <p>\u200b\u63d0\u9192\u200b\uff1a \u200b\u4e4b\u540e\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u5173\u4e8e\u200b<code>reduction</code> \u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u4f9d\u65e7\u4f1a\u200b\u5b58\u5728\u200b\u3002\u200b\u6240\u4ee5\u200b\uff0c\u200b\u4e4b\u540e\u200b\u5c31\u200b\u4e0d\u518d\u200b\u5355\u72ec\u200b\u8bf4\u660e\u200b\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $ \\operatorname{loss}(x, y)=\\frac{1}{n} \\sum_{i=1}^{n} z_{i} $ \u200b\u5176\u4e2d\u200b\uff0c $ z_{i}=\\left{\\begin{array}{ll} 0.5\\left(x_{i}-y_{i}\\right)^{2}, &amp; \\text { if }\\left|x_{i}-y_{i}\\right|&lt;1 \\ \\left|x_{i}-y_{i}\\right|-0.5, &amp; \\text { otherwise } \\end{array}\\right. $</p> <pre><code>loss = nn.SmoothL1Loss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\noutput = loss(input, target)\noutput.backward()\n</code></pre> <pre><code>print('SmoothL1Loss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>SmoothL1Loss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.7808, grad_fn=&lt;SmoothL1LossBackward&gt;)\n</code></pre> <p>\u200b\u5e73\u6ed1\u200bL1\u200b\u4e0e\u200bL1\u200b\u7684\u200b\u5bf9\u6bd4\u200b</p> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u53ef\u89c6\u5316\u200b\u4e24\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u66f2\u7ebf\u200b\u6765\u200b\u5bf9\u6bd4\u200b\u5e73\u6ed1\u200bL1\u200b\u548c\u200bL1\u200b\u4e24\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u533a\u522b\u200b\u3002</p> <pre><code>inputs = torch.linspace(-10, 10, steps=5000)\ntarget = torch.zeros_like(inputs)\n\nloss_f_smooth = nn.SmoothL1Loss(reduction='none')\nloss_smooth = loss_f_smooth(inputs, target)\nloss_f_l1 = nn.L1Loss(reduction='none')\nloss_l1 = loss_f_l1(inputs,target)\n\nplt.plot(inputs.numpy(), loss_smooth.numpy(), label='Smooth L1 Loss')\nplt.plot(inputs.numpy(), loss_l1, label='L1 loss')\nplt.xlabel('x_i - y_i')\nplt.ylabel('loss value')\nplt.legend()\nplt.grid()\nplt.show()\n</code></pre> <p></p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\uff0c\u200b\u5bf9\u4e8e\u200b<code>smoothL1</code>\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5728\u200b 0 \u200b\u8fd9\u4e2a\u200b\u5c16\u7aef\u200b\u5904\u200b\uff0c\u200b\u8fc7\u6e21\u200b\u66f4\u4e3a\u200b\u5e73\u6ed1\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#356","title":"3.5.6 \u200b\u76ee\u6807\u200b\u6cca\u677e\u200b\u5206\u5e03\u200b\u7684\u200b\u8d1f\u200b\u5bf9\u6570\u200b\u4f3c\u7136\u200b\u635f\u5931","text":"<p><pre><code>torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u6cca\u677e\u200b\u5206\u5e03\u200b\u7684\u200b\u8d1f\u200b\u5bf9\u6570\u200b\u4f3c\u7136\u200b\u635f\u5931\u200b\u51fd\u6570\u200b</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b\uff1a</p> <p><code>log_input</code>\uff1a\u200b\u8f93\u5165\u200b\u662f\u5426\u200b\u4e3a\u200b\u5bf9\u6570\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u51b3\u5b9a\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u3002</p> <p><code>full</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6240\u6709\u200b loss\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b False\u3002</p> <p><code>eps</code>\uff1a\u200b\u4fee\u6b63\u200b\u9879\u200b\uff0c\u200b\u907f\u514d\u200b input \u200b\u4e3a\u200b 0 \u200b\u65f6\u200b\uff0clog(input) \u200b\u4e3a\u200b nan \u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6570\u5b66\u516c\u5f0f\u200b\uff1a</p> <ul> <li> <p>\u200b\u5f53\u200b\u53c2\u6570\u200b<code>log_input=True</code>\uff1a $ \\operatorname{loss}\\left(x_{n}, y_{n}\\right)=e^{x_{n}}-x_{n} \\cdot y_{n} $</p> </li> <li> <p>\u200b\u5f53\u200b\u53c2\u6570\u200b<code>log_input=False</code>\uff1a</p> <p>$ \\operatorname{loss}\\left(x_{n}, y_{n}\\right)=x_{n}-y_{n} \\cdot \\log \\left(x_{n}+\\text { eps }\\right) $</p> </li> </ul> <pre><code>loss = nn.PoissonNLLLoss()\nlog_input = torch.randn(5, 2, requires_grad=True)\ntarget = torch.randn(5, 2)\noutput = loss(log_input, target)\noutput.backward()\n</code></pre> <pre><code>print('PoissonNLLLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>PoissonNLLLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.7358, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#357-kl","title":"3.5.7 KL\u200b\u6563\u5ea6","text":"<p><pre><code>torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200bKL\u200b\u6563\u5ea6\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u8ba1\u7b97\u200b\u76f8\u5bf9\u200b\u71b5\u200b\u3002\u200b\u7528\u4e8e\u200b\u8fde\u7eed\u200b\u5206\u5e03\u200b\u7684\u200b\u8ddd\u79bb\u200b\u5ea6\u91cf\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5bf9\u200b\u79bb\u6563\u200b\u91c7\u7528\u200b\u7684\u200b\u8fde\u7eed\u200b\u8f93\u51fa\u200b\u7a7a\u95f4\u200b\u5206\u5e03\u200b\u8fdb\u884c\u200b\u56de\u5f52\u200b\u901a\u5e38\u200b\u5f88\u200b\u6709\u7528\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b <code>none</code>/<code>sum</code>/<code>mean</code>/<code>batchmean</code>\u3002</p> <pre><code>none\uff1a\u200b\u9010\u4e2a\u200b\u5143\u7d20\u200b\u8ba1\u7b97\u200b\u3002\n\nsum\uff1a\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002\n\nmean\uff1a\u200b\u52a0\u6743\u200b\u5e73\u5747\u200b\uff0c\u200b\u8fd4\u56de\u200b\u6807\u91cf\u200b\u3002\n\nbatchmean\uff1abatchsize \u200b\u7ef4\u5ea6\u200b\u6c42\u200b\u5e73\u5747\u503c\u200b\u3002\n</code></pre> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ \\begin{aligned} D_{\\mathrm{KL}}(P, Q)=\\mathrm{E}{X \\sim P}\\left[\\log \\frac{P(X)}{Q(X)}\\right] &amp;=\\mathrm{E}{X \\sim P}[\\log P(X)-\\log Q(X)] \\ &amp;=\\sum_{i=1}^{n} P\\left(x_{i}\\right)\\left(\\log P\\left(x_{i}\\right)-\\log Q\\left(x_{i}\\right)\\right) \\end{aligned} $</p> <pre><code>inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\ntarget = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\nloss = nn.KLDivLoss()\noutput = loss(inputs,target)\n\nprint('KLDivLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>KLDivLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(-0.3335)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#358-marginrankingloss","title":"3.5.8 MarginRankingLoss","text":"<p><pre><code>torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u76f8\u4f3c\u200b\u5ea6\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6392\u5e8f\u200b\u4efb\u52a1\u200b\u3002\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u4e24\u7ec4\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>margin</code>\uff1a\u200b\u8fb9\u754c\u503c\u200b\uff0c\\(x_{1}\\) \u200b\u4e0e\u200b\\(x_{2}\\) \u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u503c\u200b\u3002</p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ \\operatorname{loss}(x 1, x 2, y)=\\max (0,-y *(x 1-x 2)+\\operatorname{margin}) $</p> <pre><code>loss = nn.MarginRankingLoss()\ninput1 = torch.randn(3, requires_grad=True)\ninput2 = torch.randn(3, requires_grad=True)\ntarget = torch.randn(3).sign()\noutput = loss(input1, input2, target)\noutput.backward()\n\nprint('MarginRankingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>MarginRankingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.7740, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#359","title":"3.5.9 \u200b\u591a\u200b\u6807\u7b7e\u200b\u8fb9\u754c\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<pre><code>torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u200b\u529f\u80fd\u200b\uff1a \u200b\u5bf9\u4e8e\u200b\u591a\u200b\u6807\u7b7e\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a $ \\operatorname{loss}(x, y)=\\sum_{i j} \\frac{\\max (0,1-x[y[j]]-x[i])}{x \\cdot \\operatorname{size}(0)} $</p> <p>$ \\begin{array}{l} \\text { \u200b\u5176\u4e2d\u200b, } i=0, \\ldots, x \\cdot \\operatorname{size}(0), j=0, \\ldots, y \\cdot \\operatorname{size}(0), \\text { \u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b\u7684\u200b } i \\text { \u200b\u548c\u200b } j \\text {, \u200b\u90fd\u200b\u6709\u200b } y[j] \\geq 0 \\text { \u200b\u5e76\u4e14\u200b }\\ i \\neq y[j] \\end{array} $</p> <pre><code>loss = nn.MultiLabelMarginLoss()\nx = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n# for target y, only consider labels 3 and 0, not after label -1\ny = torch.LongTensor([[3, 0, -1, 1]])# \u200b\u771f\u5b9e\u200b\u7684\u200b\u5206\u7c7b\u200b\u662f\u200b\uff0c\u200b\u7b2c\u200b3\u200b\u7c7b\u200b\u548c\u200b\u7b2c\u200b0\u200b\u7c7b\u200b\noutput = loss(x, y)\n\nprint('MultiLabelMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>MultiLabelMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.4500)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3510","title":"3.5.10 \u200b\u4e8c\u200b\u5206\u7c7b\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p><pre><code>torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')torch.nn.(size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b logistic \u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ \\operatorname{loss}(x, y)=\\sum_{i} \\frac{\\log (1+\\exp (-y[i] \\cdot x[i]))}{x \\cdot \\operatorname{nelement}()} $</p> <p>$ \\ \\text { \u200b\u5176\u4e2d\u200b, } x . \\text { nelement() \u200b\u4e3a\u200b\u8f93\u5165\u200b } x \\text { \u200b\u4e2d\u200b\u7684\u200b\u6837\u672c\u200b\u4e2a\u6570\u200b\u3002\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b } y \\text { \u200b\u4e5f\u200b\u6709\u200b } 1 \\text { \u200b\u548c\u200b }-1 \\text { \u200b\u4e24\u79cd\u200b\u6a21\u5f0f\u200b\u3002 } \\ $</p> <pre><code>inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])  # \u200b\u4e24\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u4e24\u4e2a\u200b\u795e\u7ecf\u5143\u200b\ntarget = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)  # \u200b\u8be5\u200b loss \u200b\u4e3a\u200b\u9010\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u9700\u8981\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u5355\u72ec\u200b\u8bbe\u7f6e\u200b\u6807\u7b7e\u200b\n\nloss_f = nn.SoftMarginLoss()\noutput = loss_f(inputs, target)\n\nprint('SoftMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>SoftMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.6764)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3511","title":"3.5.11 \u200b\u591a\u200b\u5206\u7c7b\u200b\u7684\u200b\u6298\u9875\u200b\u635f\u5931","text":"<p><pre><code>torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u591a\u200b\u5206\u7c7b\u200b\u7684\u200b\u6298\u9875\u200b\u635f\u5931\u200b</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p><code>p\uff1a</code>\u200b\u53ef\u200b\u9009\u200b 1 \u200b\u6216\u200b 2\u3002</p> <p><code>weight</code>\uff1a\u200b\u5404\u7c7b\u200b\u522b\u7684\u200b loss \u200b\u8bbe\u7f6e\u200b\u6743\u503c\u200b\u3002</p> <p><code>margin</code>\uff1a\u200b\u8fb9\u754c\u503c\u200b</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ \\operatorname{loss}(x, y)=\\frac{\\sum_{i} \\max (0, \\operatorname{margin}-x[y]+x[i])^{p}}{x \\cdot \\operatorname{size}(0)} $</p> <p>$ \\begin{array}{l} \\text { \u200b\u5176\u4e2d\u200b, } x \\in{0, \\ldots, x \\cdot \\operatorname{size}(0)-1}, y \\in{0, \\ldots, y \\cdot \\operatorname{size}(0)-1} \\text {, \u200b\u5e76\u4e14\u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b\u7684\u200b } i \\text { \u200b\u548c\u200b } j \\text {, }\\ \\text { \u200b\u90fd\u200b\u6709\u200b } 0 \\leq y[j] \\leq x \\cdot \\operatorname{size}(0)-1, \\text { \u200b\u4ee5\u53ca\u200b } i \\neq y[j] \\text { \u3002 } \\end{array} $</p> <pre><code>inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]]) \ntarget = torch.tensor([0, 1], dtype=torch.long) \n\nloss_f = nn.MultiMarginLoss()\noutput = loss_f(inputs, target)\n\nprint('MultiMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>MultiMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.6000)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3512","title":"3.5.12 \u200b\u4e09\u5143\u7ec4\u200b\u635f\u5931","text":"<p><pre><code>torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u8ba1\u7b97\u200b\u4e09\u5143\u7ec4\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u4e09\u5143\u7ec4\u200b: \u200b\u8fd9\u662f\u200b\u4e00\u79cd\u200b\u6570\u636e\u200b\u7684\u200b\u5b58\u50a8\u200b\u6216\u8005\u200b\u4f7f\u7528\u200b\u683c\u5f0f\u200b\u3002&lt;\u200b\u5b9e\u4f53\u200b1\uff0c\u200b\u5173\u7cfb\u200b\uff0c\u200b\u5b9e\u4f53\u200b2&gt;\u3002\u200b\u5728\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4e3a\u200b&lt; <code>anchor</code>, <code>positive examples</code> , <code>negative examples</code>&gt;</p> <p>\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u53bb\u200b<code>anchor</code>\u200b\u7684\u200b\u8ddd\u79bb\u200b\u66f4\u200b\u63a5\u8fd1\u200b<code>positive examples</code>\uff0c\u200b\u800c\u200b\u8fdc\u79bb\u200b<code>negative examples</code></p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p><code>p\uff1a</code>\u200b\u53ef\u200b\u9009\u200b 1 \u200b\u6216\u200b 2\u3002</p> <p><code>margin</code>\uff1a\u200b\u8fb9\u754c\u503c\u200b</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ L(a, p, n)=\\max \\left{d\\left(a_{i}, p_{i}\\right)-d\\left(a_{i}, n_{i}\\right)+\\operatorname{margin}, 0\\right} $</p> <p>$ \\text { \u200b\u5176\u4e2d\u200b, } d\\left(x_{i}, y_{i}\\right)=\\left|\\mathbf{x}{i}-\\mathbf{y}{i}\\right|_{\\text {\u30fb }} $</p> <pre><code>triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\nanchor = torch.randn(100, 128, requires_grad=True)\npositive = torch.randn(100, 128, requires_grad=True)\nnegative = torch.randn(100, 128, requires_grad=True)\noutput = triplet_loss(anchor, positive, negative)\noutput.backward()\nprint('TripletMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>TripletMarginLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(1.1667, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3513-hingembeddingloss","title":"3.5.13 HingEmbeddingLoss","text":"<p><pre><code>torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u5bf9\u200b\u8f93\u51fa\u200b\u7684\u200bembedding\u200b\u7ed3\u679c\u200b\u505a\u200bHing\u200b\u635f\u5931\u200b\u8ba1\u7b97\u200b</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p><code>margin</code>\uff1a\u200b\u8fb9\u754c\u503c\u200b</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ l_{n}=\\left{\\begin{array}{ll} x_{n}, &amp; \\text { if } y_{n}=1 \\ \\max \\left{0, \\Delta-x_{n}\\right}, &amp; \\text { if } y_{n}=-1 \\end{array}\\right. $ \u200b\u6ce8\u610f\u4e8b\u9879\u200b\uff1a \u200b\u8f93\u5165\u200bx\u200b\u5e94\u4e3a\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\u4e4b\u5dee\u200b\u7684\u200b\u7edd\u5bf9\u503c\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u7406\u89e3\u200b\uff0c\u200b\u8ba9\u200b\u4e2a\u200b\u8f93\u51fa\u200b\u7684\u200b\u662f\u200b\u6b63\u4f8b\u200byn=1,\u200b\u90a3\u4e48\u200bloss\u200b\u5c31\u662f\u200bx\uff0c\u200b\u5982\u679c\u200b\u8f93\u51fa\u200b\u7684\u200b\u662f\u200b\u8d1f\u4f8b\u200by=-1\uff0c\u200b\u90a3\u4e48\u200b\u8f93\u51fa\u200b\u7684\u200bloss\u200b\u5c31\u662f\u200b\u8981\u200b\u505a\u200b\u4e00\u4e2a\u200b\u6bd4\u8f83\u200b\u3002</p> <pre><code>loss_f = nn.HingeEmbeddingLoss()\ninputs = torch.tensor([[1., 0.8, 0.5]])\ntarget = torch.tensor([[1, 1, -1]])\noutput = loss_f(inputs,target)\n\nprint('HingEmbeddingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>HingEmbeddingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.7667)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3514","title":"3.5.14 \u200b\u4f59\u5f26\u200b\u76f8\u4f3c\u200b\u5ea6","text":"<p><pre><code>torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u5bf9\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\u505a\u200b\u4f59\u5f26\u200b\u76f8\u4f3c\u200b\u5ea6\u200b</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p><code>margin</code>\uff1a\u200b\u53ef\u200b\u53d6\u503c\u200b[-1,1] \uff0c\u200b\u63a8\u8350\u200b\u4e3a\u200b[0,0.5] \u3002</p> <p>\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\uff1a</p> <p>$ \\operatorname{loss}(x, y)=\\left{\\begin{array}{ll} 1-\\cos \\left(x_{1}, x_{2}\\right), &amp; \\text { if } y=1 \\ \\max \\left{0, \\cos \\left(x_{1}, x_{2}\\right)-\\text { margin }\\right}, &amp; \\text { if } y=-1 \\end{array}\\right. $ \u200b\u5176\u4e2d\u200b, $ \\cos (\\theta)=\\frac{A \\cdot B}{|A||B|}=\\frac{\\sum_{i=1}^{n} A_{i} \\times B_{i}}{\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}\\right)^{2}} \\times \\sqrt{\\sum_{i=1}^{n}\\left(B_{i}\\right)^{2}}} $</p> <p>\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5e94\u8be5\u200b\u662f\u200b\u6700\u200b\u5e7f\u4e3a\u4eba\u77e5\u200b\u7684\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\uff0c\u200b\u505a\u200b\u4f59\u5f26\u200b\u76f8\u4f3c\u200b\u5ea6\u200b\u3002\u200b\u5c06\u200b\u4f59\u5f26\u200b\u76f8\u4f3c\u200b\u5ea6\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u8ddd\u79bb\u200b\u7684\u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u679c\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\u7684\u200b\u8ddd\u79bb\u200b\u8fd1\u200b\uff0c\u200b\u5219\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\u5c0f\u200b\uff0c\u200b\u53cd\u4e4b\u4ea6\u7136\u200b\u3002</p> <pre><code>loss_f = nn.CosineEmbeddingLoss()\ninputs_1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\ninputs_2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\ntarget = torch.tensor([1, -1], dtype=torch.float)\noutput = loss_f(inputs_1,inputs_2,target)\n\nprint('CosineEmbeddingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',output)\n</code></pre> <pre><code>CosineEmbeddingLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(0.5000)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#3515-ctc","title":"3.5.15 CTC\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p><pre><code>torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n</code></pre> \u200b\u529f\u80fd\u200b\uff1a \u200b\u7528\u4e8e\u200b\u89e3\u51b3\u200b\u65f6\u5e8f\u200b\u7c7b\u200b\u6570\u636e\u200b\u7684\u200b\u5206\u7c7b\u200b</p> <p>\u200b\u8ba1\u7b97\u200b\u8fde\u7eed\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u548c\u200b\u76ee\u6807\u200b\u5e8f\u5217\u200b\u4e4b\u95f4\u200b\u7684\u200b\u635f\u5931\u200b\u3002CTCLoss\u200b\u5bf9\u200b\u8f93\u5165\u200b\u548c\u200b\u76ee\u6807\u200b\u7684\u200b\u53ef\u80fd\u200b\u6392\u5217\u200b\u7684\u200b\u6982\u7387\u200b\u8fdb\u884c\u200b\u6c42\u548c\u200b\uff0c\u200b\u4ea7\u751f\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u503c\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u503c\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u8f93\u5165\u200b\u8282\u70b9\u200b\u6765\u8bf4\u200b\u662f\u200b\u53ef\u5206\u200b\u7684\u200b\u3002\u200b\u8f93\u5165\u200b\u4e0e\u200b\u76ee\u6807\u200b\u7684\u200b\u5bf9\u9f50\u200b\u65b9\u5f0f\u200b\u88ab\u200b\u5047\u5b9a\u200b\u4e3a\u200b \"\u200b\u591a\u200b\u5bf9\u200b\u4e00\u200b\"\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u9650\u5236\u200b\u4e86\u200b\u76ee\u6807\u200b\u5e8f\u5217\u200b\u7684\u200b\u957f\u5ea6\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u5fc5\u987b\u200b\u662f\u200b\u2264\u200b\u8f93\u5165\u200b\u957f\u5ea6\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u53c2\u6570\u200b: </p> <p><code>reduction</code>\uff1a\u200b\u8ba1\u7b97\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u53ef\u200b\u4e3a\u200b none/sum/mean\u3002</p> <p><code>blank</code>\uff1ablank label\u3002</p> <p><code>zero_infinity</code>\uff1a\u200b\u65e0\u7a77\u5927\u200b\u7684\u200b\u503c\u200b\u6216\u200b\u68af\u5ea6\u200b\u503c\u4e3a\u200b </p> <pre><code># Target are to be padded\nT = 50      # Input sequence length\nC = 20      # Number of classes (including blank)\nN = 16      # Batch size\nS = 30      # Target sequence length of longest target in batch (padding length)\nS_min = 10  # Minimum target length, for demonstration purposes\n\n# Initialize random batch of input vectors, for *size = (T,N,C)\ninput = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n\n# Initialize random batch of targets (0 = blank, 1:C = classes)\ntarget = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n\ninput_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\ntarget_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\nctc_loss = nn.CTCLoss()\nloss = ctc_loss(input, target, input_lengths, target_lengths)\nloss.backward()\n\n\n# Target are to be un-padded\nT = 50      # Input sequence length\nC = 20      # Number of classes (including blank)\nN = 16      # Batch size\n\n# Initialize random batch of input vectors, for *size = (T,N,C)\ninput = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\ninput_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n\n# Initialize random batch of targets (0 = blank, 1:C = classes)\ntarget_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\ntarget = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\nctc_loss = nn.CTCLoss()\nloss = ctc_loss(input, target, input_lengths, target_lengths)\nloss.backward()\n\nprint('CTCLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b',loss)\n</code></pre> <pre><code>CTCLoss\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u4e3a\u200b tensor(16.0885, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.7%20%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0/","title":"3.7 \u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u5b8c\u6210\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u200b/\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u7684\u200b\u9a8c\u8bc1\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3001\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u8fc7\u200b\u62df\u5408\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u7684\u200b\u6d41\u7a0b\u200b\u662f\u200b\u4e00\u81f4\u200b\u7684\u200b\uff0c\u200b\u53ea\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9700\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u8bc4\u4f30\u200b\u8fc7\u7a0b\u200b\u4e2d\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u7684\u200b\u8bad\u7ec3\u200b/\u200b\u8bc4\u4f30\u200b\u6a21\u5f0f\u200b\u7684\u200b\u5f00\u542f\u200b</li> <li>\u200b\u5b8c\u6574\u200b\u7684\u200b\u8bad\u7ec3\u200b/\u200b\u8bc4\u4f30\u200b\u6d41\u7a0b\u200b</li> </ul> <p>\u200b\u5b8c\u6210\u200b\u4e86\u200b\u4e0a\u8ff0\u200b\u8bbe\u5b9a\u200b\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002\u200b\u9996\u5148\u200b\u5e94\u8be5\u200b\u8bbe\u7f6e\u200b\u6a21\u578b\u200b\u7684\u200b\u72b6\u6001\u200b\uff1a\u200b\u5982\u679c\u200b\u662f\u200b\u8bad\u7ec3\u200b\u72b6\u6001\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u5e94\u8be5\u200b\u652f\u6301\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u4fee\u6539\u200b\uff1b\u200b\u5982\u679c\u200b\u662f\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u72b6\u6001\u200b\uff0c\u200b\u5219\u200b\u4e0d\u200b\u5e94\u8be5\u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u3002\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u72b6\u6001\u200b\u8bbe\u7f6e\u200b\u975e\u5e38\u200b\u7b80\u4fbf\u200b\uff0c\u200b\u5982\u4e0b\u200b\u7684\u200b\u4e24\u4e2a\u200b\u64cd\u4f5c\u200b\u4e8c\u9009\u200b\u4e00\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>model.train()   # \u200b\u8bad\u7ec3\u200b\u72b6\u6001\u200b\nmodel.eval()   # \u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u72b6\u6001\u200b\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u524d\u9762\u200b\u5728\u200bDataLoader\u200b\u6784\u5efa\u200b\u5b8c\u6210\u200b\u540e\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u5982\u4f55\u200b\u4ece\u4e2d\u200b\u8bfb\u53d6\u6570\u636e\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u64cd\u4f5c\u200b\u5373\u53ef\u200b\uff0c\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b\u6b64\u65f6\u200b\u8981\u200b\u7528\u200bfor\u200b\u5faa\u73af\u200b\u8bfb\u53d6\u200bDataLoader\u200b\u4e2d\u200b\u7684\u200b\u5168\u90e8\u200b\u6570\u636e\u200b\u3002</p> <pre><code>for data, label in train_loader:\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\u5c06\u200b\u6570\u636e\u200b\u653e\u5230\u200bGPU\u200b\u4e0a\u200b\u7528\u4e8e\u200b\u540e\u7eed\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6b64\u5904\u200b\u4ee5\u200b.cuda()\u200b\u4e3a\u4f8b\u200b</p> <pre><code>data, label = data.cuda(), label.cuda()\n</code></pre> <p>\u200b\u5f00\u59cb\u200b\u7528\u200b\u5f53\u524d\u200b\u6279\u6b21\u200b\u6570\u636e\u200b\u505a\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u5e94\u5f53\u200b\u5148\u200b\u5c06\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u68af\u5ea6\u200b\u7f6e\u200b\u96f6\u200b\uff1a</p> <pre><code>optimizer.zero_grad()\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\u5c06\u200bdata\u200b\u9001\u5165\u200b\u6a21\u578b\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\uff1a</p> <pre><code>output = model(data)\n</code></pre> <p>\u200b\u6839\u636e\u200b\u9884\u5148\u200b\u5b9a\u4e49\u200b\u7684\u200bcriterion\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff1a</p> <pre><code>loss = criterion(output, label)\n</code></pre> <p>\u200b\u5c06\u200bloss\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u56de\u200b\u7f51\u7edc\u200b\uff1a</p> <pre><code>loss.backward()\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u4f18\u5316\u200b\u5668\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff1a</p> <pre><code>optimizer.step()\n</code></pre> <p>\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u5c31\u200b\u5b8c\u6210\u200b\u4e86\u200b\uff0c\u200b\u540e\u7eed\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u51c6\u786e\u7387\u200b\u7b49\u200b\u6307\u6807\u200b\uff0c\u200b\u8fd9\u90e8\u5206\u200b\u4f1a\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u5b9e\u6218\u200b\u4e2d\u200b\u52a0\u4ee5\u200b\u4ecb\u7ecd\u200b\u3002</p> <p>\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u6d41\u7a0b\u200b\u57fa\u672c\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e00\u81f4\u200b\uff0c\u200b\u4e0d\u540c\u70b9\u200b\u5728\u4e8e\u200b\uff1a</p> <ul> <li>\u200b\u9700\u8981\u200b\u9884\u5148\u200b\u8bbe\u7f6e\u200btorch.no_grad\uff0c\u200b\u4ee5\u53ca\u200b\u5c06\u200bmodel\u200b\u8c03\u200b\u81f3\u200beval\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u4e0d\u200b\u9700\u8981\u200b\u5c06\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u68af\u5ea6\u200b\u7f6e\u200b\u96f6\u200b</li> <li>\u200b\u4e0d\u200b\u9700\u8981\u200b\u5c06\u200bloss\u200b\u53cd\u5411\u200b\u56de\u200b\u4f20\u5230\u200b\u7f51\u7edc\u200b</li> <li>\u200b\u4e0d\u200b\u9700\u8981\u200b\u66f4\u65b0\u200boptimizer</li> </ul> <p>\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>def train(epoch):\n    model.train()\n    train_loss = 0\n    for data, label in train_loader:\n        data, label = data.cuda(), label.cuda()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n    train_loss = train_loss/len(train_loader.dataset)\n        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n</code></pre> <p>\u200b\u5bf9\u5e94\u200b\u7684\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p><pre><code>def val(epoch):       \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data, label in val_loader:\n            data, label = data.cuda(), label.cuda()\n            output = model(data)\n            preds = torch.argmax(output, 1)\n            loss = criterion(output, label)\n            val_loss += loss.item()*data.size(0)\n            running_accu += torch.sum(preds == label.data)\n    val_loss = val_loss/len(val_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))\n</code></pre> \u200b\u5bf9\u4e8e\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bsklearn.metrics\u200b\u4e2d\u200b\u7684\u200bclassification_report\u200b\u51fd\u6570\u200b\u6765\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3001\u200b\u53ec\u56de\u200b\u7387\u200b\u3001F1\u200b\u503c\u200b\u7b49\u200b\u6307\u6807\u200b\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p><pre><code>from sklearn.metrics import classification_report\n\"\"\"\n\u200b\u5c06\u200b\u4e0b\u65b9\u200b\u4ee3\u7801\u200b\u7684\u200blabels\u200b\u548c\u200bpreds\u200b\u66ff\u6362\u200b\u4e3a\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u51fa\u6765\u200b\u7684\u200b\u6240\u6709\u200blabel\u200b\u548c\u200bpreds\uff0c\ntarget_names\u200b\u66ff\u6362\u200b\u4e3a\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\uff0c\n\u200b\u65e2\u200b\u53ef\u200b\u5f97\u5230\u200b\u6a21\u578b\u200b\u7684\u200b\u5206\u7c7b\u200b\u62a5\u544a\u200b\n\"\"\"\nprint(classification_report(labels.cpu(), preds.cpu(), target_names=class_names))\n</code></pre> \u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torcheval</code>\u200b\u6216\u200b<code>torchmetric</code>\u200b\u6765\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.8%20%E5%8F%AF%E8%A7%86%E5%8C%96/","title":"3.8 \u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5728\u200bPyTorch\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u662f\u200b\u4e00\u4e2a\u200b\u53ef\u9009\u9879\u200b\uff0c\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u67d0\u4e9b\u200b\u4efb\u52a1\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5bf9\u200b\u4e00\u4e9b\u200b\u5fc5\u8981\u200b\u7684\u200b\u5185\u5bb9\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u6bd4\u5982\u200b\u5206\u7c7b\u200b\u7684\u200bROC\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u66f2\u7ebf\u200b\u7b49\u7b49\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u7b2c\u4e03\u7ae0\u200b\u3002</p>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.9%20%E4%BC%98%E5%8C%96%E5%99%A8/","title":"3.9 PyTorch\u200b\u4f18\u5316\u200b\u5668","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u901a\u8fc7\u200b\u4e0d\u65ad\u200b\u6539\u53d8\u200b\u7f51\u7edc\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u53c2\u6570\u200b\u80fd\u591f\u200b\u5bf9\u200b\u8f93\u5165\u200b\u505a\u200b\u5404\u79cd\u200b\u975e\u7ebf\u6027\u200b\u53d8\u6362\u200b\u62df\u5408\u200b\u8f93\u51fa\u200b\uff0c\u200b\u672c\u8d28\u200b\u4e0a\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u53bb\u200b\u5bfb\u627e\u200b\u6700\u4f18\u200b\u89e3\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u4e2a\u200b\u6700\u4f18\u200b\u89e3\u662f\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b\uff0c\u200b\u800c\u200b\u5982\u4f55\u200b\u5feb\u901f\u200b\u6c42\u5f97\u200b\u8fd9\u4e2a\u200b\u6700\u4f18\u200b\u89e3\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u70b9\u200b\uff0c\u200b\u4ee5\u200b\u7ecf\u5178\u200b\u7684\u200bresnet-50\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5b83\u200b\u5927\u7ea6\u200b\u6709\u200b2000\u200b\u4e07\u4e2a\u200b\u7cfb\u6570\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u8fd9\u4e48\u200b\u591a\u200b\u7cfb\u6570\u200b\uff0c\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u4e24\u79cd\u200b\u65b9\u6cd5\u200b\uff1a</p> <ol> <li>\u200b\u7b2c\u4e00\u79cd\u200b\u662f\u200b\u76f4\u63a5\u200b\u66b4\u529b\u200b\u7a77\u4e3e\u200b\u4e00\u904d\u200b\u53c2\u6570\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u4ece\u200b\u7406\u8bba\u200b\u4e0a\u200b\u884c\u5f97\u901a\u200b\uff0c\u200b\u4f46\u662f\u200b\u5b9e\u65bd\u200b\u4e0a\u200b\u53ef\u80fd\u6027\u200b\u57fa\u672c\u200b\u4e3a\u200b0\uff0c\u200b\u56e0\u4e3a\u200b\u53c2\u200b\u6570\u91cf\u200b\u8fc7\u4e8e\u200b\u5e9e\u5927\u200b\u3002</li> <li>\u200b\u4e3a\u4e86\u200b\u4f7f\u200b\u6c42\u89e3\u200b\u53c2\u6570\u200b\u8fc7\u7a0b\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u4eba\u4eec\u200b\u63d0\u51fa\u200b\u4e86\u200b\u7b2c\u4e8c\u79cd\u200b\u529e\u6cd5\u200b\uff0c\u200b\u5373\u200bBP+\u200b\u4f18\u5316\u200b\u5668\u200b\u903c\u8fd1\u200b\u6c42\u89e3\u200b\u3002</li> </ol> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4f18\u5316\u200b\u5668\u662f\u200b\u6839\u636e\u200b\u7f51\u7edc\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u68af\u5ea6\u200b\u4fe1\u606f\u200b\u6765\u200b\u66f4\u65b0\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u8d77\u5230\u200b\u964d\u4f4e\u200bloss\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u503c\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u66f4\u52a0\u200b\u63a5\u8fd1\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e86\u89e3\u200bPyTorch\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b</li> <li>\u200b\u5b66\u4f1a\u200b\u4f7f\u7528\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5c5e\u6027\u200b\u548c\u200b\u6784\u9020\u200b</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5bf9\u6bd4\u200b</li> </ul>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.9%20%E4%BC%98%E5%8C%96%E5%99%A8/#391-pytorch","title":"3.9.1 PyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4f18\u5316\u200b\u5668","text":"<p>PyTorch\u200b\u5f88\u200b\u4eba\u6027\u5316\u200b\u7684\u200b\u7ed9\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5e93\u200b<code>torch.optim</code>\uff0c\u200b\u5728\u200b\u8fd9\u200b\u91cc\u9762\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u79cd\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <ul> <li>torch.optim.SGD </li> <li>torch.optim.ASGD</li> <li>torch.optim.Adadelta</li> <li>torch.optim.Adagrad</li> <li>torch.optim.Adam</li> <li>torch.optim.AdamW</li> <li>torch.optim.Adamax</li> <li>torch.optim.RAdam</li> <li>torch.optim.NAdam</li> <li>torch.optim.SparseAdam</li> <li>torch.optim.LBFGS</li> <li>torch.optim.RMSprop</li> <li>torch.optim.Rprop</li> </ul> <p>\u200b\u800c\u200b\u4ee5\u4e0a\u200b\u8fd9\u4e9b\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\u5747\u200b\u7ee7\u627f\u200b\u4e8e\u200b<code>Optimizer</code>\uff0c\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5148\u200b\u6765\u770b\u200b\u4e0b\u200b\u6240\u6709\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u57fa\u7c7b\u200b<code>Optimizer</code>\u3002\u200b\u5b9a\u4e49\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class Optimizer(object):\n    def __init__(self, params, defaults):        \n        self.defaults = defaults\n        self.state = defaultdict(dict)\n        self.param_groups = []\n</code></pre> <p><code>Optimizer</code>\u200b\u6709\u200b\u4e09\u4e2a\u200b\u5c5e\u6027\u200b\uff1a</p> <ul> <li><code>defaults</code>\uff1a\u200b\u5b58\u50a8\u200b\u7684\u200b\u662f\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f8b\u5b50\u200b\u5982\u4e0b\u200b\uff1a</li> </ul> <pre><code>{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n</code></pre> <ul> <li><code>state</code>\uff1a\u200b\u53c2\u6570\u200b\u7684\u200b\u7f13\u5b58\u200b\uff0c\u200b\u4f8b\u5b50\u200b\u5982\u4e0b\u200b\uff1a</li> </ul> <pre><code>defaultdict(&lt;class 'dict'&gt;, {tensor([[ 0.3864, -0.0131],\n        [-0.1911, -0.4511]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n        [0.0052, 0.0052]])}})\n</code></pre> <ul> <li><code>param_groups</code>\uff1a\u200b\u7ba1\u7406\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u200b\uff0c\u200b\u662f\u200b\u4e00\u4e2a\u200blist\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\uff0c\u200b\u987a\u5e8f\u200b\u662f\u200bparams\uff0clr\uff0cmomentum\uff0cdampening\uff0cweight_decay\uff0cnesterov\uff0c\u200b\u4f8b\u5b50\u200b\u5982\u4e0b\u200b\uff1a</li> </ul> <pre><code>[{'params': [tensor([[-0.1022, -1.6890],[-1.5116, -1.7846]], requires_grad=True)], 'lr': 1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n</code></pre> <p><code>Optimizer</code>\u200b\u8fd8\u6709\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1a</p> <ul> <li><code>zero_grad()</code>\uff1a\u200b\u6e05\u7a7a\u200b\u6240\u200b\u7ba1\u7406\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0cPyTorch\u200b\u7684\u200b\u7279\u6027\u200b\u662f\u200b\u5f20\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e0d\u200b\u81ea\u52a8\u200b\u6e05\u96f6\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6bcf\u6b21\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u540e\u200b\u90fd\u200b\u9700\u8981\u200b\u6e05\u7a7a\u200b\u68af\u5ea6\u200b\u3002</li> </ul> <pre><code>def zero_grad(self, set_to_none: bool = False):\n    for group in self.param_groups:\n        for p in group['params']:\n            if p.grad is not None:  #\u200b\u68af\u5ea6\u200b\u4e0d\u4e3a\u200b\u7a7a\u200b\n                if set_to_none: \n                    p.grad = None\n                else:\n                    if p.grad.grad_fn is not None:\n                        p.grad.detach_()\n                    else:\n                        p.grad.requires_grad_(False)\n                    p.grad.zero_()# \u200b\u68af\u5ea6\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\n</code></pre> <ul> <li><code>step()</code>\uff1a\u200b\u6267\u884c\u200b\u4e00\u6b65\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b</li> </ul> <pre><code>def step(self, closure): \n    raise NotImplementedError\n</code></pre> <ul> <li><code>add_param_group()</code>\uff1a\u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b\u7ec4\u200b</li> </ul> <pre><code>def add_param_group(self, param_group):\n    assert isinstance(param_group, dict), \"param group must be a dict\"\n# \u200b\u68c0\u67e5\u200b\u7c7b\u578b\u200b\u662f\u5426\u200b\u4e3a\u200btensor\n    params = param_group['params']\n    if isinstance(params, torch.Tensor):\n        param_group['params'] = [params]\n    elif isinstance(params, set):\n        raise TypeError('optimizer parameters need to be organized in ordered collections, but '\n                        'the ordering of tensors in sets will change between runs. Please use a list instead.')\n    else:\n        param_group['params'] = list(params)\n    for param in param_group['params']:\n        if not isinstance(param, torch.Tensor):\n            raise TypeError(\"optimizer can only optimize Tensors, \"\n                            \"but one of the params is \" + torch.typename(param))\n        if not param.is_leaf:\n            raise ValueError(\"can't optimize a non-leaf Tensor\")\n\n    for name, default in self.defaults.items():\n        if default is required and name not in param_group:\n            raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n                             name)\n        else:\n            param_group.setdefault(name, default)\n\n    params = param_group['params']\n    if len(params) != len(set(params)):\n        warnings.warn(\"optimizer contains a parameter group with duplicate parameters; \"\n                      \"in future, this will cause an error; \"\n                      \"see github.com/PyTorch/PyTorch/issues/40967 for more information\", stacklevel=3)\n# \u200b\u4e0a\u9762\u200b\u597d\u50cf\u200b\u90fd\u200b\u5728\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u7c7b\u200b\u7684\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u62a5\u200bWarning\u200b\u548c\u200bError\n    param_set = set()\n    for group in self.param_groups:\n        param_set.update(set(group['params']))\n\n    if not param_set.isdisjoint(set(param_group['params'])):\n        raise ValueError(\"some parameters appear in more than one parameter group\")\n# \u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b\n    self.param_groups.append(param_group)\n</code></pre> <ul> <li><code>load_state_dict()</code> \uff1a\u200b\u52a0\u8f7d\u200b\u72b6\u6001\u53c2\u6570\u200b\u5b57\u5178\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u65ad\u70b9\u200b\u7eed\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7ee7\u7eed\u200b\u4e0a\u6b21\u200b\u7684\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b</li> </ul> <pre><code>def load_state_dict(self, state_dict):\n    r\"\"\"Loads the optimizer state.\n\n    Arguments:\n        state_dict (dict): optimizer state. Should be an object returned\n            from a call to :meth:`state_dict`.\n    \"\"\"\n    # deepcopy, to be consistent with module API\n    state_dict = deepcopy(state_dict)\n    # Validate the state_dict\n    groups = self.param_groups\n    saved_groups = state_dict['param_groups']\n\n    if len(groups) != len(saved_groups):\n        raise ValueError(\"loaded state dict has a different number of \"\n                         \"parameter groups\")\n    param_lens = (len(g['params']) for g in groups)\n    saved_lens = (len(g['params']) for g in saved_groups)\n    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n        raise ValueError(\"loaded state dict contains a parameter group \"\n                         \"that doesn't match the size of optimizer's group\")\n\n    # Update the state\n    id_map = {old_id: p for old_id, p in\n              zip(chain.from_iterable((g['params'] for g in saved_groups)),\n                  chain.from_iterable((g['params'] for g in groups)))}\n\n    def cast(param, value):\n        r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\n        .....\n\n    # Copy state assigned to params (and cast tensors to appropriate types).\n    # State that is not assigned to params is copied as is (needed for\n    # backward compatibility).\n    state = defaultdict(dict)\n    for k, v in state_dict['state'].items():\n        if k in id_map:\n            param = id_map[k]\n            state[param] = cast(param, v)\n        else:\n            state[k] = v\n\n    # Update parameter groups, setting their 'params' value\n    def update_group(group, new_group):\n       ...\n    param_groups = [\n        update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n    self.__setstate__({'state': state, 'param_groups': param_groups})\n</code></pre> <ul> <li><code>state_dict()</code>\uff1a\u200b\u83b7\u53d6\u200b\u4f18\u5316\u200b\u5668\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b\u4fe1\u606f\u200b\u5b57\u5178\u200b</li> </ul> <pre><code>def state_dict(self):\n    r\"\"\"Returns the state of the optimizer as a :class:`dict`.\n\n    It contains two entries:\n\n    * state - a dict holding current optimization state. Its content\n        differs between optimizer classes.\n    * param_groups - a dict containing all parameter groups\n    \"\"\"\n    # Save order indices instead of Tensors\n    param_mappings = {}\n    start_index = 0\n\n    def pack_group(group):\n        ......\n    param_groups = [pack_group(g) for g in self.param_groups]\n    # Remap state to use order indices as keys\n    packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v\n                    for k, v in self.state.items()}\n    return {\n        'state': packed_state,\n        'param_groups': param_groups,\n    }\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.9%20%E4%BC%98%E5%8C%96%E5%99%A8/#392","title":"3.9.2 \u200b\u5b9e\u9645\u64cd\u4f5c","text":"<pre><code>import os\nimport torch\n\n# \u200b\u8bbe\u7f6e\u200b\u6743\u91cd\u200b\uff0c\u200b\u670d\u4ece\u200b\u6b63\u6001\u5206\u5e03\u200b  --&gt; 2 x 2\nweight = torch.randn((2, 2), requires_grad=True)\n# \u200b\u8bbe\u7f6e\u200b\u68af\u5ea6\u200b\u4e3a\u200b\u5168\u200b1\u200b\u77e9\u9635\u200b  --&gt; 2 x 2\nweight.grad = torch.ones((2, 2))\n# \u200b\u8f93\u51fa\u200b\u73b0\u6709\u200b\u7684\u200bweight\u200b\u548c\u200bdata\nprint(\"The data of weight before step:\\n{}\".format(weight.data))\nprint(\"The grad of weight before step:\\n{}\".format(weight.grad))\n# \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4f18\u5316\u200b\u5668\u200b\noptimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n# \u200b\u8fdb\u884c\u200b\u4e00\u6b65\u200b\u64cd\u4f5c\u200b\noptimizer.step()\n# \u200b\u67e5\u770b\u200b\u8fdb\u884c\u200b\u4e00\u6b65\u200b\u540e\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u68af\u5ea6\u200b\nprint(\"The data of weight after step:\\n{}\".format(weight.data))\nprint(\"The grad of weight after step:\\n{}\".format(weight.grad))\n# \u200b\u6743\u91cd\u200b\u6e05\u96f6\u200b\noptimizer.zero_grad()\n# \u200b\u68c0\u9a8c\u200b\u6743\u91cd\u200b\u662f\u5426\u200b\u4e3a\u200b0\nprint(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n# \u200b\u8f93\u51fa\u200b\u53c2\u6570\u200b\nprint(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n# \u200b\u67e5\u770b\u200b\u53c2\u6570\u200b\u4f4d\u7f6e\u200b\uff0coptimizer\u200b\u548c\u200bweight\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u200b\u89c9\u5f97\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPython\u200b\u662f\u200b\u57fa\u4e8e\u200b\u503c\u200b\u7ba1\u7406\u200b\nprint(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n# \u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b\uff1aweight2\nweight2 = torch.randn((3, 3), requires_grad=True)\noptimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n# \u200b\u67e5\u770b\u200b\u73b0\u6709\u200b\u7684\u200b\u53c2\u6570\u200b\nprint(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n# \u200b\u67e5\u770b\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b\u4fe1\u606f\u200b\nopt_state_dict = optimizer.state_dict()\nprint(\"state_dict before step:\\n\", opt_state_dict)\n# \u200b\u8fdb\u884c\u200b5\u200b\u6b21\u200bstep\u200b\u64cd\u4f5c\u200b\nfor _ in range(50):\n    optimizer.step()\n# \u200b\u8f93\u51fa\u200b\u73b0\u6709\u200b\u72b6\u6001\u200b\u4fe1\u606f\u200b\nprint(\"state_dict after step:\\n\", optimizer.state_dict())\n# \u200b\u4fdd\u5b58\u200b\u53c2\u6570\u4fe1\u606f\u200b\ntorch.save(optimizer.state_dict(),os.path.join(r\"D:\\pythonProject\\Attention_Unet\", \"optimizer_state_dict.pkl\"))\nprint(\"----------done-----------\")\n# \u200b\u52a0\u8f7d\u200b\u53c2\u6570\u4fe1\u606f\u200b\nstate_dict = torch.load(r\"D:\\pythonProject\\Attention_Unet\\optimizer_state_dict.pkl\") # \u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u4e3a\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u8def\u5f84\u200b\noptimizer.load_state_dict(state_dict)\nprint(\"load state_dict successfully\\n{}\".format(state_dict))\n# \u200b\u8f93\u51fa\u200b\u6700\u540e\u200b\u5c5e\u6027\u200b\u4fe1\u606f\u200b\nprint(\"\\n{}\".format(optimizer.defaults))\nprint(\"\\n{}\".format(optimizer.state))\nprint(\"\\n{}\".format(optimizer.param_groups))\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.9%20%E4%BC%98%E5%8C%96%E5%99%A8/#393","title":"3.9.3 \u200b\u8f93\u51fa\u200b\u7ed3\u679c","text":"<pre><code># \u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u524d\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u68af\u5ea6\u200b\nThe data of weight before step:\ntensor([[-0.3077, -0.1808],\n        [-0.7462, -1.5556]])\nThe grad of weight before step:\ntensor([[1., 1.],\n        [1., 1.]])\n# \u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u540e\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u68af\u5ea6\u200b\nThe data of weight after step:\ntensor([[-0.4077, -0.2808],\n        [-0.8462, -1.6556]])\nThe grad of weight after step:\ntensor([[1., 1.],\n        [1., 1.]])\n# \u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u6e05\u96f6\u200b\u7684\u200b\u68af\u5ea6\u200b\nThe grad of weight after optimizer.zero_grad():\ntensor([[0., 0.],\n        [0., 0.]])\n# \u200b\u8f93\u51fa\u200b\u4fe1\u606f\u200b\noptimizer.params_group is \n[{'params': [tensor([[-0.4077, -0.2808],\n        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n\n# \u200b\u8bc1\u660e\u200b\u4e86\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u548c\u200bweight\u200b\u7684\u200b\u50a8\u5b58\u200b\u662f\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5730\u65b9\u200b\uff0cPython\u200b\u57fa\u4e8e\u200b\u503c\u200b\u7ba1\u7406\u200b\nweight in optimizer:1841923407424\nweight in weight:1841923407424\n\n# \u200b\u8f93\u51fa\u200b\u53c2\u6570\u200b\noptimizer.param_groups is\n[{'params': [tensor([[-0.4077, -0.2808],\n        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n        [ 0.6630, -1.5178, -0.8708],\n        [-2.0222,  1.4573,  0.8657]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n\n# \u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u524d\u200b\u7684\u200b\u53c2\u6570\u200b\u67e5\u770b\u200b\uff0c\u200b\u7528\u200bstate_dict\nstate_dict before step:\n {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n# \u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u540e\u200b\u7684\u200b\u53c2\u6570\u200b\u67e5\u770b\u200b\uff0c\u200b\u7528\u200bstate_dict\nstate_dict after step:\n {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n\n# \u200b\u5b58\u50a8\u200b\u4fe1\u606f\u200b\u5b8c\u6bd5\u200b\n----------done-----------\n# \u200b\u52a0\u8f7d\u200b\u53c2\u6570\u4fe1\u606f\u200b\u6210\u529f\u200b\nload state_dict successfully\n# \u200b\u52a0\u8f7d\u200b\u53c2\u6570\u4fe1\u606f\u200b\n{'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n\n# defaults\u200b\u7684\u200b\u5c5e\u6027\u200b\u8f93\u51fa\u200b\n{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n\n# state\u200b\u5c5e\u6027\u200b\u8f93\u51fa\u200b\ndefaultdict(&lt;class 'dict'&gt;, {tensor([[-1.3031, -1.1761],\n        [-1.7415, -2.5510]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n        [0.0052, 0.0052]])}})\n\n# param_groups\u200b\u5c5e\u6027\u200b\u8f93\u51fa\u200b\n[{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [tensor([[-1.3031, -1.1761],\n        [-1.7415, -2.5510]], requires_grad=True)]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n        [ 0.6630, -1.5178, -0.8708],\n        [-2.0222,  1.4573,  0.8657]], requires_grad=True)]}]\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ol> <li>\u200b\u6bcf\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u90fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u8fdb\u884c\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u624d\u80fd\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6bd4\u5982\u200b\u4e0b\u65b9\u200b\u5b9e\u73b0\u200b\uff1a</li> </ol> <pre><code>class Net(nn.Moddule):\n    \u00b7\u00b7\u00b7\nnet = Net()\noptim = torch.optim.SGD(net.parameters(),lr=lr)\noptim.step()\n</code></pre> <ol> <li>optimizer\u200b\u5728\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200bepoch\u200b\u4e2d\u200b\u9700\u8981\u200b\u5b9e\u73b0\u200b\u4e0b\u9762\u200b\u4e24\u4e2a\u200b\u6b65\u9aa4\u200b\uff1a</li> <li>\u200b\u68af\u5ea6\u200b\u7f6e\u200b\u96f6\u200b</li> <li>\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b</li> </ol> <p><pre><code>optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\nfor epoch in range(EPOCH):\n    ...\n    optimizer.zero_grad()  #\u200b\u68af\u5ea6\u200b\u7f6e\u200b\u96f6\u200b\n    loss = ...             #\u200b\u8ba1\u7b97\u200bloss\n    loss.backward()        #BP\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\n    optimizer.step()       #\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\n</code></pre> 3. \u200b\u7ed9\u200b\u7f51\u7edc\u200b\u4e0d\u540c\u200b\u7684\u200b\u5c42\u200b\u8d4b\u4e88\u200b\u4e0d\u540c\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u53c2\u6570\u200b\u3002</p> <pre><code>from torch import optim\nfrom torchvision.models import resnet18\n\nnet = resnet18()\n\noptimizer = optim.SGD([\n    {'params':net.fc.parameters()},#fc\u200b\u7684\u200blr\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u7684\u200b1e-5\n    {'params':net.layer4[0].conv1.parameters(),'lr':1e-2}],lr=1e-5)\n\n# \u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bparam_groups\u200b\u67e5\u770b\u200b\u5c5e\u6027\u200b\n</code></pre>"},{"location":"03-%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.9%20%E4%BC%98%E5%8C%96%E5%99%A8/#394","title":"3.9.4 \u200b\u5b9e\u9a8c","text":"<p>\u200b\u4e3a\u4e86\u200b\u66f4\u597d\u200b\u7684\u200b\u5e2e\u200b\u5927\u5bb6\u200b\u4e86\u89e3\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u6211\u4eec\u200b\u5bf9\u200bPyTorch\u200b\u4e2d\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u6d4b\u8bd5\u200b</p> <p>\u200b\u6570\u636e\u200b\u751f\u6210\u200b\uff1a</p> <pre><code>a = torch.linspace(-1, 1, 1000)\n# \u200b\u5347\u7ef4\u200b\u64cd\u4f5c\u200b\nx = torch.unsqueeze(a, dim=1)\ny = x.pow(2) + 0.1 * torch.normal(torch.zeros(x.size()))\n</code></pre> <p>\u200b\u6570\u636e\u5206\u5e03\u200b\u66f2\u7ebf\u200b\uff1a</p> <p></p> <p>\u200b\u7f51\u7edc\u7ed3\u6784\u200b</p> <pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.hidden = nn.Linear(1, 20)\n        self.predict = nn.Linear(20, 1)\n\n    def forward(self, x):\n        x = self.hidden(x)\n        x = F.relu(x)\n        x = self.predict(x)\n        return x\n</code></pre> <p>\u200b\u4e0b\u9762\u200b\u8fd9\u90e8\u5206\u200b\u662f\u200b\u6d4b\u8bd5\u200b\u56fe\u200b\uff0c\u200b\u7eb5\u5750\u6807\u200b\u4ee3\u8868\u200bLoss\uff0c\u200b\u6a2a\u5750\u6807\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200bStep\uff1a</p> <p></p> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u56fe\u7247\u200b\u4e0a\u200b\uff0c\u200b\u66f2\u7ebf\u200b\u4e0b\u964d\u200b\u7684\u200b\u8d8b\u52bf\u200b\u548c\u200b\u5bf9\u5e94\u200b\u7684\u200bsteps\u200b\u4ee3\u8868\u200b\u4e86\u200b\u5728\u200b\u8fd9\u8f6e\u200b\u6570\u636e\u200b\uff0c\u200b\u6a21\u578b\u200b\u4e0b\u200b\u7684\u200b\u6536\u655b\u200b\u901f\u5ea6\u200b</p> <p>\u200b\u6ce8\u610f\u200b:</p> <p>\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u9009\u62e9\u200b\u662f\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6539\u53d8\u200b\u7684\u200b\uff0c\u200b\u4e0d\u200b\u5b58\u5728\u200b\u7edd\u5bf9\u200b\u7684\u200b\u597d\u574f\u200b\u4e4b\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u591a\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u6d4b\u8bd5\u200b\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/","title":"4.1 ResNet","text":"<p>\u200b\u6b8b\u5dee\u200b\u795e\u7ecf\u7f51\u7edc\u200b(ResNet)\u200b\u662f\u200b\u7531\u200b\u5fae\u8f6f\u200b\u7814\u7a76\u9662\u200b\u7684\u200b\u4f55\u607a\u660e\u200b\u3001\u200b\u5f20\u7965\u96e8\u200b\u3001\u200b\u4efb\u5c11\u537f\u200b\u3001\u200b\u5b59\u5251\u200b\u7b49\u200b\u4eba\u200b\u63d0\u51fa\u200b\u7684\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u4e3b\u8981\u200b\u8d21\u732e\u200b\u662f\u200b\u53d1\u73b0\u200b\u4e86\u200b\u5728\u200b\u589e\u52a0\u200b\u7f51\u7edc\u5c42\u200b\u6570\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200b\u7cbe\u5ea6\u200b(Training accuracy)\u200b\u9010\u6e10\u200b\u8d8b\u4e8e\u200b\u9971\u548c\u200b\uff0c\u200b\u7ee7\u7eed\u200b\u589e\u52a0\u200b\u5c42\u6570\u200b\uff0ctraining accuracy \u200b\u5c31\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e0b\u964d\u200b\u7684\u200b\u73b0\u8c61\u200b\uff0c\u200b\u800c\u200b\u8fd9\u79cd\u200b\u4e0b\u964d\u200b\u4e0d\u662f\u200b\u7531\u8fc7\u200b\u62df\u5408\u200b\u9020\u6210\u200b\u7684\u200b\u3002\u200b\u4ed6\u4eec\u200b\u5c06\u200b\u8fd9\u200b\u4e00\u200b\u73b0\u8c61\u200b\u79f0\u4e4b\u4e3a\u200b\u201c\u200b\u9000\u5316\u200b\u73b0\u8c61\u200b\uff08Degradation\uff09\u201d\uff0c\u200b\u5e76\u200b\u9488\u5bf9\u200b\u9000\u5316\u200b\u73b0\u8c61\u200b\u53d1\u660e\u200b\u4e86\u200b \u201c\u200b\u5feb\u6377\u200b\u8fde\u63a5\u200b\uff08Shortcut connection\uff09\u201d\uff0c\u200b\u6781\u5927\u200b\u7684\u200b\u6d88\u9664\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u8fc7\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u56f0\u96be\u200b\u95ee\u9898\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u201c\u200b\u6df1\u5ea6\u200b\u201d\u200b\u9996\u6b21\u200b\u7a81\u7834\u200b\u4e86\u200b100\u200b\u5c42\u200b\u3001\u200b\u6700\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u751a\u81f3\u200b\u8d85\u8fc7\u200b\u4e86\u200b1000\u200b\u5c42\u200b\u3002\uff08\u200b\u5728\u200b\u6b64\u200b\uff0c\u200b\u5411\u200b\u5df2\u6545\u200b\u7684\u200b\u5b59\u5251\u200b\u535a\u58eb\u200b\u8868\u793a\u200b\u5d07\u9ad8\u200b\u7684\u200b\u656c\u610f\u200b\uff09</p> <p>\u200b\u901a\u8fc7\u200b\u672c\u6587\u200b\u4f60\u200b\u5c06\u200b\u5b66\u4e60\u200b\u5230\u200b\uff1a</p> <ul> <li> <p>\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b/\u200b\u7206\u70b8\u200b\u7684\u200b\u7b80\u4ecb\u200b</p> </li> <li> <p>\u200b\u4ee3\u7801\u200b\u91cc\u9762\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8bbe\u8ba1\u200bBasicBlock\u200b\u548c\u200bBottleneck\u200b\u4e24\u79cd\u200b\u7ed3\u6784\u200b</p> </li> <li> <p>\u200b\u4ee3\u7801\u200b\u91cc\u9762\u200b\u7684\u200bexpansion\u200b\u4f5c\u7528\u200b</p> </li> </ul>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#1","title":"1 \u200b\u57fa\u672c\u200b\u4ecb\u7ecd","text":"<p>\u200b\u968f\u7740\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u4eba\u4eec\u200b\u53d1\u73b0\u200b\u591a\u5c42\u200b\u5377\u79ef\u200b\u6216\u8005\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u5927\u4e8e\u200b\u5355\u5c42\u200b\u5377\u79ef\u200b\u6216\u8005\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u3002\u200b\u4e8e\u662f\u200b\u5f88\u591a\u200b\u4eba\u200b\u6f5c\u610f\u8bc6\u200b\u8ba4\u4e3a\u200b\u7f51\u7edc\u200b\u7684\u200b\u5c42\u6570\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u5176\u200b\u6548\u679c\u200b\u5c31\u200b\u4f1a\u200b\u8d8a\u200b\u597d\u200b\u3002\u200b\u4f46\u662f\u200b\u5f53\u65f6\u200b\u5fae\u8f6f\u200b\u7814\u7a76\u9662\u200b\u7684\u200b\u4f55\u607a\u660e\u200b\u3001\u200b\u5f20\u7965\u96e8\u200b\u3001\u200b\u4efb\u5c11\u537f\u200b\u3001\u200b\u5b59\u5251\u200b\u7b49\u200b\u4eba\u200b\u53d1\u73b0\u200b\u52a0\u6df1\u200b\u7f51\u7edc\u200b\u7684\u200b\u6df1\u5ea6\u200b\u540e\u200b\uff0c\u200b\u6574\u4e2a\u200b\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u53cd\u800c\u200b\u53d8\u5dee\u200b\u4e86\u200b\u8bb8\u591a\u200b\u3002\u200b\u4ed6\u4eec\u200b\u8ba4\u4e3a\u200b\u5f88\u6df1\u200b\u7684\u200b\u7f51\u7edc\u200b\u65e0\u6cd5\u200b\u8bad\u7ec3\u200b\u7684\u200b\u539f\u56e0\u200b\u53ef\u80fd\u200b\u662f\u200b\u7f51\u7edc\u200b\u5728\u200b\u4fe1\u606f\u200b\u4f20\u9012\u200b\u7684\u200b\u65f6\u5019\u200b\u6216\u591a\u6216\u5c11\u200b\u4f1a\u200b\u5b58\u5728\u200b\u4fe1\u606f\u200b\u4e22\u5931\u200b\uff0c\u200b\u635f\u8017\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u6216\u8005\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u73b0\u8c61\u200b\u3002\u200b\u9488\u5bf9\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u63d0\u51fa\u200b\u4e86\u200bResNet\u200b\u4ee5\u200b\u671f\u671b\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0cResNet\u200b\u7684\u200b\u51fa\u73b0\u200b\u4e5f\u200b\u8ba9\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9010\u6e10\u200b\u771f\u6b63\u200b\u8d70\u5411\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002ResNet\u200b\u6700\u5927\u200b\u7684\u200b\u8d21\u732e\u200b\u5728\u4e8e\u200b\u6dfb\u52a0\u200b\u4e86\u200bshortcut connection\u200b\u5c06\u200b\u8f93\u5165\u200b\u76f4\u63a5\u200b\u8fde\u63a5\u200b\u5230\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u7f13\u89e3\u200b\u4e86\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u5e76\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u8be6\u7ec6\u200b\u7684\u200b\u89e3\u91ca\u4e00\u4e0b\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u3002</p> <p>\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u7684\u200b\u6839\u6e90\u200b\u4e3b\u8981\u200b\u662f\u56e0\u4e3a\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ed3\u6784\u200b\u4ee5\u53ca\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u76ee\u524d\u200b\u4f18\u5316\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u601d\u60f3\u200b\uff0c\u200b\u5373\u200b\u6839\u636e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u7684\u200b\u8bef\u5dee\u200b\u901a\u8fc7\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u6307\u5bfc\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u6743\u503c\u200b\u7684\u200b\u66f4\u65b0\u200b\u3002\u200b\u8bef\u5dee\u200b\u68af\u5ea6\u200b\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u7684\u200b\u65b9\u5411\u200b\u548c\u200b\u6570\u91cf\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4ee5\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5411\u200b\u548c\u200b\u5408\u9002\u200b\u7684\u200b\u91cf\u200b\u66f4\u65b0\u200b\u7f51\u7edc\u200b\u6743\u91cd\u200b\u3002 \u200b\u5728\u200b\u6df1\u5c42\u200b\u7f51\u7edc\u200b\u6216\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u8bef\u5dee\u200b\u68af\u5ea6\u200b\u53ef\u200b\u5728\u200b\u66f4\u65b0\u200b\u4e2d\u200b\u7d2f\u79ef\u200b\uff0c\u200b\u53d8\u6210\u200b\u975e\u5e38\u200b\u5927\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u5bfc\u81f4\u200b\u7f51\u7edc\u200b\u6743\u91cd\u200b\u7684\u200b\u5927\u5e45\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5e76\u200b\u56e0\u6b64\u200b\u4f7f\u200b\u7f51\u7edc\u200b\u53d8\u5f97\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\u3002\u200b\u5728\u200b\u6781\u7aef\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6743\u91cd\u200b\u7684\u200b\u503c\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u5927\u200b\uff0c\u200b\u4ee5\u81f3\u4e8e\u200b\u6ea2\u51fa\u200b\uff0c\u200b\u5bfc\u81f4\u200b NaN \u200b\u503c\u200b\u3002 \u200b\u7f51\u7edc\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u503c\u200b\u5927\u4e8e\u200b 1.0\uff09\u200b\u91cd\u590d\u200b\u76f8\u4e58\u200b\u5bfc\u81f4\u200b\u7684\u200b\u6307\u6570\u200b\u7ea7\u200b\u589e\u957f\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u3002 \u200b\u5728\u200b\u6df1\u5ea6\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u4f1a\u200b\u5f15\u8d77\u200b\u7f51\u7edc\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\uff0c\u200b\u6700\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u65e0\u6cd5\u200b\u4ece\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\uff0c\u200b\u800c\u200b\u6700\u574f\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u51fa\u73b0\u200b\u65e0\u6cd5\u200b\u518d\u200b\u66f4\u65b0\u200b\u7684\u200b NaN \u200b\u6743\u91cd\u200b\u503c\u200b\u3002</p> <p>\u200b\u800c\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u68af\u5ea6\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c \u200b\u7f51\u7edc\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u503c\u200b\u5c0f\u4e8e\u200b 1.0\uff09\u200b\u91cd\u590d\u200b\u76f8\u4e58\u200b\u5bfc\u81f4\u200b\u7684\u200b\u6307\u6570\u200b\u7ea7\u200b\u53d8\u5c0f\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u3002\u200b\u5728\u200b\u6700\u574f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5b8c\u5168\u200b\u505c\u6b62\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u8fdb\u4e00\u6b65\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f20\u7edf\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b(\u200b\u5982\u200b\u53cc\u66f2\u200b\u6b63\u5207\u200b\u51fd\u6570\u200b)\u200b\u5177\u6709\u200b\u8303\u56f4\u200b(0,1)\u200b\u5185\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u901a\u8fc7\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002\u200b\u8fd9\u6837\u200b\u505a\u200b\u7684\u200b\u6548\u679c\u200b\u662f\u200b\uff0c\u200b\u7528\u200b\u8fd9\u4e9b\u200b\u5c0f\u200b\u6570\u5b57\u200b\u7684\u200bn\u200b\u4e58\u4ee5\u200bn\u200b\u6765\u200b\u8ba1\u7b97\u200bn\u200b\u5c42\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u201c\u200b\u524d\u7aef\u200b\u201d\u200b\u5c42\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u68af\u5ea6\u200b(\u200b\u8bef\u5dee\u200b\u4fe1\u53f7\u200b)\u200b\u968f\u200bn\u200b\u5448\u200b\u6307\u6570\u200b\u9012\u51cf\u200b\uff0c\u200b\u800c\u200b\u524d\u7aef\u200b\u5c42\u200b\u7684\u200b\u8bad\u7ec3\u200b\u975e\u5e38\u200b\u7f13\u6162\u200b\u3002\u200b\u6700\u7ec8\u200b\u5bfc\u81f4\u200b\u66f4\u65b0\u200b\u505c\u6ede\u200b\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#2","title":"2 \u200b\u6e90\u7801\u200b\u89e3\u8bfb","text":"<p>\u200b\u4e3a\u4e86\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u5bf9\u200bResNet\u200b\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200btorchvision\u200b\u7684\u200bResNet\u200b\u6e90\u7801\u200b\u8fdb\u884c\u200b\u89e3\u8bfb\u200b\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#21","title":"2.1 \u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u5c01\u88c5","text":"<p>\u200b\u5728\u200b\u4ee3\u7801\u200b\u7684\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9996\u5148\u200b\u5c01\u88c5\u200b\u4e86\u200b3x3\u200b\u548c\u200b1x1\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u589e\u52a0\u200b\u4ee3\u7801\u200b\u7684\u200b\u53ef\u8bfb\u6027\u200b\u3002\u200b\u9664\u4e86\u200b\u8fd9\u79cd\u200b\u4ee3\u7801\u200b\u5199\u6cd5\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u5728\u200b\u5f00\u59cb\u200b\u4e5f\u200b\u4f1a\u200b\u5c06\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5c42\u200b\u548c\u200bBN\u200b\u5c42\u200b\u5c01\u88c5\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u540c\u6837\u200b\u662f\u200b\u4e3a\u4e86\u200b\u589e\u52a0\u200b\u4ee3\u7801\u200b\u7684\u200b\u53ef\u8bfb\u6027\u200b\u3002</p> <pre><code>def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -&gt; nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -&gt; nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n</code></pre>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#22","title":"2.2 \u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u7684\u200b\u8bbe\u8ba1","text":"<p>ResNet\u200b\u7f51\u7edc\u200b\u662f\u200b\u7531\u200b\u5f88\u591a\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u5806\u53e0\u200b\u8d77\u6765\u200b\u7684\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4fdd\u8bc1\u200b\u4ee3\u7801\u200b\u5177\u6709\u200b\u53ef\u8bfb\u6027\u200b\u548c\u200b\u53ef\u6269\u5c55\u6027\u200b\uff0cResNet\u200b\u5728\u200b\u8bbe\u8ba1\u200b\u65f6\u200b\u91c7\u7528\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u9488\u5bf9\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200bResNet\uff0c\u200b\u4e66\u5199\u200b\u4e86\u200bBasicBlock\u200b\u548c\u200bBottleNeck\u200b\u4e24\u4e2a\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u3002\u200b\u8fd9\u79cd\u200b\u6a21\u5757\u5316\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u5728\u200b\u73b0\u5728\u200b\u8bb8\u591a\u200b\u5e38\u89c1\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ecf\u5e38\u200b\u770b\u5230\u200b\u3002</p> <p>ResNet\u200b\u5e38\u89c1\u200b\u7684\u200b\u5927\u5c0f\u200b\u6709\u200b\u4e0b\u56fe\u200b\u7684\u200bResNet-18\uff0cResNet-34\uff0cResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\uff0c\u200b\u5176\u4e2d\u200b\u7f51\u7edc\u200b\u540e\u9762\u200b\u7684\u200b\u6570\u5b57\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u7f51\u7edc\u200b\u7684\u200b\u5c42\u6570\u200b\u3002</p> <p></p> <p>\u200b\u4e3a\u4e86\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u66f4\u597d\u200b\u7684\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ee5\u200bResNet101\u200b\u4e3a\u4f8b\u200b\u3002</p> layer_name \u200b\u6b21\u6570\u200b conv1 \u200b\u5377\u79ef\u200b1\u200b\u6b21\u200b conv2_x \u200b\u5377\u79ef\u200b3 x 3 = 9\u200b\u6b21\u200b conv3_x \u200b\u5377\u79ef\u200b4 x 3 = 12\u200b\u6b21\u200b conv4_x \u200b\u5377\u79ef\u200b23 x 3 = 69\u200b\u6b21\u200b conv5_x \u200b\u5377\u79ef\u200b3 x 3 = 9\u200b\u6b21\u200b fc average pool 1\u200b\u6b21\u200b \u200b\u5408\u8ba1\u200b 1 + 9 + 12 + 69 + 9 + 1 = 101\u200b\u6b21\u200b <p>\u200b\u89c2\u5bdf\u200b\u4e0a\u9762\u200b\u5404\u4e2a\u200bResNet\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u5185\u200b\uff0c\u200b\u6570\u636e\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e0d\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u4f46\u662f\u200bResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u5185\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200bchannel\u200b\u6570\u76ee\u200b\u4e0d\u200b\u4e00\u6837\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7684\u200bchannel\u200b\u6269\u5927\u200b\u4e3a\u200b\u8f93\u5165\u200bchannel\u200b\u7684\u200b4\u200b\u500d\u200b\uff0c\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u5377\u79ef\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e5f\u200b\u53d8\u6362\u200b\u4e3a\u200b1\uff0c3\uff0c1\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u57fa\u4e8e\u200b\u8fd9\u4e2a\u200b\u53d1\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u7684\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u5f53\u4f5c\u200b\u4e00\u7c7b\u200b\uff0cResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\u200b\u8fd9\u4e09\u7c7b\u200b\u7f51\u7edc\u200b\u7684\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u5f53\u4f5c\u200b\u4e00\u7c7b\u200b\u3002\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0ctorchvision\u200b\u7684\u200b\u6e90\u7801\u200b\u4e5f\u200b\u662f\u200b\u57fa\u4e8e\u200b\u8fd9\u79cd\u200b\u8bbe\u8ba1\u200b\u601d\u60f3\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u56fe\u200b\u7684\u200bBasicBlock\uff08\u200b\u5de6\u200b\uff09\u200b\u548c\u200bBottleNeck\uff08\u200b\u53f3\u200b\uff09\u200b\u6a21\u5757\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4e3a\u4e86\u200b\u63a7\u5236\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u76ee\u200b\u7684\u200b\u53d8\u5316\u200b\uff0c\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u7ef4\u5ea6\u200b\u4e5f\u200b\u901a\u8fc7\u200bexpansion\u200b\u8fdb\u884c\u200b\u63a7\u5236\u200b\uff0c\u200b\u4e24\u4e2a\u200bblock\u200b\u7c7b\u200b\u8f93\u5165\u200b\u4e00\u4e2a\u200b\u901a\u9053\u200b\u4e3a\u200bin_planes\u200b\u7ef4\u7684\u5ea6\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200bplanes*block.expansion\u200b\u7ef4\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u5176\u4e2d\u200bplanes\u200b\u7684\u200b\u6570\u76ee\u200b\u5927\u5c0f\u200b\u7b49\u4e8e\u200bin_planes\u3002\u200b\u9664\u6b64\u4ee5\u5916\u200b\uff0c\u200b\u4ee3\u7801\u200b\u53f3\u4fa7\u200b\u7684\u200b\u66f2\u7ebf\u200b\u5c31\u662f\u200b\u672c\u6587\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200bshortcut\u200b\u652f\u8def\u200b\uff0c\u200b\u8be5\u652f\u200b\u8def\u4e0a\u200b\u7684\u200bdownsample\u200b\u64cd\u4f5c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5bf9\u200bshortcut\u200b\u652f\u8def\u200b\u8fdb\u884c\u200b\u5927\u5c0f\u200b\u6216\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u8c03\u6574\u200b\uff0c\u200b\u4ee5\u200b\u5e0c\u671b\u200b\u6267\u884c\u200b\u76f8\u52a0\u200b\u64cd\u4f5c\u200b</p> <p></p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#221-shortcut-connection","title":"2.2.1 Shortcut Connection.","text":"<p>\u200b\u8fd9\u91cc\u200b\u518d\u200b\u5206\u6790\u200b\u4e00\u4e0b\u200bshortcut connection\uff1a</p> <p></p> <p>shortcut connection\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u201c\u200b\u6284\u8fd1\u200b\u9053\u200b\u201d\uff0c\u200b\u5b83\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5176\u4e00\u200b\u4e3a\u200b\u540c\u7b49\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u5373\u200b\u8f93\u5165\u8f93\u51fa\u200b\u76f4\u63a5\u200b\u76f8\u52a0\u200b\uff08\u200b\u5373\u200b\u4e0a\u200b\u56fe\u200b\u4e2d\u200b\u7684\u200bF(x) + x\uff09\uff0c\u200b\u53e6\u200b\u4e00\u79cd\u200b\u4e3a\u200b\u4e0d\u540c\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u8fd9\u65f6\u5019\u200b\u5c31\u200b\u9700\u8981\u200b\u7ed9\u200bx\u200b\u8865\u5145\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u6765\u200b\u5339\u914d\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\u4e0b\u9762\u200b\u8fd9\u4e2a\u200b\u56fe\u200b\uff1a</p> <p></p> <p>\u200b\u5de6\u200b\uff1aVGG-19\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u53c2\u8003\u200b\u3002 \u200b\u4e2d\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u6709\u200b34\u200b\u4e2a\u200b\u53c2\u6570\u200b\u5c42\u200b\u7684\u200b\u666e\u901a\u200b\u7f51\u7edc\u200b\u3002 \u200b\u53f3\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u6709\u200b34\u200b\u4e2a\u200b\u53c2\u6570\u200b\u5c42\u200b\u7684\u200b\u6b8b\u5dee\u200b\u7f51\u7edc\u200b\uff08\u200b\u5373\u200bresnet34\uff09</p> <p>\u200b\u5728\u200b\u4e0a\u200b\u56fe\u200b\u6700\u200b\u53f3\u4fa7\u200b\u7684\u200b\u8def\u5f84\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u660e\u663e\u200b\u7684\u200b\u770b\u5230\u200bshortcut connection\u200b\u52a0\u5165\u200b\u4e86\u200b\u7f51\u7edc\u200b\u4e4b\u4e2d\u200b\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u56fe\u4e2d\u200b\u4e5f\u200b\u5f88\u200b\u660e\u663e\u200b\u7684\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5b9e\u7ebf\u200b\u90e8\u5206\u200b\u5c31\u662f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5355\u7eaf\u200b\u7684\u200bF(x)+x\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u800c\u200b\u865a\u7ebf\u200b\u90e8\u5206\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u5c42\u200b\u7684\u200bstride\u200b\u662f\u200b2\uff08\u200b\u90a3\u4e2a\u200b/2\u200b\u7684\u200b\u610f\u601d\u200b\u5c31\u662f\u200bstride\u200b\u662f\u200b2\uff09\uff1b\u200b\u540c\u65f6\u200b\u6ce8\u610f\u200b\u5230\u200b\u6df1\u5ea6\u200b\u4e5f\u200b\u53d1\u751f\u200b\u4e86\u200b\u53d8\u6362\u200b\uff0cchannel\u200b\u6570\u76ee\u200b\u589e\u52a0\u4e00\u500d\u200b\uff08\u200b\u6269\u5927\u200b\u4e24\u500d\u200b\uff09\uff0c\u200b\u8fd9\u6837\u200bF(x)\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\u6bd4\u200bx\u200b\u5c0f\u200b\u4e00\u534a\u200b\uff0c\u200b\u539a\u5ea6\u200b\u6bd4\u200bx\u200b\u5927\u200b\u4e00\u500d\u200b\u3002\u200b\u5728\u200b\u8fd9\u6837\u200b\u7684\u200bshortcut connection\u200b\u4e2d\u200b\uff0c\u200b\u5c31\u200b\u9700\u8981\u200b\u8865\u5145\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u6765\u200b\u589e\u52a0\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u5728\u200bResNet\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4f7f\u7528\u200b\u4e86\u200b1 x 1\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u6765\u200b\u8fbe\u5230\u200b\u8fd9\u4e2a\u200b\u76ee\u7684\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53c8\u200b\u63d0\u5230\u200b\u8bf4\u200b\uff1a\u201c\u2026\u2026where both designs have similar time complexity.\u201d \u200b\u65e2\u7136\u200bBasicBlock\u200b\u548c\u200bBottleneck\u200b\u4e8c\u8005\u200b\u7684\u200b\u65f6\u95f4\u200b\u590d\u6742\u5ea6\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4e3a\u4ec0\u4e48\u200b\u8fd8\u8981\u200b\u989d\u5916\u200b\u8bbe\u8ba1\u200b\u4e00\u4e2a\u200bBottleneck\u200b\u7ed3\u6784\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6839\u636e\u200b\u524d\u9762\u200b\u7684\u200b\u53d9\u8ff0\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\uff0cBasicBlock\u200b\u7ed3\u6784\u200b\u6bd4\u200b\u4f20\u7edf\u200b\u7684\u200b\u5377\u79ef\u200b\u7ed3\u6784\u200b\u591a\u200b\u4e86\u200b\u4e00\u4e2a\u200bshortcut\u200b\u652f\u8def\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4f20\u9012\u200b\u4f4e\u5c42\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u7f51\u7edc\u200b\u80fd\u591f\u200b\u8bad\u7ec3\u200b\u5730\u200b\u5f88\u200b\u6df1\u200b\u3002\u200b\u800c\u200bBottleNeck\u200b\u5148\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b1x1\u200b\u7684\u200b\u5377\u79ef\u200b\u51cf\u5c11\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u4e2d\u95f4\u200b\u5377\u79ef\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u51cf\u5c11\u200b\u4e3a\u200b1/4\uff1b\u200b\u4e2d\u95f4\u200b\u7684\u200b\u666e\u901a\u200b\u5377\u79ef\u200b\u505a\u200b\u5b8c\u200b\u5377\u79ef\u200b\u540e\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\u7b49\u4e8e\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u6570\u200b\uff1b\u200b\u7b2c\u4e09\u4e2a\u200b\u5377\u79ef\u200b\u7528\u4e8e\u200b\u6062\u590d\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u5f97\u200bBottleNeck\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\u7b49\u4e8e\u200bBottleNeck\u200b\u7684\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u6570\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b1x1\u200b\u5377\u79ef\u200b\u6709\u6548\u200b\u5730\u200b\u51cf\u5c11\u200b\u4e86\u200b\u5377\u79ef\u200b\u7684\u200b\u53c2\u6570\u200b\u4e2a\u6570\u200b\u548c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\uff0c\u200b\u540c\u65f6\u200b\u51cf\u5c11\u200b\u4e86\u200b\u4e2d\u95f4\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u200b\u5355\u4e2a\u200bBlock\u200b\u6d88\u8017\u200b\u7684\u200b\u663e\u5b58\u200b\u66f4\u200b\u5c11\u200b\uff0c\u200b\u5728\u200b\u8f83\u200b\u6df1\u200b\u7684\u200b\u7f51\u7edc\u200b\u4e2d\u200bBottleNeck\u200b\u4f1a\u200b\u5728\u200b\u53c2\u6570\u200b\u4e0a\u200b\u66f4\u52a0\u200b\u8282\u7ea6\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u6709\u5229\u4e8e\u200b\u6784\u5efa\u200b\u5c42\u6570\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7f51\u7edc\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u80fd\u200b\u4fdd\u6301\u200b\u6027\u80fd\u200b\u7684\u200b\u63d0\u5347\u200b\u3002\u200b\u6240\u4ee5\u200bresnet50, resnet101\u200b\u548c\u200bresnet152\u200b\u4f7f\u7528\u200b\u4e86\u200b\u53e6\u5916\u200b\u8bbe\u8ba1\u200b\u7684\u200bBottleNeck\u200b\u7ed3\u6784\u200b\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#222-basicblock","title":"2.2.2 BasicBlock","text":"<p>BasicBlock\u200b\u6a21\u5757\u200b\u7528\u6765\u200b\u6784\u5efa\u200bresnet18\u200b\u548c\u200bresnet34</p> <pre><code>class BasicBlock(nn.Module):\n    expansion: int = 1\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n        if dilation &gt; 1:\n            raise NotImplementedError(\"Dilation &gt; 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x  # x  \u200b\u7ed9\u200b\u81ea\u5df1\u200b\u5148\u200b\u5907\u4efd\u200b\u4e00\u4efd\u200b\n\n        out = self.conv1(x)  # \u200b\u5bf9\u200bx\u200b\u505a\u200b\u5377\u79ef\u200b \n        out = self.bn1(out)  # \u200b\u5bf9\u200bx\u200b\u5f52\u4e00\u5316\u200b \n        out = self.relu(out)  # \u200b\u5bf9\u200bx\u200b\u7528\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\n\n        out = self.conv2(out)  # \u200b\u5bf9\u200bx\u200b\u505a\u200b\u5377\u79ef\u200b\n        out = self.bn2(out)  # \u200b\u5f52\u4e00\u5316\u200b\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity  # \u200b\u8fdb\u884c\u200bdownsample\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#223-bottleneck","title":"2.2.3 BottleNeck","text":"<p>BottleNeck\u200b\u6a21\u5757\u200b\u7528\u6765\u200b\u6784\u5efa\u200bresnet50\uff0cresnet101\u200b\u548c\u200bresnet152</p> <pre><code>class Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4  # \u200b\u5bf9\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u8fdb\u884c\u200b\u500d\u589e\u200b\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        # Bottleneckd forward\u200b\u51fd\u6570\u200b\u548c\u200bBasicBlock\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4e0d\u518d\u200b\u989d\u5916\u200b\u6ce8\u91ca\u200b\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u518d\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u4e2d\u200bexpansion\u200b\u7684\u200b\u4f5c\u7528\u200b\u505a\u200b\u4e00\u4e2a\u200b\u8bf4\u660e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\u4e0b\u9762\u200b\u8fd9\u200b\u5f20\u56fe\u200b\u3002</p> <p></p> <p>expansion\u200b\u7b80\u5355\u200b\u6765\u8bf4\u200b\u5c31\u662f\u200b\u5bf9\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u7684\u200b\u500d\u200b\u4e58\u200b\u3002\u200b\u5728\u200bBasicBlock\u200b\u548c\u200bBottleNeck\u200b\u4e2d\u200b\uff0c\u201c__init__\u201d\u200b\u51fd\u6570\u200b\u4e2d\u6709\u200b\u4e09\u4e2a\u200b\u6bd4\u8f83\u200b\u5173\u952e\u200b\u7684\u200b\u53c2\u6570\u200b\uff1ainplanes,planes\u200b\u548c\u200bstride\uff0c\u200b\u8fd9\u200b\u4e09\u8005\u200b\u5206\u522b\u200b\u8868\u793a\u200b\u8f93\u5165\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u548c\u200b\u6b65\u5e45\u200b\u3002\u200b\u5728\u200b\u4e24\u4e2a\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c__init__\u200b\u4f20\u5165\u200b\u7684\u200bplanes\u200b\u90fd\u200b\u662f\u200b64,128,156,512\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u89c2\u5bdf\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u683c\u200b\uff0c\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u5bf9\u4e8e\u200bResNet-50\uff0cResNet-101\u200b\u548c\u200bResNet-152\u200b\u800c\u8a00\u200b\uff0c\u200b\u5b83\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u5e94\u8be5\u200b\u4e3a\u200b256,512,1024,2048\u200b\u624d\u200b\u5bf9\u200b\u3002\u200b\u56e0\u6b64\u200b\u5728\u200b\u8fd9\u91cc\u200b\u8bbe\u7f6e\u200bexpansion=4\uff0c\u200b\u5bf9\u5e94\u200b\u4e0a\u9762\u200bBottleNeck\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u7684\u200b30\u200b\u884c\u200b\u548c\u200b31\u200b\u884c\u200b\uff0c\u200b\u5c06\u200b\u6bcf\u200b\u4e00\u4e2a\u200bplanes\u200b\u90fd\u200b\u4e58\u200b\u4e0a\u200b\u8fd9\u4e2a\u200bexpansion\uff0c\u200b\u5c31\u200b\u5f97\u5230\u200b\u4e86\u200b\u9700\u8981\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff1b\u200b\u800c\u200b\u5bf9\u4e8e\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u800c\u8a00\u200b\uff0c\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u548c\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u6ca1\u6709\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u56e0\u6b64\u200bexpansion\u200b\u4e5f\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b1\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#23","title":"2.3 \u200b\u7f51\u7edc\u200b\u6574\u4f53\u200b\u7ed3\u6784","text":"<p>\u200b\u5728\u200b\u5b9a\u4e49\u200b\u597d\u200b\u6700\u200b\u57fa\u672c\u200b\u7684\u200bBottlenneck\u200b\u548c\u200bBasicBlock\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u6784\u5efa\u200bResNet\u200b\u7f51\u7edc\u200b\u4e86\u200b\u3002</p> <pre><code>class ResNet(nn.Module):\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]], # \u200b\u9009\u62e9\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\n        layers: List[int], # \u200b\u6bcf\u200b\u4e00\u5c42\u200bblock\u200b\u7684\u200b\u6570\u76ee\u200b\u6784\u6210\u200b -&gt; [3,4,6,3]\n        num_classes: int = 1000, # \u200b\u5206\u7c7b\u200b\u6570\u76ee\u200b\n        zero_init_residual: bool = False, # \u200b\u521d\u59cb\u5316\u200b\n\n        #######\u200b\u5176\u4ed6\u200b\u5377\u79ef\u200b\u6784\u6210\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200bResNet\u200b\u65e0\u5173\u200b######\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        #########################################\n\n        norm_layer: Optional[Callable[..., nn.Module]] = None, # norm\u200b\u5c42\u200b\n    ) -&gt; None:\n        super().__init__()\n        _log_api_usage_once(self)\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64 # \u200b\u8f93\u5165\u200b\u901a\u9053\u200b\n\n        #######\u200b\u5176\u4ed6\u200b\u5377\u79ef\u200b\u6784\u6210\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200bResNet\u200b\u65e0\u5173\u200b######\n        self.dilation = 1 # \u200b\u7a7a\u6d1e\u200b\u5377\u79ef\u200b\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n        #########################################\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        # \u200b\u901a\u8fc7\u200b_make_layer\u200b\u5e26\u5230\u200b\u5c42\u6b21\u5316\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u6548\u679c\u200b\n        self.layer1 = self._make_layer(block, 64, layers[0])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv2_x\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv3_x\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv4_x\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv5_x\n        # \u200b\u5206\u7c7b\u200b\u5934\u200b\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        # \u200b\u6a21\u578b\u200b\u521d\u59cb\u5316\u200b\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n    # \u200b\u5c42\u6b21\u5316\u200b\u8bbe\u8ba1\u200b\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]], # \u200b\u57fa\u672c\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u9009\u62e9\u200b\n        planes: int,  # \u200b\u8f93\u5165\u200b\u7684\u200b\u901a\u9053\u200b\n        blocks: int, # \u200b\u6a21\u5757\u200b\u6570\u76ee\u200b\n        stride: int = 1, # \u200b\u6b65\u957f\u200b\n        dilate: bool = False, # \u200b\u7a7a\u6d1e\u200b\u5377\u79ef\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200b\u65e0\u5173\u200b\n    ) -&gt; nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None # \u200b\u662f\u5426\u200b\u91c7\u7528\u200b\u4e0b\u200b\u91c7\u6837\u200b\n        ####################\u200b\u65e0\u5173\u200b#####################\n        previous_dilation = self.dilation \n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        #############################################\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        # \u200b\u4f7f\u7528\u200blayers\u200b\u5b58\u50a8\u200b\u6bcf\u4e2a\u200blayer\n        layers = []\n        layers.append(\n            block(\n                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    dilation=self.dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n        # \u200b\u5c06\u200blayers\u200b\u901a\u8fc7\u200bnn.Sequential\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u7f51\u7edc\u200b\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -&gt; Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)  # conv1   x shape [1 64 112 112]\n        x = self.bn1(x)   # \u200b\u5f52\u4e00\u5316\u200b\u5904\u7406\u200b   \n        x = self.relu(x)  # \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\n        x = self.maxpool(x)  # conv2_x\u200b\u7684\u200b3x3 maxpool        x shape [1 64 56 56]\n\n        x = self.layer1(x) # layer 1\n        x = self.layer2(x) # layer 2\n        x = self.layer3(x) # layer 3\n        x = self.layer4(x) # layer 4\n\n        x = self.avgpool(x) # \u200b\u81ea\u200b\u9002\u5e94\u200b\u6c60\u5316\u200b\n        x = torch.flatten(x, 1) \n        x = self.fc(x) # \u200b\u5206\u7c7b\u200b\n\n        return x\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self._forward_impl(x) \n</code></pre> <p>\u200b\u89c2\u5bdf\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u96be\u200b\u770b\u5230\u200b\uff0c\u200b\u9996\u5148\u200b\u662f\u200b\u4e00\u4e2a\u200b7 x 7\u200b\u7684\u200b\u5377\u79ef\u200b\u4f5c\u7528\u200b\u5728\u200b\u8f93\u5165\u200b\u7684\u200b3\u200b\u7ef4\u200b\u56fe\u7247\u200b\u4e0a\u200b\uff0c\u200b\u5e76\u200b\u8f93\u5165\u200b\u4e00\u4e2a\u200b64\u200b\u7ef4\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff08\u200b\u5373\u200bself.inplanes\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\uff09\uff0c\u200b\u901a\u8fc7\u200bBatchNorm\u200b\u5c42\u200b\uff0cReLU\u200b\u5c42\u200b\uff0cMaxPool\u200b\u5c42\u200b\uff1b\u200b\u7136\u540e\u200b\u7ecf\u8fc7\u200b_make_layer()\u200b\u51fd\u6570\u200b\u6784\u5efa\u200b\u7684\u200b4\u200b\u5c42\u200blayer\uff0c\u200b\u6700\u540e\u200b\u7ecf\u8fc7\u200b\u4e00\u4e2a\u200bAveragePooling\u200b\u5c42\u200b\uff0c\u200b\u518d\u200b\u7ecf\u8fc7\u200b\u4e00\u4e2a\u200bfc\u200b\u5c42\u200b\u5f97\u5230\u200b\u5206\u7c7b\u200b\u8f93\u51fa\u200b\u3002\u200b\u5728\u200b\u7f51\u7edc\u200b\u642d\u5efa\u200b\u8d77\u6765\u200b\u540e\u200b\uff0c\u200b\u8fd8\u200b\u5bf9\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b(Conv2d\u3001BatchNorm2d\u3001last BN)\u200b\u8fdb\u884c\u200b\u4e86\u200b\u521d\u59cb\u5316\u200b\u3002</p> <p>\u200b\u800c\u200b\u5bf9\u4e8e\u200b_make_layer\u200b\u51fd\u6570\u200b\uff0c\u200b\u4e00\u4e2a\u200b_make_layer()\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200blayer\u200b\u5c42\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200blayer\u200b\u5c42\u200b\u662f\u200b\u4e0a\u8ff0\u200b\u4e24\u79cd\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u7684\u200b\u5806\u53e0\u200b\u3002\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u4e2d\u200bblock\u200b\u4ee3\u8868\u200b\u8be5\u200blayer\u200b\u5806\u53e0\u200b\u6a21\u5757\u200b\u7684\u200b\u7c7b\u578b\u200b\uff0c\u200b\u53ef\u200b\u9009\u200bBasicBlock\u200b\u6216\u8005\u200bBottleNeck\uff1bblocks\u200b\u4ee3\u8868\u200b\u8be5\u200blayer\u200b\u4e2d\u200b\u5806\u53e0\u200b\u7684\u200bblock\u200b\u7684\u200b\u6570\u76ee\u200b\uff1bplanes\u200b\u4e0e\u200b\u8be5\u200blayer\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u200b\u6709\u5173\u200b\uff0c\u200b\u6ce8\u610f\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u4e3a\u200bplanes * block.expansion\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c _make_layer()\u200b\u662f\u200b\u7528\u6765\u200b\u751f\u6210\u200b\u6b8b\u5dee\u200b\u5757\u200b\u7684\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u7275\u626f\u200b\u5230\u200b\u5b83\u200b\u7684\u200b\u7b2c\u56db\u4e2a\u200b\u53c2\u6570\u200b\uff1astride\uff0c\u200b\u5373\u200b\u5377\u79ef\u200b\u6b65\u5e45\u200b\u3002\u200b\u8be5\u200b\u51fd\u6570\u200b\u4e2d\u200b\u9996\u5148\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u5982\u679c\u200bstride\u200b\u4e0d\u200b\u7b49\u4e8e\u200b1\u200b\u6216\u8005\u200b\u7ef4\u5ea6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff08\u200b\u5373\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u4e0d\u200b\u6ee1\u8db3\u200b\u5bf9\u5e94\u200b\u5173\u7cfb\u200b\uff09\u200b\u7684\u200b\u65f6\u5019\u200b\u7684\u200bdownsample\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200bBN\u200b\u64cd\u4f5c\u200b\u3002\u200b\u63a5\u7740\u200b\u5bf9\u200binplanes\u200b\u548c\u200bplanes\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u7684\u200b\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u6b21\u200bdownsample \uff0c\u200b\u5373\u5c06\u200b\u5e26\u200bdownsample\u200b\u7684\u200bblock\u200b\u6dfb\u52a0\u200b\u81f3\u200blayers\u3002\u200b\u8fd9\u6837\u200b\u4fdd\u8bc1\u200b\u4e86\u200bx\u200b\u548c\u200bout\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u5faa\u73af\u200b\u6dfb\u52a0\u200b\u4e86\u200b\u6307\u5b9a\u200b\u4e2a\u6570\u200b\u7684\u200bBlock\uff0c\u200b\u7531\u4e8e\u200bx\u200b\u5df2\u7ecf\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\u4e86\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6dfb\u52a0\u200b\u7684\u200b\u5176\u4ed6\u200b\u7684\u200bBlock\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e0d\u7528\u200b\u964d\u7ef4\u200b\u4e86\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5faa\u73af\u200b\u6dfb\u52a0\u200b\u4e0d\u200b\u542b\u200bDownsample\u200b\u7684\u200bBlock\u3002\u200b\u6b63\u5982\u200b\u4e0b\u9762\u200b\u4ee3\u7801\u200b\u6240\u793a\u200b</p> <pre><code>if stride != 1 or self.inplanes != planes * block.expansion:\n    downsample = nn.Sequential(\n        conv1x1(self.inplanes, planes * block.expansion, stride),\n        norm_layer(planes * block.expansion),\n    )\n</code></pre> <p>\u200b\u5f53\u200b\u4e00\u4e2a\u200blayer\u200b\u5305\u542b\u200b\u591a\u4e2a\u200bblock\u200b\u65f6\u200b\uff0c\u200b\u662f\u200b\u901a\u8fc7\u200b\u5411\u200blayers\u200b\u5217\u8868\u200b\u4e2d\u200b\u4f9d\u6b21\u200b\u52a0\u5165\u200b\u6bcf\u4e2a\u200bblock\uff0c\u200b\u6765\u200b\u5b9e\u73b0\u200bblock\u200b\u7684\u200b\u5806\u53e0\u200b\u7684\u200b\u3002\u200b\u7b2c\u4e00\u4e2a\u200bblock\u200b\u9700\u8981\u200b\u7279\u6b8a\u200b\u5904\u7406\u200b\uff0c\u200b\u8be5\u200bblock\u200b\u4f9d\u636e\u200b\u4f20\u5165\u200b\u7684\u200bself.inplanes, planes\u200b\u4ee5\u53ca\u200bstride\u200b\u5224\u65ad\u200b\uff0c\u200b\u53ef\u80fd\u200b\u542b\u6709\u200bdownsample\u200b\u652f\u8def\u200b\uff1b\u200b\u8fd9\u4e2a\u200bblock\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u662f\u200bplanes*block.expansion\u3002\u200b\u7d27\u63a5\u7740\u200b\u4fbf\u200b\u628a\u200bself.inplanes\u200b\u66f4\u65b0\u200b\u4e3a\u6b64\u200b\u503c\u200b\u4f5c\u4e3a\u200b\u540e\u7eed\u200bblock\u200b\u7684\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u540e\u9762\u200b\u7684\u200bblock\u200b\u7684\u200bstride\u200b\u4e3a\u200b\u9ed8\u8ba4\u503c\u200b1\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8f93\u5165\u200b\u4e3a\u200bself.inplanes\uff0c\u200b\u8f93\u51fa\u200b\u4e3a\u200bplanes*block.expansion\uff0c\u200b\u800c\u200bself.inplanes = planes * block.expansion\uff0c\u200b\u56e0\u6b64\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u7279\u5f81\u200b\u56fe\u200b\u5927\u5c0f\u200b\u6216\u8005\u200b\u5c3a\u5bf8\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200bdownsample\u200b\u64cd\u4f5c\u200b\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.1%20ResNet/#3","title":"3 \u200b\u603b\u7ed3","text":"<p>\u200b\u4e0e\u200b\u666e\u901a\u200b\u7684\u200b\u7f51\u7edc\u200b\u76f8\u6bd4\u200b\uff0cResNet\u200b\u6700\u5927\u200b\u7684\u200b\u4f18\u52bf\u200b\u5c31\u662f\u200b\u5f15\u5165\u200b\u4e86\u200bShortcut\u200b\u8fd9\u4e2a\u200b\u652f\u8def\u200b\uff0c\u200b\u8ba9\u200b\u67d0\u200b\u4e00\u5c42\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8fde\u63a5\u200b\u5230\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5b66\u4e60\u200b\u6b8b\u5dee\u200b\u3002\u200b\u4f20\u7edf\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u6216\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u5728\u200b\u4fe1\u606f\u200b\u4f20\u9012\u200b\u65f6\u200b\uff0c\u200b\u6216\u591a\u6216\u5c11\u200b\u4f1a\u200b\u5b58\u5728\u200b\u4fe1\u606f\u200b\u4e22\u5931\u200b\u3001\u200b\u635f\u8017\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002ResNet \u200b\u5728\u200b\u67d0\u79cd\u7a0b\u5ea6\u200b\u4e0a\u200b\u89e3\u51b3\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u5c06\u200b\u8f93\u5165\u200b\u4fe1\u606f\u200b\u7ed5\u9053\u200b\u4f20\u5230\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4fdd\u62a4\u200b\u4fe1\u606f\u200b\u7684\u200b\u5b8c\u6574\u6027\u200b\uff0c\u200b\u6574\u4e2a\u200b\u7f51\u7edc\u200b\u5219\u200b\u53ea\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\u8f93\u5165\u200b\u3001\u200b\u8f93\u51fa\u200b\u5dee\u522b\u200b\u7684\u200b\u90a3\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u7b80\u5316\u200b\u5b66\u4e60\u200b\u76ee\u6807\u200b\u548c\u200b\u96be\u5ea6\u200b\u3002</p> <p>ResNet\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u89e3\u51b3\u200b\u4e86\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u968f\u200b\u6df1\u5ea6\u200b\u7684\u200b\u589e\u52a0\u200b\uff0c\u200b\u4f46\u662f\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u5374\u200b\u53d8\u5dee\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u7528\u200b\u4f5c\u8005\u200b\u7684\u8bdd\u200b\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b: \u201cOur deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\u201d\u3002\u200b\u539f\u59cb\u200b\u7684\u200bResNet\u200b\u5bf9\u4e8e\u200b\u8bad\u7ec3\u200b\u5377\u79ef\u200b\u795e\u7ecf\u200b\u7f51\u8def\u200b\u505a\u51fa\u200b\u4e86\u200b\u5f88\u5927\u200b\u7684\u200b\u8d21\u732e\u200b\uff0c\u200b\u4f46\u662f\u200b\u540c\u6837\u200b\u4e5f\u200b\u6709\u7740\u200b\u8bb8\u591a\u200b\u53ef\u4ee5\u200b\u6539\u8fdb\u200b\u7684\u200b\u5730\u65b9\u200b\u3002\u200b\u968f\u7740\u200b\u65f6\u4ee3\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u539f\u7248\u200b\u7684\u200bResNet\u200b\u5728\u200b\u4e00\u6b21\u200b\u53c8\u200b\u4e00\u6b21\u200b\u7684\u200b\u7814\u7a76\u200b\u4e2d\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e30\u5bcc\u200b\u548c\u200b\u5b8c\u5584\u200b\uff0c\u200b\u884d\u751f\u200b\u51fa\u200b\u4e86\u200b\u4e30\u5bcc\u200b\u7684\u200b\u6539\u8fdb\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200bResNeXt\u3002\u200b\u5b83\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u4ecb\u4e8e\u200b\u666e\u901a\u200b\u5377\u79ef\u200b\u6838\u200b\u6df1\u5ea6\u200b\u53ef\u200b\u5206\u79bb\u200b\u5377\u79ef\u200b\u7684\u200b\u8fd9\u79cd\u200b\u7b56\u7565\u200b\uff1a\u200b\u5206\u7ec4\u200b\u5377\u79ef\u200b\u3002\u200b\u901a\u8fc7\u200b\u63a7\u5236\u200b\u5206\u7ec4\u200b\u7684\u200b\u6570\u91cf\u200b\uff08\u200b\u57fa\u6570\u200b\uff09\u200b\u6765\u200b\u8fbe\u5230\u200b\u4e24\u79cd\u200b\u7b56\u7565\u200b\u7684\u200b\u5e73\u8861\u200b\u3002\u200b\u5206\u7ec4\u200b\u5377\u79ef\u200b\u7684\u200b\u601d\u60f3\u200b\u662f\u200b\u6e90\u81ea\u200bInception\uff0c\u200b\u4e0d\u540c\u4e8e\u200bInception\u200b\u7684\u200b\u9700\u8981\u200b\u4eba\u5de5\u200b\u8bbe\u8ba1\u200b\u6bcf\u4e2a\u200b\u5206\u652f\u200b\uff0cResNeXt\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5206\u652f\u200b\u7684\u200b\u62d3\u6251\u200b\u7ed3\u6784\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002\u200b\u6700\u540e\u200b\u518d\u200b\u7ed3\u5408\u200b\u6b8b\u5dee\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5f97\u5230\u200b\u7684\u200b\u4fbf\u662f\u200b\u6700\u7ec8\u200b\u7684\u200bResNeXt\u3002</p> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0cResNet\u200b\u8fd8\u6709\u200b\u5176\u5b83\u200b\u53d8\u4f53\u200b\u5982\u200bWider ResNet\uff0cDarkNet53\u200b\u7b49\u200b\u3002\u200b\u5b83\u4eec\u200b\u7684\u200b\u6539\u8fdb\u200b\u76f8\u5bf9\u200b\u8f83\u5927\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200bDarkNet53\uff0c\u200b\u5b83\u200b\u548c\u200bResNet\u200b\u5df2\u7ecf\u200b\u6709\u200b\u5f88\u5927\u200b\u4e0d\u540c\u200b\u4e86\u200b\uff0c\u200b\u53ea\u662f\u200b\u4f7f\u7528\u200b\u5230\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u4ece\u800c\u200b\u590d\u7528\u200b\u7279\u5f81\u200b\u800c\u5df2\u200b\u3002\u200b\u603b\u800c\u8a00\u4e4b\u200b\uff0cResNet\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u4e00\u4e2a\u200b\u91cc\u7a0b\u7891\u5f0f\u200b\u7684\u200b\u5de5\u4f5c\u200b\u3002\u3002</p>"},{"location":"04-%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/4.4%20FashionMNIST%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/","title":"\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014FashionMNIST\u200b\u65f6\u88c5\u200b\u5206\u7c7b","text":"<p>\u200b\u7ecf\u8fc7\u200b\u524d\u9762\u200b\u4e09\u7ae0\u200b\u5185\u5bb9\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a - \u200b\u5bf9\u200bPyTorch\u200b\u6709\u200b\u4e86\u200b\u521d\u6b65\u200b\u7684\u200b\u8ba4\u8bc6\u200b - \u200b\u5b66\u4f1a\u200b\u4e86\u200b\u5982\u4f55\u200b\u5b89\u88c5\u200bPyTorch\u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7f16\u7a0b\u200b\u73af\u5883\u200b - \u200b\u5b66\u4e60\u200b\u4e86\u200bPyTorch\u200b\u6700\u200b\u6838\u5fc3\u200b\u7684\u200b\u7406\u8bba\u200b\u57fa\u7840\u200b\uff08\u200b\u5f20\u91cf\u200b&amp;\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\uff09 - \u200b\u68b3\u7406\u200b\u4e86\u200b\u5229\u7528\u200bPyTorch\u200b\u5b8c\u6210\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e3b\u8981\u200b\u6b65\u9aa4\u200b\u548c\u200b\u5bf9\u5e94\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b  </p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u6848\u4f8b\u200b\uff0c\u200b\u5c06\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u6240\u200b\u6d89\u53ca\u200b\u7684\u200bPyTorch\u200b\u5165\u95e8\u200b\u77e5\u8bc6\u200b\u4e32\u200b\u8d77\u6765\u200b\uff0c\u200b\u4fbf\u4e8e\u200b\u5927\u5bb6\u200b\u52a0\u6df1\u200b\u7406\u89e3\u200b\u3002\u200b\u540c\u65f6\u200b\u4e3a\u200b\u540e\u7eed\u200b\u7684\u200b\u8fdb\u9636\u200b\u5b66\u4e60\u200b\u6253\u200b\u597d\u200b\u57fa\u7840\u200b\u3002 </p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u5404\u4e2a\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b</li> </ul> <p>\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u5bf9\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u201c\u200b\u65f6\u88c5\u200b\u201d\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u4f7f\u7528\u200bFashionMNIST\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 \u200b\u4e0a\u200b\u56fe\u200b\u7ed9\u51fa\u200b\u4e86\u200bFashionMNIST\u200b\u4e2d\u200b\u6570\u636e\u200b\u7684\u200b\u82e5\u5e72\u200b\u6837\u4f8b\u200b\u56fe\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u5c0f\u56fe\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u6837\u672c\u200b\u3002 FashionMNIST\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u5305\u542b\u200b\u5df2\u7ecf\u200b\u9884\u5148\u200b\u5212\u5206\u200b\u597d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bad\u7ec3\u200b\u96c6\u5171\u200b60,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u96c6\u5171\u200b10,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\u3002\u200b\u6bcf\u5f20\u200b\u56fe\u50cf\u200b\u5747\u200b\u4e3a\u200b\u5355\u901a\u9053\u200b\u9ed1\u767d\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b28*28pixel\uff0c\u200b\u5206\u5c5e\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002  </p> <p>\u200b\u4e0b\u9762\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u8d77\u200b\u5c06\u200b\u7b2c\u4e09\u7ae0\u200b\u5404\u200b\u90e8\u5206\u200b\u5185\u5bb9\u200b\u9010\u6b65\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6765\u200b\u8dd1\u200b\u5b8c\u6574\u200b\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6d41\u7a0b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u7684\u200b\u5305\u200b </p> <pre><code>import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>\u200b\u914d\u7f6e\u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b\u548c\u200b\u8d85\u200b\u53c2\u6570\u200b </p> <pre><code># \u200b\u914d\u7f6e\u200bGPU\uff0c\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\n## \u200b\u65b9\u6848\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200bos.environ\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n# \u200b\u65b9\u6848\u200b\u4e8c\u200b\uff1a\u200b\u4f7f\u7528\u200b\u201cdevice\u201d\uff0c\u200b\u540e\u7eed\u200b\u5bf9\u8981\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u53d8\u91cf\u200b\u7528\u200b.to(device)\u200b\u5373\u53ef\u200b\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\n## \u200b\u914d\u7f6e\u200b\u5176\u4ed6\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200bbatch_size, num_workers, learning rate, \u200b\u4ee5\u53ca\u200b\u603b\u200b\u7684\u200bepochs\nbatch_size = 256\nnum_workers = 4   # \u200b\u5bf9\u4e8e\u200bWindows\u200b\u7528\u6237\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u5e94\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u591a\u7ebf\u7a0b\u200b\u9519\u8bef\u200b\nlr = 1e-4\nepochs = 20\n</code></pre> <p>\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u548c\u200b\u52a0\u8f7d\u200b \u200b\u8fd9\u91cc\u200b\u540c\u65f6\u200b\u5c55\u793a\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b: - \u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u4f7f\u7528\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b - \u200b\u4ece\u200b\u7f51\u7ad9\u200b\u4e0b\u8f7d\u200b\u4ee5\u200bcsv\u200b\u683c\u5f0f\u200b\u5b58\u50a8\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u8bfb\u5165\u200b\u5e76\u200b\u8f6c\u6210\u200b\u9884\u671f\u200b\u7684\u200b\u683c\u5f0f\u200b   \u200b\u7b2c\u4e00\u79cd\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u65b9\u5f0f\u200b\u53ea\u200b\u9002\u7528\u200b\u4e8e\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5982\u200bMNIST\uff0cCIFAR10\u200b\u7b49\u200b\uff0cPyTorch\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u3002\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u5f80\u5f80\u200b\u9002\u7528\u200b\u4e8e\u200b\u5feb\u901f\u200b\u6d4b\u8bd5\u65b9\u6cd5\u200b\uff08\u200b\u6bd4\u5982\u200b\u6d4b\u8bd5\u200b\u4e0b\u200b\u67d0\u4e2a\u200bidea\u200b\u5728\u200bMNIST\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u662f\u5426\u200b\u6709\u6548\u200b\uff09 \u200b\u7b2c\u4e8c\u79cd\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u65b9\u5f0f\u200b\u9700\u8981\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200bDataset\uff0c\u200b\u8fd9\u200b\u5bf9\u4e8e\u200bPyTorch\u200b\u5e94\u7528\u200b\u4e8e\u200b\u81ea\u5df1\u200b\u7684\u200b\u5de5\u4f5c\u200b\u4e2d\u200b\u5341\u5206\u200b\u91cd\u8981\u200b  </p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5fc5\u8981\u200b\u7684\u200b\u53d8\u6362\u200b\uff0c\u200b\u6bd4\u5982\u8bf4\u200b\u9700\u8981\u200b\u5c06\u200b\u56fe\u7247\u200b\u7edf\u4e00\u200b\u4e3a\u200b\u4e00\u81f4\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u540e\u7eed\u200b\u80fd\u591f\u200b\u8f93\u5165\u200b\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\uff1b\u200b\u9700\u8981\u200b\u5c06\u200b\u6570\u636e\u683c\u5f0f\u200b\u8f6c\u4e3a\u200bTensor\u200b\u7c7b\u200b\uff0c\u200b\u7b49\u7b49\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u5730\u200b\u501f\u52a9\u200btorchvision\u200b\u5305\u6765\u200b\u5b8c\u6210\u200b\uff0c\u200b\u8fd9\u662f\u200bPyTorch\u200b\u5b98\u65b9\u200b\u7528\u4e8e\u200b\u56fe\u50cf\u5904\u7406\u200b\u7684\u200b\u5de5\u5177\u200b\u5e93\u200b\uff0c\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e5f\u200b\u8981\u200b\u7528\u5230\u200b\u3002PyTorch\u200b\u7684\u200b\u4e00\u5927\u200b\u65b9\u4fbf\u200b\u4e4b\u5904\u200b\u5c31\u200b\u5728\u4e8e\u200b\u5b83\u200b\u662f\u200b\u4e00\u6574\u5957\u200b\u201c\u200b\u751f\u6001\u200b\u201d\uff0c\u200b\u6709\u7740\u200b\u5b98\u65b9\u200b\u548c\u200b\u7b2c\u4e09\u65b9\u200b\u5404\u4e2a\u9886\u57df\u200b\u7684\u200b\u652f\u6301\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u540e\u7eed\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u3002</p> <pre><code># \u200b\u9996\u5148\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\nfrom torchvision import transforms\n\nimage_size = 28\ndata_transform = transforms.Compose([\n    transforms.ToPILImage(),  \n     # \u200b\u8fd9\u200b\u4e00\u6b65\u200b\u53d6\u51b3\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\n    transforms.Resize(image_size),\n    transforms.ToTensor()\n])\n</code></pre> <pre><code>## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200btorchvision\u200b\u81ea\u5e26\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\nfrom torchvision import datasets\n\ntrain_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform)\ntest_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform)\n</code></pre> <pre><code>/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n</code></pre> <pre><code>## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e8c\u200b\uff1a\u200b\u8bfb\u5165\u200bcsv\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u81ea\u884c\u200b\u6784\u5efa\u200bDataset\u200b\u7c7b\u200b\n# csv\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff1ahttps://www.kaggle.com/zalando-research/fashionmnist\nclass FMDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.images = df.iloc[:,1:].values.astype(np.uint8)\n        self.labels = df.iloc[:, 0].values\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx].reshape(28,28,1)\n        label = int(self.labels[idx])\n        if self.transform is not None:\n            image = self.transform(image)\n        else:\n            image = torch.tensor(image/255., dtype=torch.float)\n        label = torch.tensor(label, dtype=torch.long)\n        return image, label\n\ntrain_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_train.csv\")\ntest_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_test.csv\")\ntrain_data = FMDataset(train_df, data_transform)\ntest_data = FMDataset(test_df, data_transform)\n</code></pre> <p>\u200b\u5728\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5b9a\u4e49\u200bDataLoader\u200b\u7c7b\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b  </p> <pre><code>train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n</code></pre> <p>\u200b\u8bfb\u5165\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u505a\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u53ef\u89c6\u5316\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4e3b\u8981\u200b\u662f\u200b\u9a8c\u8bc1\u200b\u6211\u4eec\u200b\u8bfb\u5165\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u5426\u200b\u6b63\u786e\u200b</p> <pre><code>import matplotlib.pyplot as plt\nimage, label = next(iter(train_loader))\nprint(image.shape, label.shape)\nplt.imshow(image[0][0], cmap=\"gray\")\n</code></pre> <pre><code>torch.Size([256, 1, 28, 28]) \ntorch.Size([256])   \n&lt;matplotlib.image.AxesImage at 0x7f19a043cc10&gt;\n</code></pre> <p></p> <p>\u200b\u6a21\u578b\u200b\u8bbe\u8ba1\u200b \u200b\u7531\u4e8e\u200b\u4efb\u52a1\u200b\u8f83\u4e3a\u7b80\u5355\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u624b\u200b\u642d\u200b\u4e00\u4e2a\u200bCNN\uff0c\u200b\u800c\u200b\u4e0d\u200b\u8003\u8651\u200b\u5f53\u4e0b\u200b\u5404\u79cd\u200b\u6a21\u578b\u200b\u7684\u200b\u590d\u6742\u200b\u7ed3\u6784\u200b\uff0c\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u5230\u200bGPU\u200b\u4e0a\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u3002  </p> <pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout(0.3),\n            nn.Conv2d(32, 64, 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout(0.3)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64*4*4, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1, 64*4*4)\n        x = self.fc(x)\n        # x = nn.functional.normalize(x)\n        return x\n\nmodel = Net()\nmodel = model.cuda()\n# model = nn.DataParallel(model).cuda()   # \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u5199\u6cd5\u200b\uff0c\u200b\u4e4b\u540e\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u4f1a\u200b\u8fdb\u4e00\u6b65\u200b\u8bb2\u89e3\u200b\n</code></pre> <p>\u200b\u8bbe\u5b9a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u4f7f\u7528\u200btorch.nn\u200b\u6a21\u5757\u200b\u81ea\u5e26\u200b\u7684\u200bCrossEntropy\u200b\u635f\u5931\u200b PyTorch\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u628a\u200b\u6574\u6570\u578b\u200b\u7684\u200blabel\u200b\u8f6c\u4e3a\u200bone-hot\u200b\u578b\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200bCE loss \u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u786e\u4fdd\u200blabel\u200b\u662f\u4ece\u200b0\u200b\u5f00\u59cb\u200b\u7684\u200b\uff0c\u200b\u540c\u65f6\u200b\u6a21\u578b\u200b\u4e0d\u200b\u52a0\u200bsoftmax\u200b\u5c42\u200b\uff08\u200b\u4f7f\u7528\u200blogits\u200b\u8ba1\u7b97\u200b\uff09,\u200b\u8fd9\u200b\u4e5f\u200b\u8bf4\u660e\u200b\u4e86\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u4e0d\u662f\u200b\u72ec\u7acb\u200b\u7684\u200b\uff0c\u200b\u9700\u8981\u200b\u901a\u76d8\u8003\u8651\u200b</p> <pre><code>criterion = nn.CrossEntropyLoss()\n# criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])\n</code></pre> <pre><code>?nn.CrossEntropyLoss # \u200b\u8fd9\u91cc\u200b\u65b9\u4fbf\u200b\u770b\u200b\u4e00\u4e0b\u200bweighting\u200b\u7b49\u200b\u7b56\u7565\u200b\n</code></pre> <p>\u200b\u8bbe\u5b9a\u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bAdam\u200b\u4f18\u5316\u200b\u5668\u200b  </p> <pre><code>optimizer = optim.Adam(model.parameters(), lr=0.001)\n</code></pre> <p>\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\uff08\u200b\u9a8c\u8bc1\u200b\uff09 \u200b\u5404\u81ea\u200b\u5c01\u88c5\u200b\u6210\u200b\u51fd\u6570\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u8c03\u7528\u200b \u200b\u5173\u6ce8\u200b\u4e24\u8005\u200b\u7684\u200b\u4e3b\u8981\u200b\u533a\u522b\u200b\uff1a - \u200b\u6a21\u578b\u200b\u72b6\u6001\u200b\u8bbe\u7f6e\u200b - \u200b\u662f\u5426\u200b\u9700\u8981\u200b\u521d\u59cb\u5316\u200b\u4f18\u5316\u200b\u5668\u200b - \u200b\u662f\u5426\u200b\u9700\u8981\u200b\u5c06\u200bloss\u200b\u4f20\u200b\u56de\u5230\u200b\u7f51\u7edc\u200b - \u200b\u662f\u5426\u200b\u9700\u8981\u200b\u6bcf\u6b65\u200b\u66f4\u65b0\u200boptimizer  </p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6d4b\u8bd5\u200b\u6216\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u5206\u7c7b\u200b\u51c6\u786e\u7387\u200b</p> <pre><code>def train(epoch):\n    model.train()\n    train_loss = 0\n    for data, label in train_loader:\n        data, label = data.cuda(), label.cuda()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n    train_loss = train_loss/len(train_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n</code></pre> <pre><code>def val(epoch):       \n    model.eval()\n    val_loss = 0\n    gt_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data, label in test_loader:\n            data, label = data.cuda(), label.cuda()\n            output = model(data)\n            preds = torch.argmax(output, 1)\n            gt_labels.append(label.cpu().data.numpy())\n            pred_labels.append(preds.cpu().data.numpy())\n            loss = criterion(output, label)\n            val_loss += loss.item()*data.size(0)\n    val_loss = val_loss/len(test_loader.dataset)\n    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))\n</code></pre> <pre><code>for epoch in range(1, epochs+1):\n    train(epoch)\n    val(epoch)\n</code></pre> <pre><code>/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nEpoch: 1    Training Loss: 0.659050\nEpoch: 1    Validation Loss: 0.420328, Accuracy: 0.852000\nEpoch: 2    Training Loss: 0.403703\nEpoch: 2    Validation Loss: 0.350373, Accuracy: 0.872300\nEpoch: 3    Training Loss: 0.350197\nEpoch: 3    Validation Loss: 0.293053, Accuracy: 0.893200\nEpoch: 4    Training Loss: 0.322463\nEpoch: 4    Validation Loss: 0.283335, Accuracy: 0.892300\nEpoch: 5    Training Loss: 0.300117\nEpoch: 5    Validation Loss: 0.268653, Accuracy: 0.903500\nEpoch: 6    Training Loss: 0.282179\nEpoch: 6    Validation Loss: 0.247219, Accuracy: 0.907200\nEpoch: 7    Training Loss: 0.268283\nEpoch: 7    Validation Loss: 0.242937, Accuracy: 0.907800\nEpoch: 8    Training Loss: 0.257615\nEpoch: 8    Validation Loss: 0.234324, Accuracy: 0.912200\nEpoch: 9    Training Loss: 0.245795\nEpoch: 9    Validation Loss: 0.231515, Accuracy: 0.914100\nEpoch: 10   Training Loss: 0.238739\nEpoch: 10   Validation Loss: 0.229616, Accuracy: 0.914400\nEpoch: 11   Training Loss: 0.230499\nEpoch: 11   Validation Loss: 0.228124, Accuracy: 0.915200\nEpoch: 12   Training Loss: 0.221574\nEpoch: 12   Validation Loss: 0.211928, Accuracy: 0.921200\nEpoch: 13   Training Loss: 0.217924\nEpoch: 13   Validation Loss: 0.209744, Accuracy: 0.921700\nEpoch: 14   Training Loss: 0.206033\nEpoch: 14   Validation Loss: 0.215477, Accuracy: 0.921400\nEpoch: 15   Training Loss: 0.203349\nEpoch: 15   Validation Loss: 0.215550, Accuracy: 0.919400\nEpoch: 16   Training Loss: 0.196319\nEpoch: 16   Validation Loss: 0.210800, Accuracy: 0.923700\nEpoch: 17   Training Loss: 0.191969\nEpoch: 17   Validation Loss: 0.207266, Accuracy: 0.923700\nEpoch: 18   Training Loss: 0.185466\nEpoch: 18   Validation Loss: 0.207138, Accuracy: 0.924200\nEpoch: 19   Training Loss: 0.178241\nEpoch: 19   Validation Loss: 0.204093, Accuracy: 0.924900\nEpoch: 20   Training Loss: 0.176674\nEpoch: 20   Validation Loss: 0.197495, Accuracy: 0.928300\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b \u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200btorch.save\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6216\u8005\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b \u200b\u8fd9\u90e8\u5206\u200b\u4f1a\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b</p> <pre><code>save_path = \"./FahionModel.pkl\"\ntorch.save(model, save_path)\n</code></pre> <p>\u200b\u4f5c\u4e1a\u200b - \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b 1. \u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u200b6.2 \u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u540e\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u65b9\u6cd5\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002 2. \u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u200b6.3 \u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u540e\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002 3. \u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u200b6.5 \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u540e\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200bimgaug\u200b\u5e93\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002 4. \u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u200b7.3 \u200b\u4f7f\u7528\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u540e\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002 5. \u200b\u4f7f\u7528\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002</p> <ul> <li>\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</li> <li>\u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u200b5.4 PyTorh\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u4e0e\u200b\u8bfb\u53d6\u200b\u540e\u200b\uff0c\u200b\u52a0\u8f7d\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200bsklearn.metrics\u200b\u4e2d\u200b\u7684\u200bclassification_report\u200b\u51fd\u6570\u200b\uff0c\u200b\u8f93\u51fa\u200b\u6a21\u578b\u200b\u7684\u200b\u5206\u7c7b\u200b\u62a5\u544a\u200b\u3002</li> <li>\u200b\u5b66\u4e60\u200b\u5b8c\u200b9.1 \u200b\u4f7f\u7528\u200bONNX\u200b\u8fdb\u884c\u200b\u90e8\u7f72\u200b\u5e76\u200b\u63a8\u7406\u200b\u540e\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f\u200b\uff0c\u200b\u4f7f\u7528\u200bONNXRuntime\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002</li> </ul>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/","title":"5.1 PyTorch\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u7684\u200b\u65b9\u5f0f","text":"<p>\u200b\u6a21\u578b\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u626e\u6f14\u7740\u200b\u91cd\u8981\u200b\u7684\u200b\u89d2\u8272\u200b\uff0c\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u6781\u5927\u200b\u5730\u200b\u4fc3\u8fdb\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\u8fdb\u6b65\u200b\uff0c\u200b\u6bd4\u5982\u200bCNN\u200b\u7684\u200b\u63d0\u51fa\u200b\u89e3\u51b3\u200b\u4e86\u200b\u56fe\u50cf\u200b\u3001\u200b\u89c6\u9891\u200b\u5904\u7406\u200b\u4e2d\u200b\u7684\u200b\u8bf8\u591a\u200b\u95ee\u9898\u200b\uff0cRNN/LSTM\u200b\u6a21\u578b\u200b\u89e3\u51b3\u200b\u4e86\u200b\u5e8f\u5217\u200b\u6570\u636e\u5904\u7406\u200b\u7684\u200b\u95ee\u9898\u200b\uff0cGNN\u200b\u5728\u200b\u56fe\u200b\u6a21\u578b\u200b\u4e0a\u200b\u53d1\u6325\u200b\u7740\u200b\u91cd\u8981\u200b\u7684\u200b\u4f5c\u7528\u200b\u3002\u200b\u5f53\u200b\u6211\u4eec\u200b\u5728\u200b\u5411\u200b\u4ed6\u4eba\u200b\u4ecb\u7ecd\u200b\u4e00\u9879\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5de5\u4f5c\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5bf9\u65b9\u200b\u53ef\u80fd\u200b\u9996\u5148\u200b\u8981\u200b\u95ee\u200b\u7684\u200b\u5c31\u662f\u200b\u4f7f\u7528\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u6a21\u578b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200bPyTorch\u200b\u8fdb\u9636\u200b\u64cd\u4f5c\u200b\u7684\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u6765\u200b\u5b66\u4e60\u200bPyTorch\u200b\u6a21\u578b\u200b\u76f8\u5173\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u5728\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u7b2c\u4e09\u7ae0\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b66\u4e60\u200b\u4e86\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u201c\u200b\u5c42\u200b\u201c\u200b\u662f\u200b\u5982\u4f55\u200b\u5b9a\u4e49\u200b\u7684\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u57fa\u7840\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u7684\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u6765\u200b\u66f4\u4e3a\u200b\u7cfb\u7edf\u5730\u200b\u5b66\u4e60\u200bPyTorch\u200b\u4e2d\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\u5c06\u200b\u4e3a\u200b\u540e\u7eed\u200b\u7075\u6d3b\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u6253\u4e0b\u200b\u575a\u5b9e\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b*\uff1a</p> <ul> <li>\u200b\u719f\u6089\u200bPyTorch\u200b\u4e2d\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u7684\u200b\u4e09\u79cd\u200b\u65b9\u5f0f\u200b</li> <li>\u200b\u8bfb\u61c2\u200bGitHub\u200b\u4e0a\u200b\u5343\u5947\u767e\u602a\u200b\u7684\u200b\u5199\u6cd5\u200b</li> <li>\u200b\u81ea\u5df1\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u7075\u6d3b\u200b\u9009\u53d6\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b</li> </ul>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#511","title":"5.1.1 \u200b\u5fc5\u8981\u200b\u7684\u200b\u77e5\u8bc6\u200b\u56de\u987e","text":"<ul> <li><code>Module</code> \u200b\u7c7b\u200b\u662f\u200b <code>torch.nn</code> \u200b\u6a21\u5757\u200b\uf9e9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6784\u9020\u200b\u7c7b\u200b (<code>nn.Module</code>)\uff0c\u200b\u662f\u200b\u6240\u6709\u200b\u795e\u7ecf\u200b\u2f79\u200b\u7f51\u7edc\u200b\u6a21\u5757\u200b\u7684\u200b\u57fa\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ee7\u627f\u200b\u5b83\u200b\u6765\u200b\u5b9a\u4e49\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\uff1b</li> <li>PyTorch\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u5e94\u200b\u5305\u62ec\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u5206\u200b\uff1a\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\uff08<code>__init__</code>\uff09\uff1b\u200b\u6570\u636e\u6d41\u200b\u5411\u200b\u5b9a\u4e49\u200b\uff08<code>forward</code>\uff09</li> </ul> <p>\u200b\u57fa\u4e8e\u200b<code>nn.Module</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>Sequential</code>\uff0c<code>ModuleList</code>\u200b\u548c\u200b<code>ModuleDict</code>\u200b\u4e09\u79cd\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200bPyTorch\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c31\u200b\u6765\u200b\u9010\u4e2a\u200b\u63a2\u7d22\u200b\u8fd9\u200b\u4e09\u79cd\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#512-sequential","title":"5.1.2 Sequential","text":"<p>\u200b\u5bf9\u5e94\u200b\u6a21\u5757\u200b\u4e3a\u200b<code>nn.Sequential()</code>\u3002</p> <p>\u200b\u5f53\u200b\u6a21\u578b\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u4e3a\u200b\u7b80\u5355\u200b\uf905\u200b\u8054\u200b\u5404\u4e2a\u200b\u5c42\u200b\u7684\u200b\u8ba1\u7b97\u200b\u65f6\u200b\uff0c <code>Sequential</code> \u200b\u7c7b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\uf901\u200b\u52a0\u200b\u7b80\u5355\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u3002\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u63a5\u6536\u200b\u4e00\u4e2a\u200b\u5b50\u200b\u6a21\u5757\u200b\u7684\u200b\u6709\u5e8f\u200b\u5b57\u5178\u200b(OrderedDict) \u200b\u6216\u8005\u200b\u4e00\u7cfb\u5217\u200b\u5b50\u200b\u6a21\u5757\u200b\u4f5c\u4e3a\u200b\u53c2\u6570\u200b\u6765\u200b\u9010\u4e00\u200b\u6dfb\u52a0\u200b <code>Module</code> \u200b\u7684\u200b\u5b9e\u200b\uf9b5\u200b\uff0c\u200b\u2f7d\u200b\u6a21\u578b\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u5c31\u662f\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5b9e\u200b\uf9b5\u200b\u6309\u200b\u6dfb\u52a0\u200b\u7684\u200b\u987a\u5e8f\u200b\u9010\u200b\u2f00\u200b\u8ba1\u7b97\u200b\u3002\u200b\u6211\u4eec\u200b\u7ed3\u5408\u200b<code>Sequential</code>\u200b\u548c\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u52a0\u4ee5\u200b\u7406\u89e3\u200b\uff1a</p> <pre><code>from collections import OrderedDict\nclass MySequential(nn.Module):\n    def __init__(self, *args):\n        super(MySequential, self).__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict): # \u200b\u5982\u679c\u200b\u4f20\u5165\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200bOrderedDict\n            for key, module in args[0].items():\n                self.add_module(key, module)  \n                # add_module\u200b\u65b9\u6cd5\u200b\u4f1a\u200b\u5c06\u200bmodule\u200b\u6dfb\u52a0\u200b\u8fdb\u200bself._modules(\u200b\u4e00\u4e2a\u200bOrderedDict)\n        else:  # \u200b\u4f20\u5165\u200b\u7684\u200b\u662f\u200b\u4e00\u4e9b\u200bModule\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n    def forward(self, input):\n        # self._modules\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b OrderedDict\uff0c\u200b\u4fdd\u8bc1\u200b\u4f1a\u200b\u6309\u7167\u200b\u6210\u5458\u200b\u6dfb\u52a0\u200b\u65f6\u200b\u7684\u200b\u987a\u5e8f\u200b\u904d\u5386\u200b\u6210\u200b\n        for module in self._modules.values():\n            input = module(input)\n        return input\n</code></pre> <p>\u200b\u4e0b\u9762\u200b\u6765\u770b\u200b\u4e0b\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b<code>Sequential</code>\u200b\u6765\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u3002\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u5c42\u200b\u6309\u5e8f\u200b\u6392\u5217\u200b\u8d77\u6765\u200b\u5373\u53ef\u200b\uff0c\u200b\u6839\u636e\u200b\u5c42\u540d\u200b\u7684\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6392\u5217\u200b\u7684\u200b\u65f6\u5019\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff1a</p> <ul> <li>\u200b\u76f4\u63a5\u200b\u6392\u5217\u200b </li> </ul> <pre><code>import torch.nn as nn\nnet = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 10), \n        )\nprint(net)\n</code></pre> <pre><code>Sequential(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n</code></pre> <ul> <li>\u200b\u4f7f\u7528\u200bOrderedDict\uff1a</li> </ul> <pre><code>import collections\nimport torch.nn as nn\nnet2 = nn.Sequential(collections.OrderedDict([\n          ('fc1', nn.Linear(784, 256)),\n          ('relu1', nn.ReLU()),\n          ('fc2', nn.Linear(256, 10))\n          ]))\nprint(net2)\n</code></pre> <pre><code>Sequential(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (relu1): ReLU()\n  (fc2): Linear(in_features=256, out_features=10, bias=True)\n)\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>Sequential</code>\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7684\u200b\u597d\u5904\u200b\u5728\u4e8e\u200b\u7b80\u5355\u200b\u3001\u200b\u6613\u8bfb\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f7f\u7528\u200b<code>Sequential</code>\u200b\u5b9a\u4e49\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0d\u200b\u9700\u8981\u200b\u518d\u200b\u5199\u200b<code>forward</code>\uff0c\u200b\u56e0\u4e3a\u200b\u987a\u5e8f\u200b\u5df2\u7ecf\u200b\u5b9a\u4e49\u200b\u597d\u200b\u4e86\u200b\u3002\u200b\u4f46\u200b\u4f7f\u7528\u200b<code>Sequential</code>\u200b\u4e5f\u200b\u4f1a\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4e27\u5931\u200b\u7075\u6d3b\u6027\u200b\uff0c\u200b\u6bd4\u5982\u200b\u9700\u8981\u200b\u5728\u200b\u6a21\u578b\u200b\u4e2d\u95f4\u200b\u52a0\u5165\u200b\u4e00\u4e2a\u200b\u5916\u90e8\u200b\u8f93\u5165\u200b\u65f6\u200b\u5c31\u200b\u4e0d\u200b\u9002\u5408\u200b\u7528\u200b<code>Sequential</code>\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9e\u73b0\u200b\u3002\u200b\u4f7f\u7528\u200b\u65f6\u9700\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u9700\u6c42\u200b\u52a0\u4ee5\u200b\u9009\u62e9\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#513-modulelist","title":"5.1.3 ModuleList","text":"<p>\u200b\u5bf9\u5e94\u200b\u6a21\u5757\u200b\u4e3a\u200b<code>nn.ModuleList()</code>\u3002</p> <p><code>ModuleList</code> \u200b\u63a5\u6536\u200b\u4e00\u4e2a\u200b\u5b50\u200b\u6a21\u5757\u200b\uff08\u200b\u6216\u5c42\u200b\uff0c\u200b\u9700\u200b\u5c5e\u4e8e\u200b<code>nn.Module</code>\u200b\u7c7b\u200b\uff09\u200b\u7684\u200b\uf99c\u200b\u8868\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\uff0c\u200b\u7136\u540e\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u7c7b\u4f3c\u200b<code>List</code>\u200b\u90a3\u6837\u200b\u8fdb\u884c\u200bappend\u200b\u548c\u200bextend\u200b\u64cd\u4f5c\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u5b50\u200b\u6a21\u5757\u200b\u6216\u5c42\u200b\u7684\u200b\u6743\u91cd\u200b\u4e5f\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u6dfb\u52a0\u200b\u5230\u200b\u7f51\u7edc\u200b\u4e2d\u6765\u200b\u3002</p> <pre><code>net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\nnet.append(nn.Linear(256, 10)) # # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200bappend\u200b\u64cd\u4f5c\u200b\nprint(net[-1])  # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200b\u7d22\u5f15\u200b\u8bbf\u95ee\u200b\nprint(net)\n</code></pre> <pre><code>Linear(in_features=256, out_features=10, bias=True)\nModuleList(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n</code></pre> <p>\u200b\u8981\u200b\u7279\u522b\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c<code>nn.ModuleList</code> \u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002<code>ModuleList</code>\u200b\u4e2d\u200b\u5143\u7d20\u200b\u7684\u200b\u5148\u540e\u987a\u5e8f\u200b\u5e76\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u5176\u200b\u5728\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u771f\u5b9e\u200b\u4f4d\u7f6e\u200b\u987a\u5e8f\u200b\uff0c\u200b\u9700\u8981\u200b\u7ecf\u8fc7\u200bforward\u200b\u51fd\u6570\u200b\u6307\u5b9a\u200b\u5404\u4e2a\u200b\u5c42\u200b\u7684\u200b\u5148\u540e\u987a\u5e8f\u200b\u540e\u200b\u624d\u200b\u7b97\u200b\u5b8c\u6210\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u5b9a\u4e49\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u65f6\u7528\u200bfor\u200b\u5faa\u73af\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\uff1a</p> <pre><code>class model(nn.Module):\n  def __init__(self, ...):\n    super().__init__()\n    self.modulelist = ...\n    ...\n\n  def forward(self, x):\n    for layer in self.modulelist:\n      x = layer(x)\n    return x\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#514-moduledict","title":"5.1.4 ModuleDict","text":"<p>\u200b\u5bf9\u5e94\u200b\u6a21\u5757\u200b\u4e3a\u200b<code>nn.ModuleDict()</code>\u3002</p> <p><code>ModuleDict</code>\u200b\u548c\u200b<code>ModuleList</code>\u200b\u7684\u200b\u4f5c\u7528\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u53ea\u662f\u200b<code>ModuleDict</code>\u200b\u80fd\u591f\u200b\u66f4\u200b\u65b9\u4fbf\u200b\u5730\u4e3a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5c42\u200b\u6dfb\u52a0\u200b\u540d\u79f0\u200b\u3002</p> <pre><code>net = nn.ModuleDict({\n    'linear': nn.Linear(784, 256),\n    'act': nn.ReLU(),\n})\nnet['output'] = nn.Linear(256, 10) # \u200b\u6dfb\u52a0\u200b\nprint(net['linear']) # \u200b\u8bbf\u95ee\u200b\nprint(net.output)\nprint(net)\n</code></pre> <pre><code>Linear(in_features=784, out_features=256, bias=True)\nLinear(in_features=256, out_features=10, bias=True)\nModuleDict(\n  (act): ReLU()\n  (linear): Linear(in_features=784, out_features=256, bias=True)\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#515","title":"5.1.5 \u200b\u4e09\u79cd\u200b\u65b9\u6cd5\u200b\u7684\u200b\u6bd4\u8f83\u200b\u4e0e\u200b\u9002\u7528\u200b\u573a\u666f","text":"<p><code>Sequential</code>\u200b\u9002\u7528\u200b\u4e8e\u200b\u5feb\u901f\u200b\u9a8c\u8bc1\u200b\u7ed3\u679c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5df2\u7ecf\u200b\u660e\u786e\u200b\u4e86\u200b\u8981\u200b\u7528\u200b\u54ea\u4e9b\u200b\u5c42\u200b\uff0c\u200b\u76f4\u63a5\u200b\u5199\u200b\u4e00\u4e0b\u200b\u5c31\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u540c\u65f6\u200b\u5199\u200b<code>__init__</code>\u200b\u548c\u200b<code>forward</code>\uff1b</p> <p>ModuleList\u200b\u548c\u200bModuleDict\u200b\u5728\u200b\u67d0\u4e2a\u200b\u5b8c\u5168\u76f8\u540c\u200b\u7684\u200b\u5c42\u200b\u9700\u8981\u200b\u91cd\u590d\u200b\u51fa\u73b0\u200b\u591a\u6b21\u200b\u65f6\u200b\uff0c\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u201d\u200b\u4e00\u884c\u200b\u9876\u591a\u200b\u884c\u200b\u201c\uff1b</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e4b\u524d\u200b\u5c42\u200b\u7684\u200b\u4fe1\u606f\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6bd4\u5982\u200b ResNets \u200b\u4e2d\u200b\u7684\u200b\u6b8b\u5dee\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5f53\u524d\u200b\u5c42\u200b\u7684\u200b\u7ed3\u679c\u200b\u9700\u8981\u200b\u548c\u200b\u4e4b\u524d\u200b\u5c42\u4e2d\u200b\u7684\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u878d\u5408\u200b\uff0c\u200b\u4e00\u822c\u200b\u4f7f\u7528\u200b ModuleList/ModuleDict \u200b\u6bd4\u8f83\u200b\u65b9\u4fbf\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.1%20PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%B9%E5%BC%8F/#_1","title":"\u672c\u200b\u8282\u200b\u53c2\u8003","text":"<p>\u30101\u3011https://zhuanlan.zhihu.com/p/64990232</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/","title":"5.2 \u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u5feb\u901f\u200b\u642d\u5efa\u200b\u590d\u6742\u200b\u7f51\u7edc","text":"<p>\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e2d\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u600e\u6837\u200b\u5b9a\u4e49\u200bPyTorch\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5176\u4e2d\u200b\u7ed9\u51fa\u200b\u7684\u200b\u793a\u4f8b\u200b\u90fd\u200b\u662f\u200b\u7528\u200b<code>torch.nn</code>\u200b\u4e2d\u200b\u7684\u200b\u5c42\u200b\u6765\u200b\u5b8c\u6210\u200b\u7684\u200b\u3002\u200b\u8fd9\u79cd\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u6613\u4e8e\u200b\u7406\u89e3\u200b\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u573a\u666f\u200b\u4e0b\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u5229\u4e8e\u200b\u4f7f\u7528\u200b\u3002\u200b\u5f53\u200b\u6a21\u578b\u200b\u7684\u200b\u6df1\u5ea6\u200b\u975e\u5e38\u200b\u5927\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>Sequential</code>\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u9700\u8981\u200b\u5411\u200b\u5176\u4e2d\u200b\u6dfb\u52a0\u200b\u51e0\u767e\u200b\u884c\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8d77\u6765\u200b\u4e0d\u200b\u751a\u200b\u65b9\u4fbf\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u5927\u90e8\u5206\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff08\u200b\u6bd4\u5982\u200bResNet\u3001DenseNet\u200b\u7b49\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u4ed4\u7ec6\u89c2\u5bdf\u200b\u5c31\u200b\u4f1a\u200b\u53d1\u73b0\u200b\uff0c\u200b\u867d\u7136\u200b\u6a21\u578b\u200b\u6709\u200b\u5f88\u591a\u200b\u5c42\u200b\uff0c \u200b\u4f46\u662f\u200b\u5176\u4e2d\u200b\u6709\u200b\u5f88\u591a\u200b\u91cd\u590d\u200b\u51fa\u73b0\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u8003\u8651\u200b\u5230\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u6709\u200b\u5176\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\uff0c\u200b\u82e5\u5e72\u5c42\u200b\u4e32\u8054\u200b\u6210\u200b\u7684\u200b\u201d\u200b\u6a21\u5757\u200b\u201c\u200b\u4e5f\u200b\u6709\u200b\u5176\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u80fd\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u91cd\u590d\u200b\u51fa\u73b0\u200b\u7684\u200b\u5c42\u200b\u5b9a\u4e49\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u201d\u200b\u6a21\u5757\u200b\u201c\uff0c\u200b\u6bcf\u6b21\u200b\u53ea\u200b\u9700\u8981\u200b\u5411\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6a21\u5757\u200b\u6765\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c06\u4f1a\u200b\u6781\u5927\u200b\u4fbf\u5229\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200bU-Net\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u5feb\u901f\u200b\u642d\u5efa\u200b\u590d\u6742\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5229\u7528\u200b\u4e0a\u200b\u4e00\u8282\u200b\u5b66\u5230\u200b\u7684\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u5c06\u200b\u7b80\u5355\u200b\u5c42\u200b\u6784\u5efa\u200b\u6210\u200b\u5177\u6709\u200b\u7279\u5b9a\u200b\u529f\u80fd\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b</li> <li>\u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u6784\u5efa\u200b\u590d\u6742\u200b\u7f51\u7edc\u200b</li> </ul>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/#521-u-net","title":"5.2.1 U-Net\u200b\u7b80\u4ecb","text":"<p>U-Net\u200b\u662f\u200b\u5206\u5272\u200b (Segmentation) \u200b\u6a21\u578b\u200b\u7684\u200b\u6770\u4f5c\u200b\uff0c\u200b\u5728\u200b\u4ee5\u200b\u533b\u5b66\u5f71\u50cf\u200b\u4e3a\u200b\u4ee3\u8868\u200b\u7684\u200b\u8bf8\u591a\u200b\u9886\u57df\u200b\u6709\u7740\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u5e94\u7528\u200b\u3002U-Net\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u901a\u8fc7\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u7ed3\u6784\u200b\u89e3\u51b3\u200b\u4e86\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u9000\u5316\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6df1\u5ea6\u200b\u80fd\u591f\u200b\u4e0d\u65ad\u200b\u6269\u5c55\u200b\u3002</p> <p></p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/#522-u-net","title":"5.2.2 U-Net\u200b\u6a21\u578b\u200b\u5757\u200b\u5206\u6790","text":"<p>\u200b\u7ed3\u5408\u200b\u4e0a\u200b\u56fe\u200b\uff0c\u200b\u4e0d\u96be\u200b\u53d1\u73b0\u200bU-Net\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u975e\u5e38\u200b\u597d\u200b\u7684\u200b\u5bf9\u79f0\u6027\u200b\u3002\u200b\u6a21\u578b\u200b\u4ece\u4e0a\u5230\u4e0b\u200b\u5206\u4e3a\u200b\u82e5\u5e72\u5c42\u200b\uff0c\u200b\u6bcf\u5c42\u200b\u7531\u200b\u5de6\u4fa7\u200b\u548c\u200b\u53f3\u4fa7\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u5757\u200b\u7ec4\u6210\u200b\uff0c\u200b\u6bcf\u4fa7\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\u4e0e\u5176\u200b\u4e0a\u4e0b\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u6709\u200b\u8fde\u63a5\u200b\uff1b\u200b\u540c\u65f6\u200b\u4f4d\u4e8e\u200b\u540c\u200b\u4e00\u5c42\u200b\u5de6\u53f3\u200b\u4e24\u4fa7\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u4e5f\u200b\u6709\u200b\u8fde\u63a5\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u201cSkip-connection\u201d\u3002\u200b\u6b64\u5916\u200b\u8fd8\u6709\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5904\u7406\u200b\u7b49\u200b\u5176\u4ed6\u200b\u7ec4\u6210\u90e8\u5206\u200b\u3002\u200b\u7531\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u5f62\u72b6\u200b\u975e\u5e38\u200b\u50cf\u200b\u82f1\u6587\u5b57\u6bcd\u200b\u7684\u200b\u201cU\u201d\uff0c\u200b\u56e0\u6b64\u200b\u88ab\u200b\u547d\u540d\u200b\u4e3a\u200b\u201cU-Net\u201d\u3002</p> <p>\u200b\u7ec4\u6210\u200bU-Net\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\u4e3b\u8981\u200b\u6709\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ul> <li> <p>\u200b\u6bcf\u4e2a\u200b\u5b50\u5757\u200b\u5185\u90e8\u200b\u7684\u200b\u4e24\u6b21\u200b\u5377\u79ef\u200b\uff08Double Convolution\uff09</p> </li> <li> <p>\u200b\u5de6\u4fa7\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e0b\u200b\u91c7\u6837\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5373\u200b\u6700\u5927\u200b\u6c60\u5316\u200b\uff08Max pooling\uff09</p> </li> <li> <p>\u200b\u53f3\u4fa7\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e0a\u200b\u91c7\u6837\u200b\u8fde\u63a5\u200b\uff08Up sampling\uff09</p> </li> <li> <p>\u200b\u8f93\u51fa\u200b\u5c42\u200b\u7684\u200b\u5904\u7406\u200b</p> </li> </ul> <p>\u200b\u9664\u200b\u6a21\u578b\u200b\u5757\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6a2a\u5411\u200b\u8fde\u63a5\u200b\uff0c\u200b\u8f93\u5165\u200b\u548c\u200bU-Net\u200b\u5e95\u90e8\u200b\u7684\u200b\u8fde\u63a5\u200b\u7b49\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5355\u72ec\u200b\u7684\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bforward\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u7528\u200bPyTorch\u200b\u5148\u200b\u5b9e\u73b0\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u5229\u7528\u200b\u5b9a\u4e49\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\u6784\u5efa\u200bU-Net\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/#523-u-net","title":"5.2.3 U-Net\u200b\u6a21\u578b\u200b\u5757\u200b\u5b9e\u73b0","text":"<p>\u200b\u5728\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5b9e\u73b0\u200bU-Net\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u5fc5\u200b\u628a\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u6309\u5e8f\u200b\u6392\u5217\u200b\u663e\u5f0f\u200b\u5199\u51fa\u200b\uff0c\u200b\u8fd9\u6837\u200b\u592a\u200b\u9ebb\u70e6\u200b\u4e14\u200b\u4e0d\u5b9c\u200b\u8bfb\u200b\uff0c\u200b\u4e00\u79cd\u200b\u6bd4\u8f83\u200b\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5148\u200b\u5b9a\u4e49\u200b\u597d\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u518d\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8fde\u63a5\u200b\u987a\u5e8f\u200b\u548c\u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\u3002\u200b\u5c31\u200b\u597d\u6bd4\u200b\u88c5\u914d\u200b\u96f6\u4ef6\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u88c5\u914d\u200b\u597d\u200b\u4e00\u4e9b\u200b\u57fa\u7840\u200b\u7684\u200b\u90e8\u4ef6\u200b\uff0c\u200b\u4e4b\u540e\u200b\u518d\u7528\u200b\u8fd9\u4e9b\u200b\u53ef\u4ee5\u200b\u590d\u7528\u200b\u7684\u200b\u90e8\u4ef6\u200b\u5f97\u5230\u200b\u6574\u4e2a\u200b\u88c5\u914d\u200b\u4f53\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u57fa\u7840\u200b\u90e8\u4ef6\u200b\u5bf9\u5e94\u200b\u4e0a\u200b\u4e00\u8282\u200b\u5206\u6790\u200b\u7684\u200b\u56db\u4e2a\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u6839\u636e\u200b\u529f\u80fd\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u547d\u540d\u200b\u4e3a\u200b\uff1a<code>DoubleConv</code>, <code>Down</code>, <code>Up</code>, <code>OutConv</code>\u3002\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200bU-Net\u200b\u4e2d\u200b\u6a21\u578b\u200b\u5757\u200b\u7684\u200bPyTorch \u200b\u5b9e\u73b0\u200b\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre> <pre><code>class DoubleConv(nn.Module):\n    \"\"\"(convolution =&gt; [BN] =&gt; ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n</code></pre> <pre><code>class Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n</code></pre> <pre><code>class Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=False):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n</code></pre> <pre><code>class OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/#524-u-net","title":"5.2.4 \u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u7ec4\u88c5\u200bU-Net","text":"<p>\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\u5730\u200b\u7ec4\u88c5\u200bU-Net\u200b\u6a21\u578b\u200b\u3002\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u901a\u8fc7\u200b\u6a21\u578b\u200b\u5757\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4ee3\u7801\u200b\u590d\u7528\u200b\uff0c\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u5b9a\u4e49\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u4ee3\u7801\u200b\u603b\u884c\u200b\u6570\u200b\u660e\u663e\u200b\u51cf\u5c11\u200b\uff0c\u200b\u4ee3\u7801\u200b\u53ef\u8bfb\u6027\u200b\u4e5f\u200b\u5f97\u5230\u200b\u4e86\u200b\u63d0\u5347\u200b\u3002</p> <pre><code>class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.2%20%E5%88%A9%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%9D%97%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C/#_1","title":"\u53c2\u8003\u8d44\u6599","text":"<ol> <li>https://github.com/milesial/Pytorch-UNet</li> </ol>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.3%20PyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/","title":"5.3 PyTorch\u200b\u4fee\u6539\u200b\u6a21\u578b","text":"<p>\u200b\u9664\u4e86\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200bPyTorch\u200b\u6a21\u578b\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u53e6\u200b\u4e00\u79cd\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e00\u4e2a\u200b\u73b0\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u8be5\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u90e8\u5206\u200b\u7ed3\u6784\u200b\u4e0d\u200b\u7b26\u5408\u200b\u6211\u4eec\u200b\u7684\u200b\u8981\u6c42\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4f7f\u7528\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u5fc5\u8981\u200b\u7684\u200b\u4fee\u6539\u200b\u3002\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\u548c\u200bPyTorch\u200b\u8d8a\u6765\u8d8a\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6709\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u5f00\u6e90\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u4f9b\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5f88\u591a\u200b\u65f6\u5019\u200b\u6211\u4eec\u200b\u4e5f\u200b\u4e0d\u5fc5\u200b\u4ece\u5934\u5f00\u59cb\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u638c\u63e1\u200b\u5982\u4f55\u200b\u4fee\u6539\u200bPyTorch\u200b\u6a21\u578b\u200b\u5c31\u200b\u663e\u5f97\u200b\u5c24\u4e3a\u91cd\u8981\u200b\u3002</p> <p>\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u5c31\u200b\u6765\u200b\u63a2\u7d22\u200b\u8fd9\u4e00\u200b\u95ee\u9898\u200b\u3002\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5982\u4f55\u200b\u5728\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff1a</li> <li>\u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u82e5\u5e72\u5c42\u200b</li> <li>\u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u5165\u200b</li> <li>\u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u51fa\u200b</li> </ul>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.3%20PyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/#531","title":"5.3.1 \u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u5c42","text":"<p>\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u4ee5\u200bPyTorch\u200b\u5b98\u65b9\u200b\u89c6\u89c9\u200b\u5e93\u200btorchvision\u200b\u9884\u5b9a\u200b\u4e49\u597d\u200b\u7684\u200b\u6a21\u578b\u200bResNet50\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u63a2\u7d22\u200b\u5982\u4f55\u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u7684\u200b\u67d0\u200b\u4e00\u5c42\u200b\u6216\u8005\u200b\u67d0\u200b\u51e0\u5c42\u200b\u3002\u200b\u6211\u4eec\u200b\u5148\u200b\u770b\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u5b9a\u4e49\u200b\u662f\u200b\u600e\u6837\u200b\u7684\u200b\uff1a</p> <pre><code># \u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u7684\u200bpackage\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\nimport torchvision.models as models\nnet = models.resnet50()\nprint(net)\n</code></pre> <pre><code>ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n..............\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u662f\u200b\u4e3a\u4e86\u200b\u9002\u914d\u200bImageNet\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6700\u540e\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff08fc\uff09\u200b\u7684\u200b\u8f93\u51fa\u200b\u8282\u70b9\u200b\u6570\u662f\u200b1000\u3002</p> <p>\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u8981\u200b\u7528\u200b\u8fd9\u4e2a\u200bresnet\u200b\u6a21\u578b\u200b\u53bb\u200b\u505a\u200b\u4e00\u4e2a\u200b10\u200b\u5206\u7c7b\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5c31\u200b\u5e94\u8be5\u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u7684\u200bfc\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b\u5176\u200b\u8f93\u51fa\u200b\u8282\u70b9\u200b\u6570\u200b\u66ff\u6362\u200b\u4e3a\u200b10\u3002\u200b\u53e6\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u89c9\u5f97\u200b\u4e00\u5c42\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u53ef\u80fd\u200b\u592a\u5c11\u200b\u4e86\u200b\uff0c\u200b\u60f3\u200b\u518d\u52a0\u200b\u4e00\u5c42\u200b\u3002\u200b\u53ef\u4ee5\u200b\u505a\u200b\u5982\u4e0b\u200b\u4fee\u6539\u200b\uff1a</p> <pre><code>classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(2048, 128)),\n                          ('relu1', nn.ReLU()), \n                          ('dropout1',nn.Dropout(0.5)),\n                          ('fc2', nn.Linear(128, 10)),\n                          ('output', nn.Softmax(dim=1))\n                          ]))\n\nnet.fc = classifier\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u64cd\u4f5c\u200b\u76f8\u5f53\u4e8e\u200b\u5c06\u200b\u6a21\u578b\u200b\uff08net\uff09\u200b\u6700\u540e\u200b\u540d\u79f0\u200b\u4e3a\u200b\u201cfc\u201d\u200b\u7684\u200b\u5c42\u66ff\u6362\u200b\u6210\u200b\u4e86\u200b\u540d\u79f0\u200b\u4e3a\u200b\u201cclassifier\u201d\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u8be5\u200b\u7ed3\u6784\u200b\u662f\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u5b9a\u4e49\u200b\u7684\u200b\u3002\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u4e86\u200b\u7b2c\u4e00\u8282\u200b\u4ecb\u7ecd\u200b\u7684\u200bSequential+OrderedDict\u200b\u7684\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u3002\u200b\u81f3\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5b8c\u6210\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u73b0\u5728\u200b\u7684\u200b\u6a21\u578b\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u53bb\u200b\u505a\u200b10\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4e86\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.3%20PyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/#532","title":"5.3.2 \u200b\u6dfb\u52a0\u200b\u5916\u90e8\u200b\u8f93\u5165","text":"<p>\u200b\u6709\u65f6\u5019\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0c\u200b\u9664\u4e86\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u200b\u4e4b\u5916\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u8f93\u5165\u200b\u989d\u5916\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u6bd4\u5982\u200b\u5728\u200bCNN\u200b\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9664\u4e86\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u540c\u65f6\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5176\u4ed6\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u65f6\u5019\u200b\u5c31\u200b\u9700\u8981\u200b\u5728\u200b\u5df2\u6709\u200b\u7684\u200bCNN\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u7684\u200b\u8f93\u5165\u200b\u53d8\u91cf\u200b\u3002\u200b\u57fa\u672c\u601d\u8def\u200b\u662f\u200b\uff1a\u200b\u5c06\u200b\u539f\u200b\u6a21\u578b\u200b\u6dfb\u52a0\u200b\u8f93\u5165\u200b\u4f4d\u7f6e\u200b\u524d\u200b\u7684\u200b\u90e8\u5206\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6574\u4f53\u200b\uff0c\u200b\u540c\u65f6\u200b\u5728\u200bforward\u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u597d\u539f\u200b\u6a21\u578b\u200b\u4e0d\u53d8\u200b\u7684\u200b\u90e8\u5206\u200b\u3001\u200b\u6dfb\u52a0\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u540e\u7eed\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8fde\u63a5\u200b\u5173\u7cfb\u200b\uff0c\u200b\u4ece\u800c\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u7684\u200b\u4fee\u6539\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4ee5\u200btorchvision\u200b\u7684\u200bresnet50\u200b\u6a21\u578b\u200b\u4e3a\u200b\u57fa\u7840\u200b\uff0c\u200b\u4efb\u52a1\u200b\u8fd8\u662f\u200b10\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u3002\u200b\u4e0d\u540c\u70b9\u200b\u5728\u4e8e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5229\u7528\u200b\u5df2\u6709\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5728\u200b\u5012\u6570\u200b\u7b2c\u4e8c\u5c42\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u8f93\u5165\u200b\u53d8\u91cf\u200badd_variable\u200b\u6765\u200b\u8f85\u52a9\u200b\u9884\u6d4b\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class Model(nn.Module):\n    def __init__(self, net):\n        super(Model, self).__init__()\n        self.net = net\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.fc_add = nn.Linear(1001, 10, bias=True)\n        self.output = nn.Softmax(dim=1)\n\n    def forward(self, x, add_variable):\n        x = self.net(x)\n        x = torch.cat((self.dropout(self.relu(x)), add_variable.unsqueeze(1)),1)\n        x = self.fc_add(x)\n        x = self.output(x)\n        return x\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u5b9e\u73b0\u200b\u8981\u70b9\u200b\u662f\u200b\u901a\u8fc7\u200btorch.cat\u200b\u5b9e\u73b0\u200b\u4e86\u200btensor\u200b\u7684\u200b\u62fc\u63a5\u200b\u3002torchvision\u200b\u4e2d\u200b\u7684\u200bresnet50\u200b\u8f93\u51fa\u200b\u662f\u200b\u4e00\u4e2a\u200b1000\u200b\u7ef4\u200b\u7684\u200btensor\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200bforward\u200b\u51fd\u6570\u200b\uff08\u200b\u914d\u5957\u200b\u5b9a\u4e49\u200b\u4e00\u4e9b\u200b\u5c42\u200b\uff09\uff0c\u200b\u5148\u200b\u5c06\u200b1000\u200b\u7ef4\u200b\u7684\u200btensor\u200b\u901a\u8fc7\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5c42\u200b\u548c\u200bdropout\u200b\u5c42\u200b\uff0c\u200b\u518d\u200b\u548c\u200b\u5916\u90e8\u200b\u8f93\u5165\u200b\u53d8\u91cf\u200b\"add_variable\"\u200b\u62fc\u63a5\u200b\uff0c\u200b\u6700\u540e\u200b\u901a\u8fc7\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u6620\u5c04\u200b\u5230\u200b\u6307\u5b9a\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b10\u3002</p> <p>\u200b\u53e6\u5916\u200b\u8fd9\u91cc\u200b\u5bf9\u5916\u90e8\u200b\u8f93\u5165\u200b\u53d8\u91cf\u200b\"add_variable\"\u200b\u8fdb\u884c\u200bunsqueeze\u200b\u64cd\u4f5c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u548c\u200bnet\u200b\u8f93\u51fa\u200b\u7684\u200btensor\u200b\u4fdd\u6301\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\uff0c\u200b\u5e38\u7528\u200b\u4e8e\u200badd_variable\u200b\u662f\u200b\u5355\u4e00\u200b\u6570\u503c\u200b (scalar) \u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6b64\u65f6\u200badd_variable\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u662f\u200b (batch_size, )\uff0c\u200b\u9700\u8981\u200b\u5728\u200b\u7b2c\u4e8c\u200b\u7ef4\u200b\u8865\u5145\u200b\u7ef4\u6570\u200b1\uff0c\u200b\u4ece\u800c\u200b\u53ef\u4ee5\u200b\u548c\u200btensor\u200b\u8fdb\u884c\u200btorch.cat\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5bf9\u4e8e\u200bunsqueeze\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u590d\u4e60\u200b\u4e0b\u200b2.1\u200b\u8282\u200b\u7684\u200b\u5185\u5bb9\u200b\u548c\u200b\u914d\u5957\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4e4b\u540e\u200b\u5bf9\u200b\u6211\u4eec\u200b\u4fee\u6539\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u5b9e\u4f8b\u200b\u5316\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e86\u200b\uff1a</p> <pre><code>net = models.resnet50()\nmodel = Model(net).cuda()\n</code></pre> <p>\u200b\u53e6\u5916\u200b\u522b\u5fd8\u4e86\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u5728\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u65f6\u5019\u200b\u8981\u200b\u7ed9\u200b\u4e24\u4e2a\u200binputs\uff1a</p> <pre><code>outputs = model(inputs, add_var)\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.3%20PyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/#533","title":"5.3.3 \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u51fa","text":"<p>\u200b\u6709\u65f6\u5019\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0c\u200b\u9664\u4e86\u200b\u6a21\u578b\u200b\u6700\u540e\u200b\u7684\u200b\u8f93\u51fa\u200b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8f93\u51fa\u200b\u6a21\u578b\u200b\u67d0\u4e00\u200b\u4e2d\u95f4\u5c42\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4ee5\u200b\u65bd\u52a0\u200b\u989d\u5916\u200b\u7684\u200b\u76d1\u7763\u200b\uff0c\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u4e2d\u95f4\u5c42\u200b\u7ed3\u679c\u200b\u3002\u200b\u57fa\u672c\u200b\u7684\u200b\u601d\u8def\u200b\u662f\u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4e2d\u200bforward\u200b\u51fd\u6570\u200b\u7684\u200breturn\u200b\u53d8\u91cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f9d\u7136\u200b\u4ee5\u200bresnet50\u200b\u505a\u200b10\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5728\u200b\u5df2\u7ecf\u200b\u5b9a\u4e49\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u4e0a\u200b\uff0c\u200b\u540c\u65f6\u200b\u8f93\u51fa\u200b1000\u200b\u7ef4\u200b\u7684\u200b\u5012\u6570\u200b\u7b2c\u4e8c\u5c42\u200b\u548c\u200b10\u200b\u7ef4\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\u7ed3\u679c\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class Model(nn.Module):\n    def __init__(self, net):\n        super(Model, self).__init__()\n        self.net = net\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(1000, 10, bias=True)\n        self.output = nn.Softmax(dim=1)\n\n    def forward(self, x, add_variable):\n        x1000 = self.net(x)\n        x10 = self.dropout(self.relu(x1000))\n        x10 = self.fc1(x10)\n        x10 = self.output(x10)\n        return x10, x1000\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\u5bf9\u200b\u6211\u4eec\u200b\u4fee\u6539\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u5b9e\u4f8b\u200b\u5316\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e86\u200b\uff1a</p> <pre><code>import torchvision.models as models\nnet = models.resnet50()\nmodel = Model(net).cuda()\n</code></pre> <p>\u200b\u53e6\u5916\u200b\u522b\u5fd8\u4e86\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u5728\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u540e\u4f1a\u200b\u6709\u200b\u4e24\u4e2a\u200boutputs\uff1a</p> <pre><code>out10, out1000 = model(inputs, add_var)\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/","title":"5.4 PyTorch\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u4e0e\u200b\u8bfb\u53d6","text":"<p>\u200b\u5728\u200b\u524d\u9762\u200b\u51e0\u8282\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u548c\u200b\u4fee\u6539\u200bPyTorch\u200b\u6a21\u578b\u200b\u3002\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u6765\u200b\u8ba8\u8bba\u200bPyTorch\u200b\u5982\u4f55\u200b\u4fdd\u5b58\u200b\u548c\u200b\u8bfb\u53d6\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u5728\u200b\u5f88\u591a\u200b\u573a\u666f\u200b\u4e0b\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u591a\u200bGPU\u200b\u8bad\u7ec3\u200b\u3002\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6a21\u578b\u200b\u4f1a\u200b\u5206\u5e03\u200b\u4e8e\u200b\u5404\u4e2a\u200bGPU\u200b\u4e0a\u200b\uff08\u200b\u53c2\u52a0\u200b2.3\u200b\u8282\u200b\u5206\u5e03\u200b\u6570\u636e\u200b\u5f0f\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6682\u200b\u4e0d\u200b\u8003\u8651\u200b\u5206\u5e03\u200b\u6a21\u578b\u200b\u5f0f\u200b\u8bad\u7ec3\u200b\uff09\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u4fdd\u5b58\u200b\u548c\u200b\u8bfb\u53d6\u200b\u4e0e\u200b\u5355\u200bGPU\u200b\u8bad\u7ec3\u200b\u60c5\u666f\u200b\u4e0b\u200b\u662f\u5426\u200b\u6709\u6240\u4e0d\u540c\u200b\uff1f</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u5b58\u50a8\u200b\u683c\u5f0f\u200b</li> <li>PyTorch\u200b\u5982\u4f55\u200b\u5b58\u50a8\u200b\u6a21\u578b\u200b</li> <li>\u200b\u5355\u5361\u200b\u4e0e\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u4e0b\u200b\u6a21\u578b\u200b\u7684\u200b\u4fdd\u5b58\u200b\u4e0e\u200b\u52a0\u8f7d\u200b\u65b9\u6cd5\u200b</li> <li>\u200b\u4f18\u5316\u200b\u5668\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u4fdd\u5b58\u200b</li> </ul>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#541","title":"5.4.1 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u683c\u5f0f","text":"<p>PyTorch\u200b\u5b58\u50a8\u200b\u6a21\u578b\u200b\u4e3b\u8981\u200b\u91c7\u7528\u200bpkl\uff0cpt\uff0cpth\u200b\u4e09\u79cd\u200b\u683c\u5f0f\u200b\u3002\u200b\u5c31\u200b\u4f7f\u7528\u200b\u5c42\u9762\u200b\u6765\u8bf4\u200b\u6ca1\u6709\u200b\u533a\u522b\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4e0d\u200b\u505a\u200b\u5177\u4f53\u200b\u7684\u200b\u8ba8\u8bba\u200b\u3002\u200b\u672c\u200b\u8282\u200b\u6700\u540e\u200b\u7684\u200b\u53c2\u8003\u200b\u5185\u5bb9\u200b\u4e2d\u200b\u5217\u51fa\u200b\u4e86\u200b\u67e5\u9605\u200b\u5230\u200b\u7684\u200b\u4e00\u4e9b\u200b\u8d44\u6599\u200b\uff0c\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u7814\u7a76\u200b\uff0c\u200b\u6b22\u8fce\u200b\u7559\u8a00\u200b\u8ba8\u8bba\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#542","title":"5.4.2 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5185\u5bb9","text":"<p>\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u6a21\u578b\u200b\u4e3b\u8981\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff1a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u548c\u200b\u6743\u91cd\u200b\u3002\u200b\u5176\u4e2d\u200b\u6a21\u578b\u200b\u662f\u200b\u7ee7\u627f\u200bnn.Module\u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u6743\u91cd\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\uff08key\u200b\u662f\u200b\u5c42\u540d\u200b\uff0cvalue\u200b\u662f\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\uff09\u3002\u200b\u5b58\u50a8\u200b\u4e5f\u200b\u7531\u6b64\u200b\u5206\u4e3a\u200b\u4e24\u79cd\u200b\u5f62\u5f0f\u200b\uff1a\u200b\u5b58\u50a8\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff08\u200b\u5305\u62ec\u200b\u7ed3\u6784\u200b\u548c\u200b\u6743\u91cd\u200b\uff09\uff0c\u200b\u548c\u200b\u53ea\u200b\u5b58\u50a8\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u3002</p> <pre><code>from torchvision import models\nmodel = models.resnet152(pretrained=True)\nsave_dir = './resnet152.pth'\n\n# \u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\ntorch.save(model, save_dir)\n# \u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\ntorch.save(model.state_dict, save_dir)\n</code></pre> <p>\u200b\u5bf9\u4e8e\u200bPyTorch\u200b\u800c\u8a00\u200b\uff0cpt, pth\u200b\u548c\u200bpkl\u200b\u4e09\u79cd\u200b\u6570\u636e\u683c\u5f0f\u200b\u5747\u200b\u652f\u6301\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u548c\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u5b58\u50a8\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f7f\u7528\u200b\u4e0a\u200b\u6ca1\u6709\u200b\u5dee\u522b\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#543","title":"5.4.3 \u200b\u5355\u5361\u200b\u548c\u200b\u591a\u5361\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u7684\u200b\u533a\u522b","text":"<p>PyTorch\u200b\u4e2d\u5c06\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u653e\u5230\u200bGPU\u200b\u4e0a\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u2014\u2014<code>.cuda()</code>\u200b\u548c\u200b<code>.to(device)</code>\uff0c\u200b\u672c\u200b\u8282\u200b\u540e\u7eed\u200b\u5185\u5bb9\u200b\u9488\u5bf9\u200b\u524d\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u8ba8\u8bba\u200b\u3002\u200b\u5982\u679c\u200b\u8981\u200b\u4f7f\u7528\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u7684\u8bdd\u200b\uff0c\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u4f7f\u7528\u200b<code>torch.nn.DataParallel</code>\u3002\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0' # \u200b\u5982\u679c\u200b\u662f\u200b\u591a\u5361\u200b\u6539\u6210\u200b\u7c7b\u4f3c\u200b0,1,2\nmodel = model.cuda()  # \u200b\u5355\u5361\u200b\nmodel = torch.nn.DataParallel(model).cuda()  # \u200b\u591a\u5361\u200b\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\u6211\u4eec\u200b\u628a\u200b<code>model</code>\u200b\u5bf9\u5e94\u200b\u7684\u200b<code>layer</code>\u200b\u540d\u79f0\u200b\u6253\u5370\u200b\u51fa\u6765\u200b\u770b\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u89c2\u5bdf\u200b\u5230\u200b\u5dee\u522b\u200b\u5728\u4e8e\u200b\u591a\u5361\u200b\u5e76\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\u6bcf\u5c42\u200b\u7684\u200b\u540d\u79f0\u200b\u524d\u591a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u201cmodule\u201d\u3002</p> <ul> <li>\u200b\u5355\u5361\u200b\u6a21\u578b\u200b\u7684\u200b\u5c42\u540d\u200b\uff1a</li> </ul> <p></p> <ul> <li>\u200b\u591a\u5361\u200b\u6a21\u578b\u200b\u7684\u200b\u5c42\u540d\u200b\uff1a</li> </ul> <p></p> <p>\u200b\u8fd9\u79cd\u200b\u6a21\u578b\u8868\u793a\u200b\u7684\u200b\u4e0d\u540c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9700\u8981\u200b\u5904\u7406\u200b\u4e00\u4e9b\u200b\u77db\u76fe\u200b\u70b9\u200b\uff0c\u200b\u4e0b\u9762\u200b\u5bf9\u200b\u5404\u79cd\u200b\u53ef\u80fd\u200b\u7684\u200b\u60c5\u51b5\u200b\u505a\u200b\u5206\u7c7b\u200b\u8ba8\u8bba\u200b\u3002</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#544","title":"5.4.4 \u200b\u5355\u5361\u200b/\u200b\u591a\u5361\u200b\u60c5\u51b5\u200b\u5206\u7c7b\u200b\u8ba8\u8bba","text":"<p>\u200b\u7531\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\u6761\u4ef6\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5728\u200b\u6a21\u578b\u200b\u7684\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u56e0\u4e3a\u200b\u5355\u200bGPU\u200b\u548c\u200b\u591a\u200bGPU\u200b\u73af\u5883\u200b\u7684\u200b\u4e0d\u540c\u200b\u5e26\u6765\u200b\u6a21\u578b\u200b\u4e0d\u200b\u5339\u914d\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002\u200b\u8fd9\u91cc\u200b\u5bf9\u200bPyTorch\u200b\u6846\u67b6\u200b\u4e0b\u200b\u5355\u5361\u200b/\u200b\u591a\u5361\u4e0b\u200b\u6a21\u578b\u200b\u7684\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u95ee\u9898\u200b\u8fdb\u884c\u200b\u6392\u5217\u7ec4\u5408\u200b\uff08=4\uff09\uff0c\u200b\u6837\u4f8b\u200b\u6a21\u578b\u200b\u662f\u200btorchvision\u200b\u4e2d\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200bresnet152\uff0c\u200b\u4e0d\u5c3d\u200b\u4e4b\u200b\u5904\u200b\u6b22\u8fce\u200b\u5927\u5bb6\u200b\u8865\u5145\u200b\u3002</p> <ul> <li>\u200b\u5355\u5361\u200b\u4fdd\u5b58\u200b+\u200b\u5355\u5361\u200b\u52a0\u8f7d\u200b</li> </ul> <p>\u200b\u5728\u200b\u4f7f\u7528\u200bos.envision\u200b\u547d\u4ee4\u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u540e\u200b\uff0c\u200b\u5373\u53ef\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u548c\u200b\u8bfb\u53d6\u200b\u64cd\u4f5c\u200b\u3002\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u5373\u4fbf\u200b\u4fdd\u5b58\u200b\u548c\u200b\u8bfb\u53d6\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u4e0d\u540c\u200b\u4e5f\u200b\u65e0\u59a8\u200b\u3002</p> <pre><code>import os\nimport torch\nfrom torchvision import models\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nmodel = models.resnet152(pretrained=True)\nmodel.cuda()\n\nsave_dir = 'resnet152.pt'   #\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\ntorch.save(model, save_dir)\nloaded_model = torch.load(save_dir)\nloaded_model.cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\ntorch.save(model.state_dict(), save_dir)\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.load_state_dict(torch.load(save_dir))\nloaded_model.cuda()\n</code></pre> <ul> <li>\u200b\u5355\u5361\u200b\u4fdd\u5b58\u200b+\u200b\u591a\u5361\u200b\u52a0\u8f7d\u200b</li> </ul> <p>\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u7684\u200b\u5904\u7406\u200b\u6bd4\u8f83\u7b80\u5355\u200b\uff0c\u200b\u8bfb\u53d6\u200b\u5355\u5361\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>nn.DataParallel</code>\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\u5373\u53ef\u200b\uff08\u200b\u76f8\u5f53\u4e8e\u200b3.1\u200b\u4ee3\u7801\u200b\u4e2d\u200b.cuda()\u200b\u66ff\u6362\u200b\u4e00\u4e0b\u200b\uff09\uff1a</p> <pre><code>import os\nimport torch\nfrom torchvision import models\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nmodel = models.resnet152(pretrained=True)\nmodel.cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\ntorch.save(model, save_dir)\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nloaded_model = torch.load(save_dir)\nloaded_model = nn.DataParallel(loaded_model).cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\ntorch.save(model.state_dict(), save_dir)\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.load_state_dict(torch.load(save_dir))\nloaded_model = nn.DataParallel(loaded_model).cuda()\n</code></pre> <ul> <li>\u200b\u591a\u5361\u200b\u4fdd\u5b58\u200b+\u200b\u5355\u5361\u200b\u52a0\u8f7d\u200b</li> </ul> <p>\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u7684\u200b\u6838\u5fc3\u200b\u95ee\u9898\u200b\u662f\u200b\uff1a\u200b\u5982\u4f55\u200b\u53bb\u6389\u200b\u6743\u91cd\u200b\u5b57\u5178\u200b\u952e\u540d\u200b\u4e2d\u200b\u7684\u200b\"module\"\uff0c\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u6a21\u578b\u200b\u7684\u200b\u7edf\u4e00\u6027\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u52a0\u8f7d\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u76f4\u63a5\u200b\u63d0\u53d6\u200b\u6a21\u578b\u200b\u7684\u200bmodule\u200b\u5c5e\u6027\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>import os\nimport torch\nfrom torchvision import models\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\n\nmodel = models.resnet152(pretrained=True)\nmodel = nn.DataParallel(model).cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\ntorch.save(model, save_dir)\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nloaded_model = torch.load(save_dir).module\n</code></pre> <p>\u200b\u5bf9\u4e8e\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0c\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\u601d\u8def\u200b\uff1a \u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u65f6\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200bmodule\u200b\u5c5e\u6027\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6743\u91cd\u200b <pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nimport torch\nfrom torchvision import models\n\nsave_dir = 'resnet152.pth'   #\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\nmodel = models.resnet152(pretrained=True)\nmodel = nn.DataParallel(model).cuda()\n\n# \u200b\u4fdd\u5b58\u200b\u6743\u91cd\u200b\ntorch.save(model.module.state_dict(), save_dir)\n</code></pre> \u200b\u8fd9\u6837\u200b\u4fdd\u5b58\u200b\u4e0b\u6765\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5c31\u200b\u548c\u200b\u5355\u5361\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4e00\u6837\u200b\u4e86\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u52a0\u8f7d\u200b\u3002\u200b\u4e5f\u200b\u662f\u200b\u6bd4\u8f83\u200b\u63a8\u8350\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u3002 \u200b\u53bb\u9664\u200b\u5b57\u5178\u200b\u91cc\u200b\u7684\u200bmodule\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u5f80\u200bmodel\u200b\u91cc\u200b\u6dfb\u52a0\u200bmodule\u200b\u7b80\u5355\u200b</p> <pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nimport torch\nfrom torchvision import models\n\nmodel = models.resnet152(pretrained=True)\nmodel = nn.DataParallel(model).cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\ntorch.save(model.state_dict(), save_dir)\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.load_state_dict(torch.load(save_dir))\nloaded_model = nn.DataParallel(loaded_model).cuda()\nloaded_model.state_dict = loaded_dict\n</code></pre> <p>\u200b\u8fd9\u6837\u200b\u5373\u4fbf\u200b\u662f\u200b\u5355\u5361\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e86\u200b\uff08\u200b\u76f8\u5f53\u4e8e\u200b\u5206\u5e03\u200b\u5230\u200b\u5355\u5361\u200b\u4e0a\u200b\uff09</p> <p>\u200b\u904d\u5386\u200b\u5b57\u5178\u200b\u53bb\u9664\u200bmodule</p> <pre><code>from collections import OrderedDict\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\n\nloaded_dict = torch.load(save_dir)\n\nnew_state_dict = OrderedDict()\nfor k, v in loaded_dict.items():\n    name = k[7:] # module\u200b\u5b57\u6bb5\u200b\u5728\u200b\u6700\u200b\u524d\u9762\u200b\uff0c\u200b\u4ece\u200b\u7b2c\u200b7\u200b\u4e2a\u5b57\u7b26\u200b\u5f00\u59cb\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u53bb\u6389\u200bmodule\n    new_state_dict[name] = v #\u200b\u65b0\u200b\u5b57\u5178\u200b\u7684\u200bkey\u200b\u503c\u200b\u5bf9\u5e94\u200b\u7684\u200bvalue\u200b\u4e00\u4e00\u5bf9\u5e94\u200b\n\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.state_dict = new_state_dict\nloaded_model = loaded_model.cuda()\n</code></pre> <p>\u200b\u4f7f\u7528\u200breplace\u200b\u64cd\u4f5c\u200b\u53bb\u9664\u200bmodule</p> <pre><code>loaded_model = models.resnet152()    \nloaded_dict = torch.load(save_dir)\nloaded_model.load_state_dict({k.replace('module.', ''): v for k, v in loaded_dict.items()})\n</code></pre> <ul> <li>\u200b\u591a\u5361\u200b\u4fdd\u5b58\u200b+\u200b\u591a\u5361\u200b\u52a0\u8f7d\u200b</li> </ul> <p>\u200b\u7531\u4e8e\u200b\u662f\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u90fd\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u591a\u5361\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e0d\u200b\u5b58\u5728\u200b\u6a21\u578b\u200b\u5c42\u540d\u200b\u524d\u7f00\u200b\u4e0d\u540c\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u4f46\u591a\u5361\u200b\u72b6\u6001\u200b\u4e0b\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200bdevice\uff08\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\uff09\u200b\u5339\u914d\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5373\u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u65f6\u4f1a\u200b\u540c\u65f6\u200b\u4fdd\u5b58\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200bGPU id\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bfb\u53d6\u200b\u65f6\u82e5\u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u548c\u200b\u5f53\u524d\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u4fe1\u606f\u200b\u4e0d\u7b26\u200b\u5219\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u62a5\u9519\u200b\u6216\u8005\u200b\u7a0b\u5e8f\u200b\u4e0d\u200b\u6309\u200b\u9884\u5b9a\u200b\u72b6\u6001\u200b\u8fd0\u884c\u200b\u3002\u200b\u5177\u4f53\u8868\u73b0\u200b\u4e3a\u200b\u4ee5\u4e0b\u200b\u4e24\u70b9\u200b\uff1a</p> <p>\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u518d\u200b\u4f7f\u7528\u200bnn.DataParallel\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b</p> <p>\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9020\u6210\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u4e2d\u200bGPU id\u200b\u548c\u200b\u8bfb\u53d6\u200b\u73af\u5883\u200b\u4e0b\u200b\u8bbe\u7f6e\u200b\u7684\u200bGPU id\u200b\u4e0d\u7b26\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u6570\u636e\u200b\u6240\u5728\u200bdevice\u200b\u548c\u200b\u6a21\u578b\u200b\u6240\u5728\u200bdevice\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u800c\u200b\u62a5\u9519\u200b\u3002</p> <p>\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u800c\u200b\u4e0d\u200b\u4f7f\u7528\u200bnn.DataParallel\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b</p> <p>\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u53ef\u80fd\u200b\u4e0d\u4f1a\u200b\u62a5\u9519\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u7a0b\u5e8f\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u7684\u200b\u524d\u200bn\u200b\u4e2a\u200bGPU\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff08n\u200b\u662f\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u4e2a\u6570\u200b\uff09\u3002\u200b\u6b64\u65f6\u200b\u5982\u679c\u200b\u6307\u5b9a\u200b\u7684\u200bGPU\u200b\u4e2a\u6570\u200b\u5c11\u4e8e\u200bn\uff0c\u200b\u5219\u200b\u4f1a\u200b\u62a5\u9519\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ea\u6709\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u65f6\u200b\u73af\u5883\u200b\u7684\u200bdevice id\u200b\u548c\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u65f6\u200b\u73af\u5883\u200b\u7684\u200bdevice id\u200b\u4e00\u81f4\u200b\uff0c\u200b\u7a0b\u5e8f\u200b\u624d\u200b\u4f1a\u200b\u6309\u7167\u200b\u9884\u671f\u200b\u5728\u200b\u6307\u5b9a\u200b\u7684\u200bGPU\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u76f8\u6bd4\u4e4b\u4e0b\u200b\uff0c\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0c\u200b\u4e4b\u540e\u200b\u518d\u200b\u4f7f\u7528\u200bnn.DataParallel\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\u5219\u200b\u6ca1\u6709\u200b\u95ee\u9898\u200b\u3002\u200b\u56e0\u6b64\u200b\u591a\u5361\u200b\u6a21\u5f0f\u200b\u4e0b\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u6743\u200b\u91cd\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b58\u50a8\u200b\u548c\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>import os\nimport torch\nfrom torchvision import models\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #\u200b\u8fd9\u91cc\u200b\u66ff\u6362\u6210\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u7f16\u53f7\u200b\n\nmodel = models.resnet152(pretrained=True)\nmodel = nn.DataParallel(model).cuda()\n\n# \u200b\u4fdd\u5b58\u200b+\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0c\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\uff01\uff01\ntorch.save(model.state_dict(), save_dir)\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.load_state_dict(torch.load(save_dir)))\nloaded_model = nn.DataParallel(loaded_model).cuda()\n</code></pre> <p>\u200b\u5982\u679c\u200b\u53ea\u6709\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u63d0\u53d6\u200b\u6743\u91cd\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6784\u5efa\u200b\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code># \u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\nloaded_whole_model = torch.load(save_dir)\nloaded_model = models.resnet152()   #\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u200b\u5b9a\u4e49\u200b\nloaded_model.state_dict = loaded_whole_model.state_dict\nloaded_model = nn.DataParallel(loaded_model).cuda()\n</code></pre> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u4e0a\u9762\u200b\u6240\u6709\u200b\u5bf9\u4e8e\u200bloaded_model\u200b\u4fee\u6539\u200b\u6743\u91cd\u200b\u5b57\u5178\u200b\u7684\u200b\u5f62\u5f0f\u200b\u90fd\u200b\u662f\u200b\u901a\u8fc7\u200b\u8d4b\u503c\u200b\u6765\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff0c\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\"load_state_dict\"\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002\u200b\u56e0\u6b64\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4e24\u79cd\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b\u3002</p> <pre><code>loaded_model.load_state_dict(loaded_dict)\n</code></pre>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#545","title":"5.4.5 \u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u7684\u200b\u4fdd\u5b58\u200b\u548c\u200b\u8bfb\u53d6","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u91cc\u200b\uff0c\u200b\u6709\u65f6\u5019\u200b\u6211\u4eec\u200b\u4e0d\u4ec5\u4ec5\u200b\u9700\u8981\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u4fdd\u5b58\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u6bd4\u5982\u200b\u8bad\u7ec3\u200b\u7684\u200bepoch\u200b\u6570\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7684\u200bloss\uff0c\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u7b56\u7565\u200b\u7684\u200b\u53c2\u6570\u200b\u7b49\u7b49\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5b57\u5178\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4fdd\u5b58\u200b\u5728\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u200b\u91cc\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u65f6\u200b\u4e00\u8d77\u200b\u8bfb\u53d6\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ee5\u4e0b\u200b\u65b9\u200b\u4ee3\u7801\u200b\u4e3a\u4f8b\u200b\uff1a <pre><code>torch.save({\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'lr_scheduler': lr_scheduler.state_dict(),\n        'epoch': epoch,\n        'args': args,\n    }, checkpoint_path)\n</code></pre> \u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u7684\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e5f\u200b\u662f\u200b\u7c7b\u4f3c\u200b\u7684\u200b\uff1a <pre><code>checkpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint['model'])\noptimizer.load_state_dict(checkpoint['optimizer'])\nlr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\nepoch = checkpoint['epoch']\nargs = checkpoint['args']\n</code></pre></p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#_1","title":"\u9644\u200b\uff1a\u200b\u6d4b\u8bd5\u73af\u5883","text":"<p>OS: Ubuntu 20.04 LTS GPU: GeForce RTX 2080 Ti (x3)</p>"},{"location":"05-%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/5.4%20PyTorh%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/#_2","title":"\u53c2\u8003\u8d44\u6599","text":"<p>\u200b\u672c\u7ae0\u200b\u5185\u5bb9\u200b\u540c\u65f6\u200b\u53d1\u5e03\u200b\u4e8e\u200b\u77e5\u4e4e\u200b\u548c\u200bCSDN</p> <ol> <li>pytorch \u200b\u4e2d\u200bpkl\u200b\u548c\u200bpth\u200b\u7684\u200b\u533a\u522b\u200b\uff1f </li> <li>What is the difference between .pt, .pth and .pwf extentions in PyTorch?</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.1%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"6.1 \u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p>PyTorch\u200b\u5728\u200btorch.nn\u200b\u6a21\u5757\u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5e38\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u6bd4\u5982\u200b\uff1aMSELoss\uff0cL1Loss\uff0cBCELoss...... \u200b\u4f46\u662f\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u51fa\u73b0\u200b\u4e86\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u975e\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200bLoss\uff0c\u200b\u6bd4\u5982\u200bDiceLoss\uff0cHuberLoss\uff0cSobolevLoss...... \u200b\u8fd9\u4e9b\u200bLoss Function\u200b\u4e13\u95e8\u200b\u9488\u5bf9\u200b\u4e00\u4e9b\u200b\u975e\u200b\u901a\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\uff0cPyTorch\u200b\u4e0d\u80fd\u200b\u5c06\u200b\u4ed6\u4eec\u200b\u5168\u90e8\u200b\u6dfb\u52a0\u200b\u5230\u5e93\u200b\u4e2d\u200b\u53bb\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5219\u200b\u9700\u8981\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002\u200b\u53e6\u5916\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e9b\u200b\u7b97\u6cd5\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0c\u200b\u7814\u7a76\u8005\u200b\u5f80\u5f80\u200b\u4f1a\u200b\u63d0\u51fa\u200b\u5168\u65b0\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6765\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7684\u200b\u8868\u73b0\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u6211\u4eec\u200b\u65e2\u200b\u65e0\u6cd5\u200b\u4f7f\u7528\u200bPyTorch\u200b\u81ea\u5e26\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u4e5f\u200b\u6ca1\u6709\u200b\u76f8\u5173\u200b\u7684\u200b\u535a\u5ba2\u200b\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u6b64\u65f6\u200b\u81ea\u5df1\u200b\u5b9e\u73b0\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5c31\u200b\u663e\u5f97\u200b\u66f4\u4e3a\u91cd\u8981\u200b\u4e86\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u638c\u63e1\u200b\u5982\u4f55\u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b</li> </ul>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.1%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#611","title":"6.1.1 \u200b\u4ee5\u200b\u51fd\u6570\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49","text":"<p>\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4ec5\u4ec5\u200b\u662f\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u800c\u5df2\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u4ee5\u200b\u51fd\u6570\u200b\u5b9a\u4e49\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u81ea\u5df1\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>def my_loss(output, target):\n    loss = torch.mean((output - target)**2)\n    return loss\n</code></pre>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.1%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#612","title":"6.1.2 \u200b\u4ee5\u7c7b\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49","text":"<p>\u200b\u867d\u7136\u200b\u4ee5\u200b\u51fd\u6570\u200b\u5b9a\u4e49\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u4f46\u662f\u200b\u4ee5\u7c7b\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200b\u66f4\u52a0\u200b\u5e38\u7528\u200b\uff0c\u200b\u5728\u200b\u4ee5\u7c7b\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u679c\u200b\u770b\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u7ee7\u627f\u200b\u5173\u7cfb\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b<code>Loss</code>\u200b\u51fd\u6570\u200b\u90e8\u5206\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>_loss</code>, \u200b\u90e8\u5206\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>_WeightedLoss</code>, \u200b\u800c\u200b<code>_WeightedLoss</code>\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>_loss</code>\uff0c<code>_loss</code>\u200b\u7ee7\u627f\u200b\u81ea\u200b nn.Module\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u5f53\u4f5c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u5c42\u200b\u6765\u200b\u5bf9\u5f85\u200b\uff0c\u200b\u540c\u6837\u200b\u5730\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7c7b\u200b\u5c31\u200b\u9700\u8981\u200b\u7ee7\u627f\u200b\u81ea\u200bnn.Module\u200b\u7c7b\u200b\uff0c\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\u6211\u4eec\u200b\u4ee5\u200bDiceLoss\u200b\u4e3a\u4f8b\u200b\u5411\u200b\u5927\u5bb6\u200b\u8bb2\u8ff0\u200b\u3002</p> <p>Dice Loss\u200b\u662f\u200b\u4e00\u79cd\u200b\u5728\u200b\u5206\u5272\u200b\u9886\u57df\u200b\u5e38\u89c1\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b9a\u4e49\u200b\u5982\u4e0b\u200b\uff1a</p> <p>$$ DSC = \\frac{2|X\u2229Y|}{|X|+|Y|} $$ \u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class DiceLoss(nn.Module):\n    def __init__(self,weight=None,size_average=True):\n        super(DiceLoss,self).__init__()\n\n    def forward(self,inputs,targets,smooth=1):\n        inputs = F.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (inputs * targets).sum()                   \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        return 1 - dice\n\n# \u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b    \ncriterion = DiceLoss()\nloss = criterion(input,targets)\n</code></pre> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8fd8\u6709\u200bBCE-Dice Loss\uff0cJaccard/Intersection over Union (IoU) Loss\uff0cFocal Loss......</p> <p><pre><code>class DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        inputs = F.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (inputs * targets).sum()                     \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n\n        return Dice_BCE\n</code></pre> <pre><code>class IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        inputs = F.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n\n        IoU = (intersection + smooth)/(union + smooth)\n\n        return 1 - IoU\n</code></pre> <pre><code>ALPHA = 0.8\nGAMMA = 2\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        inputs = F.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n\n        return focal_loss\n# \u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u94fe\u63a5\u200b1\n</code></pre></p> <p>\u200b\u6ce8\u200b\uff1a</p> <p>\u200b\u5728\u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u65f6\u200b\uff0c\u200b\u6d89\u53ca\u200b\u5230\u200b\u6570\u5b66\u200b\u8fd0\u7b97\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u6700\u597d\u200b\u5168\u7a0b\u200b\u4f7f\u7528\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5f20\u91cf\u200b\u8ba1\u7b97\u200b\u63a5\u53e3\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4e0d\u200b\u9700\u8981\u200b\u6211\u4eec\u200b\u5b9e\u73b0\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u529f\u80fd\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8c03\u7528\u200bcuda\uff0c\u200b\u4f7f\u7528\u200bnumpy\u200b\u6216\u8005\u200bscipy\u200b\u7684\u200b\u6570\u5b66\u200b\u8fd0\u7b97\u200b\u65f6\u200b\uff0c\u200b\u64cd\u4f5c\u200b\u4f1a\u200b\u6709\u4e9b\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u4e0b\u53bb\u200b\u8fdb\u884c\u200b\u63a2\u7d22\u200b\u3002\u200b\u5173\u4e8e\u200bPyTorch\u200b\u4f7f\u7528\u200bClass\u200b\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u539f\u56e0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPyTorch\u200b\u7684\u200b\u8ba8\u8bba\u533a\u200b\uff08\u200b\u94fe\u63a5\u200b6\uff09</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.1%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/#_1","title":"\u672c\u200b\u8282\u200b\u53c2\u8003","text":"<p>\u30101\u3011https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook \u30102\u3011https://www.zhihu.com/question/66988664/answer/247952270 \u30103\u3011https://blog.csdn.net/dss_dssssd/article/details/84103834 \u30104\u3011https://zj-image-processing.readthedocs.io/zh_CN/latest/pytorch/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/ \u30105\u3011https://blog.csdn.net/qq_27825451/article/details/95165265 \u30106\u3011https://discuss.pytorch.org/t/should-i-define-my-custom-loss-function-as-a-class/89468  </p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.2%20%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87/","title":"6.2 \u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387","text":"<p>\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u9009\u62e9\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u4e00\u4e2a\u200b\u56f0\u6270\u200b\u4eba\u4eec\u200b\u8bb8\u4e45\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5b66\u4e60\u200b\u901f\u7387\u200b\u8bbe\u7f6e\u200b\u8fc7\u200b\u5c0f\u200b\uff0c\u200b\u4f1a\u200b\u6781\u5927\u200b\u964d\u4f4e\u200b\u6536\u655b\u200b\u901f\u5ea6\u200b\uff0c\u200b\u589e\u52a0\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff1b\u200b\u5b66\u4e60\u200b\u7387\u592a\u5927\u200b\uff0c\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u53c2\u6570\u200b\u5728\u200b\u6700\u4f18\u200b\u89e3\u200b\u4e24\u4fa7\u200b\u6765\u56de\u200b\u632f\u8361\u200b\u3002\u200b\u4f46\u662f\u200b\u5f53\u200b\u6211\u4eec\u200b\u9009\u5b9a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5408\u9002\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u540e\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u8bb8\u591a\u200b\u8f6e\u200b\u7684\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u51c6\u786e\u7387\u200b\u9707\u8361\u200b\u6216\u200bloss\u200b\u4e0d\u518d\u200b\u4e0b\u964d\u200b\u7b49\u200b\u60c5\u51b5\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5f53\u524d\u200b\u5b66\u4e60\u200b\u7387\u200b\u5df2\u200b\u4e0d\u80fd\u200b\u6ee1\u8db3\u200b\u6a21\u578b\u200b\u8c03\u4f18\u200b\u7684\u200b\u9700\u6c42\u200b\u3002\u200b\u6b64\u65f6\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u9002\u5f53\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u7b56\u7565\u200b\u6765\u200b\u6539\u5584\u200b\u8fd9\u79cd\u200b\u73b0\u8c61\u200b\uff0c\u200b\u63d0\u9ad8\u200b\u6211\u4eec\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u8fd9\u79cd\u200b\u8bbe\u7f6e\u200b\u65b9\u5f0f\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u88ab\u200b\u79f0\u4e3a\u200bscheduler\uff0c\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u672c\u8282\u200b\u6240\u200b\u7814\u7a76\u200b\u7684\u200b\u5bf9\u8c61\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5982\u4f55\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u9009\u53d6\u200b\u5df2\u6709\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u6574\u200b\u7b56\u7565\u200b</li> <li>\u200b\u5982\u4f55\u200b\u81ea\u5b9a\u4e49\u200b\u8bbe\u7f6e\u200b\u5b66\u4e60\u200b\u8c03\u6574\u200b\u7b56\u7565\u200b\u5e76\u200b\u5b9e\u73b0\u200b</li> </ul>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.2%20%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87/#621-scheduler","title":"6.2.1 \u200b\u4f7f\u7528\u200b\u5b98\u65b9\u200bscheduler","text":"<ul> <li>\u200b\u4e86\u89e3\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200bAPI</li> </ul> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u662f\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u5f53\u524d\u200b\u8f83\u4e3a\u200b\u6d41\u884c\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\uff0cPyTorch\u200b\u5df2\u7ecf\u200b\u5728\u200b<code>torch.optim.lr_scheduler</code>\u200b\u4e3a\u200b\u6211\u4eec\u200b\u5c01\u88c5\u200b\u597d\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4f9b\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5982\u200b\u4e0b\u9762\u200b\u5217\u51fa\u200b\u7684\u200b\u8fd9\u4e9b\u200bscheduler\u3002</p> <ul> <li><code>lr_scheduler.LambdaLR</code></li> <li><code>lr_scheduler.MultiplicativeLR</code></li> <li><code>lr_scheduler.StepLR</code></li> <li><code>lr_scheduler.MultiStepLR</code></li> <li><code>lr_scheduler.ExponentialLR</code></li> <li><code>lr_scheduler.CosineAnnealingLR</code></li> <li><code>lr_scheduler.ReduceLROnPlateau</code></li> <li><code>lr_scheduler.CyclicLR</code></li> <li><code>lr_scheduler.OneCycleLR</code></li> <li><code>lr_scheduler.CosineAnnealingWarmRestarts</code></li> <li><code>lr_scheduler.ConstantLR</code></li> <li><code>lr_scheduler.LinearLR</code></li> <li><code>lr_scheduler.PolynomialLR</code></li> <li><code>lr_scheduler.ChainedScheduler</code></li> <li><code>lr_scheduler.SequentialLR</code></li> </ul> <p>\u200b\u8fd9\u4e9b\u200bscheduler\u200b\u90fd\u200b\u662f\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>_LRScheduler</code>\u200b\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>help(torch.optim.lr_scheduler)</code>\u200b\u6765\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u7c7b\u200b\u7684\u200b\u5177\u4f53\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>help(torch.optim.lr_scheduler._LRScheduler)</code>\u200b\u6765\u200b\u67e5\u770b\u200b<code>_LRScheduler</code>\u200b\u7c7b\u200b\u7684\u200b\u5177\u4f53\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u3002 - \u200b\u4f7f\u7528\u200b\u5b98\u65b9\u200bAPI</p> <p>\u200b\u5173\u4e8e\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u7b56\u7565\u200b\uff0c<code>PyTorch</code>\u200b\u5b98\u65b9\u200b\u4e5f\u200b\u5f88\u200b\u4eba\u6027\u5316\u200b\u7684\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u4f7f\u7528\u200b\u5b9e\u4f8b\u200b\u4ee3\u7801\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5c06\u200b\u7ed3\u5408\u200b\u5b98\u65b9\u200b\u7ed9\u51fa\u200b\u7684\u200b\u4ee3\u7801\u200b\u6765\u200b\u8fdb\u884c\u200b\u89e3\u91ca\u200b\u3002</p> <pre><code># \u200b\u9009\u62e9\u200b\u4e00\u79cd\u200b\u4f18\u5316\u200b\u5668\u200b\noptimizer = torch.optim.Adam(...) \n# \u200b\u9009\u62e9\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u4e00\u79cd\u200b\u6216\u200b\u591a\u79cd\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u65b9\u6cd5\u200b\nscheduler1 = torch.optim.lr_scheduler.... \nscheduler2 = torch.optim.lr_scheduler....\n...\nschedulern = torch.optim.lr_scheduler....\n# \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\nfor epoch in range(100):\n    train(...)\n    validate(...)\n    optimizer.step()\n    # \u200b\u9700\u8981\u200b\u5728\u200b\u4f18\u5316\u200b\u5668\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u4e4b\u540e\u200b\u518d\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\n# scheduler\u200b\u7684\u200b\u4f18\u5316\u200b\u662f\u200b\u5728\u200b\u6bcf\u200b\u4e00\u8f6e\u200b\u540e\u9762\u200b\u8fdb\u884c\u200b\u7684\u200b\nscheduler1.step() \n...\nschedulern.step()\n</code></pre> <p>\u200b\u6ce8\u200b\uff1a</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u4f7f\u7528\u200b\u5b98\u65b9\u200b\u7ed9\u51fa\u200b\u7684\u200b<code>torch.optim.lr_scheduler</code>\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b<code>scheduler.step()</code>\u200b\u653e\u5728\u200b<code>optimizer.step()</code>\u200b\u540e\u9762\u200b\u8fdb\u884c\u200b\u4f7f\u7528\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.2%20%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87/#622-scheduler","title":"6.2.2 \u200b\u81ea\u5b9a\u4e49\u200bscheduler","text":"<p>\u200b\u867d\u7136\u200bPyTorch\u200b\u5b98\u65b9\u200b\u7ed9\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u7684\u200bAPI\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u4e5f\u200b\u6709\u200b\u53ef\u80fd\u200b\u78b0\u5230\u200b\u9700\u8981\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u5b9a\u4e49\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u6574\u200b\u7b56\u7565\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u81ea\u5b9a\u4e49\u200b\u51fd\u6570\u200b<code>adjust_learning_rate</code>\u200b\u6765\u200b\u6539\u53d8\u200b<code>param_group</code>\u200b\u4e2d\u200b<code>lr</code>\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u53d9\u8ff0\u200b\u4e2d\u200b\u4f1a\u200b\u7ed9\u51fa\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u6b63\u5728\u200b\u505a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\u7387\u200b\u6bcf\u200b30\u200b\u8f6e\u200b\u4e0b\u964d\u200b\u4e3a\u200b\u539f\u6765\u200b\u7684\u200b1/10\uff0c\u200b\u5047\u8bbe\u200b\u5df2\u6709\u200b\u7684\u200b\u5b98\u65b9\u200bAPI\u200b\u4e2d\u200b\u6ca1\u6709\u200b\u7b26\u5408\u200b\u6211\u4eec\u200b\u9700\u6c42\u200b\u7684\u200b\uff0c\u200b\u90a3\u200b\u5c31\u200b\u9700\u8981\u200b\u81ea\u5b9a\u4e49\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u6539\u53d8\u200b\u3002</p> <pre><code>def adjust_learning_rate(optimizer, epoch):\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n</code></pre> <p>\u200b\u6709\u200b\u4e86\u200b<code>adjust_learning_rate</code>\u200b\u51fd\u6570\u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u52a8\u6001\u53d8\u5316\u200b</p> <pre><code>def adjust_learning_rate(optimizer,...):\n    ...\noptimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = 0.9)\nfor epoch in range(10):\n    train(...)\n    validate(...)\n    adjust_learning_rate(optimizer,epoch)\n</code></pre>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.2%20%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87/#_1","title":"\u672c\u200b\u8282\u200b\u53c2\u8003","text":"<ol> <li>PyTorch\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b </li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/","title":"6.3 \u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b - timm","text":"<p>\u200b\u9664\u4e86\u200b\u4f7f\u7528\u200b<code>torchvision.models</code>\u200b\u8fdb\u884c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u4ee5\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u5e93\u200b\uff0c\u200b\u53eb\u505a\u200b<code>timm</code>\uff0c\u200b\u8fd9\u4e2a\u200b\u5e93\u662f\u200b\u7531\u200bRoss Wightman\u200b\u521b\u5efa\u200b\u7684\u200b\u3002\u200b\u91cc\u9762\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200bSOTA\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f53\u4f5c\u200b\u662f\u200btorchvision\u200b\u7684\u200b\u6269\u5145\u200b\u7248\u672c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u91cc\u9762\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u51c6\u786e\u5ea6\u200b\u4e0a\u200b\u4e5f\u200b\u8f83\u200b\u9ad8\u200b\u3002\u200b\u5728\u200b\u672c\u7ae0\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u662f\u200b\u9488\u5bf9\u200b\u8fd9\u4e2a\u200b\u5e93\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u4f7f\u7528\u200b\u505a\u200b\u53d9\u8ff0\u200b\uff0c\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u5185\u5bb9\u200b\uff08\u200b\u6570\u636e\u200b\u6269\u589e\u200b\uff0c\u200b\u4f18\u5316\u200b\u5668\u200b\u7b49\u200b\uff09\u200b\u5982\u679c\u200b\u5927\u5bb6\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4ee5\u4e0b\u200b\u4e24\u4e2a\u200b\u94fe\u63a5\u200b\u3002 - Github\u200b\u94fe\u63a5\u200b\uff1ahttps://github.com/rwightman/pytorch-image-models - \u200b\u5b98\u7f51\u200b\u94fe\u63a5\u200b\uff1ahttps://fastai.github.io/timmdocs/                     https://rwightman.github.io/pytorch-image-models/</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/#631-timm","title":"6.3.1 timm\u200b\u7684\u200b\u5b89\u88c5","text":"<p>\u200b\u5173\u4e8e\u200btimm\u200b\u7684\u200b\u5b89\u88c5\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4ee5\u4e0b\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\uff1a 1. \u200b\u901a\u8fc7\u200bpip\u200b\u5b89\u88c5\u200b <pre><code>pip install timm\n</code></pre> 2. \u200b\u901a\u8fc7\u200b\u6e90\u7801\u200b\u7f16\u8bd1\u200b\u5b89\u88c5\u200b <pre><code>git clone https://github.com/rwightman/pytorch-image-models\ncd pytorch-image-models &amp;&amp; pip install -e .\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/#632","title":"6.3.2 \u200b\u5982\u4f55\u200b\u67e5\u770b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u79cd\u7c7b","text":"<ol> <li>\u200b\u67e5\u770b\u200btimm\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u622a\u6b62\u200b\u5230\u200b2022.3.27\u200b\u65e5\u200b\u4e3a\u6b62\u200b\uff0ctimm\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u8fbe\u5230\u200b\u4e86\u200b592\u200b\u4e2a\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>timm.list_models()</code>\u200b\u65b9\u6cd5\u200b\u67e5\u770b\u200btimm\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff08\u200b\u6ce8\u200b\uff1a\u200b\u672c\u7ae0\u200b\u6d4b\u8bd5\u4ee3\u7801\u200b\u5747\u200b\u662f\u200b\u5728\u200bjupyter notebook\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff09 <pre><code>import timm\navail_pretrained_models = timm.list_models(pretrained=True)\nlen(avail_pretrained_models)\n</code></pre></li> </ol> <p><pre><code>592\n</code></pre> 2. \u200b\u67e5\u770b\u200b\u7279\u5b9a\u200b\u6a21\u578b\u200b\u7684\u200b\u6240\u6709\u200b\u79cd\u7c7b\u200b \u200b\u6bcf\u200b\u4e00\u79cd\u200b\u7cfb\u5217\u200b\u53ef\u80fd\u200b\u5bf9\u5e94\u200b\u7740\u200b\u4e0d\u540c\u200b\u65b9\u6848\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6bd4\u5982\u200bResnet\u200b\u7cfb\u5217\u200b\u5c31\u200b\u5305\u62ec\u200b\u4e86\u200bResNet18\uff0c50\uff0c101\u200b\u7b49\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b<code>timm.list_models()</code>\u200b\u4f20\u5165\u200b\u60f3\u200b\u67e5\u8be2\u200b\u7684\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b\uff08\u200b\u6a21\u7cca\u200b\u67e5\u8be2\u200b\uff09\uff0c\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u60f3\u200b\u67e5\u8be2\u200bdensenet\u200b\u7cfb\u5217\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u3002 <pre><code>all_densnet_models = timm.list_models(\"*densenet*\")\nall_densnet_models\n</code></pre> \u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u4ee5\u200b\u5217\u8868\u200b\u7684\u200b\u5f62\u5f0f\u200b\u8fd4\u56de\u200b\u4e86\u200b\u6240\u6709\u200bdensenet\u200b\u7cfb\u5217\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u3002 <pre><code>['densenet121',\n 'densenet121d',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenet264',\n 'densenet264d_iabn',\n 'densenetblur121d',\n 'tv_densenet121']\n</code></pre> 3. \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u5177\u4f53\u200b\u53c2\u6570\u200b \u200b\u5f53\u200b\u6211\u4eec\u200b\u60f3\u200b\u67e5\u770b\u200b\u4e0b\u200b\u6a21\u578b\u200b\u7684\u200b\u5177\u4f53\u200b\u53c2\u6570\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200b\u6a21\u578b\u200b\u7684\u200b<code>default_cfg</code>\u200b\u5c5e\u6027\u200b\u6765\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\uff0c\u200b\u5177\u4f53\u64cd\u4f5c\u200b\u5982\u4e0b\u200b <pre><code>model = timm.create_model('resnet34',num_classes=10,pretrained=True)\nmodel.default_cfg\n</code></pre> <pre><code>{'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth',\n 'num_classes': 1000,\n 'input_size': (3, 224, 224),\n 'pool_size': (7, 7),\n 'crop_pct': 0.875,\n 'interpolation': 'bilinear',\n 'mean': (0.485, 0.456, 0.406),\n 'std': (0.229, 0.224, 0.225),\n 'first_conv': 'conv1',\n 'classifier': 'fc',\n 'architecture': 'resnet34'}\n</code></pre> \u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200b\u8fd9\u4e2a\u200b\u94fe\u63a5\u200b \u200b\u67e5\u770b\u200b\u63d0\u4f9b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u5ea6\u200b\u7b49\u200b\u4fe1\u606f\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/#633","title":"6.3.3 \u200b\u4f7f\u7528\u200b\u548c\u200b\u4fee\u6539\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>\u200b\u5728\u200b\u5f97\u5230\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>timm.create_model()</code>\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u521b\u5efa\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f20\u5165\u200b\u53c2\u6570\u200b<code>pretrained=True</code>\uff0c\u200b\u6765\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002\u200b\u540c\u6837\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8ddf\u200btorchvision\u200b\u91cc\u9762\u200b\u7684\u200b\u6a21\u578b\u200b\u4e00\u6837\u200b\u7684\u200b\u65b9\u6cd5\u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u7c7b\u578b\u200b/ <pre><code>import timm\nimport torch\n\nmodel = timm.create_model('resnet34',pretrained=True)\nx = torch.randn(1,3,224,224)\noutput = model(x)\noutput.shape\n</code></pre></p> <p><pre><code>torch.Size([1, 1000])\n</code></pre> - \u200b\u67e5\u770b\u200b\u67d0\u200b\u4e00\u5c42\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff08\u200b\u4ee5\u200b\u7b2c\u4e00\u5c42\u200b\u5377\u79ef\u200b\u4e3a\u4f8b\u200b\uff09 <pre><code>model = timm.create_model('resnet34',pretrained=True)\nlist(dict(model.named_children())['conv1'].parameters())\n</code></pre> <pre><code>[Parameter containing:\n tensor([[[[-2.9398e-02, -3.6421e-02, -2.8832e-02,  ..., -1.8349e-02,\n            -6.9210e-03,  1.2127e-02],\n           [-3.6199e-02, -6.0810e-02, -5.3891e-02,  ..., -4.2744e-02,\n            -7.3169e-03, -1.1834e-02],\n            ...\n           [ 8.4563e-03, -1.7099e-02, -1.2176e-03,  ...,  7.0081e-02,\n             2.9756e-02, -4.1400e-03]]]], requires_grad=True)]\n</code></pre> - \u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\uff08\u200b\u5c06\u200b1000\u200b\u7c7b\u200b\u6539\u4e3a\u200b10\u200b\u7c7b\u200b\u8f93\u51fa\u200b\uff09 <pre><code>model = timm.create_model('resnet34',num_classes=10,pretrained=True)\nx = torch.randn(1,3,224,224)\noutput = model(x)\noutput.shape\n</code></pre> <pre><code>torch.Size([1, 10])\n</code></pre> - \u200b\u6539\u53d8\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u6570\u200b\uff08\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u4f20\u5165\u200b\u7684\u200b\u56fe\u7247\u200b\u662f\u200b\u5355\u901a\u9053\u200b\u7684\u200b\uff0c\u200b\u4f46\u662f\u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u7684\u200b\u662f\u200b\u4e09\u200b\u901a\u9053\u200b\u56fe\u7247\u200b\uff09 \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6dfb\u52a0\u200b<code>in_chans=1</code>\u200b\u6765\u200b\u6539\u53d8\u200b <pre><code>model = timm.create_model('resnet34',num_classes=10,pretrained=True,in_chans=1)\nx = torch.randn(1,1,224,224)\noutput = model(x)\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/#634","title":"6.3.4 \u200b\u6a21\u578b\u200b\u7684\u200b\u4fdd\u5b58","text":"<p>timm\u200b\u5e93\u6240\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b<code>torch.model</code>\u200b\u7684\u200b\u5b50\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200btorch\u200b\u5e93\u4e2d\u200b\u5185\u7f6e\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5177\u4f53\u64cd\u4f5c\u200b\u5982\u4e0b\u200b\u65b9\u200b\u4ee3\u7801\u200b\u6240\u793a\u200b <pre><code>torch.save(model.state_dict(),'./checkpoint/timm_model.pth')\nmodel.load_state_dict(torch.load('./checkpoint/timm_model.pth'))\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-timm/#_1","title":"\u53c2\u8003\u200b\u6750\u6599","text":"<ol> <li>https://www.aiuai.cn/aifarm1967.html</li> <li>https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055</li> <li>https://chowdera.com/2022/03/202203170834122729.html</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-torchvision/","title":"6.3 \u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b-torchvision","text":"<p>\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u8d8a\u6765\u8d8a\u200b\u5927\u200b\uff0c\u200b\u8bb8\u591a\u200b\u5f00\u6e90\u200b\u6a21\u578b\u200b\u90fd\u200b\u662f\u200b\u5728\u200b\u8f83\u5927\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u6bd4\u5982\u200bImagenet-1k\uff0cImagenet-11k\uff0c\u200b\u751a\u81f3\u200b\u662f\u200bImageNet-21k\u200b\u7b49\u200b\u3002\u200b\u4f46\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u53ef\u80fd\u200b\u53ea\u6709\u200b\u51e0\u5343\u200b\u5f20\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u5177\u6709\u200b\u51e0\u5343\u4e07\u200b\u53c2\u6570\u200b\u7684\u200b\u5927\u578b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u4e0d\u200b\u73b0\u5b9e\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8d8a\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u8981\u6c42\u200b\u8d8a\u5927\u200b\uff0c\u200b\u8fc7\u200b\u62df\u5408\u200b\u65e0\u6cd5\u200b\u907f\u514d\u200b\u3002</p> <p>\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u60f3\u200b\u4ece\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u8bc6\u522b\u200b\u51fa\u200b\uf967\u200b\u540c\u200b\u79cd\u7c7b\u200b\u7684\u200b\u6905\u200b\u2f26\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8d2d\u4e70\u200b\u94fe\u63a5\u200b\u63a8\u8350\u200b\u7ed9\u200b\u7528\u6237\u200b\u3002\u200b\u4e00\u79cd\u200b\u53ef\u80fd\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5148\u200b\u627e\u51fa\u200b100\u200b\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u6905\u5b50\u200b\uff0c\u200b\u4e3a\u200b\u6bcf\u79cd\u200b\u6905\u5b50\u200b\u62cd\u6444\u200b1000\u200b\u5f20\u200b\u4e0d\u540c\u200b\u2ec6\u200b\u5ea6\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6536\u96c6\u200b\u5230\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u6905\u5b50\u200b\u6570\u636e\u200b\u96c6\u200b\u867d\u7136\u200b\u53ef\u80fd\u200b\u6bd4\u200bFashion-MNIST\u200b\u6570\u636e\u200b\u96c6\u8981\u200b\u5e9e\u200b\u2f24\u200b\uff0c\u200b\u4f46\u200b\u6837\u672c\u6570\u200b\u4ecd\u7136\u200b\u4e0d\u53ca\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u6837\u672c\u6570\u200b\u7684\u200b\u5341\u5206\u200b\u4e4b\u200b\u2f00\u200b\u3002\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u9002\u7528\u200b\u4e8e\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u590d\u6742\u200b\u6a21\u578b\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u6905\u200b\u2f26\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6570\u636e\u91cf\u200b\u6709\u9650\u200b\uff0c\u200b\u6700\u7ec8\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u8fbe\u200b\uf967\u200b\u5230\u200b\u5b9e\u7528\u200b\u7684\u200b\u8981\u6c42\u200b\u3002</p> <p>\u200b\u4e3a\u200b\uf9ba\u200b\u5e94\u5bf9\u200b\u4e0a\u8ff0\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u663e\u200b\u2f7d\u200b\uf9e0\u200b\u2ec5\u200b\u7684\u200b\u89e3\u51b3\u529e\u6cd5\u200b\u662f\u200b\u6536\u96c6\u200b\uf901\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6536\u96c6\u200b\u548c\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\u4f1a\u200b\u82b1\u8d39\u200b\u5927\u200b\uf97e\u200b\u7684\u200b\u65f6\u95f4\u200b\u548c\u8d44\u200b\u2fa6\u200b\u3002\u200b\uf9b5\u200b\u5982\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u6536\u96c6\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u82b1\u8d39\u200b\uf9ba\u200b\u6570\u767e\u4e07\u7f8e\u5143\u200b\u7684\u200b\u7814\u7a76\u200b\u7ecf\u8d39\u200b\u3002\u200b\u867d\u7136\u200b\u76ee\u524d\u200b\u7684\u200b\u6570\u636e\u200b\u91c7\u96c6\u200b\u6210\u672c\u200b\u5df2\u200b\u964d\u4f4e\u200b\uf9ba\u200b\uf967\u200b\u5c11\u200b\uff0c\u200b\u4f46\u200b\u5176\u200b\u6210\u672c\u200b\u4ecd\u7136\u200b\uf967\u200b\u53ef\u200b\u5ffd\u200b\uf976\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\u4e00\u79cd\u200b\u89e3\u51b3\u529e\u6cd5\u200b\u662f\u200b\u5e94\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b(transfer learning)\uff0c\u200b\u5c06\u200b\u4ece\u6e90\u200b\u6570\u636e\u200b\u96c6\u200b\u5b66\u5230\u200b\u7684\u200b\u77e5\u8bc6\u200b\u8fc1\u79fb\u200b\u5230\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u3002\u200b\uf9b5\u200b\u5982\u200b\uff0c\u200b\u867d\u7136\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u56fe\u50cf\u200b\u5927\u591a\u200b\u8ddf\u200b\u6905\u5b50\u200b\u65e0\u5173\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u62bd\u53d6\u200b\u8f83\u200b\u901a\u7528\u200b\u7684\u200b\u56fe\u50cf\u200b\u7279\u5f81\u200b\uff0c\u200b\u4ece\u800c\u200b\u80fd\u591f\u200b\u5e2e\u52a9\u200b\u8bc6\u522b\u200b\u8fb9\u7f18\u200b\u3001\u200b\u7eb9\u200b\uf9e4\u200b\u3001\u200b\u5f62\u72b6\u200b\u548c\u200b\u7269\u4f53\u200b\u7ec4\u6210\u200b\u7b49\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u7279\u5f81\u200b\u5bf9\u4e8e\u200b\u8bc6\u522b\u200b\u6905\u5b50\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u540c\u6837\u200b\u6709\u6548\u200b\u3002</p> <p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u5927\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u662f\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\uff08finetune\uff09\u3002\u200b\u7b80\u5355\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u5148\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u540c\u7c7b\u200b\u7684\u200b\u522b\u4eba\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u628a\u200b\u522b\u4eba\u200b\u73b0\u6210\u200b\u7684\u200b\u8bad\u7ec3\u200b\u597d\u200b\u4e86\u200b\u7684\u200b\u6a21\u578b\u200b\u62ff\u200b\u8fc7\u6765\u200b\uff0c\u200b\u6362\u6210\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u8c03\u6574\u200b\u4e00\u4e0b\u200b\u53c2\u6570\u200b\u3002 \u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u7f51\u7edc\u200b\u6a21\u578b\u200b\uff08VGG\uff0cResNet\u200b\u7cfb\u5217\u200b\uff0cmobilenet\u200b\u7cfb\u5217\u200b......\uff09\uff0c\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u90fd\u200b\u662f\u200bPyTorch\u200b\u5b98\u65b9\u200b\u5728\u200b\u76f8\u5e94\u200b\u7684\u200b\u5927\u578b\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u3002\u200b\u5b66\u4e60\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u65b9\u4fbf\u200b\u6211\u4eec\u200b\u5feb\u901f\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5b8c\u6210\u200b\u81ea\u5df1\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u638c\u63e1\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u7684\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u4e86\u89e3\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5e38\u7528\u200bmodel</li> <li>\u200b\u638c\u63e1\u200b\u5982\u4f55\u200b\u6307\u5b9a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u5206\u200b\u5c42\u200b</li> </ul>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-torchvision/#631","title":"6.3.1 \u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u7684\u200b\u6d41\u7a0b","text":"<ol> <li>\u200b\u5728\u200b\u6e90\u200b\u6570\u636e\u200b\u96c6\u200b(\u200b\u5982\u200bImageNet\u200b\u6570\u636e\u200b\u96c6\u200b)\u200b\u4e0a\u9884\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\uff0c\u200b\u5373\u6e90\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\uff0c\u200b\u5373\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u3002\u200b\u5b83\u200b\u590d\u5236\u200b\uf9ba\u200b\u6e90\u200b\u6a21\u578b\u200b\u4e0a\u9664\u200b\uf9ba\u200b\u8f93\u51fa\u200b\u5c42\u5916\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u8bbe\u8ba1\u200b\u53ca\u5176\u200b\u53c2\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5305\u542b\u200b\uf9ba\u200b\u6e90\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u4e14\u200b\u8fd9\u4e9b\u200b\u77e5\u8bc6\u200b\u540c\u6837\u200b\u9002\u7528\u200b\u4e8e\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5047\u8bbe\u200b\u6e90\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\u8ddf\u6e90\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6807\u7b7e\u200b\u7d27\u5bc6\u200b\u76f8\u5173\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u4e2d\u200b\u4e0d\u4e88\u200b\u91c7\u7528\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u2f24\u200b\u5c0f\u4e3a\u200b\u2f6c\u200b\u6807\u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u522b\u200b\u4e2a\u6570\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff0c\u200b\u5e76\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u8be5\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u3002</li> <li>\u200b\u5728\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u5934\u200b\u8bad\u7ec3\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff0c\u200b\u800c\u200b\u5176\u4f59\u200b\u5c42\u200b\u7684\u200b\u53c2\u6570\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6e90\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u5fae\u8c03\u200b\u5f97\u5230\u200b\u7684\u200b\u3002</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-torchvision/#632","title":"6.3.2 \u200b\u4f7f\u7528\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u7ed3\u6784","text":"<p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ee5\u200btorchvision\u200b\u4e2d\u200b\u7684\u200b\u5e38\u89c1\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5217\u51fa\u200b\u4e86\u200b\u5982\u4f55\u200b\u5728\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u4f7f\u7528\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5e38\u89c1\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u548c\u200b\u53c2\u6570\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u200b\u4efb\u52a1\u200b\u548c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\uff0c\u200b\u4f7f\u7528\u200b\u65b9\u5f0f\u200b\u662f\u200b\u7c7b\u4f3c\u200b\u7684\u200b\uff1a</p> <ul> <li>\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u7f51\u7edc\u200b</li> </ul> <p><code>``python   import torchvision.models as models   resnet18 = models.resnet18()   # resnet18 = models.resnet18(pretrained=False)  \u200b\u7b49\u4ef7\u200b\u4e8e\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b   alexnet = models.alexnet()   vgg16 = models.vgg16()   squeezenet = models.squeezenet1_0()   densenet = models.densenet161()   inception = models.inception_v3()   googlenet = models.googlenet()   shufflenet = models.shufflenet_v2_x1_0()   mobilenet_v2 = models.mobilenet_v2()   mobilenet_v3_large = models.mobilenet_v3_large()   mobilenet_v3_small = models.mobilenet_v3_small()   resnext50_32x4d = models.resnext50_32x4d()   wide_resnet50_2 = models.wide_resnet50_2()   mnasnet = models.mnasnet1_0() - \u200b\u4f20\u9012\u200b</code>pretrained`\u200b\u53c2\u6570\u200b</p> <p>\u200b\u901a\u8fc7\u200b<code>True</code>\u200b\u6216\u8005\u200b<code>False</code>\u200b\u6765\u200b\u51b3\u5b9a\u200b\u662f\u5426\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u5728\u200b\u9ed8\u8ba4\u200b\u72b6\u6001\u200b\u4e0b\u200b<code>pretrained = False</code>\uff0c\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u5f53\u200b<code>pretrained = True</code>\uff0c\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5728\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200b\u6743\u91cd\u200b\u3002</p> <p><code>python import torchvision.models as models resnet18 = models.resnet18(pretrained=True) alexnet = models.alexnet(pretrained=True) squeezenet = models.squeezenet1_0(pretrained=True) vgg16 = models.vgg16(pretrained=True) densenet = models.densenet161(pretrained=True) inception = models.inception_v3(pretrained=True) googlenet = models.googlenet(pretrained=True) shufflenet = models.shufflenet_v2_x1_0(pretrained=True) mobilenet_v2 = models.mobilenet_v2(pretrained=True) mobilenet_v3_large = models.mobilenet_v3_large(pretrained=True) mobilenet_v3_small = models.mobilenet_v3_small(pretrained=True) resnext50_32x4d = models.resnext50_32x4d(pretrained=True) wide_resnet50_2 = models.wide_resnet50_2(pretrained=True) mnasnet = models.mnasnet1_0(pretrained=True)</code></p> <p>\u200b\u6ce8\u610f\u4e8b\u9879\u200b\uff1a</p> <ol> <li> <p>\u200b\u901a\u5e38\u200bPyTorch\u200b\u6a21\u578b\u200b\u7684\u200b\u6269\u5c55\u200b\u4e3a\u200b<code>.pt</code>\u200b\u6216\u200b<code>.pth</code>\uff0c\u200b\u7a0b\u5e8f\u8fd0\u884c\u200b\u65f6\u4f1a\u200b\u9996\u5148\u200b\u68c0\u67e5\u200b\u9ed8\u8ba4\u200b\u8def\u5f84\u200b\u4e2d\u200b\u662f\u5426\u200b\u6709\u200b\u5df2\u7ecf\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0c\u200b\u4e00\u65e6\u200b\u6743\u91cd\u200b\u88ab\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u4e0b\u6b21\u200b\u52a0\u8f7d\u200b\u5c31\u200b\u4e0d\u200b\u9700\u8981\u200b\u4e0b\u8f7d\u200b\u4e86\u200b\u3002</p> </li> <li> <p>\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u4e0b\u8f7d\u200b\u4f1a\u200b\u6bd4\u8f83\u6162\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b\u8fc5\u96f7\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u53bb\u200b \u200b\u8fd9\u91cc\u200b \u200b\u67e5\u770b\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u91cc\u9762\u200b<code>model_urls</code>\uff0c\u200b\u7136\u540e\u200b\u624b\u52a8\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u5728\u200b<code>Linux</code>\u200b\u548c\u200b<code>Mac</code>\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u4e0b\u8f7d\u200b\u8def\u5f84\u200b\u662f\u200b\u7528\u6237\u200b\u6839\u76ee\u5f55\u200b\u4e0b\u200b\u7684\u200b<code>.cache</code>\u200b\u6587\u4ef6\u5939\u200b\u3002\u200b\u5728\u200b<code>Windows</code>\u200b\u4e0b\u200b\u5c31\u662f\u200b<code>C:\\Users\\&lt;username&gt;\\.cache\\torch\\hub\\checkpoint</code>\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b <code>torch.utils.model_zoo.load_url()</code>\u200b\u8bbe\u7f6e\u200b\u6743\u91cd\u200b\u7684\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\u3002</p> </li> <li> <p>\u200b\u5982\u679c\u200b\u89c9\u5f97\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u81ea\u5df1\u200b\u7684\u200b\u6743\u91cd\u200b\u4e0b\u8f7d\u200b\u4e0b\u6765\u200b\u653e\u5230\u200b\u540c\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u5c06\u200b\u53c2\u6570\u200b\u52a0\u8f7d\u200b\u7f51\u7edc\u200b\u3002</p> </li> </ol> <pre><code>self.model = models.resnet50(pretrained=False)\nself.model.load_state_dict(torch.load('./model/resnet50-19c8e357.pth'))\n</code></pre> <ol> <li>\u200b\u5982\u679c\u200b\u4e2d\u9014\u200b\u5f3a\u884c\u200b\u505c\u6b62\u200b\u4e0b\u8f7d\u200b\u7684\u8bdd\u200b\uff0c\u200b\u4e00\u5b9a\u200b\u8981\u200b\u53bb\u200b\u5bf9\u5e94\u200b\u8def\u5f84\u200b\u4e0b\u200b\u5c06\u200b\u6743\u91cd\u200b\u6587\u4ef6\u200b\u5220\u9664\u200b\u5e72\u51c0\u200b\uff0c\u200b\u8981\u4e0d\u7136\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u62a5\u9519\u200b\u3002</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-torchvision/#633","title":"6.3.3 \u200b\u8bad\u7ec3\u200b\u7279\u5b9a\u200b\u5c42","text":"<p>\u200b\u5728\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53c2\u6570\u200b\u7684\u200b\u5c5e\u6027\u200b<code>.requires_grad = True</code>\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u6216\u200b\u5fae\u8c03\u200b\u4e0d\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u63d0\u53d6\u200b\u7279\u5f81\u200b\u5e76\u4e14\u200b\u53ea\u60f3\u200b\u4e3a\u200b\u65b0\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u5c42\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u6539\u53d8\u200b\u3002\u200b\u90a3\u200b\u6211\u4eec\u200b\u5c31\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b<code>requires_grad = False</code>\u200b\u6765\u200b\u51bb\u7ed3\u200b\u90e8\u5206\u200b\u5c42\u200b\u3002\u200b\u5728\u200bPyTorch\u200b\u5b98\u65b9\u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u4f8b\u7a0b\u200b\u3002</p> <pre><code>def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n</code></pre> <p>\u200b\u5728\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u4ecd\u65e7\u200b\u4f7f\u7528\u200b<code>resnet18</code>\u200b\u4e3a\u4f8b\u200b\u7684\u200b\u5c06\u200b1000\u200b\u7c7b\u200b\u6539\u4e3a\u200b4\u200b\u7c7b\u200b\uff0c\u200b\u4f46\u662f\u200b\u4ec5\u200b\u6539\u53d8\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff0c\u200b\u4e0d\u200b\u6539\u53d8\u200b\u7279\u5f81\u63d0\u53d6\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff1b\u200b\u6ce8\u610f\u200b\u6211\u4eec\u200b\u5148\u200b\u51bb\u7ed3\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u518d\u200b\u5bf9\u6a21\u578b\u200b\u8f93\u51fa\u200b\u90e8\u5206\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\uff0c\u200b\u8fd9\u6837\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u7684\u200b\u53c2\u6570\u200b\u5c31\u662f\u200b\u53ef\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u7684\u200b\u3002</p> <pre><code>import torchvision.models as models\n# \u200b\u51bb\u7ed3\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\nfeature_extract = True\nmodel = models.resnet18(pretrained=True)\nset_parameter_requires_grad(model, feature_extract)\n# \u200b\u4fee\u6539\u200b\u6a21\u578b\u200b\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(in_features=num_ftrs, out_features=4, bias=True)\n</code></pre> <p>\u200b\u4e4b\u540e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0cmodel\u200b\u4ecd\u4f1a\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u56de\u4f20\u200b\uff0c\u200b\u4f46\u662f\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u5219\u200b\u53ea\u4f1a\u200b\u53d1\u751f\u200b\u5728\u200bfc\u200b\u5c42\u200b\u3002\u200b\u901a\u8fc7\u200b\u8bbe\u5b9a\u200b\u53c2\u6570\u200b\u7684\u200brequires_grad\u200b\u5c5e\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u4e86\u200b\u6307\u5b9a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u7279\u5b9a\u200b\u5c42\u200b\u7684\u200b\u76ee\u6807\u200b\uff0c\u200b\u8fd9\u200b\u5bf9\u200b\u5b9e\u73b0\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.3%20%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-torchvision/#_1","title":"\u672c\u200b\u8282\u200b\u53c2\u8003","text":"<ol> <li>\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b </li> <li>\u200b\u7ed9\u200b\u4e0d\u540c\u200b\u5c42\u200b\u5206\u914d\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.4%20%E5%8D%8A%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/","title":"6.4 \u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3","text":"<p>\u200b\u6211\u4eec\u200b\u63d0\u5230\u200bPyTorch\u200b\u65f6\u5019\u200b\uff0c\u200b\u603b\u4f1a\u200b\u60f3\u5230\u200b\u8981\u200b\u7528\u200b\u786c\u4ef6\u200b\u8bbe\u5907\u200bGPU\u200b\u7684\u200b\u652f\u6301\u200b\u3002\u200b\u800c\u200bGPU\u200b\u7684\u200b\u6027\u80fd\u200b\u4e3b\u8981\u200b\u5206\u4e3a\u200b\u4e24\u200b\u90e8\u5206\u200b\uff1a\u200b\u7b97\u529b\u200b\u548c\u200b\u663e\u5b58\u200b\uff0c\u200b\u524d\u8005\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u663e\u5361\u200b\u8ba1\u7b97\u200b\u7684\u200b\u901f\u5ea6\u200b\uff0c\u200b\u540e\u8005\u200b\u5219\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u663e\u5361\u200b\u53ef\u4ee5\u200b\u540c\u65f6\u200b\u653e\u5165\u200b\u591a\u5c11\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u3002\u200b\u5728\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u7684\u200b\u663e\u5b58\u200b\u6570\u91cf\u200b\u4e00\u5b9a\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u8bad\u7ec3\u200b\u80fd\u591f\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u66f4\u200b\u591a\u200b\uff08\u200b\u4e5f\u200b\u5c31\u662f\u200bbatch size\u200b\u66f4\u5927\u200b\uff09\uff0c\u200b\u5219\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u63d0\u9ad8\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b\u3002\u200b\u53e6\u5916\u200b\uff0c\u200b\u6709\u65f6\u5019\u200b\u6570\u636e\u200b\u672c\u8eab\u200b\u4e5f\u200b\u6bd4\u8f83\u200b\u5927\u200b\uff08\u200b\u6bd4\u5982\u200b3D\u200b\u56fe\u50cf\u200b\u3001\u200b\u89c6\u9891\u200b\u7b49\u200b\uff09\uff0c\u200b\u663e\u5b58\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u53ef\u80fd\u200b\u751a\u81f3\u200bbatch size\u200b\u4e3a\u200b1\u200b\u7684\u200b\u60c5\u51b5\u200b\u90fd\u200b\u65e0\u6cd5\u200b\u5b9e\u73b0\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5408\u7406\u200b\u4f7f\u7528\u200b\u663e\u5b58\u200b\u4e5f\u200b\u5c31\u200b\u663e\u5f97\u200b\u5341\u5206\u200b\u91cd\u8981\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u89c2\u5bdf\u200bPyTorch\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u6d6e\u70b9\u6570\u200b\u5b58\u50a8\u200b\u65b9\u5f0f\u200b\u7528\u200b\u7684\u200b\u662f\u200b<code>torch.float32</code>\uff0c\u200b\u5c0f\u6570\u70b9\u200b\u540e\u200b\u4f4d\u6570\u200b\u66f4\u200b\u591a\u200b\u56fa\u7136\u200b\u80fd\u200b\u4fdd\u8bc1\u6570\u636e\u200b\u7684\u200b\u7cbe\u786e\u6027\u200b\uff0c\u200b\u4f46\u200b\u7edd\u5927\u591a\u6570\u200b\u573a\u666f\u200b\u5176\u5b9e\u200b\u5e76\u4e0d\u9700\u8981\u200b\u8fd9\u4e48\u200b\u7cbe\u786e\u200b\uff0c\u200b\u53ea\u200b\u4fdd\u7559\u200b\u4e00\u534a\u200b\u7684\u200b\u4fe1\u606f\u200b\u4e5f\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u4f7f\u7528\u200b<code>torch.float16</code>\u200b\u683c\u5f0f\u200b\u3002\u200b\u7531\u4e8e\u200b\u6570\u4f4d\u200b\u51cf\u4e86\u200b\u4e00\u534a\u200b\uff0c\u200b\u56e0\u6b64\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u201d\uff0c\u200b\u5177\u4f53\u200b\u5982\u4e0b\u200b\u56fe\u200b\uff1a</p> <p></p> <p>\u200b\u663e\u7136\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u80fd\u591f\u200b\u51cf\u5c11\u200b\u663e\u5b58\u200b\u5360\u7528\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u663e\u5361\u200b\u53ef\u4ee5\u200b\u540c\u65f6\u200b\u52a0\u8f7d\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002\u200b\u672c\u8282\u200b\u4f1a\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u4f7f\u7528\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5982\u4f55\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b</li> <li>\u200b\u4f7f\u7528\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6ce8\u610f\u4e8b\u9879\u200b</li> </ul>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.4%20%E5%8D%8A%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/#641","title":"6.4.1 \u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8bbe\u7f6e","text":"<p>\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u4f7f\u7528\u200bautocast\u200b\u914d\u7f6e\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u540c\u65f6\u200b\u9700\u8981\u200b\u5728\u200b\u4e0b\u9762\u200b\u4e09\u5904\u200b\u52a0\u4ee5\u200b\u8bbe\u7f6e\u200b\uff1a</p> <ul> <li>import autocast</li> </ul> <pre><code>from torch.cuda.amp import autocast\n</code></pre> <ul> <li>\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b</li> </ul> <p>\u200b\u5728\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200bpython\u200b\u7684\u200b\u88c5\u9970\u200b\u5668\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u7528\u200bautocast\u200b\u88c5\u9970\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200bforward\u200b\u51fd\u6570\u200b\u3002\u200b\u5173\u4e8e\u200b\u88c5\u9970\u200b\u5668\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u8fd9\u91cc\u200b\uff1a</p> <pre><code>@autocast()   \ndef forward(self, x):\n    ...\n    return x\n</code></pre> <ul> <li>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b</li> </ul> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u5728\u200b\u5c06\u200b\u6570\u636e\u200b\u8f93\u5165\u200b\u6a21\u578b\u200b\u53ca\u5176\u200b\u4e4b\u540e\u200b\u7684\u200b\u90e8\u5206\u200b\u653e\u5165\u200b\u201cwith autocast():\u201c\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code> for x in train_loader:\n    x = x.cuda()\n    with autocast():\n            output = model(x)\n        ...\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <p>\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\u4e3b\u8981\u200b\u9002\u7528\u200b\u4e8e\u200b\u6570\u636e\u200b\u672c\u8eab\u200b\u7684\u200bsize\u200b\u6bd4\u8f83\u200b\u5927\u200b\uff08\u200b\u6bd4\u5982\u8bf4\u200b3D\u200b\u56fe\u50cf\u200b\u3001\u200b\u89c6\u9891\u200b\u7b49\u200b\uff09\u3002\u200b\u5f53\u200b\u6570\u636e\u200b\u672c\u8eab\u200b\u7684\u200bsize\u200b\u5e76\u4e0d\u5927\u200b\u65f6\u200b\uff08\u200b\u6bd4\u5982\u200b\u624b\u5199\u200b\u6570\u5b57\u200bMNIST\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u53ea\u6709\u200b28*28\uff09\uff0c\u200b\u4f7f\u7528\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\u5219\u200b\u53ef\u80fd\u200b\u4e0d\u4f1a\u200b\u5e26\u6765\u200b\u663e\u8457\u200b\u7684\u200b\u63d0\u5347\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/","title":"6.5 \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b-imgaug","text":"<p>\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b/\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u9047\u5230\u200b\u6a21\u578b\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fc7\u200b\u62df\u5408\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u52a0\u5165\u200b\u6b63\u5219\u200b\u9879\u200b\u6216\u8005\u200b\u51cf\u5c11\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\u6765\u200b\u89e3\u51b3\u200b\uff0c\u200b\u4f46\u662f\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u907f\u514d\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u589e\u52a0\u200b\u6570\u636e\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u8bb8\u591a\u200b\u573a\u666f\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u83b7\u5f97\u200b\u5927\u91cf\u200b\u6570\u636e\u200b\uff0c\u200b\u4f8b\u5982\u200b\u533b\u5b66\u200b\u56fe\u50cf\u200b\u5206\u6790\u200b\u3002\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6280\u672f\u200b\u7684\u200b\u5b58\u5728\u200b\u662f\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u8fd9\u662f\u200b\u9488\u5bf9\u200b\u6709\u9650\u200b\u6570\u636e\u200b\u95ee\u9898\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\u3002\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u4e00\u5957\u200b\u6280\u672f\u200b\uff0c\u200b\u53ef\u200b\u63d0\u9ad8\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5927\u5c0f\u200b\u548c\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b83\u4eec\u200b\u6765\u200b\u6784\u5efa\u200b\u66f4\u597d\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u3002 \u200b\u5728\u200b\u8ba1\u7b97\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\uff0c\u200b\u751f\u6210\u200b\u589e\u5f3a\u200b\u56fe\u50cf\u200b\u76f8\u5bf9\u200b\u5bb9\u6613\u200b\u3002\u200b\u5373\u4f7f\u200b\u5f15\u5165\u200b\u566a\u58f0\u200b\u6216\u200b\u88c1\u526a\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u6a21\u578b\u200b\u4ecd\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6709\u200b\u4e00\u7cfb\u5217\u200b\u7b80\u5355\u200b\u6709\u6548\u200b\u7684\u200b\u65b9\u6cd5\u200b\u53ef\u200b\u4f9b\u9009\u62e9\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e9b\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e93\u6765\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u6bd4\u5982\u200b\uff1aimgaug \u200b\u5b98\u7f51\u200b\u5b83\u200b\u5c01\u88c5\u200b\u4e86\u200b\u5f88\u591a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u7ed9\u200b\u5f00\u53d1\u8005\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u65b9\u4fbf\u200b\u3002\u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u5185\u5bb9\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u5b66\u4f1a\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a - imgaug\u200b\u7684\u200b\u7b80\u4ecb\u200b\u548c\u200b\u5b89\u88c5\u200b - \u200b\u4f7f\u7528\u200bimgaug\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u589e\u5f3a\u200b</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#651-imgaug","title":"6.5.1 imgaug\u200b\u7b80\u4ecb\u200b\u548c\u200b\u5b89\u88c5","text":""},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#6511-imgaug","title":"6.5.1.1 imgaug\u200b\u7b80\u4ecb","text":"<p><code>imgaug</code>\u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u5e38\u7528\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u5305\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200b<code>torchvision.transforms</code>\uff0c\u200b\u5b83\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u5404\u79cd\u200b\u7ade\u8d5b\u200b\u4e2d\u200b\uff0c\u200b\u4eba\u4eec\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b<code>imgaug</code>\u200b\u6765\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0cimgaug\u200b\u5b98\u65b9\u200b\u8fd8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u4f8b\u7a0b\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5b66\u4e60\u200b\uff0c\u200b\u672c\u7ae0\u200b\u5185\u5bb9\u200b\u4ec5\u200b\u662f\u200b\u7b80\u4ecb\u200b\uff0c\u200b\u5e0c\u671b\u200b\u8d77\u5230\u200b\u629b\u7816\u5f15\u7389\u200b\u7684\u200b\u529f\u80fd\u200b\u3002 1. Github\u200b\u5730\u5740\u200b\uff1aimgaug 2. Readthedocs\uff1aimgaug 3. \u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200bnotebook\u200b\u4f8b\u7a0b\u200b\uff1anotebook</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#6512-imgaug","title":"6.5.1.2 imgaug\u200b\u7684\u200b\u5b89\u88c5","text":"<p>imgaug\u200b\u7684\u200b\u5b89\u88c5\u200b\u65b9\u6cd5\u200b\u548c\u200b\u5176\u4ed6\u200b\u7684\u200bPython\u200b\u5305\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#conda","title":"conda","text":"<pre><code>conda config --add channels conda-forge\nconda install imgaug\n</code></pre>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#pip","title":"pip","text":"<pre><code>#  install imgaug either via pypi\n\npip install imgaug\n\n#  install the latest version directly from github\n\npip install git+https://github.com/aleju/imgaug.git\n</code></pre>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#652-imgaug","title":"6.5.2 imgaug\u200b\u7684\u200b\u4f7f\u7528","text":"<p>imgaug\u200b\u4ec5\u4ec5\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u56fe\u50cf\u589e\u5f3a\u200b\u7684\u200b\u4e00\u4e9b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u662f\u200b\u5e76\u672a\u200b\u63d0\u4f9b\u200b\u56fe\u50cf\u200b\u7684\u200bIO\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u4e00\u4e9b\u200b\u5e93\u6765\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5bfc\u5165\u200b\uff0c\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200bimageio\u200b\u8fdb\u884c\u200b\u8bfb\u5165\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bopencv\u200b\u8fdb\u884c\u200b\u6587\u4ef6\u200b\u8bfb\u53d6\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u624b\u52a8\u200b\u6539\u53d8\u200b\u901a\u9053\u200b\uff0c\u200b\u5c06\u200b\u8bfb\u53d6\u200b\u7684\u200bBGR\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200bRGB\u200b\u56fe\u50cf\u200b\u3002\u200b\u9664\u6b64\u4ee5\u5916\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u7528\u200bPIL.Image\u200b\u8fdb\u884c\u200b\u8bfb\u53d6\u200b\u65f6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8bfb\u53d6\u200b\u7684\u200b\u56fe\u7247\u200b\u6ca1\u6709\u200bshape\u200b\u7684\u200b\u5c5e\u6027\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u8bfb\u53d6\u200b\u5230\u200b\u7684\u200bimg\u200b\u8f6c\u6362\u200b\u4e3a\u200bnp.array()\u200b\u7684\u200b\u5f62\u5f0f\u200b\u518d\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002\u200b\u56e0\u6b64\u200b\u5b98\u65b9\u200b\u7684\u200b\u4f8b\u7a0b\u200b\u4e2d\u200b\u4e5f\u200b\u662f\u200b\u4f7f\u7528\u200bimageio\u200b\u8fdb\u884c\u200b\u56fe\u7247\u200b\u8bfb\u53d6\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_1","title":"\u5355\u5f20\u200b\u56fe\u7247\u200b\u5904\u7406","text":"<p>\u200b\u5728\u200b\u8be5\u200b\u5355\u5143\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ec5\u4ee5\u200b\u51e0\u79cd\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4e3b\u8981\u200b\u76ee\u7684\u200b\u662f\u200b\u6559\u4f1a\u200b\u5927\u5bb6\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bimgaug\u200b\u6765\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\u3002 <pre><code>import imageio\nimport imgaug as ia\n%matplotlib inline\n\n# \u200b\u56fe\u7247\u200b\u7684\u200b\u8bfb\u53d6\u200b\nimg = imageio.imread(\"./Lenna.jpg\")\n\n# \u200b\u4f7f\u7528\u200bImage\u200b\u8fdb\u884c\u200b\u8bfb\u53d6\u200b\n# img = Image.open(\"./Lenna.jpg\")\n# image = np.array(img)\n# ia.imshow(image)\n\n# \u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b\nia.imshow(img)\n</code></pre> </p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5f97\u5230\u200b\u4e86\u200b\u9700\u8981\u200b\u5904\u7406\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c<code>imgaug</code>\u200b\u5305\u542b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u4ece\u200b<code>Augmenter</code>\u200b\u7ee7\u627f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ee5\u200b<code>Affine</code>\u200b\u4e3a\u200b\u4f8b\u5b50\u200b\u3002 <pre><code>from imgaug import augmenters as iaa\n\n# \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\u79cd\u5b50\u200b\nia.seed(4)\n\n# \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u65b9\u6cd5\u200b\nrotate = iaa.Affine(rotate=(-4,45))\nimg_aug = rotate(image=img)\nia.imshow(img_aug)\n</code></pre> </p> <p>\u200b\u8fd9\u200b\u662f\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u4e00\u79cd\u200b\u64cd\u4f5c\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4f46\u200b\u5b9e\u9645\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u505a\u200b\u591a\u79cd\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5904\u7406\u200b\u3002\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u9700\u8981\u200b\u5229\u7528\u200b<code>imgaug.augmenters.Sequential()</code>\u200b\u6765\u200b\u6784\u9020\u200b\u6211\u4eec\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bpipline\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u4e0e\u200b<code>torchvison.transforms.Compose()</code>\u200b\u76f8\u200b\u7c7b\u4f3c\u200b\u3002 <pre><code>iaa.Sequential(children=None, # Augmenter\u200b\u96c6\u5408\u200b\n               random_order=False, # \u200b\u662f\u5426\u200b\u5bf9\u200b\u6bcf\u4e2a\u200bbatch\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u987a\u5e8f\u200b\u7684\u200bAugmenter list\n               name=None,\n               deterministic=False,\n               random_state=None)\n</code></pre></p> <p><pre><code># \u200b\u6784\u5efa\u200b\u5904\u7406\u200b\u5e8f\u5217\u200b\naug_seq = iaa.Sequential([\n    iaa.Affine(rotate=(-25,25)),\n    iaa.AdditiveGaussianNoise(scale=(10,60)),\n    iaa.Crop(percent=(0,0.2))\n])\n# \u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0cimage\u200b\u4e0d\u200b\u53ef\u4ee5\u200b\u7701\u7565\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u80fd\u200b\u5199\u6210\u200bimages\nimage_aug = aug_seq(image=img)\nia.imshow(image_aug)\n</code></pre> </p> <p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0c\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u5904\u7406\u200b\u7684\u200b\u65b9\u5f0f\u200b\u57fa\u672c\u76f8\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u9700\u6c42\u200b\uff0c\u200b\u9009\u62e9\u200b\u5408\u9002\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\u6765\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_2","title":"\u5bf9\u200b\u6279\u6b21\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5904\u7406","text":"<p>\u200b\u5728\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u5904\u7406\u200b\u66f4\u200b\u591a\u4efd\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u3002\u200b\u6b64\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u56fe\u5f62\u200b\u6570\u636e\u200b\u6309\u7167\u200bNHWC\u200b\u7684\u200b\u5f62\u5f0f\u200b\u6216\u8005\u200b\u7531\u200b\u5217\u8868\u200b\u7ec4\u6210\u200b\u7684\u200bHWC\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5bf9\u200b\u6279\u91cf\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002\u200b\u4e3b\u8981\u200b\u5206\u4e3a\u200b\u4ee5\u4e0b\u200b\u4e24\u200b\u90e8\u5206\u200b\uff0c\u200b\u5bf9\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u4ee5\u540c\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u5904\u7406\u200b\u548c\u200b\u5bf9\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5206\u200b\u90e8\u5206\u200b\u5904\u7406\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_3","title":"\u5bf9\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u4ee5\u540c\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u5904\u7406","text":"<p>\u200b\u5bf9\u200b\u4e00\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u5f85\u5904\u7406\u200b\u7684\u200b\u56fe\u7247\u200b\u653e\u5728\u200b\u4e00\u4e2a\u200b<code>list</code>\u200b\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u51fd\u6570\u200b\u7684\u200bimage\u200b\u6539\u4e3a\u200bimages\u200b\u5373\u53ef\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5177\u4f53\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u5982\u4e0b\u200b\uff1a <pre><code>images = [img,img,img,img,]\nimages_aug = rotate(images=images)\nia.imshow(np.hstack(images_aug))\n</code></pre> \u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u7684\u200b\u5c55\u793a\u200b\u6548\u679c\u200b\uff1a</p> <p> \u200b\u5728\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ec5\u4ec5\u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4eff\u5c04\u53d8\u6362\u200b\uff0c\u200b\u540c\u6837\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u4f7f\u7528\u200b\u591a\u79cd\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4e0e\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u65b9\u6cd5\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6211\u4eec\u200b\u540c\u6837\u200b\u9700\u8981\u200b\u501f\u52a9\u200b<code>Sequential</code>\u200b\u6765\u200b\u6784\u9020\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bpipline\u3002 <pre><code>aug_seq = iaa.Sequential([\n    iaa.Affine(rotate=(-25, 25)),\n    iaa.AdditiveGaussianNoise(scale=(10, 60)),\n    iaa.Crop(percent=(0, 0.2))\n])\n\n# \u200b\u4f20\u5165\u200b\u65f6\u200b\u9700\u8981\u200b\u6307\u660e\u200b\u662f\u200bimages\u200b\u53c2\u6570\u200b\nimages_aug = aug_seq.augment_images(images = images)\n#images_aug = aug_seq(images = images) \nia.imshow(np.hstack(images_aug))\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_4","title":"\u5bf9\u200b\u6279\u6b21\u200b\u7684\u200b\u56fe\u7247\u200b\u5206\u200b\u90e8\u5206\u200b\u5904\u7406","text":"<p>imgaug\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u5176\u4ed6\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u5e93\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u6709\u610f\u601d\u200b\u7684\u200b\u7279\u6027\u200b\uff0c\u200b\u5373\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>imgaug.augmenters.Sometimes()</code>\u200b\u5bf9\u200bbatch\u200b\u4e2d\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u56fe\u7247\u200b\u5e94\u7528\u200b\u4e00\u90e8\u5206\u200bAugmenters,\u200b\u5269\u4e0b\u200b\u7684\u200b\u56fe\u7247\u200b\u5e94\u7528\u200b\u53e6\u5916\u200b\u7684\u200bAugmenters\u3002 <pre><code>iaa.Sometimes(p=0.5,  # \u200b\u4ee3\u8868\u200b\u5212\u5206\u200b\u6bd4\u4f8b\u200b\n              then_list=None,  # Augmenter\u200b\u96c6\u5408\u200b\u3002p\u200b\u6982\u7387\u200b\u7684\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u7684\u200bAugmenters\u3002\n              else_list=None,  #1-p\u200b\u6982\u7387\u200b\u7684\u200b\u56fe\u7247\u200b\u4f1a\u200b\u88ab\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u7684\u200bAugmenters\u3002\u200b\u6ce8\u610f\u200b\u53d8\u6362\u200b\u7684\u200b\u56fe\u7247\u200b\u5e94\u7528\u200b\u7684\u200bAugmenter\u200b\u53ea\u80fd\u200b\u662f\u200bthen_list\u200b\u6216\u8005\u200belse_list\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u3002\n              name=None,\n              deterministic=False,\n              random_state=None)\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_5","title":"\u5bf9\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5904\u7406","text":"<p>\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u56fe\u7247\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b\u76f8\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u793a\u4f8b\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ece\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u52a0\u8f7d\u200b\u4e09\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u5c06\u200b\u5b83\u4eec\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u8fdb\u884c\u200b\u6269\u5145\u200b\uff0c\u200b\u7136\u540e\u200b\u4e00\u5f20\u200b\u4e00\u5f20\u200b\u5730\u200b\u663e\u793a\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\u64cd\u4f5c\u200b\u8ddf\u200b\u5355\u5f20\u200b\u7684\u200b\u56fe\u7247\u200b\u90fd\u200b\u662f\u200b\u5341\u5206\u76f8\u4f3c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e0d\u200b\u505a\u200b\u8fc7\u200b\u591a\u200b\u8d58\u8ff0\u200b\u3002 <pre><code># \u200b\u6784\u5efa\u200bpipline\nseq = iaa.Sequential([\n    iaa.CropAndPad(percent=(-0.2, 0.2), pad_mode=\"edge\"),  # crop and pad images\n    iaa.AddToHueAndSaturation((-60, 60)),  # change their color\n    iaa.ElasticTransformation(alpha=90, sigma=9),  # water-like effect\n    iaa.Cutout()  # replace one squared area within the image by a constant intensity value\n], random_order=True)\n\n# \u200b\u52a0\u8f7d\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u56fe\u7247\u200b\nimages_different_sizes = [\n    imageio.imread(\"https://upload.wikimedia.org/wikipedia/commons/e/ed/BRACHYLAGUS_IDAHOENSIS.jpg\"),\n    imageio.imread(\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Southern_swamp_rabbit_baby.jpg\"),\n    imageio.imread(\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Lower_Keys_marsh_rabbit.jpg\")\n]\n\n# \u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u589e\u5f3a\u200b\nimages_aug = seq(images=images_different_sizes)\n\n# \u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\nprint(\"Image 0 (input shape: %s, output shape: %s)\" % (images_different_sizes[0].shape, images_aug[0].shape))\nia.imshow(np.hstack([images_different_sizes[0], images_aug[0]]))\n\nprint(\"Image 1 (input shape: %s, output shape: %s)\" % (images_different_sizes[1].shape, images_aug[1].shape))\nia.imshow(np.hstack([images_different_sizes[1], images_aug[1]]))\n\nprint(\"Image 2 (input shape: %s, output shape: %s)\" % (images_different_sizes[2].shape, images_aug[2].shape))\nia.imshow(np.hstack([images_different_sizes[2], images_aug[2]]))\n</code></pre> </p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#653-imgaugpytorch","title":"6.5.3 imgaug\u200b\u5728\u200bPyTorch\u200b\u7684\u200b\u5e94\u7528","text":"<p>\u200b\u5173\u4e8e\u200bPyTorch\u200b\u4e2d\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bimgaug\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u4eba\u200b\u7684\u200b\u6a21\u677f\u200b\u662f\u200b\u4e0d\u200b\u4e00\u6837\u200b\u7684\u200b\uff0c\u200b\u6211\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4e5f\u200b\u4ec5\u4ec5\u200b\u7ed9\u51fa\u200bimgaug\u200b\u7684\u200bissue\u200b\u91cc\u9762\u200b\u63d0\u51fa\u200b\u7684\u200b\u4e00\u79cd\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff0c\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u7684\u200b\u5b9e\u9645\u200b\u9700\u6c42\u200b\u8fdb\u884c\u200b\u6539\u53d8\u200b\u3002 \u200b\u5177\u4f53\u200b\u94fe\u63a5\u200b\uff1ahow to use imgaug with pytorch <pre><code>import numpy as np\nfrom imgaug import augmenters as iaa\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# \u200b\u6784\u5efa\u200bpipline\ntfs = transforms.Compose([\n    iaa.Sequential([\n        iaa.flip.Fliplr(p=0.5),\n        iaa.flip.Flipud(p=0.5),\n        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n        iaa.MultiplyBrightness(mul=(0.65, 1.35)),\n    ]).augment_image,\n    # \u200b\u4e0d\u8981\u200b\u5fd8\u8bb0\u200b\u4e86\u200b\u4f7f\u7528\u200bToTensor()\n    transforms.ToTensor()\n])\n\n# \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\nclass CustomDataset(Dataset):\n    def __init__(self, n_images, n_classes, transform=None):\n        # \u200b\u56fe\u7247\u200b\u7684\u200b\u8bfb\u53d6\u200b\uff0c\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200bimageio\n        self.images = np.random.randint(0, 255,\n                                        (n_images, 224, 224, 3),\n                                        dtype=np.uint8)\n        self.targets = np.random.randn(n_images, n_classes)\n        self.transform = transform\n\n    def __getitem__(self, item):\n        image = self.images[item]\n        target = self.targets[item]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.images)\n\n\ndef worker_init_fn(worker_id):\n    imgaug.seed(np.random.get_state()[1][0] + worker_id)\n\n\ncustom_ds = CustomDataset(n_images=50, n_classes=10, transform=tfs)\ncustom_dl = DataLoader(custom_ds, batch_size=64,\n                       num_workers=4, pin_memory=True, \n                       worker_init_fn=worker_init_fn)\n</code></pre> \u200b\u5173\u4e8e\u200bnum_workers\u200b\u5728\u200bWindows\u200b\u7cfb\u7edf\u200b\u4e0a\u200b\u53ea\u80fd\u200b\u8bbe\u7f6e\u200b\u6210\u200b0\uff0c\u200b\u4f46\u662f\u200b\u5f53\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bLinux\u200b\u8fdc\u7a0b\u200b\u670d\u52a1\u5668\u65f6\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200bnum_workers\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u5c31\u200b\u9700\u8981\u200b\u6ce8\u610f\u200bworker_init_fn()\u200b\u51fd\u6570\u200b\u7684\u200b\u4f5c\u7528\u200b\u4e86\u200b\u3002\u200b\u5b83\u200b\u4fdd\u8bc1\u200b\u4e86\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5728\u200bnum_workers&gt;0\u200b\u65f6\u200b\u662f\u200b\u5bf9\u200b\u6570\u636e\u200b\u7684\u200b\u589e\u5f3a\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#654","title":"6.5.4 \u200b\u603b\u7ed3","text":"<p>\u200b\u6570\u636e\u200b\u6269\u5145\u200b\u662f\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u638c\u63e1\u200b\u7684\u200b\u57fa\u672c\u6280\u80fd\u200b\uff0c\u200b\u9664\u4e86\u200bimgaug\u200b\u4ee5\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u53bb\u200b\u5b66\u4e60\u200b\u5176\u4ed6\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5e93\u200b\uff0c\u200b\u5305\u62ec\u200b\u4f46\u200b\u4e0d\u200b\u5c40\u9650\u4e8e\u200bAlbumentations\uff0cAugmentor\u3002\u200b\u9664\u53bb\u200bimgaug\u200b\u4ee5\u5916\u200b\uff0c\u200b\u6211\u200b\u8fd8\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\u5927\u5bb6\u200b\u5b66\u4e0b\u200bAlbumentations\uff0c\u200b\u56e0\u4e3a\u200bAlbumentations\u200b\u8ddf\u200bimgaug\u200b\u90fd\u200b\u6709\u7740\u200b\u4e30\u5bcc\u200b\u7684\u200b\u6559\u7a0b\u200b\u8d44\u6e90\u200b\uff0c\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u6709\u200b\u9700\u6c42\u200b\u8bbf\u95ee\u200bAlbumentations\u200b\u6559\u7a0b\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.5%20%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-imgaug/#_6","title":"\u53c2\u8003\u8d44\u6599","text":"<ol> <li>kaggle-data-augmentation-packages-overview</li> <li>how to use imgaug with pytorch</li> <li>Kaggle\u200b\u77e5\u8bc6\u70b9\u200b\uff1a\u200b\u6570\u636e\u200b\u6269\u589e\u200b\u65b9\u6cd5\u200b</li> <li>PyTorch Classification Model Based On Imgaug.</li> <li>Tutorial Notebooks</li> </ol>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.6%20%E4%BD%BF%E7%94%A8argparse%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%8F%82/","title":"6.6 \u200b\u4f7f\u7528\u200bargparse\u200b\u8fdb\u884c\u200b\u8c03\u53c2","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u65f6\u200b\uff0c\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u4fee\u6539\u200b\u548c\u200b\u4fdd\u5b58\u200b\u662f\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u4e00\u6b65\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u5f53\u200b\u6211\u4eec\u200b\u5728\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u8dd1\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u5982\u4f55\u200b\u66f4\u200b\u65b9\u4fbf\u200b\u7684\u200b\u4fee\u6539\u200b\u8d85\u200b\u53c2\u6570\u200b\u662f\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u7684\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u3002\u200b\u8fd9\u65f6\u5019\u200b\uff0c\u200b\u8981\u662f\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5e93\u200b\u6216\u8005\u200b\u51fd\u6570\u200b\u53ef\u4ee5\u200b\u89e3\u6790\u200b\u6211\u4eec\u200b\u8f93\u5165\u200b\u7684\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u518d\u200b\u4f20\u5165\u200b\u6a21\u578b\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u4e2d\u8be5\u200b\u591a\u200b\u597d\u200b\u3002\u200b\u5230\u5e95\u200b\u6709\u6ca1\u6709\u200b\u8fd9\u6837\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u5462\u200b\uff1f\u200b\u7b54\u6848\u200b\u662f\u200b\u80af\u5b9a\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5c31\u662f\u200b Python \u200b\u6807\u51c6\u200b\u5e93\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff1aArgparse\u3002\u200b\u90a3\u4e48\u200b\u4e0b\u9762\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u4ed6\u200b\u662f\u200b\u591a\u4e48\u200b\u65b9\u4fbf\u200b\u3002\u200b\u901a\u8fc7\u200b\u672c\u8282\u200b\u8bfe\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u4f1a\u200b\u6536\u83b7\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b - argparse\u200b\u7684\u200b\u7b80\u4ecb\u200b - argparse\u200b\u7684\u200b\u4f7f\u7528\u200b - \u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bargparse\u200b\u4fee\u6539\u200b\u8d85\u200b\u53c2\u6570\u200b</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.6%20%E4%BD%BF%E7%94%A8argparse%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%8F%82/#661-argparse","title":"6.6.1 argparse\u200b\u7b80\u4ecb","text":"<p>argsparse\u200b\u662f\u200bpython\u200b\u7684\u200b\u547d\u4ee4\u884c\u200b\u89e3\u6790\u200b\u7684\u200b\u6807\u51c6\u200b\u6a21\u5757\u200b\uff0c\u200b\u5185\u200b\u7f6e\u4e8e\u200bpython\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u5e93\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u76f4\u63a5\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5411\u200b\u7a0b\u5e8f\u200b\u4e2d\u200b\u4f20\u5165\u200b\u53c2\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>python file.py</code>\u200b\u6765\u200b\u8fd0\u884c\u200bpython\u200b\u6587\u4ef6\u200b\u3002\u200b\u800c\u200bargparse\u200b\u7684\u200b\u4f5c\u7528\u200b\u5c31\u662f\u200b\u5c06\u200b\u547d\u4ee4\u884c\u200b\u4f20\u5165\u200b\u7684\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u89e3\u6790\u200b\u3001\u200b\u4fdd\u5b58\u200b\u548c\u200b\u4f7f\u7528\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200bargparse\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u8f93\u5165\u200b\u7684\u200b\u53c2\u6570\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4ee5\u200b\u8fd9\u79cd\u200b\u5f62\u5f0f\u200b<code>python file.py --lr 1e-4 --batch_size 32</code>\u200b\u6765\u200b\u5b8c\u6210\u200b\u5bf9\u200b\u5e38\u89c1\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.6%20%E4%BD%BF%E7%94%A8argparse%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%8F%82/#662-argparse","title":"6.6.2 argparse\u200b\u7684\u200b\u4f7f\u7528","text":"<p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200bargparse\u200b\u7684\u200b\u4f7f\u7528\u200b\u5f52\u7eb3\u200b\u4e3a\u200b\u4ee5\u4e0b\u200b\u4e09\u4e2a\u200b\u6b65\u9aa4\u200b\u3002 - \u200b\u521b\u5efa\u200b<code>ArgumentParser()</code>\u200b\u5bf9\u8c61\u200b - \u200b\u8c03\u7528\u200b<code>add_argument()</code>\u200b\u65b9\u6cd5\u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b - \u200b\u4f7f\u7528\u200b<code>parse_args()</code>\u200b\u89e3\u6790\u200b\u53c2\u6570\u200b \u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u6765\u200b\u5b66\u4e60\u200bargparse\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u3002 <pre><code># demo.py\nimport argparse\n\n# \u200b\u521b\u5efa\u200bArgumentParser()\u200b\u5bf9\u8c61\u200b\nparser = argparse.ArgumentParser()\n\n# \u200b\u6dfb\u52a0\u200b\u53c2\u6570\u200b\nparser.add_argument('-o', '--output', action='store_true', \n    help=\"shows output\")\n# action = `store_true` \u200b\u4f1a\u200b\u5c06\u200boutput\u200b\u53c2\u6570\u200b\u8bb0\u5f55\u200b\u4e3a\u200bTrue\n# type \u200b\u89c4\u5b9a\u200b\u4e86\u200b\u53c2\u6570\u200b\u7684\u200b\u683c\u5f0f\u200b\n# default \u200b\u89c4\u5b9a\u200b\u4e86\u200b\u9ed8\u8ba4\u503c\u200b\nparser.add_argument('--lr', type=float, default=3e-5, help='select the learning rate, default=1e-3') \n\nparser.add_argument('--batch_size', type=int, required=True, help='input batch size')  \n# \u200b\u4f7f\u7528\u200bparse_args()\u200b\u89e3\u6790\u200b\u51fd\u6570\u200b\nargs = parser.parse_args()\n\nif args.output:\n    print(\"This is some output\")\n    print(f\"learning rate:{args.lr} \")\n</code></pre> \u200b\u6211\u4eec\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4f7f\u7528\u200b<code>python demo.py --lr 3e-4 --batch_size 32</code>\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u8f93\u51fa\u200b <pre><code>This is some output\nlearning rate: 3e-4\n</code></pre> argparse\u200b\u7684\u200b\u53c2\u6570\u200b\u4e3b\u8981\u200b\u53ef\u4ee5\u200b\u5206\u4e3a\u200b\u53ef\u200b\u9009\u200b\u53c2\u6570\u200b\u548c\u200b\u5fc5\u9009\u200b\u53c2\u6570\u200b\u3002\u200b\u53ef\u200b\u9009\u200b\u53c2\u6570\u200b\u5c31\u200b\u8ddf\u200b\u6211\u4eec\u200b\u7684\u200b<code>lr</code>\u200b\u53c2\u6570\u200b\u76f8\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u672a\u200b\u8f93\u5165\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u4f1a\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u9ed8\u8ba4\u503c\u200b\u3002\u200b\u5fc5\u9009\u200b\u53c2\u6570\u200b\u5c31\u200b\u8ddf\u200b\u6211\u4eec\u200b\u7684\u200b<code>batch_size</code>\u200b\u53c2\u6570\u200b\u76f8\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u7ed9\u200b\u53c2\u6570\u8bbe\u7f6e\u200b<code>required =True</code>\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5fc5\u987b\u200b\u4f20\u5165\u200b\u8be5\u200b\u53c2\u6570\u200b\uff0c\u200b\u5426\u5219\u200b\u5c31\u200b\u4f1a\u200b\u62a5\u9519\u200b\u3002\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u683c\u5f0f\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u7591\u95ee\u200b\uff0c\u200b\u6211\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u7684\u200b\u65f6\u5019\u200b\u4e0d\u200b\u4f7f\u7528\u200b--\u200b\u53ef\u4ee5\u200b\u5417\u200b\uff1f\u200b\u7b54\u6848\u200b\u662f\u200b\u80af\u5b9a\u200b\u7684\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b\u8bbe\u7f6e\u200b\u4e0a\u200b\u505a\u51fa\u200b\u4e00\u4e9b\u200b\u6539\u53d8\u200b\u3002 <pre><code># positional.py\nimport argparse\n\n# \u200b\u4f4d\u7f6e\u200b\u53c2\u6570\u200b\nparser = argparse.ArgumentParser()\n\nparser.add_argument('name')\nparser.add_argument('age')\n\nargs = parser.parse_args()\n\nprint(f'{args.name} is {args.age} years old')\n</code></pre> \u200b\u5f53\u200b\u6211\u4eec\u200b\u4e0d\u200b\u5b9e\u7528\u200b--\u200b\u540e\u200b\uff0c\u200b\u5c06\u4f1a\u200b\u4e25\u683c\u200b\u6309\u7167\u200b\u53c2\u6570\u200b\u4f4d\u7f6e\u200b\u8fdb\u884c\u200b\u89e3\u6790\u200b\u3002 <pre><code>$ positional_arg.py Peter 23\nPeter is 23 years old\n</code></pre> \u200b\u603b\u7684\u6765\u8bf4\u200b\uff0cargparse\u200b\u7684\u200b\u4f7f\u7528\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u4ee5\u4e0a\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u6211\u200b\u5c06\u200b\u4f1a\u200b\u5206\u4eab\u200b\u6211\u200b\u662f\u200b\u5982\u4f55\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u4f7f\u7528\u200bargparse\u200b\u8fdb\u884c\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u4fee\u6539\u200b\u3002</p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.6%20%E4%BD%BF%E7%94%A8argparse%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%8F%82/#663-argparse","title":"6.6.3 \u200b\u66f4\u52a0\u200b\u9ad8\u6548\u200b\u4f7f\u7528\u200bargparse\u200b\u4fee\u6539\u200b\u8d85\u200b\u53c2\u6570","text":"<p>\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u6709\u7740\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u7ba1\u7406\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u200b\u5c06\u200b\u5206\u4eab\u200b\u6211\u200b\u4f7f\u7528\u200bargparse\u200b\u7ba1\u7406\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5e0c\u671b\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u5927\u5bb6\u200b\u6709\u200b\u4e00\u4e9b\u200b\u501f\u9274\u200b\u610f\u4e49\u200b\u3002\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4f7f\u200b\u4ee3\u7801\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\u548c\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u6211\u200b\u4e00\u822c\u200b\u4f1a\u200b\u5c06\u200b\u6709\u5173\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u64cd\u4f5c\u200b\u5199\u200b\u5728\u200b<code>config.py</code>\uff0c\u200b\u7136\u540e\u200b\u5728\u200b<code>train.py</code>\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u6587\u4ef6\u200b\u5bfc\u5165\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b<code>config.py</code>\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b\u5185\u5bb9\u200b\u3002 <pre><code>import argparse  \n\ndef get_options(parser=argparse.ArgumentParser()):  \n\n    parser.add_argument('--workers', type=int, default=0,  \n                        help='number of data loading workers, you had better put it '  \n                              '4 times of your gpu')  \n\n    parser.add_argument('--batch_size', type=int, default=4, help='input batch size, default=64')  \n\n    parser.add_argument('--niter', type=int, default=10, help='number of epochs to train for, default=10')  \n\n    parser.add_argument('--lr', type=float, default=3e-5, help='select the learning rate, default=1e-3')  \n\n    parser.add_argument('--seed', type=int, default=118, help=\"random seed\")  \n\n    parser.add_argument('--cuda', action='store_true', default=True, help='enables cuda')  \n    parser.add_argument('--checkpoint_path',type=str,default='',  \n                        help='Path to load a previous trained model if not empty (default empty)')  \n    parser.add_argument('--output',action='store_true',default=True,help=\"shows output\")  \n\n    opt = parser.parse_args()  \n\n    if opt.output:  \n        print(f'num_workers: {opt.workers}')  \n        print(f'batch_size: {opt.batch_size}')  \n        print(f'epochs (niters) : {opt.niter}')  \n        print(f'learning rate : {opt.lr}')  \n        print(f'manual_seed: {opt.seed}')  \n        print(f'cuda enable: {opt.cuda}')  \n        print(f'checkpoint_path: {opt.checkpoint_path}')  \n\n    return opt  \n\nif __name__ == '__main__':  \n    opt = get_options()\n</code></pre></p> <pre><code>$ python config.py\n\nnum_workers: 0\nbatch_size: 4\nepochs (niters) : 10\nlearning rate : 3e-05\nmanual_seed: 118\ncuda enable: True\ncheckpoint_path:\n</code></pre> <p>\u200b\u968f\u540e\u200b\u5728\u200b<code>train.py</code>\u200b\u7b49\u200b\u5176\u4ed6\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u8fd9\u6837\u200b\u7684\u200b\u7ed3\u6784\u200b\u6765\u200b\u8c03\u7528\u200b\u53c2\u6570\u200b\u3002 <pre><code># \u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u5e93\u200b\n...\nimport config\n\nopt = config.get_options()\n\nmanual_seed = opt.seed\nnum_workers = opt.workers\nbatch_size = opt.batch_size\nlr = opt.lr\nniters = opt.niters\ncheckpoint_path = opt.checkpoint_path\n\n# \u200b\u968f\u673a\u6570\u200b\u7684\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u590d\u73b0\u200b\u7ed3\u679c\u200b\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n...\n\n\nif __name__ == '__main__':\n    set_seed(manual_seed)\n    for epoch in range(niters):\n        train(model,lr,batch_size,num_workers,checkpoint_path)\n        val(model,lr,batch_size,num_workers,checkpoint_path)\n</code></pre></p>"},{"location":"06-%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/6.6%20%E4%BD%BF%E7%94%A8argparse%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%8F%82/#_1","title":"\u603b\u7ed3","text":"<p>argparse\u200b\u7ed9\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u79cd\u200b\u65b0\u200b\u7684\u200b\u66f4\u52a0\u200b\u4fbf\u6377\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u4e00\u4e9b\u200b\u5927\u578b\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u4e2d\u200b\u4eba\u4eec\u200b\u4e5f\u200b\u4f1a\u200b\u4f7f\u7528\u200bjson\u3001dict\u3001yaml\u200b\u7b49\u200b\u6587\u4ef6\u683c\u5f0f\u200b\u53bb\u200b\u4fdd\u5b58\u200b\u8d85\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002\u200b\u5982\u679c\u200b\u5927\u5bb6\u200b\u8fd8\u200b\u60f3\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u4e86\u89e3\u200bargparse\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u4e0b\u9762\u200b\u63d0\u4f9b\u200b\u7684\u200b\u8fde\u63a5\u200b\u8fdb\u884c\u200b\u66f4\u6df1\u200b\u7684\u200b\u5b66\u4e60\u200b\u548c\u200b\u4e86\u89e3\u200b\u3002 1. Python argparse \u200b\u6559\u7a0b\u200b 2. argparse \u200b\u5b98\u65b9\u200b\u6559\u7a0b\u200b</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.1%20%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/","title":"7.1 \u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784","text":"<p>\u200b\u968f\u7740\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u505a\u200b\u7684\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u7f51\u7edc\u200b\u7684\u200b\u7ed3\u6784\u200b\u8d8a\u6765\u8d8a\u200b\u590d\u6742\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5f88\u200b\u96be\u200b\u786e\u5b9a\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u7ed3\u6784\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7ed3\u6784\u200b\u4ee5\u53ca\u200b\u53c2\u6570\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5bfc\u81f4\u200b\u6211\u4eec\u200b\u5f88\u96be\u200b\u5728\u200b\u77ed\u65f6\u95f4\u200b\u5185\u200b\u5b8c\u6210\u200bdebug\u3002\u200b\u56e0\u6b64\u200b\u638c\u63e1\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u5de5\u5177\u200b\u662f\u200b\u5341\u5206\u200b\u6709\u200b\u5fc5\u8981\u200b\u7684\u200b\u3002\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u529f\u80fd\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200bKeras\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b\u4e00\u4e2a\u200b\u53eb\u505a\u200b<code>model.summary()</code>\u200b\u7684\u200bAPI\u200b\u6765\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u5730\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u8c03\u7528\u200b\u540e\u200b\u5c31\u200b\u4f1a\u200b\u663e\u793a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff0c\u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u6574\u4f53\u200b\u53c2\u6570\u200b\u7b49\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u6ca1\u6709\u200b\u8fd9\u6837\u200b\u4e00\u79cd\u200b\u4fbf\u5229\u200b\u7684\u200b\u5de5\u5177\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u53ef\u89c6\u5316\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4eba\u4eec\u200b\u5f00\u53d1\u200b\u4e86\u200btorchinfo\u200b\u5de5\u5177\u5305\u200b ( torchinfo\u200b\u662f\u200b\u7531\u200btorchsummary\u200b\u548c\u200btorchsummaryX\u200b\u91cd\u6784\u200b\u51fa\u200b\u7684\u200b\u5e93\u200b) \u3002\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200btorchinfo\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> </ul>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.1%20%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/#711-print","title":"7.1.1 \u200b\u4f7f\u7528\u200bprint\u200b\u51fd\u6570\u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u57fa\u7840\u200b\u4fe1\u606f","text":"<p>\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bResNet18\u200b\u7684\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u5c55\u793a\u200b\u3002</p> <pre><code>import torchvision.models as models\nmodel = models.resnet18()\n</code></pre> <p>\u200b\u901a\u8fc7\u200b\u4e0a\u9762\u200b\u7684\u200b\u4e24\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5f97\u5230\u200bresnet18\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3002\u200b\u5728\u200b\u5b66\u4e60\u200btorchinfo\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u770b\u200b\u4e0b\u200b\u76f4\u63a5\u200bprint(model)\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <pre><code>ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n   ... ...\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u5355\u7eaf\u200b\u7684\u200bprint(model)\uff0c\u200b\u53ea\u80fd\u200b\u5f97\u51fa\u200b\u57fa\u7840\u200b\u6784\u4ef6\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u65e2\u200b\u4e0d\u80fd\u200b\u663e\u793a\u200b\u51fa\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200bshape\uff0c\u200b\u4e5f\u200b\u4e0d\u80fd\u200b\u663e\u793a\u200b\u5bf9\u5e94\u200b\u53c2\u200b\u6570\u91cf\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u9700\u8981\u200b\u4ecb\u7ecd\u200b\u51fa\u200b\u6211\u4eec\u200b\u4eca\u5929\u200b\u7684\u200b\u4e3b\u4eba\u516c\u200b<code>torchinfo</code>\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.1%20%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/#712-torchinfo","title":"7.1.2 \u200b\u4f7f\u7528\u200btorchinfo\u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u7ed3\u6784","text":"<ul> <li>torchinfo\u200b\u7684\u200b\u5b89\u88c5\u200b </li> </ul> <pre><code># \u200b\u5b89\u88c5\u200b\u65b9\u6cd5\u200b\u4e00\u200b\npip install torchinfo \n# \u200b\u5b89\u88c5\u200b\u65b9\u6cd5\u200b\u4e8c\u200b\nconda install -c conda-forge torchinfo\n</code></pre> <ul> <li>torchinfo\u200b\u7684\u200b\u4f7f\u7528\u200b</li> </ul> <p>trochinfo\u200b\u7684\u200b\u4f7f\u7528\u200b\u4e5f\u200b\u662f\u200b\u5341\u5206\u200b\u7b80\u5355\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b<code>torchinfo.summary()</code>\u200b\u5c31\u884c\u4e86\u200b\uff0c\u200b\u5fc5\u9700\u200b\u7684\u200b\u53c2\u6570\u200b\u5206\u522b\u200b\u662f\u200bmodel\uff0cinput_size[batch_size,channel,h,w]\uff0c\u200b\u66f4\u200b\u591a\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bdocumentation\uff0c\u200b\u4e0b\u9762\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u8d77\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u5b9e\u4f8b\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\u3002</p> <pre><code>import torchvision.models as models\nfrom torchinfo import summary\nresnet18 = models.resnet18() # \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u6a21\u578b\u200b\nsummary(resnet18, (1, 3, 224, 224)) # 1\uff1abatch_size 3:\u200b\u56fe\u7247\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b 224: \u200b\u56fe\u7247\u200b\u7684\u200b\u9ad8\u200b\u5bbd\u200b\n</code></pre> <ul> <li>torchinfo\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u8f93\u51fa\u200b</li> </ul> <pre><code>=========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n=========================================================================================\nResNet                                   --                        --\n\u251c\u2500Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n\u251c\u2500BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n\u251c\u2500ReLU: 1-3                              [1, 64, 112, 112]         --\n\u251c\u2500MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n\u251c\u2500Sequential: 1-5                        [1, 64, 56, 56]           --\n\u2502    \u2514\u2500BasicBlock: 2-1                   [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500ReLU: 3-3                    [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500ReLU: 3-6                    [1, 64, 56, 56]           --\n\u2502    \u2514\u2500BasicBlock: 2-2                   [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500ReLU: 3-9                    [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500ReLU: 3-12                   [1, 64, 56, 56]           --\n\u251c\u2500Sequential: 1-6                        [1, 128, 28, 28]          --\n\u2502    \u2514\u2500BasicBlock: 2-3                   [1, 128, 28, 28]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n\u2502    \u2502    \u2514\u2500ReLU: 3-15                   [1, 128, 28, 28]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n\u2502    \u2502    \u2514\u2500Sequential: 3-18             [1, 128, 28, 28]          8,448\n\u2502    \u2502    \u2514\u2500ReLU: 3-19                   [1, 128, 28, 28]          --\n\u2502    \u2514\u2500BasicBlock: 2-4                   [1, 128, 28, 28]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n\u2502    \u2502    \u2514\u2500ReLU: 3-22                   [1, 128, 28, 28]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n\u2502    \u2502    \u2514\u2500ReLU: 3-25                   [1, 128, 28, 28]          --\n\u251c\u2500Sequential: 1-7                        [1, 256, 14, 14]          --\n\u2502    \u2514\u2500BasicBlock: 2-5                   [1, 256, 14, 14]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n\u2502    \u2502    \u2514\u2500ReLU: 3-28                   [1, 256, 14, 14]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n\u2502    \u2502    \u2514\u2500Sequential: 3-31             [1, 256, 14, 14]          33,280\n\u2502    \u2502    \u2514\u2500ReLU: 3-32                   [1, 256, 14, 14]          --\n\u2502    \u2514\u2500BasicBlock: 2-6                   [1, 256, 14, 14]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n\u2502    \u2502    \u2514\u2500ReLU: 3-35                   [1, 256, 14, 14]          --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n\u2502    \u2502    \u2514\u2500ReLU: 3-38                   [1, 256, 14, 14]          --\n\u251c\u2500Sequential: 1-8                        [1, 512, 7, 7]            --\n\u2502    \u2514\u2500BasicBlock: 2-7                   [1, 512, 7, 7]            --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n\u2502    \u2502    \u2514\u2500ReLU: 3-41                   [1, 512, 7, 7]            --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n\u2502    \u2502    \u2514\u2500Sequential: 3-44             [1, 512, 7, 7]            132,096\n\u2502    \u2502    \u2514\u2500ReLU: 3-45                   [1, 512, 7, 7]            --\n\u2502    \u2514\u2500BasicBlock: 2-8                   [1, 512, 7, 7]            --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n\u2502    \u2502    \u2514\u2500ReLU: 3-48                   [1, 512, 7, 7]            --\n\u2502    \u2502    \u2514\u2500Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n\u2502    \u2502    \u2514\u2500BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n\u2502    \u2502    \u2514\u2500ReLU: 3-51                   [1, 512, 7, 7]            --\n\u251c\u2500AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n\u251c\u2500Linear: 1-10                           [1, 1000]                 513,000\n=========================================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\nTotal mult-adds (G): 1.81\n=========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 39.75\nParams size (MB): 46.76\nEstimated Total Size (MB): 87.11\n=========================================================================================\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200btorchinfo\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u52a0\u200b\u8be6\u7ec6\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5305\u62ec\u200b\u6a21\u5757\u200b\u4fe1\u606f\u200b\uff08\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u7c7b\u578b\u200b\u3001\u200b\u8f93\u51fa\u200bshape\u200b\u548c\u200b\u53c2\u200b\u6570\u91cf\u200b\uff09\u3001\u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u3001\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u3001\u200b\u4e00\u6b21\u200b\u524d\u5411\u200b\u6216\u8005\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u9700\u8981\u200b\u7684\u200b\u5185\u5b58\u5927\u5c0f\u200b\u7b49\u200b</p> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <p>\u200b\u4f46\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bcolab\u200b\u6216\u8005\u200bjupyter notebook\u200b\u65f6\u200b\uff0c\u200b\u60f3\u8981\u200b\u5b9e\u73b0\u200b\u8be5\u200b\u65b9\u6cd5\u200b\uff0c<code>summary()</code>\u200b\u4e00\u5b9a\u200b\u662f\u200b\u8be5\u200b\u5355\u5143\u200b\uff08\u200b\u5373\u200bnotebook\u200b\u4e2d\u200b\u7684\u200bcell\uff09\u200b\u7684\u200b\u8fd4\u56de\u503c\u200b\uff0c\u200b\u5426\u5219\u200b\u6211\u4eec\u200b\u5c31\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b<code>print(summary(...))</code>\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/","title":"7.2 CNN\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN\uff09\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5b83\u200b\u5e7f\u6cdb\u200b\u5730\u200b\u7528\u4e8e\u200b\u56fe\u50cf\u5904\u7406\u200b\uff0c\u200b\u6781\u5927\u200b\u5730\u200b\u63d0\u5347\u200b\u4e86\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\uff0c\u200b\u63a8\u52a8\u200b\u4e86\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u53d1\u5c55\u200b\u548c\u200b\u8fdb\u6b65\u200b\u3002\u200b\u4f46\u200bCNN\u200b\u662f\u200b\u4e00\u4e2a\u200b\u201c\u200b\u9ed1\u76d2\u200b\u6a21\u578b\u200b\u201d\uff0c\u200b\u4eba\u4eec\u200b\u5e76\u4e0d\u77e5\u9053\u200bCNN\u200b\u662f\u200b\u5982\u4f55\u200b\u83b7\u5f97\u200b\u8f83\u200b\u597d\u200b\u8868\u73b0\u200b\u7684\u200b\uff0c\u200b\u7531\u6b64\u200b\u5e26\u6765\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53ef\u200b\u89e3\u91ca\u6027\u200b\u95ee\u9898\u200b\u3002\u200b\u5982\u679c\u200b\u80fd\u200b\u7406\u89e3\u200bCNN\u200b\u5de5\u4f5c\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4eba\u4eec\u200b\u4e0d\u4ec5\u200b\u80fd\u591f\u200b\u89e3\u91ca\u200b\u6240\u200b\u83b7\u5f97\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7684\u200b\u9c81\u68d2\u6027\u200b\uff0c\u200b\u800c\u4e14\u200b\u8fd8\u200b\u80fd\u200b\u6709\u200b\u9488\u5bf9\u6027\u200b\u5730\u200b\u6539\u8fdb\u200bCNN\u200b\u7684\u200b\u7ed3\u6784\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u6548\u679c\u200b\u63d0\u5347\u200b\u3002</p> <p>\u200b\u7406\u89e3\u200bCNN\u200b\u7684\u200b\u91cd\u8981\u200b\u4e00\u6b65\u200b\u662f\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u5305\u62ec\u200b\u53ef\u89c6\u5316\u200b\u7279\u5f81\u200b\u662f\u200b\u5982\u4f55\u200b\u63d0\u53d6\u200b\u7684\u200b\u3001\u200b\u63d0\u53d6\u200b\u5230\u200b\u7684\u200b\u7279\u5f81\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u5728\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4e0a\u200b\u7684\u200b\u5173\u6ce8\u70b9\u200b\u7b49\u200b\u3002\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u5c31\u200b\u4ece\u200b\u4e0a\u8ff0\u200b\u4e09\u4e2a\u200b\u65b9\u9762\u200b\u51fa\u53d1\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5728\u200bPyTorch\u200b\u7684\u200b\u6846\u67b6\u200b\u4e0b\u200b\u5b8c\u6210\u200bCNN\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u53ef\u89c6\u5316\u200bCNN\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200bCNN\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200bCNN\u200b\u663e\u8457\u200b\u56fe\u200b\uff08class activation map\uff09\u200b\u7684\u200b\u65b9\u6cd5\u200b</li> </ul>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/#721-cnn","title":"7.2.1 CNN\u200b\u5377\u79ef\u200b\u6838\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5377\u79ef\u200b\u6838\u5728\u200bCNN\u200b\u4e2d\u200b\u8d1f\u8d23\u200b\u63d0\u53d6\u200b\u7279\u5f81\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b\u80fd\u591f\u200b\u5e2e\u52a9\u200b\u4eba\u4eec\u200b\u7406\u89e3\u200bCNN\u200b\u5404\u4e2a\u200b\u5c42\u200b\u5728\u200b\u63d0\u53d6\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u7279\u5f81\u200b\uff0c\u200b\u8fdb\u800c\u200b\u7406\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\u3002\u200b\u4f8b\u5982\u200b\u5728\u200bZeiler\u200b\u548c\u200bFergus 2013\u200b\u5e74\u200b\u7684\u200bpaper\u200b\u4e2d\u200b\u5c31\u200b\u7814\u7a76\u200b\u4e86\u200bCNN\u200b\u5404\u4e2a\u200b\u5c42\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u4e0d\u540c\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u53d1\u73b0\u200b\u9760\u8fd1\u200b\u8f93\u5165\u200b\u7684\u200b\u5c42\u200b\u63d0\u53d6\u200b\u7684\u200b\u7279\u5f81\u200b\u662f\u200b\u76f8\u5bf9\u200b\u7b80\u5355\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u800c\u200b\u9760\u8fd1\u200b\u8f93\u51fa\u200b\u7684\u200b\u5c42\u200b\u63d0\u53d6\u200b\u7684\u200b\u7279\u5f81\u200b\u5c31\u200b\u548c\u200b\u56fe\u200b\u4e2d\u200b\u7684\u200b\u5b9e\u4f53\u200b\u5f62\u72b6\u200b\u76f8\u8fd1\u200b\u4e86\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p></p> <p></p> <p>\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b\u4e5f\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u6838\u5fc3\u200b\u5728\u4e8e\u200b\u7279\u5b9a\u200b\u5c42\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u5373\u200b\u7279\u5b9a\u200b\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b\u5c31\u200b\u7b49\u4ef7\u200b\u4e8e\u200b\u53ef\u89c6\u5316\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u3002\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u6848\u200b\uff0c\u200b\u4ee5\u200btorchvision\u200b\u81ea\u5e26\u200b\u7684\u200bVGG11\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u786e\u5b9a\u200b\u6a21\u578b\u200b\u7684\u200b\u5c42\u200b\u4fe1\u606f\u200b\uff1a</p> <pre><code>import torch\nfrom torchvision.models import vgg11\n\nmodel = vgg11(pretrained=True)\nprint(dict(model.features.named_children()))\n</code></pre> <pre><code>{'0': Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '1': ReLU(inplace=True),\n '2': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n '3': Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '4': ReLU(inplace=True),\n '5': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n '6': Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '7': ReLU(inplace=True),\n '8': Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '9': ReLU(inplace=True),\n '10': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n '11': Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '12': ReLU(inplace=True),\n '13': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '14': ReLU(inplace=True),\n '15': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n '16': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '17': ReLU(inplace=True),\n '18': Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n '19': ReLU(inplace=True),\n '20': MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)}\n</code></pre> <p>\u200b\u5377\u79ef\u200b\u6838\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5e94\u4e3a\u200b\u5377\u79ef\u200b\u5c42\u200b\uff08Conv2d\uff09\uff0c\u200b\u8fd9\u91cc\u200b\u4ee5\u200b\u7b2c\u200b\u201c3\u201d\u200b\u5c42\u4e3a\u4f8b\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u5bf9\u5e94\u200b\u7684\u200b\u53c2\u6570\u200b\uff1a</p> <pre><code>conv1 = dict(model.features.named_children())['3']\nkernel_set = conv1.weight.detach()\nnum = len(conv1.weight.detach())\nprint(kernel_set.shape)\nfor i in range(0,num):\n    i_kernel = kernel_set[i]\n    plt.figure(figsize=(20, 17))\n    if (len(i_kernel)) &gt; 1:\n        for idx, filer in enumerate(i_kernel):\n            plt.subplot(9, 9, idx+1) \n            plt.axis('off')\n            plt.imshow(filer[ :, :].detach(),cmap='bwr')\n</code></pre> <pre><code>torch.Size([128, 64, 3, 3])\n</code></pre> <p>\u200b\u7531\u4e8e\u200b\u7b2c\u200b\u201c3\u201d\u200b\u5c42\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u7531\u200b64\u200b\u7ef4\u200b\u53d8\u4e3a\u200b128\u200b\u7ef4\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5171\u6709\u200b128*64\u200b\u4e2a\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u5176\u4e2d\u200b\u90e8\u5206\u200b\u5377\u79ef\u200b\u6838\u200b\u53ef\u89c6\u5316\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/#722-cnn","title":"7.2.2 CNN\u200b\u7279\u5f81\u200b\u56fe\u200b\u53ef\u89c6\u5316\u200b\u65b9\u6cd5","text":"<p>\u200b\u4e0e\u200b\u5377\u79ef\u200b\u6838\u200b\u76f8\u5bf9\u200b\u5e94\u200b\uff0c\u200b\u8f93\u5165\u200b\u7684\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u7ecf\u8fc7\u200b\u6bcf\u6b21\u200b\u5377\u79ef\u200b\u5c42\u200b\u5f97\u5230\u200b\u7684\u200b\u6570\u636e\u200b\u79f0\u4e3a\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u662f\u200b\u4e3a\u4e86\u200b\u770b\u200b\u6a21\u578b\u200b\u63d0\u53d6\u200b\u54ea\u4e9b\u200b\u7279\u5f81\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\u7279\u5f81\u200b\u56fe\u5219\u200b\u662f\u200b\u4e3a\u4e86\u200b\u770b\u200b\u6a21\u578b\u200b\u63d0\u53d6\u200b\u5230\u200b\u7684\u200b\u7279\u5f81\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> <p>\u200b\u83b7\u53d6\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6709\u200b\u5f88\u200b\u591a\u79cd\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u8f93\u5165\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9010\u5c42\u200b\u505a\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u76f4\u5230\u200b\u60f3\u8981\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u5904\u200b\u5c06\u200b\u5176\u200b\u8fd4\u56de\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u884c\u200b\uff0c\u200b\u4f46\u662f\u200b\u6709\u4e9b\u200b\u9ebb\u70e6\u200b\u4e86\u200b\u3002\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4e13\u7528\u200b\u7684\u200b\u63a5\u53e3\u200b\u4f7f\u5f97\u200b\u7f51\u7edc\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u80fd\u591f\u200b\u83b7\u53d6\u200b\u5230\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u63a5\u53e3\u200b\u7684\u200b\u540d\u79f0\u200b\u975e\u5e38\u200b\u5f62\u8c61\u200b\uff0c\u200b\u53eb\u505a\u200bhook\u3002\u200b\u53ef\u4ee5\u200b\u60f3\u8c61\u200b\u8fd9\u6837\u200b\u7684\u200b\u573a\u666f\u200b\uff0c\u200b\u6570\u636e\u200b\u901a\u8fc7\u200b\u7f51\u7edc\u200b\u5411\u524d\u200b\u4f20\u64ad\u200b\uff0c\u200b\u7f51\u7edc\u200b\u67d0\u200b\u4e00\u5c42\u200b\u6211\u4eec\u200b\u9884\u5148\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u94a9\u5b50\u200b\uff0c\u200b\u6570\u636e\u200b\u4f20\u64ad\u200b\u8fc7\u540e\u200b\u94a9\u5b50\u200b\u4e0a\u200b\u4f1a\u200b\u7559\u4e0b\u200b\u6570\u636e\u200b\u5728\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u7684\u200b\u6837\u5b50\u200b\uff0c\u200b\u8bfb\u53d6\u200b\u94a9\u5b50\u200b\u7684\u200b\u4fe1\u606f\u200b\u5c31\u662f\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class Hook(object):\n    def __init__(self):\n        self.module_name = []\n        self.features_in_hook = []\n        self.features_out_hook = []\n\n    def __call__(self,module, fea_in, fea_out):\n        print(\"hooker working\", self)\n        self.module_name.append(module.__class__)\n        self.features_in_hook.append(fea_in)\n        self.features_out_hook.append(fea_out)\n        return None\n\n\ndef plot_feature(model, idx, inputs):\n    hh = Hook()\n    model.features[idx].register_forward_hook(hh)\n\n    # forward_model(model,False)\n    model.eval()\n    _ = model(inputs)\n    print(hh.module_name)\n    print((hh.features_in_hook[0][0].shape))\n    print((hh.features_out_hook[0].shape))\n\n    out1 = hh.features_out_hook[0]\n\n    total_ft  = out1.shape[1]\n    first_item = out1[0].cpu().clone()    \n\n    plt.figure(figsize=(20, 17))\n\n\n    for ftidx in range(total_ft):\n        if ftidx &gt; 99:\n            break\n        ft = first_item[ftidx]\n        plt.subplot(10, 10, ftidx+1) \n\n        plt.axis('off')\n        #plt.imshow(ft[ :, :].detach(),cmap='gray')\n        plt.imshow(ft[ :, :].detach())\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e00\u4e2a\u200bhook\u200b\u7c7b\u200b\uff0c\u200b\u4e4b\u540e\u200b\u5728\u200bplot_feature\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u8be5\u200bhook\u200b\u7c7b\u200b\u7684\u200b\u5bf9\u8c61\u200b\u6ce8\u518c\u200b\u5230\u200b\u8981\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u7f51\u7edc\u200b\u7684\u200b\u67d0\u5c42\u200b\u4e2d\u200b\u3002model\u200b\u5728\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u65f6\u5019\u200b\u4f1a\u200b\u8c03\u7528\u200bhook\u200b\u7684\u200b__call__\u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5c31\u662f\u200b\u5728\u200b\u90a3\u91cc\u200b\u5b58\u50a8\u200b\u4e86\u200b\u5f53\u524d\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u3002\u200b\u8fd9\u91cc\u200b\u7684\u200bfeatures_out_hook \u200b\u662f\u200b\u4e00\u4e2a\u200blist\uff0c\u200b\u6bcf\u6b21\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u4e00\u6b21\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u8c03\u7528\u200b\u4e00\u6b21\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200bfeatures_out_hook  \u200b\u957f\u5ea6\u200b\u4f1a\u200b\u589e\u52a0\u200b1\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/#723-cnn-class-activation-map","title":"7.2.3 CNN class activation map\u200b\u53ef\u89c6\u5316\u200b\u65b9\u6cd5","text":"<p>class activation map \uff08CAM\uff09\u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u5224\u65ad\u200b\u54ea\u4e9b\u200b\u53d8\u91cf\u200b\u5bf9\u6a21\u578b\u200b\u6765\u8bf4\u200b\u662f\u200b\u91cd\u8981\u200b\u7684\u200b\uff0c\u200b\u5728\u200bCNN\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u573a\u666f\u200b\u4e0b\u200b\uff0c\u200b\u5373\u200b\u5224\u65ad\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u54ea\u4e9b\u200b\u50cf\u7d20\u70b9\u200b\u5bf9\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u662f\u200b\u91cd\u8981\u200b\u7684\u200b\u3002\u200b\u9664\u4e86\u200b\u786e\u5b9a\u200b\u91cd\u8981\u200b\u7684\u200b\u50cf\u7d20\u70b9\u200b\uff0c\u200b\u4eba\u4eec\u200b\u4e5f\u200b\u4f1a\u200b\u5bf9\u200b\u91cd\u8981\u200b\u533a\u57df\u200b\u7684\u200b\u68af\u5ea6\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200bCAM\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u4e5f\u200b\u8fdb\u4e00\u6b65\u200b\u6539\u8fdb\u200b\u5f97\u5230\u200b\u4e86\u200bGrad-CAM\uff08\u200b\u4ee5\u53ca\u200b\u8bf8\u591a\u200b\u53d8\u79cd\u200b\uff09\u3002CAM\u200b\u548c\u200bGrad-CAM\u200b\u7684\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u76f8\u6bd4\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b\u4e0e\u200b\u53ef\u89c6\u5316\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0cCAM\u200b\u7cfb\u5217\u200b\u53ef\u89c6\u5316\u200b\u66f4\u4e3a\u200b\u76f4\u89c2\u200b\uff0c\u200b\u80fd\u591f\u200b\u4e00\u76ee\u4e86\u7136\u200b\u5730\u200b\u786e\u5b9a\u200b\u91cd\u8981\u200b\u533a\u57df\u200b\uff0c\u200b\u8fdb\u800c\u200b\u8fdb\u884c\u200b\u53ef\u200b\u89e3\u91ca\u6027\u200b\u5206\u6790\u200b\u6216\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u6539\u8fdb\u200b\u3002CAM\u200b\u7cfb\u5217\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5b9e\u73b0\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5f00\u6e90\u200b\u5de5\u5177\u5305\u200bpytorch-grad-cam\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p> <ul> <li>\u200b\u5b89\u88c5\u200b</li> </ul> <pre><code>pip install grad-cam\n</code></pre> <ul> <li>\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b</li> </ul> <pre><code>import torch\nfrom torchvision.models import vgg11,resnet18,resnet101,resnext101_32x8d\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\nmodel = vgg11(pretrained=True)\nimg_path = './dog.png'\n# resize\u200b\u64cd\u4f5c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u548c\u200b\u4f20\u5165\u795e\u7ecf\u200b\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u56fe\u7247\u5927\u5c0f\u200b\u4e00\u81f4\u200b\nimg = Image.open(img_path).resize((224,224))\n# \u200b\u9700\u8981\u200b\u5c06\u200b\u539f\u59cb\u200b\u56fe\u7247\u200b\u8f6c\u4e3a\u200bnp.float32\u200b\u683c\u5f0f\u200b\u5e76\u4e14\u200b\u5728\u200b0-1\u200b\u4e4b\u95f4\u200b \nrgb_img = np.float32(img)/255\nplt.imshow(img)\n</code></pre> <p></p> <pre><code>from pytorch_grad_cam import GradCAM,ScoreCAM,GradCAMPlusPlus,AblationCAM,XGradCAM,EigenCAM,FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# \u200b\u5c06\u200b\u56fe\u7247\u200b\u8f6c\u4e3a\u200btensor\nimg_tensor = torch.from_numpy(rgb_img).permute(2,0,1).unsqueeze(0)\n\ntarget_layers = [model.features[-1]]\n# \u200b\u9009\u53d6\u200b\u5408\u9002\u200b\u7684\u200b\u7c7b\u200b\u6fc0\u6d3b\u200b\u56fe\u200b\uff0c\u200b\u4f46\u662f\u200bScoreCAM\u200b\u548c\u200bAblationCAM\u200b\u9700\u8981\u200bbatch_size\ncam = GradCAM(model=model,target_layers=target_layers)\ntargets = [ClassifierOutputTarget(preds)]   \n# \u200b\u4e0a\u65b9\u200bpreds\u200b\u9700\u8981\u200b\u8bbe\u5b9a\u200b\uff0c\u200b\u6bd4\u5982\u200bImageNet\u200b\u6709\u200b1000\u200b\u7c7b\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u8bbe\u200b\u4e3a\u200b200\ngrayscale_cam = cam(input_tensor=img_tensor, targets=targets)\ngrayscale_cam = grayscale_cam[0, :]\ncam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\nprint(type(cam_img))\nImage.fromarray(cam_img)\n</code></pre> <p></p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/#724-flashtorchcnn","title":"7.2.4 \u200b\u4f7f\u7528\u200bFlashTorch\u200b\u5feb\u901f\u200b\u5b9e\u73b0\u200bCNN\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u806a\u660e\u200b\u7684\u200b\u4f60\u200b\u53ef\u80fd\u200b\u8981\u200b\u95ee\u200b\u4e86\u200b\uff0c\u200b\u5df2\u7ecf\u200b202x\u200b\u5e74\u200b\u4e86\u200b\uff0c\u200b\u96be\u9053\u200b\u8fd8\u8981\u200b\u6211\u4eec\u200b\u624b\u628a\u624b\u200b\u53bb\u200b\u5199\u200b\u5404\u79cd\u200bCNN\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u4ee3\u7801\u200b\u5417\u200b\uff1f\u200b\u7b54\u6848\u200b\u5f53\u7136\u200b\u662f\u200b\u5426\u5b9a\u200b\u7684\u200b\u3002\u200b\u968f\u7740\u200bPyTorch\u200b\u793e\u533a\u200b\u7684\u200b\u52aa\u529b\u200b\uff0c\u200b\u76ee\u524d\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e0d\u5c11\u200b\u5f00\u6e90\u200b\u5de5\u5177\u200b\u80fd\u591f\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5feb\u901f\u200b\u5b9e\u73b0\u200bCNN\u200b\u53ef\u89c6\u5316\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u5176\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u2014\u2014FlashTorch\u3002</p> <p>\uff08\u200b\u6ce8\u200b\uff1a\u200b\u4f7f\u7528\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u8be5\u200bpackage\u200b\u5bf9\u200b\u73af\u5883\u200b\u6709\u200b\u8981\u6c42\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0b\u65b9\u200b\u4ee3\u7801\u8fd0\u884c\u200b\u62a5\u9519\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u4f5c\u8005\u200b\u7ed9\u51fa\u200b\u7684\u200b\u914d\u7f6e\u200b\u6216\u8005\u200bColab\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff1ahttps://github.com/MisaOgura/flashtorch/issues/39\uff09</p> <ul> <li>\u200b\u5b89\u88c5\u200b</li> </ul> <pre><code>pip install flashtorch\n</code></pre> <ul> <li>\u200b\u53ef\u89c6\u5316\u200b\u68af\u5ea6\u200b</li> </ul> <pre><code># Download example images\n# !mkdir -p images\n# !wget -nv \\\n#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \\\n#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg   \\\n#    https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg    \\\n#    -P /content/images\n\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\nfrom flashtorch.utils import apply_transforms, load_image\nfrom flashtorch.saliency import Backprop\n\nmodel = models.alexnet(pretrained=True)\nbackprop = Backprop(model)\n\nimage = load_image('/content/images/great_grey_owl.jpg')\nowl = apply_transforms(image)\n\ntarget_class = 24\nbackprop.visualize(owl, target_class, guided=True, use_gpu=True)\n</code></pre> <p></p> <ul> <li>\u200b\u53ef\u89c6\u5316\u200b\u5377\u79ef\u200b\u6838\u200b</li> </ul> <pre><code>import torchvision.models as models\nfrom flashtorch.activmax import GradientAscent\n\nmodel = models.vgg16(pretrained=True)\ng_ascent = GradientAscent(model.features)\n\n# specify layer and filter info\nconv5_1 = model.features[24]\nconv5_1_filters = [45, 271, 363, 489]\n\ng_ascent.visualize(conv5_1, conv5_1_filters, title=\"VGG16: conv5_1\")\n</code></pre> <p></p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.2%20CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E8%A7%86%E5%8C%96/#_1","title":"\u53c2\u8003\u8d44\u6599","text":"<ol> <li>https://andrewhuman.github.io/cnn-hidden-layout_search  </li> <li>https://cloud.tencent.com/developer/article/1747222</li> <li>https://github.com/jacobgil/pytorch-grad-cam  </li> <li>https://github.com/MisaOgura/flashtorch  </li> </ol>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/","title":"7.3 \u200b\u4f7f\u7528\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b","text":"<p>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u626e\u6f14\u7740\u200b\u91cd\u8981\u200b\u7684\u200b\u89d2\u8272\u200b\u3002\u200b\u5b66\u4e60\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u627e\u5230\u200b\u6700\u4f18\u200b\u7684\u200b\u70b9\u200b\u4f5c\u4e3a\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u8f93\u51fa\u200b\u4ea7\u7269\u200b\u3002\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u7ed3\u5408\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u7ed8\u5236\u200b\u4e24\u6761\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u66f2\u7ebf\u200b\u6765\u200b\u786e\u5b9a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7ec8\u70b9\u200b\uff0c\u200b\u627e\u5230\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6a21\u578b\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\u3002\u200b\u90a3\u4e48\u200b\u9664\u4e86\u200b\u8bb0\u5f55\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u6bcf\u4e2a\u200bepoch\u200b\u7684\u200bloss\u200b\u503c\u200b\uff0c\u200b\u80fd\u5426\u200b\u5b9e\u65f6\u200b\u89c2\u5bdf\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u66f2\u7ebf\u200b\u7684\u200b\u53d8\u5316\u200b\uff0c\u200b\u53ca\u65f6\u200b\u6355\u6349\u200b\u6a21\u578b\u200b\u7684\u200b\u53d8\u5316\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5e0c\u671b\u200b\u53ef\u89c6\u5316\u200b\u5176\u4ed6\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5982\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\uff08\u200b\u5c24\u5176\u200b\u662f\u200b\u56fe\u7247\u200b\uff09\u3001\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3001\u200b\u53c2\u6570\u200b\u5206\u5e03\u200b\u7b49\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200bdebug\u200b\u4e2d\u200b\u67e5\u627e\u200b\u95ee\u9898\u200b\u6765\u6e90\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\uff08\u200b\u6bd4\u5982\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u548c\u200b\u6211\u4eec\u200b\u60f3\u8c61\u200b\u7684\u200b\u662f\u5426\u200b\u4e00\u81f4\u200b\uff09\u3002</p> <p>TensorBoard\u200b\u4f5c\u4e3a\u200b\u4e00\u6b3e\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200b\u80fd\u591f\u200b\u6ee1\u8db3\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u5404\u79cd\u200b\u9700\u6c42\u200b\u3002TensorBoard\u200b\u7531\u200bTensorFlow\u200b\u56e2\u961f\u200b\u5f00\u53d1\u200b\uff0c\u200b\u6700\u65e9\u200b\u548c\u200bTensorFlow\u200b\u914d\u5408\u200b\u4f7f\u7528\u200b\uff0c\u200b\u540e\u6765\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u4e8e\u200b\u5404\u79cd\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u4e2d\u6765\u200b\u3002\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u63a2\u7d22\u200bTensorBoard\u200b\u7684\u200b\u5f3a\u5927\u200b\u529f\u80fd\u200b\uff0c\u200b\u5e0c\u671b\u200b\u5e2e\u52a9\u200b\u8bfb\u8005\u200b\u201c\u200b\u4ece\u200b\u5165\u95e8\u200b\u5230\u200b\u7cbe\u901a\u200b\u201d\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5b89\u88c5\u200bTensorBoard\u200b\u5de5\u5177\u200b</li> <li>\u200b\u4e86\u89e3\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u57fa\u672c\u200b\u903b\u8f91\u200b</li> <li>\u200b\u638c\u63e1\u200b\u5229\u7528\u200bTensorBoard\u200b\u5b9e\u73b0\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u53ef\u89c6\u5316\u200b</li> <li>\u200b\u638c\u63e1\u200b\u5229\u7528\u200bTensorBoard\u200b\u5b8c\u6210\u200b\u5176\u4ed6\u200b\u5185\u5bb9\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b</li> </ul>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#731-tensorboard","title":"7.3.1 TensorBoard\u200b\u5b89\u88c5","text":"<p>\u200b\u5728\u200b\u5df2\u200b\u5b89\u88c5\u200bPyTorch\u200b\u7684\u200b\u73af\u5883\u200b\u4e0b\u200b\u4f7f\u7528\u200bpip\u200b\u5b89\u88c5\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>pip install tensorboardX\n</code></pre> <p>\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPyTorch\u200b\u81ea\u5e26\u200b\u7684\u200btensorboard\u200b\u5de5\u5177\u200b\uff0c\u200b\u6b64\u65f6\u200b\u4e0d\u200b\u9700\u8981\u200b\u989d\u5916\u200b\u5b89\u88c5\u200btensorboard\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#732-tensorboard","title":"7.3.2 TensorBoard\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u57fa\u672c\u200b\u903b\u8f91","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200bTensorBoard\u200b\u770b\u505a\u200b\u4e00\u4e2a\u200b\u8bb0\u5f55\u5458\u200b\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u8bb0\u5f55\u200b\u6211\u4eec\u200b\u6307\u5b9a\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u5305\u62ec\u200b\u6a21\u578b\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200bfeature map\uff0c\u200b\u6743\u91cd\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8bad\u7ec3\u200bloss\u200b\u7b49\u7b49\u200b\u3002TensorBoard\u200b\u5c06\u200b\u8bb0\u5f55\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4fdd\u5b58\u200b\u5728\u200b\u4e00\u4e2a\u200b\u7528\u6237\u200b\u6307\u5b9a\u200b\u7684\u200b\u6587\u4ef6\u5939\u200b\u91cc\u200b\uff0c\u200b\u7a0b\u5e8f\u200b\u4e0d\u65ad\u200b\u8fd0\u884c\u200b\u4e2d\u200bTensorBoard\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u8bb0\u5f55\u200b\u3002\u200b\u8bb0\u5f55\u200b\u4e0b\u200b\u7684\u200b\u5185\u5bb9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7f51\u9875\u200b\u7684\u200b\u5f62\u5f0f\u200b\u52a0\u4ee5\u200b\u53ef\u89c6\u5316\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#733-tensorboard","title":"7.3.3 TensorBoard\u200b\u7684\u200b\u914d\u7f6e\u200b\u4e0e\u200b\u542f\u52a8","text":"<p>\u200b\u5728\u200b\u4f7f\u7528\u200bTensorBoard\u200b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u6307\u5b9a\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u5939\u200b\u4f9b\u200bTensorBoard\u200b\u4fdd\u5b58\u200b\u8bb0\u5f55\u4e0b\u6765\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u7136\u540e\u200b\u8c03\u7528\u200btensorboard\u200b\u4e2d\u200b\u7684\u200bSummaryWriter\u200b\u4f5c\u4e3a\u200b\u4e0a\u8ff0\u200b\u201c\u200b\u8bb0\u5f55\u5458\u200b\u201d</p> <pre><code>from tensorboardX import SummaryWriter\n\nwriter = SummaryWriter('./runs')\n</code></pre> <p>\u200b\u4e0a\u9762\u200b\u7684\u200b\u64cd\u4f5c\u200b\u5b9e\u4f8b\u200b\u5316\u200bSummaryWritter\u200b\u4e3a\u200b\u53d8\u91cf\u200bwriter\uff0c\u200b\u5e76\u200b\u6307\u5b9a\u200bwriter\u200b\u7684\u200b\u8f93\u51fa\u200b\u76ee\u5f55\u200b\u4e3a\u200b\u5f53\u524d\u76ee\u5f55\u200b\u4e0b\u200b\u7684\u200b\"runs\"\u200b\u76ee\u5f55\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u4e4b\u540e\u200btensorboard\u200b\u8bb0\u5f55\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u90fd\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5728\u200bruns\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f7f\u7528\u200bPyTorch\u200b\u81ea\u5e26\u200b\u7684\u200btensorboard\uff0c\u200b\u5219\u200b\u91c7\u7528\u200b\u5982\u4e0b\u200b\u65b9\u5f0f\u200bimport\uff1a</p> <pre><code>from torch.utils.tensorboard import SummaryWriter\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u806a\u660e\u200b\u7684\u200b\u4f60\u200b\u53ef\u80fd\u200b\u53d1\u73b0\u200b\u4e86\u200b\uff0c\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u624b\u52a8\u200b\u5f80\u200bruns\u200b\u6587\u4ef6\u5939\u200b\u91cc\u200b\u6dfb\u52a0\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u6216\u8005\u200b\u628a\u200bruns\u200b\u6587\u4ef6\u5939\u200b\u91cc\u200b\u7684\u200b\u6570\u636e\u200b\u653e\u5230\u200b\u5176\u4ed6\u200b\u673a\u5668\u200b\u4e0a\u200b\u53ef\u89c6\u5316\u200b\u5462\u200b\uff1f\u200b\u7b54\u6848\u200b\u662f\u200b\u53ef\u4ee5\u200b\u7684\u200b\u3002\u200b\u53ea\u8981\u200b\u6570\u636e\u200b\u88ab\u200b\u8bb0\u5f55\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u5206\u4eab\u200b\u7ed9\u200b\u5176\u4ed6\u4eba\u200b\uff0c\u200b\u5176\u4ed6\u4eba\u200b\u5728\u200b\u5b89\u88c5\u200b\u4e86\u200btensorboard\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5c31\u200b\u4f1a\u200b\u770b\u5230\u200b\u4f60\u200b\u5206\u4eab\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u542f\u52a8\u200btensorboard\u200b\u4e5f\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u8f93\u5165\u200b</p> <pre><code>tensorboard --logdir=/path/to/logs/ --port=xxxx\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\u201cpath/to/logs/\"\u200b\u662f\u200b\u6307\u5b9a\u200b\u7684\u200b\u4fdd\u5b58\u200btensorboard\u200b\u8bb0\u5f55\u200b\u7ed3\u679c\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff08\u200b\u7b49\u4ef7\u200b\u4e8e\u200b\u4e0a\u9762\u200b\u7684\u200b\u201c./runs\"\uff0cport\u200b\u662f\u200b\u5916\u90e8\u200b\u8bbf\u95ee\u200bTensorBoard\u200b\u7684\u200b\u7aef\u53e3\u53f7\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200bip:port\u200b\u8bbf\u95ee\u200btensorboard\uff0c\u200b\u8fd9\u4e00\u200b\u64cd\u4f5c\u200b\u548c\u200bjupyter notebook\u200b\u7684\u200b\u4f7f\u7528\u200b\u7c7b\u4f3c\u200b\u3002\u200b\u5982\u679c\u200b\u4e0d\u662f\u200b\u5728\u200b\u670d\u52a1\u5668\u200b\u8fdc\u7a0b\u200b\u4f7f\u7528\u200b\u7684\u8bdd\u200b\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\u914d\u7f6e\u200bport\u3002</p> <p>\u200b\u6709\u65f6\u200b\uff0c\u200b\u4e3a\u4e86\u200btensorboard\u200b\u80fd\u591f\u200b\u4e0d\u65ad\u200b\u5730\u200b\u5728\u200b\u540e\u53f0\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bnohup\u200b\u547d\u4ee4\u200b\u6216\u8005\u200btmux\u200b\u5de5\u5177\u200b\u6765\u200b\u8fd0\u884c\u200btensorboard\u3002\u200b\u5927\u5bb6\u200b\u53ef\u4ee5\u200b\u81ea\u884c\u200b\u641c\u7d22\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4e0d\u200b\u5c55\u5f00\u8ba8\u8bba\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6a21\u62df\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u6765\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5229\u7528\u200bTensorBoard\u200b\u53ef\u89c6\u5316\u200b\u5176\u4e2d\u200b\u7684\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#734-tensorboard","title":"7.3.4 TensorBoard\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u9996\u5148\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)\n        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)\n        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))\n        self.flatten = nn.Flatten()\n        self.linear1 = nn.Linear(64,32)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(32,1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        x = self.pool(x)\n        x = self.adaptive_pool(x)\n        x = self.flatten(x)\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        y = self.sigmoid(x)\n        return y\n\nmodel = Net()\nprint(model)\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Net(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear1): Linear(in_features=64, out_features=32, bias=True)\n  (relu): ReLU()\n  (linear2): Linear(in_features=32, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)\n</code></pre> <p>\u200b\u53ef\u89c6\u5316\u200b\u6a21\u578b\u200b\u7684\u200b\u601d\u8def\u200b\u548c\u200b7.1\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4e00\u6837\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u7ed9\u5b9a\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\uff0c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u540e\u200b\u5f97\u5230\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u518d\u200b\u901a\u8fc7\u200bTensorBoard\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u4f7f\u7528\u200badd_graph\uff1a</p> <pre><code>writer.add_graph(model, input_to_model = torch.rand(1, 3, 224, 224))\nwriter.close()\n</code></pre> <p>\u200b\u5c55\u793a\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff08\u200b\u5176\u4e2d\u200b\u6846\u5185\u200b\u90e8\u5206\u200b\u521d\u59cb\u200b\u4f1a\u200b\u663e\u793a\u200b\u4e3a\u200b\u201cNet\"\uff0c\u200b\u9700\u8981\u200b\u53cc\u51fb\u200b\u540e\u200b\u624d\u200b\u4f1a\u200b\u5c55\u5f00\u200b\uff09\uff1a</p> <p></p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#735-tensorboard","title":"7.3.5 TensorBoard\u200b\u56fe\u50cf\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u505a\u200b\u56fe\u50cf\u200b\u76f8\u5173\u200b\u7684\u200b\u4efb\u52a1\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u65b9\u4fbf\u200b\u5730\u200b\u5c06\u200b\u6240\u200b\u5904\u7406\u200b\u7684\u200b\u56fe\u7247\u200b\u5728\u200btensorboard\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\u5c55\u793a\u200b\u3002</p> <ul> <li>\u200b\u5bf9\u4e8e\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u663e\u793a\u200b\u4f7f\u7528\u200badd_image</li> <li>\u200b\u5bf9\u4e8e\u200b\u591a\u200b\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u663e\u793a\u200b\u4f7f\u7528\u200badd_images</li> <li>\u200b\u6709\u65f6\u200b\u9700\u8981\u200b\u4f7f\u7528\u200btorchvision.utils.make_grid\u200b\u5c06\u200b\u591a\u200b\u5f20\u200b\u56fe\u7247\u200b\u62fc\u6210\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u540e\u200b\uff0c\u200b\u7528\u200bwriter.add_image\u200b\u663e\u793a\u200b</li> </ul> <p>\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200btorchvision\u200b\u7684\u200bCIFAR10\u200b\u6570\u636e\u200b\u96c6\u4e3a\u4f8b\u200b\uff1a</p> <pre><code>import torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntransform_train = transforms.Compose(\n    [transforms.ToTensor()])\ntransform_test = transforms.Compose(\n    [transforms.ToTensor()])\n\ntrain_data = datasets.CIFAR10(\".\", train=True, download=True, transform=transform_train)\ntest_data = datasets.CIFAR10(\".\", train=False, download=True, transform=transform_test)\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64)\n\nimages, labels = next(iter(train_loader))\n\n# \u200b\u4ec5\u200b\u67e5\u770b\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\nwriter = SummaryWriter('./pytorch_tb')\nwriter.add_image('images[0]', images[0])\nwriter.close()\n\n# \u200b\u5c06\u200b\u591a\u200b\u5f20\u200b\u56fe\u7247\u200b\u62fc\u63a5\u200b\u6210\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u7528\u200b\u9ed1\u8272\u200b\u7f51\u683c\u200b\u5206\u5272\u200b\n# create grid of images\nwriter = SummaryWriter('./pytorch_tb')\nimg_grid = torchvision.utils.make_grid(images)\nwriter.add_image('image_grid', img_grid)\nwriter.close()\n\n# \u200b\u5c06\u200b\u591a\u200b\u5f20\u200b\u56fe\u7247\u200b\u76f4\u63a5\u200b\u5199\u5165\u200b\nwriter = SummaryWriter('./pytorch_tb')\nwriter.add_images(\"images\",images,global_step = 0)\nwriter.close()\n</code></pre> <p>\u200b\u4f9d\u6b21\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u4e09\u7ec4\u200b\u53ef\u89c6\u5316\u200b\uff08\u200b\u6ce8\u610f\u200b\u4e0d\u8981\u200b\u540c\u65f6\u200b\u5728\u200bnotebook\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5355\u5143\u683c\u200b\u5185\u200b\u8fd0\u884c\u200b\uff09\uff0c\u200b\u5f97\u5230\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff08\u200b\u6700\u540e\u200b\u8fd0\u884c\u200b\u7684\u200b\u7ed3\u679c\u200b\u5728\u200b\u6700\u200b\u4e0a\u9762\u200b\uff09\uff1a</p> <p></p> <p></p> <p></p> <p>\u200b\u53e6\u5916\u200b\u6ce8\u610f\u200b\u4e0a\u65b9\u200bmenu\u200b\u90e8\u5206\u200b\uff0c\u200b\u521a\u521a\u200b\u53ea\u6709\u200b\u201cGRAPHS\"\u200b\u680f\u200b\u5bf9\u5e94\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u73b0\u5728\u200b\u5219\u200b\u591a\u51fa\u200b\u4e86\u200b\u201dIMAGES\u201c\u200b\u680f\u200b\u5bf9\u5e94\u200b\u56fe\u50cf\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u3002\u200b\u5de6\u4fa7\u200b\u7684\u200b\u6ed1\u52a8\u200b\u6309\u94ae\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u7684\u200b\u4eae\u5ea6\u200b\u548c\u200b\u5bf9\u6bd4\u5ea6\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u9664\u4e86\u200b\u53ef\u89c6\u5316\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\uff0cTensorBoard\u200b\u63d0\u4f9b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u65b9\u6848\u200b\u81ea\u7136\u200b\u4e5f\u200b\u9002\u7528\u200b\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200bPython\u200b\u4e2d\u200b\u7528\u200bmatplotlib\u200b\u7b49\u200b\u5de5\u5177\u200b\u7ed8\u5236\u200b\u7684\u200b\u5176\u4ed6\u200b\u56fe\u50cf\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5c55\u793a\u200b\u5206\u6790\u200b\u7ed3\u679c\u200b\u7b49\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#736-tensorboard","title":"7.3.6 TensorBoard\u200b\u8fde\u7eed\u53d8\u91cf\u200b\u53ef\u89c6\u5316","text":"<p>TensorBoard\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u53ef\u89c6\u5316\u200b\u8fde\u7eed\u53d8\u91cf\u200b\uff08\u200b\u6216\u200b\u65f6\u5e8f\u200b\u53d8\u91cf\u200b\uff09\u200b\u7684\u200b\u53d8\u5316\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u901a\u8fc7\u200badd_scalar\u200b\u5b9e\u73b0\u200b\uff1a</p> <pre><code>writer = SummaryWriter('./pytorch_tb')\nfor i in range(500):\n    x = i\n    y = x**2\n    writer.add_scalar(\"x\", x, i) #\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u8bb0\u5f55\u200bx\u200b\u5728\u200b\u7b2c\u200bstep i \u200b\u7684\u200b\u503c\u200b\n    writer.add_scalar(\"y\", y, i) #\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u8bb0\u5f55\u200by\u200b\u5728\u200b\u7b2c\u200bstep i \u200b\u7684\u200b\u503c\u200b\nwriter.close()\n</code></pre> <p>\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p> <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u5728\u200b\u540c\u200b\u4e00\u5f20\u200b\u56fe\u4e2d\u200b\u663e\u793a\u200b\u591a\u4e2a\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u5206\u522b\u200b\u5efa\u7acb\u200b\u5b58\u653e\u200b\u5b50\u200b\u8def\u5f84\u200b\uff08\u200b\u4f7f\u7528\u200bSummaryWriter\u200b\u6307\u5b9a\u200b\u8def\u5f84\u200b\u5373\u53ef\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\uff0c\u200b\u4f46\u200b\u9700\u8981\u200b\u5728\u200btensorboard\u200b\u8fd0\u884c\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff09\uff0c\u200b\u540c\u65f6\u200b\u5728\u200badd_scalar\u200b\u4e2d\u200b\u4fee\u6539\u200b\u66f2\u7ebf\u200b\u7684\u200b\u6807\u7b7e\u200b\u4f7f\u200b\u5176\u200b\u4e00\u81f4\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code>writer1 = SummaryWriter('./pytorch_tb/x')\nwriter2 = SummaryWriter('./pytorch_tb/y')\nfor i in range(500):\n    x = i\n    y = x*2\n    writer1.add_scalar(\"same\", x, i) #\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u8bb0\u5f55\u200bx\u200b\u5728\u200b\u7b2c\u200bstep i \u200b\u7684\u200b\u503c\u200b\n    writer2.add_scalar(\"same\", y, i) #\u200b\u65e5\u5fd7\u200b\u4e2d\u200b\u8bb0\u5f55\u200by\u200b\u5728\u200b\u7b2c\u200bstep i \u200b\u7684\u200b\u503c\u200b\nwriter1.close()\nwriter2.close()\n</code></pre> <p></p> <p>\u200b\u8fd9\u91cc\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e00\u4e2a\u200bwriter\uff0c\u200b\u4f46\u200bfor\u200b\u5faa\u73af\u200b\u4e2d\u200b\u4e0d\u65ad\u200b\u521b\u5efa\u200bSummaryWriter\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u597d\u200b\u9009\u9879\u200b\u3002\u200b\u6b64\u65f6\u200b\u5de6\u4e0b\u89d2\u200b\u7684\u200bRuns\u200b\u90e8\u5206\u200b\u51fa\u73b0\u200b\u4e86\u200b\u52fe\u200b\u9009\u9879\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u66f2\u7ebf\u200b\u3002\u200b\u66f2\u7ebf\u200b\u540d\u79f0\u200b\u5bf9\u5e94\u200b\u5b58\u653e\u200b\u5b50\u200b\u8def\u5f84\u200b\u7684\u200b\u540d\u79f0\u200b\uff08\u200b\u8fd9\u91cc\u200b\u662f\u200bx\u200b\u548c\u200by\uff09\u3002</p> <p>\u200b\u8fd9\u90e8\u5206\u200b\u529f\u80fd\u200b\u975e\u5e38\u9002\u5408\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u66f4\u52a0\u200b\u76f4\u89c2\u200b\u5730\u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4ece\u800c\u200b\u786e\u5b9a\u200b\u6700\u4f73\u200b\u7684\u200bcheckpoint\u3002\u200b\u5de6\u4fa7\u200b\u7684\u200bSmoothing\u200b\u6ed1\u52a8\u200b\u6309\u94ae\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u66f2\u7ebf\u200b\u7684\u200b\u5e73\u6ed1\u200b\u5ea6\u200b\uff0c\u200b\u5f53\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u9707\u8361\u200b\u8f83\u5927\u200b\u65f6\u200b\uff0c\u200b\u5c06\u200bSmoothing\u200b\u8c03\u5927\u200b\u6709\u52a9\u4e8e\u200b\u89c2\u5bdf\u200bloss\u200b\u7684\u200b\u6574\u4f53\u200b\u53d8\u5316\u8d8b\u52bf\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#737-tensorboard","title":"7.3.7 TensorBoard\u200b\u53c2\u6570\u200b\u5206\u5e03\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b\u53c2\u6570\u200b\uff08\u200b\u6216\u200b\u5411\u91cf\u200b\uff09\u200b\u7684\u200b\u53d8\u5316\u200b\uff0c\u200b\u6216\u8005\u200b\u5bf9\u200b\u5176\u200b\u5206\u5e03\u200b\u8fdb\u884c\u200b\u7814\u7a76\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u65b9\u4fbf\u200b\u5730\u7528\u200bTensorBoard\u200b\u6765\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u901a\u8fc7\u200badd_histogram\u200b\u5b9e\u73b0\u200b\u3002\u200b\u4e0b\u9762\u200b\u7ed9\u51fa\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code>import torch\nimport numpy as np\n\n# \u200b\u521b\u5efa\u200b\u6b63\u6001\u5206\u5e03\u200b\u7684\u200b\u5f20\u91cf\u200b\u6a21\u62df\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\ndef norm(mean, std):\n    t = std * torch.randn((100, 20)) + mean\n    return t\n\nwriter = SummaryWriter('./pytorch_tb/')\nfor step, mean in enumerate(range(-10, 10, 1)):\n    w = norm(mean, 1)\n    writer.add_histogram(\"w\", w, step)\n    writer.flush()\nwriter.close()\n</code></pre> <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <p></p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#738-tensorboard","title":"7.3.8 \u200b\u670d\u52a1\u5668\u7aef\u200b\u4f7f\u7528\u200bTensorBoard","text":"<p>\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8fde\u63a5\u200b\u8fdc\u7a0b\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u6765\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u670d\u52a1\u5668\u7aef\u200b\u662f\u200b\u6ca1\u6709\u200b\u6d4f\u89c8\u5668\u200b\u7684\u200b\uff08\u200b\u7eaf\u200b\u547d\u4ee4\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u76f8\u5e94\u200b\u7684\u200b\u914d\u7f6e\u200b\uff0c\u200b\u624d\u200b\u53ef\u4ee5\u200b\u5728\u200b\u672c\u5730\u200b\u6d4f\u89c8\u5668\u200b\uff0c\u200b\u4f7f\u7528\u200btensorboard\u200b\u67e5\u770b\u200b\u670d\u52a1\u5668\u200b\u8fd0\u884c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u3002 \u200b\u672c\u6587\u200b\u63d0\u4f9b\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\uff0c\u200b\u5176\u4e2d\u200b\u524d\u200b\u4e24\u79cd\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u662f\u200b\u5efa\u7acb\u200bSSH\u200b\u96a7\u9053\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u8fdc\u7a0b\u200b\u7aef\u53e3\u200b\u5230\u200b\u672c\u673a\u200b\u7aef\u53e3\u200b\u7684\u200b\u8f6c\u53d1\u200b\uff0c\u200b\u6700\u540e\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u9002\u7528\u200b\u4e8e\u200b\u6ca1\u6709\u200b\u4e0b\u8f7d\u200bXshell\u200b\u7b49\u200bSSH\u200b\u8fde\u63a5\u200b\u5de5\u5177\u200b\u7684\u200b\u7528\u6237\u200b - MobaXterm     1. \u200b\u5728\u200bMobaXterm\u200b\u70b9\u51fb\u200bTunneling     2. \u200b\u9009\u62e9\u200bNew SSH tunnel\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4ee5\u4e0b\u200b\u754c\u9762\u200b\u3002</p> <pre><code>![ssh_tunnel](./figures/ssh_tunnel_UI.png)\n3. \u200b\u5bf9\u200b\u65b0\u5efa\u200b\u7684\u200bSSH\u200b\u901a\u9053\u200b\u505a\u200b\u4ee5\u4e0b\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u7b2c\u4e00\u200b\u680f\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b`Local port forwarding`\uff0c`&lt; Remote Server&gt;`\u200b\u6211\u4eec\u200b\u586b\u5199\u200b**localhost**\uff0c`&lt; Remote port&gt;`\u200b\u586b\u5199\u200b6006\uff0ctensorboard\u200b\u9ed8\u8ba4\u200b\u4f1a\u200b\u5728\u200b6006\u200b\u7aef\u53e3\u200b\u8fdb\u884c\u200b\u663e\u793a\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b **tensorboard --logdir=/path/to/logs/ --port=xxxx**\u200b\u7684\u200b\u547d\u4ee4\u200b\u4e2d\u200b\u7684\u200bport\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\uff0c`&lt; SSH server&gt;` \u200b\u586b\u5199\u200b\u6211\u4eec\u200b\u8fde\u63a5\u200b\u670d\u52a1\u5668\u200b\u7684\u200bip\u200b\u5730\u5740\u200b\uff0c`&lt;SSH login&gt;`\u200b\u586b\u5199\u200b\u6211\u4eec\u200b\u8fde\u63a5\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u7684\u200b\u7528\u6237\u540d\u200b\uff0c`&lt;SSH port&gt;`\u200b\u586b\u5199\u200b\u7aef\u53e3\u53f7\u200b\uff08\u200b\u901a\u5e38\u200b\u4e3a\u200b22\uff09\uff0c`&lt; forwarded port&gt;`\u200b\u586b\u5199\u200b\u7684\u200b\u662f\u200b\u672c\u5730\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7aef\u53e3\u53f7\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u540e\u9762\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8bbf\u95ee\u200b\u3002\n4. \u200b\u8bbe\u5b9a\u200b\u597d\u200b\u4e4b\u540e\u200b\uff0c\u200b\u70b9\u51fb\u200bSave\uff0c\u200b\u7136\u540e\u200bStart\u3002\u200b\u5728\u200b\u542f\u52a8\u200btensorboard\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u672c\u5730\u200b\u7684\u200b\u6d4f\u89c8\u5668\u200b\u8f93\u5165\u200b`http://localhost:6006/`\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8bbf\u95ee\u200b\u4e86\u200b\n</code></pre> <ul> <li>Xshell <ol> <li>Xshell\u200b\u7684\u200b\u8fde\u63a5\u200b\u65b9\u6cd5\u200b\u4e0e\u200bMobaXterm\u200b\u7684\u200b\u8fde\u63a5\u200b\u65b9\u5f0f\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\uff0c\u200b\u5177\u4f53\u64cd\u4f5c\u200b\u5982\u4e0b\u200b\uff1a</li> <li>\u200b\u8fde\u63a5\u200b\u4e0a\u200b\u670d\u52a1\u5668\u200b\u540e\u200b\uff0c\u200b\u6253\u5f00\u200b\u5f53\u524d\u200b\u4f1a\u8bdd\u200b\u5c5e\u6027\u200b\uff0c\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e0b\u56fe\u200b\uff0c\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u96a7\u9053\u200b\uff0c\u200b\u70b9\u51fb\u200b\u6dfb\u52a0\u200b </li> <li>\u200b\u6309\u7167\u200b\u4e0b\u65b9\u200b\u56fe\u200b\u8fdb\u884c\u200b\u9009\u62e9\u200b\uff0c\u200b\u5176\u4e2d\u200b\u76ee\u6807\u200b\u4e3b\u673a\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u670d\u52a1\u5668\u200b\uff0c\u200b\u6e90\u200b\u4e3b\u673a\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u672c\u5730\u200b\uff0c\u200b\u7aef\u53e3\u200b\u7684\u200b\u9009\u62e9\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u60c5\u51b5\u200b\u800c\u5b9a\u200b\u3002 </li> <li>\u200b\u542f\u52a8\u200btensorboard\uff0c\u200b\u5728\u200b\u672c\u5730\u200b127.0.0.1:6006 \u200b\u6216\u8005\u200b localhost:6006\u200b\u8fdb\u884c\u200b\u8bbf\u95ee\u200b\u3002</li> </ol> </li> <li>SSH<ol> <li>\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5c06\u200b\u670d\u52a1\u5668\u200b\u7684\u200b6006\u200b\u7aef\u53e3\u200b\u91cd\u5b9a\u5411\u200b\u5230\u200b\u81ea\u5df1\u200b\u673a\u5668\u200b\u4e0a\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u672c\u5730\u200b\u7684\u200b\u7ec8\u7aef\u200b\u91cc\u200b\u8f93\u5165\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\uff1a\u200b\u5176\u4e2d\u200b16006\u200b\u4ee3\u8868\u200b\u6620\u5c04\u200b\u5230\u200b\u672c\u5730\u200b\u7684\u200b\u7aef\u53e3\u200b\uff0c6006\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u7684\u200b\u7aef\u53e3\u200b\u3002 <code>shell   ssh -L 16006:127.0.0.1:6006 username@remote_server_ip</code></li> <li>\u200b\u5728\u200b\u670d\u52a1\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u7684\u200b6006\u200b\u7aef\u53e3\u200b\u6b63\u5e38\u200b\u542f\u52a8\u200btensorboard <code>shell tensorboard --logdir=xxx --port=6006</code></li> <li>\u200b\u5728\u200b\u672c\u5730\u200b\u7684\u200b\u6d4f\u89c8\u5668\u200b\u8f93\u5165\u200b\u5730\u5740\u200b <code>shell 127.0.0.1:16006 \u200b\u6216\u8005\u200b localhost:16006</code></li> </ol> </li> </ul>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#739","title":"7.3.9 \u200b\u603b\u7ed3","text":"<p>\u200b\u5bf9\u4e8e\u200bTensorBoard\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u529f\u80fd\u200b\u662f\u200b\u5f88\u200b\u5f3a\u5927\u200b\u7684\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8bb0\u5f55\u200b\u7684\u200b\u4e1c\u897f\u200b\u4e0d\u200b\u53ea\u200b\u9650\u4e8e\u200b\u672c\u8282\u200b\u6240\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u8303\u56f4\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u6848\u200b\u662f\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200bSummaryWriter\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b<code>add_XXX()</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u5176\u5b9e\u200bTensorBoard\u200b\u7684\u200b\u903b\u8f91\u200b\u8fd8\u662f\u200b\u5f88\u200b\u7b80\u5355\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u57fa\u672c\u200b\u903b\u8f91\u200b\u5c31\u662f\u200b\u6587\u4ef6\u200b\u7684\u200b\u8bfb\u5199\u200b\u903b\u8f91\u200b\uff0c\u200b\u5199\u5165\u200b\u60f3\u8981\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200bTensorBoard\u200b\u81ea\u5df1\u200b\u4f1a\u200b\u8bfb\u51fa\u6765\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.3%20%E4%BD%BF%E7%94%A8TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#_1","title":"\u53c2\u8003\u8d44\u6599","text":"<ol> <li>https://blog.csdn.net/Python_Ai_Road/article/details/107704530</li> </ol>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.4%20%E4%BD%BF%E7%94%A8wandb%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/","title":"7.4 \u200b\u4f7f\u7528\u200bwandb\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b","text":"<p>\u200b\u5728\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200bTensorboard\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u4f46\u662f\u200bTensorboard\u200b\u5bf9\u200b\u6570\u636e\u200b\u7684\u200b\u4fdd\u5b58\u200b\u4ec5\u9650\u4e8e\u200b\u672c\u5730\u200b\uff0c\u200b\u4e5f\u200b\u5f88\u200b\u96be\u200b\u5206\u6790\u200b\u8d85\u200b\u53c2\u6570\u200b\u4e0d\u540c\u200b\u5bf9\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002wandb\u200b\u7684\u200b\u51fa\u73b0\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u89e3\u51b3\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u672c\u200b\u7ae0\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bf9\u200bwandb\u200b\u8fdb\u884c\u200b\u7b80\u8981\u200b\u4ecb\u7ecd\u200b\u3002 wandb\u200b\u662f\u200bWeights &amp; Biases\u200b\u7684\u200b\u7f29\u5199\u200b\uff0c\u200b\u5b83\u200b\u80fd\u591f\u200b\u81ea\u52a8\u8bb0\u5f55\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u548c\u200b\u8f93\u51fa\u200b\u6307\u6807\u200b\uff0c\u200b\u7136\u540e\u200b\u53ef\u89c6\u5316\u200b\u548c\u200b\u6bd4\u8f83\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5e76\u200b\u5feb\u901f\u200b\u4e0e\u200b\u5176\u4ed6\u4eba\u200b\u5171\u4eab\u200b\u7ed3\u679c\u200b\u3002\u200b\u76ee\u524d\u200b\u5b83\u200b\u80fd\u591f\u200b\u548c\u200bJupyter\u3001TensorFlow\u3001Pytorch\u3001Keras\u3001Scikit\u3001fast.ai\u3001LightGBM\u3001XGBoost\u200b\u4e00\u8d77\u200b\u7ed3\u5408\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>wandb\u200b\u7684\u200b\u5b89\u88c5\u200b</li> <li>wandb\u200b\u7684\u200b\u4f7f\u7528\u200b</li> <li>demo\u200b\u6f14\u793a\u200b</li> </ul>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.4%20%E4%BD%BF%E7%94%A8wandb%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#741-wandb","title":"7.4.1 wandb\u200b\u7684\u200b\u5b89\u88c5","text":"<p>wandb\u200b\u7684\u200b\u5b89\u88c5\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4f7f\u7528\u200bpip\u200b\u5b89\u88c5\u200b\u5373\u53ef\u200b\u3002</p> <p><pre><code>pip install wandb\n</code></pre> \u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b\u5b98\u7f51\u200b\u6ce8\u518c\u200b\u4e00\u4e2a\u200b\u8d26\u53f7\u200b\u5e76\u200b\u590d\u5236\u200b\u4e0b\u200b\u81ea\u5df1\u200b\u7684\u200bAPI keys\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u672c\u5730\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u547d\u4ee4\u200b\u767b\u5f55\u200b\u3002</p> <p><pre><code>wandb login\n</code></pre> \u200b\u8fd9\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u770b\u5230\u200b\u4e0b\u9762\u200b\u7684\u200b\u754c\u9762\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u7c98\u8d34\u200b\u4f60\u200b\u7684\u200bAPI keys\u200b\u5373\u53ef\u200b\u3002 </p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.4%20%E4%BD%BF%E7%94%A8wandb%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#742-wandb","title":"7.4.2 wandb\u200b\u7684\u200b\u4f7f\u7528","text":"<p>wandb\u200b\u7684\u200b\u4f7f\u7528\u200b\u4e5f\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u3002</p> <pre><code>import wandb\nwandb.init(project='my-project', entity='my-name')\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200bproject\u200b\u548c\u200bentity\u200b\u662f\u200b\u4f60\u200b\u5728\u200bwandb\u200b\u4e0a\u200b\u521b\u5efa\u200b\u7684\u200b\u9879\u76ee\u540d\u79f0\u200b\u548c\u200b\u7528\u6237\u540d\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u521b\u5efa\u200b\u9879\u76ee\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u3002</p>"},{"location":"07-%E7%AC%AC%E4%B8%83%E7%AB%A0%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96/7.4%20%E4%BD%BF%E7%94%A8wandb%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#743-demo","title":"7.4.3 demo\u200b\u6f14\u793a","text":"<p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200bCIFAR10\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200bdemo\u200b\u6765\u200b\u6f14\u793a\u200bwandb\u200b\u7684\u200b\u4f7f\u7528\u200b\u3002</p> <p><pre><code>import random  # to set the python random seed\nimport numpy  # to set the numpy random seed\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> \u200b\u4f7f\u7528\u200bwandb\u200b\u7684\u200b\u7b2c\u4e00\u6b65\u200b\u662f\u200b\u521d\u59cb\u5316\u200bwandb\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bwandb.init()\u200b\u51fd\u6570\u200b\u6765\u200b\u521d\u59cb\u5316\u200bwandb\uff0c\u200b\u5176\u4e2d\u200bproject\u200b\u662f\u200b\u4f60\u200b\u5728\u200bwandb\u200b\u4e0a\u200b\u521b\u5efa\u200b\u7684\u200b\u9879\u76ee\u540d\u79f0\u200b\uff0cname\u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u540d\u79f0\u200b\u3002 <pre><code># \u200b\u521d\u59cb\u5316\u200bwandb\nimport wandb\nwandb.init(project=\"thorough-pytorch\",\n           name=\"wandb_demo\",)\n</code></pre> \u200b\u4f7f\u7528\u200bwandb\u200b\u7684\u200b\u7b2c\u4e8c\u6b65\u200b\u662f\u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bwandb.config\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200bwandb\u200b\u7684\u200b\u754c\u9762\u200b\u4e0a\u200b\u770b\u5230\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u53d8\u5316\u200b\u3002wandb.config\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u548c\u200b\u5b57\u5178\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bconfig.key\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p> <p><pre><code># \u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\nconfig = wandb.config  # config\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\nconfig.batch_size = 64  \nconfig.test_batch_size = 10 \nconfig.epochs = 5  \nconfig.lr = 0.01 \nconfig.momentum = 0.1  \nconfig.use_cuda = True  \nconfig.seed = 2043  \nconfig.log_interval = 10 \n\n# \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\ndef set_seed(seed):\n    random.seed(config.seed)      \n    torch.manual_seed(config.seed) \n    numpy.random.seed(config.seed) \n</code></pre> \u200b\u7b2c\u4e09\u6b65\u200b\u662f\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200bpipeline\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bpytorch\u200b\u7684\u200bCIFAR10\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200bresnet18\u200b\u6765\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200bpipeline\u3002 <pre><code>def train(model, device, train_loader, optimizer):\n    model.train()\n\n    for batch_id, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n# wandb.log\u200b\u7528\u6765\u200b\u8bb0\u5f55\u200b\u4e00\u4e9b\u200b\u65e5\u5fd7\u200b(accuracy,loss and epoch), \u200b\u4fbf\u4e8e\u200b\u968f\u65f6\u200b\u67e5\u770b\u200b\u7f51\u8def\u200b\u7684\u200b\u6027\u80fd\u200b\ndef test(model, device, test_loader, classes):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    example_images = []\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            criterion = nn.CrossEntropyLoss()\n            test_loss += criterion(output, target).item()\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            example_images.append(wandb.Image(\n                data[0], caption=\"Pred:{} Truth:{}\".format(classes[pred[0].item()], classes[target[0]])))\n\n   # \u200b\u4f7f\u7528\u200bwandb.log \u200b\u8bb0\u5f55\u200b\u4f60\u200b\u60f3\u200b\u8bb0\u5f55\u200b\u7684\u200b\u6307\u6807\u200b\n    wandb.log({\n        \"Examples\": example_images,\n        \"Test Accuracy\": 100. * correct / len(test_loader.dataset),\n        \"Test Loss\": test_loss\n    })\n\nwandb.watch_called = False \n\n\ndef main():\n    use_cuda = config.use_cuda and torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n\n    # \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\n    set_seed(config.seed)\n    torch.backends.cudnn.deterministic = True\n\n    # \u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    # \u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\n    train_loader = DataLoader(datasets.CIFAR10(\n        root='dataset',\n        train=True,\n        download=True,\n        transform=transform\n    ), batch_size=config.batch_size, shuffle=True, **kwargs)\n\n    test_loader = DataLoader(datasets.CIFAR10(\n        root='dataset',\n        train=False,\n        download=True,\n        transform=transform\n    ), batch_size=config.batch_size, shuffle=False, **kwargs)\n\n    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n    model = resnet18(pretrained=True).to(device)\n    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n\n    wandb.watch(model, log=\"all\")\n    for epoch in range(1, config.epochs + 1):\n        train(model, device, train_loader, optimizer)\n        test(model, device, test_loader, classes)\n\n    # \u200b\u672c\u5730\u200b\u548c\u200b\u4e91\u7aef\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\n    torch.save(model.state_dict(), 'model.pth')\n    wandb.save('model.pth')\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> \u200b\u5f53\u200b\u6211\u4eec\u200b\u8fd0\u884c\u200b\u5b8c\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200bwandb\u200b\u7684\u200b\u754c\u9762\u200b\u4e0a\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u4e86\u200b\u548c\u200b\u7cfb\u7edf\u200b\u7684\u200b\u6027\u80fd\u6307\u6807\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5728\u200bsetting\u200b\u91cc\u9762\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u5b8c\u7ed9\u200b\u6211\u4eec\u200b\u53d1\u9001\u200b\u90ae\u4ef6\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5b8c\u200b\u4e4b\u540e\u200b\u53ca\u65f6\u200b\u7684\u200b\u67e5\u770b\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u4e86\u200b\u3002  </p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u4f7f\u7528\u200bwandb\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u7684\u200b\u8bb0\u5f55\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\uff0c\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0cwandb\u200b\u8fd8\u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5f88\u591a\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u6bd4\u5982\u200b\uff1a\u200b\u6a21\u578b\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u641c\u7d22\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u7248\u672c\u63a7\u5236\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u7f72\u200b\u7b49\u7b49\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u529f\u80fd\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u66f4\u597d\u200b\u7684\u200b\u7ba1\u7406\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u66f4\u597d\u200b\u7684\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u8fed\u4ee3\u200b\u548c\u200b\u4f18\u5316\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u529f\u80fd\u200b\u6211\u4eec\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u66f4\u65b0\u200b\u4e2d\u200b\u4f1a\u200b\u8fdb\u884c\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.1%20%E6%9C%AC%E7%AB%A0%E7%AE%80%E4%BB%8B/","title":"8.1 \u200b\u672c\u7ae0\u200b\u7b80\u4ecb","text":"<p>\u200b\u606d\u559c\u200b\u4f60\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u524d\u9762\u200b\u4e03\u7ae0\u200b\u5185\u5bb9\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u9010\u6b65\u200b\u719f\u6089\u200b\u4e86\u200bPyTorch\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u80fd\u591f\u200b\u5b9a\u4e49\u200b\u548c\u200b\u4fee\u6539\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b66\u4f1a\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6280\u5de7\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u53ef\u89c6\u5316\u200b\u8f85\u52a9\u200bPyTorch\u200b\u7684\u200b\u4f7f\u7528\u200b\u3002</p> <p>PyTorch\u200b\u7684\u200b\u5f3a\u5927\u200b\u5e76\u200b\u4e0d\u4ec5\u200b\u5c40\u9650\u4e8e\u200b\u81ea\u8eab\u200b\u7684\u200b\u6613\u7528\u6027\u200b\uff0c\u200b\u66f4\u200b\u5728\u4e8e\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u56f4\u7ed5\u200bPyTorch\u200b\u6240\u200b\u4ea7\u751f\u200b\u7684\u200b\u4e00\u7cfb\u5217\u200b\u5de5\u5177\u5305\u200b\uff08\u200b\u4e00\u822c\u200b\u662f\u200bPython package\uff09\u200b\u548c\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u4f18\u79c0\u200b\u7684\u200b\u5de5\u5177\u5305\u200b\u6781\u5927\u200b\u5730\u65b9\u200b\u4fbf\u200b\u4e86\u200bPyTorch\u200b\u5728\u200b\u7279\u5b9a\u200b\u9886\u57df\u200b\u7684\u200b\u4f7f\u7528\u200b\u3002\u200b\u6bd4\u5982\u200b\u5bf9\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\uff0c\u200b\u6709\u200bTorchVision\u3001TorchVideo\u200b\u7b49\u200b\u7528\u4e8e\u200b\u56fe\u7247\u200b\u548c\u200b\u89c6\u9891\u200b\u5904\u7406\u200b\uff1b\u200b\u5bf9\u4e8e\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff0c\u200b\u6709\u200btorchtext\uff1b\u200b\u5bf9\u4e8e\u200b\u56fe\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\uff0c\u200b\u6709\u200bPyTorch Geometric \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u3002\u200b\u8fd9\u91cc\u200b\u53ea\u662f\u200b\u4e3e\u4f8b\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u9886\u57df\u200b\u8fd8\u6709\u200b\u5f88\u591a\u200b\u4f18\u79c0\u200b\u7684\u200b\u5de5\u5177\u5305\u200b\u4f9b\u200b\u793e\u533a\u200b\u4f7f\u7528\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5de5\u5177\u5305\u200b\u5171\u540c\u200b\u6784\u6210\u200b\u4e86\u200bPyTorch\u200b\u7684\u200b\u751f\u6001\u200b\uff08EcoSystem\uff09\u3002</p> <p>PyTorch\u200b\u751f\u6001\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u52a9\u529b\u200b\u4e86\u200bPyTorch\u200b\u7684\u200b\u63a8\u5e7f\u200b\u4e0e\u200b\u6210\u529f\u200b\u3002\u200b\u5728\u200b\u7279\u5b9a\u200b\u9886\u57df\u200b\u4f7f\u7528\u200bPyTorch\u200b\u751f\u6001\u200b\u4e2d\u200b\u7684\u200b\u5de5\u5177\u5305\u200b\uff0c\u200b\u80fd\u591f\u200b\u6781\u5927\u200b\u5730\u200b\u964d\u4f4e\u200b\u5165\u95e8\u200b\u95e8\u69db\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u590d\u73b0\u200b\u5df2\u6709\u200b\u7684\u200b\u5de5\u4f5c\u200b\u3002\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u5728\u200b\u8ba8\u8bba\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b\u65f6\u5019\u200b\u5c31\u7528\u5230\u200b\u4e86\u200btorchvision\u200b\u4e2d\u200b\u9884\u5b9a\u200b\u4e49\u200b\u7684\u200bresnet\u200b\u7ed3\u6784\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u81ea\u5df1\u200b\u91cd\u65b0\u200b\u7f16\u5199\u200b\u3002\u200b\u540c\u65f6\u200b\uff0cPyTorch\u200b\u751f\u6001\u200b\u6709\u52a9\u4e8e\u200b\u793e\u533a\u200b\u529b\u91cf\u200b\u7684\u200b\u52a0\u5165\u200b\uff0c\u200b\u5171\u540c\u200b\u4e3a\u200b\u793e\u533a\u200b\u63d0\u4f9b\u200b\u66f4\u200b\u6709\u200b\u4ef7\u503c\u200b\u7684\u200b\u5185\u5bb9\u200b\u548c\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u5f00\u6e90\u200b\u7406\u5ff5\u200b\u6240\u200b\u575a\u6301\u200b\u7684\u200b\u4ef7\u503c\u200b\u3002</p> <p>\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u9010\u6b65\u200b\u4ecb\u7ecd\u200bPyTorch\u200b\u751f\u6001\u200b\u5728\u200b\u56fe\u50cf\u200b\u3001\u200b\u89c6\u9891\u200b\u3001\u200b\u6587\u672c\u200b\u7b49\u200b\u9886\u57df\u200b\u4e2d\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u9488\u5bf9\u200b\u67d0\u4e2a\u200b\u9886\u57df\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u5176\u4e2d\u200b\u6709\u200b\u4ee3\u8868\u6027\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5de5\u5177\u5305\u200b\u8fdb\u884c\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u4e3b\u8981\u200b\u5305\u62ec\u200b\u5de5\u5177\u5305\u200b\u7684\u200b\u4f5c\u8005\u200b\u6216\u200b\u5176\u200b\u6240\u5728\u200b\u673a\u6784\u200b\u3001\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u5de5\u5177\u200b\uff08\u200b\u8fd9\u5757\u200b\u53ef\u80fd\u200b\u518d\u200b\u5f15\u5165\u200b\u7b2c\u4e09\u65b9\u200b\u5de5\u5177\u5305\u200b\uff09\u3001\u200b\u6570\u636e\u200b\u6269\u589e\u200b\u3001\u200b\u5e38\u7528\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u7684\u200b\u9884\u5b9a\u200b\u4e49\u200b\u3001\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u3001\u200b\u5e38\u7528\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u5e38\u7528\u200b\u8bc4\u6d4b\u200b\u6307\u6807\u200b\u3001\u200b\u5c01\u88c5\u200b\u597d\u200b\u7684\u200b\u8bad\u7ec3\u200b&amp;\u200b\u6d4b\u8bd5\u200b\u6a21\u5757\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u4f7f\u7528\u200b\u5bf9\u5e94\u200b\u5de5\u5177\u5305\u200b\u65f6\u4f1a\u200b\u7528\u5230\u200b\u7684\u200b\u3002\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u81ea\u8eab\u200b\u9700\u8981\u200b\u91cd\u70b9\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u81ea\u5df1\u200b\u7814\u7a76\u6240\u200b\u4e0d\u200b\u6d89\u53ca\u200b\u7684\u200b\u5de5\u5177\u5305\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53ea\u200b\u505a\u200b\u4e86\u89e3\u200b\uff0c\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u65f6\u200b\u518d\u200b\u6765\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u6ce8\u200b\uff1a</p> <p>\u200b\u672c\u7ae0\u200b\u5185\u5bb9\u200b\u4f1a\u200b\u4e0d\u65ad\u66f4\u65b0\u200b\uff0c\u200b\u6b22\u8fce\u200b\u5927\u5bb6\u200b\u5728\u200bissue\u200b\u4e2d\u200b\u63d0\u51fa\u200b\u5b9d\u8d35\u200b\u5efa\u8bae\u200b\uff0c\u200b\u6216\u8005\u200b\u76f4\u63a5\u200bpull request~</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/","title":"8.2 torchvision","text":"<p>PyTorch\u200b\u4e4b\u6240\u4ee5\u200b\u4f1a\u200b\u5728\u200b\u77ed\u77ed\u7684\u200b\u51e0\u5e74\u200b\u65f6\u95f4\u200b\u91cc\u200b\u53d1\u5c55\u200b\u6210\u4e3a\u200b\u4e3b\u6d41\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\uff0c\u200b\u9664\u53bb\u200b\u6846\u67b6\u200b\u672c\u8eab\u200b\u7684\u200b\u4f18\u52bf\u200b\uff0c\u200b\u8fd8\u200b\u5728\u4e8e\u200bPyTorch\u200b\u6709\u7740\u200b\u826f\u597d\u200b\u7684\u200b\u751f\u6001\u5708\u200b\u3002\u200b\u5728\u200b\u524d\u9762\u200b\u7684\u200b\u5b66\u4e60\u200b\u548c\u200b\u5b9e\u6218\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u7528\u5230\u200btorchvision\u200b\u6765\u200b\u8c03\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5728\u200b\u672c\u7ae0\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ed9\u200b\u5927\u5bb6\u200b\u7b80\u5355\u200b\u4ecb\u7ecd\u200b\u4e0b\u200btorchvision\u200b\u4ee5\u53ca\u200b\u76f8\u5173\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u4e86\u89e3\u200btorchvision</li> <li>\u200b\u4e86\u89e3\u200btorchvision\u200b\u7684\u200b\u4f5c\u7528\u200b</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#821-torchvision","title":"8.2.1 torchvision\u200b\u7b80\u4ecb","text":"<p>\" The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. \"</p> <p>\u200b\u6b63\u5982\u200b\u5f15\u8a00\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u77e5\u9053\u200btorchvision\u200b\u5305\u542b\u200b\u4e86\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u5e38\u5e38\u200b\u7528\u5230\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6a21\u578b\u200b\u548c\u200b\u56fe\u50cf\u5904\u7406\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u800c\u200b\u5177\u4f53\u200b\u7684\u200btorchvision\u200b\u5219\u200b\u5305\u62ec\u200b\u4e86\u200b\u4e0b\u9762\u200b\u8fd9\u200b\u51e0\u200b\u90e8\u5206\u200b\uff0c\u200b\u5e26\u200b * \u200b\u7684\u200b\u90e8\u5206\u200b\u662f\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u5230\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5e93\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u6211\u4eec\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u5e93\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4ecb\u7ecd\u200b\uff1a</p> <ul> <li>torchvision.datasets *                           </li> <li>torchvision.models *</li> <li>torchvision.tramsforms *</li> <li>torchvision.io </li> <li>torchvision.ops</li> <li>torchvision.utils</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#822-torchvisiondatasets","title":"8.2.2 torchvision.datasets","text":"<p><code>torchvision.datasets</code>\u200b\u4e3b\u8981\u200b\u5305\u542b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6211\u4eec\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5728\u200b==0.10.0\u200b\u7248\u672c\u200b==\u200b\u7684\u200b<code>torchvision</code>\u200b\u4e0b\u200b\uff0c\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff1a</p> Caltech CelebA CIFAR Cityscapes EMNIST FakeData Fashion-MNIST Flickr ImageNet Kinetics-400 KITTI KMNIST PhotoTour Places365 QMNIST SBD SEMEION STL10 SVHN UCF101 VOC WIDERFace"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#823-torchvisiontransforms","title":"8.2.3 torchvision.transforms","text":"<p>\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u6709\u200b\u5f88\u5927\u200b\u4e00\u90e8\u5206\u200b\u662f\u200b\u56fe\u7247\u200b\u7c7b\u578b\u200b\u7684\u200b\uff0c\u200b\u5982\u679c\u200b\u83b7\u53d6\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u683c\u5f0f\u200b\u6216\u8005\u200b\u5927\u5c0f\u4e0d\u4e00\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u548c\u200b\u5927\u5c0f\u200b\u7f29\u653e\u200b\u7b49\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u5e38\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5f53\u200b\u56fe\u7247\u200b\u6570\u636e\u200b\u6709\u9650\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u73b0\u6709\u200b\u56fe\u7247\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5404\u79cd\u200b\u53d8\u6362\u200b\uff0c\u200b\u5982\u200b\u7f29\u5c0f\u200b\u6216\u200b\u653e\u5927\u200b\u3001\u200b\u6c34\u5e73\u200b\u6216\u200b\u5782\u76f4\u200b\u7ffb\u8f6c\u200b\u7b49\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\u3002\u200b\u800c\u200btorchvision.transforms\u200b\u4e2d\u200b\u5c31\u200b\u5305\u542b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u8fd9\u6837\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5728\u200b\u4e4b\u524d\u200b\u7b2c\u56db\u7ae0\u200b\u7684\u200bFashion-mnist\u200b\u5b9e\u6218\u200b\u4e2d\u200b\u5bf9\u200b\u6570\u636e\u200b\u7684\u200b\u5904\u7406\u200b\u65f6\u200b\u6211\u4eec\u200b\u5c31\u7528\u5230\u200b\u4e86\u200btorchvision.transformer\uff1a</p> <pre><code>from torchvision import transforms\ndata_transform = transforms.Compose([\n    transforms.ToPILImage(),   # \u200b\u8fd9\u200b\u4e00\u6b65\u200b\u53d6\u51b3\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\n    transforms.Resize(image_size),\n    transforms.ToTensor()\n])\n</code></pre> <p>\u200b\u9664\u4e86\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u51e0\u79cd\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5728\u200btorchvision\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u91cc\u200b\u63d0\u5230\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5177\u4f53\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u672c\u200b\u8282\u200b\u914d\u5957\u200b\u7684\u200b\u201dtransforms.ipynb\u201c\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200bnotebook\u200b\u4e2d\u200b\u6211\u4eec\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u5e38\u89c1\u200b\u7684\u200btransforms\u200b\u7684\u200bAPI\u200b\u53ca\u5176\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\u7684\u200b\u64cd\u4f5c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#824-torchvisionmodels","title":"8.2.4 torchvision.models","text":"<p>\u200b\u4e3a\u4e86\u200b\u63d0\u9ad8\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b\uff0c\u200b\u51cf\u5c11\u200b\u4e0d\u5fc5\u8981\u200b\u7684\u200b\u91cd\u590d\u52b3\u52a8\u200b\uff0cPyTorch\u200b\u5b98\u65b9\u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u4f9b\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\u73b0\u5728\u200b\u6709\u200b\u54ea\u4e9b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bf9\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u3002 \u200b\u6b64\u5904\u200b\u6211\u4eec\u200b\u4ee5\u200btorchvision0.10.0 \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5982\u679c\u200b\u5e0c\u671b\u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4f7f\u7528\u200bpretrained-models.pytorch\u200b\u4ed3\u5e93\u200b\u3002\u200b\u73b0\u6709\u200b\u9884\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5206\u4e3a\u200b\u4ee5\u4e0b\u200b\u51e0\u7c7b\u200b\uff1a</p> <ul> <li>Classification</li> </ul> <p>\u200b\u5728\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u91cc\u9762\u200b\uff0cPyTorch\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u6b63\u5728\u200b\u4e0d\u65ad\u200b\u589e\u591a\u200b\u3002</p> AlexNet VGG ResNet SqueezeNet DenseNet Inception v3 GoogLeNet ShuffleNet v2 MobileNetV2 MobileNetV3 ResNext Wide ResNet MNASNet EfficientNet RegNet \u200b\u6301\u7eed\u200b\u66f4\u65b0\u200b <p>\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200bImageNet-1k\u200b\u8fdb\u884c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u5177\u4f53\u200b\u7684\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u540e\u9762\u200b\u8fdb\u884c\u200b\u4ecb\u7ecd\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u53bb\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u5728\u200bImageNet-1k\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</p> <ul> <li>Semantic Segmentation</li> </ul> <p>\u200b\u8bed\u4e49\u200b\u5206\u5272\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200bCOCO train2017\u200b\u7684\u200b\u5b50\u96c6\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u4e86\u200b20\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5305\u62ec\u200bbackground, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa,train, tvmonitor\u3002</p> FCN ResNet50 FCN ResNet101 DeepLabV3 ResNet50 DeepLabV3 ResNet101 LR-ASPP MobileNetV3-Large DeepLabV3 MobileNetV3-Large \u200b\u672a\u5b8c\u5f85\u7eed\u200b <p>\u200b\u5177\u4f53\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b<code>mean IOU</code>\u200b\u548c\u200b<code>global pixelwise acc</code></p> <ul> <li>Object Detection\uff0cinstance Segmentation and Keypoint Detection</li> </ul> <p>\u200b\u7269\u4f53\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u5b9e\u4f8b\u200b\u5206\u5272\u200b\u548c\u200b\u4eba\u4f53\u200b\u5173\u952e\u70b9\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6a21\u578b\u200b\u6211\u4eec\u200b\u540c\u6837\u200b\u662f\u200b\u5728\u200bCOCO train2017\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u5728\u200b\u4e0b\u65b9\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5b9e\u4f8b\u200b\u5206\u5272\u200b\u7684\u200b\u7c7b\u522b\u200b\u548c\u200b\u4eba\u4f53\u200b\u5173\u952e\u70b9\u200b\u68c0\u6d4b\u200b\u7c7b\u522b\u200b\uff1a</p> <pre><code>COCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus','train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A','handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball','kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket','bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl','banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza','donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table','N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone','microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book','clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\nCOCO_PERSON_KEYPOINT_NAMES =['nose','left_eye','right_eye','left_ear','right_ear','left_shoulder','right_shoulder','left_elbow','right_elbow','left_wrist','right_wrist','left_hip','right_hip','left_knee','right_knee','left_ankle','right_ankle']\n</code></pre> Faster R-CNN Mask R-CNN RetinaNet SSDlite SSD \u200b\u672a\u5b8c\u5f85\u7eed\u200b <p>\u200b\u540c\u6837\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u5728\u200bCOCO train 2017\u200b\u4e0a\u200b\u7684\u200b<code>box AP</code>,<code>keypoint AP</code>,<code>mask AP</code></p> <ul> <li>Video classification</li> </ul> <p>\u200b\u89c6\u9891\u5206\u7c7b\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b Kinetics-400\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b</p> ResNet 3D 18 ResNet MC 18 ResNet (2+1) D \u200b\u672a\u5b8c\u5f85\u7eed\u200b <p>\u200b\u540c\u6837\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u7684\u200b<code>Clip acc@1</code>,<code>Clip acc@5</code></p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#825-torchvisionio","title":"8.2.5 torchvision.io","text":"<p>\u200b\u5728\u200b<code>torchvision.io</code>\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u89c6\u9891\u200b\u3001\u200b\u56fe\u7247\u200b\u548c\u200b\u6587\u4ef6\u200b\u7684\u200b IO \u200b\u64cd\u4f5c\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5305\u62ec\u200b\u8bfb\u53d6\u200b\u3001\u200b\u5199\u5165\u200b\u3001\u200b\u7f16\u89e3\u7801\u200b\u5904\u7406\u200b\u64cd\u4f5c\u200b\u3002\u200b\u968f\u7740\u200btorchvision\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0cio\u200b\u4e5f\u200b\u589e\u52a0\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u5e95\u5c42\u200b\u7684\u200b\u9ad8\u6548\u7387\u200b\u7684\u200bAPI\u3002\u200b\u5728\u200b\u4f7f\u7528\u200btorchvision.io\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u4ee5\u4e0b\u51e0\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u4e0d\u540c\u200b\u7248\u672c\u200b\u4e4b\u95f4\u200b\uff0c<code>torchvision.io</code>\u200b\u6709\u7740\u200b\u8f83\u5927\u200b\u53d8\u5316\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u4f7f\u7528\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u67e5\u770b\u200b\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b<code>torchvision</code>\u200b\u7248\u672c\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u4f60\u200b\u60f3\u200b\u4f7f\u7528\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u9664\u4e86\u200bread_video()\u200b\u7b49\u200b\u65b9\u6cd5\u200b\uff0ctorchvision.io\u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7ec6\u7c92\u5ea6\u200b\u7684\u200b\u89c6\u9891\u200bAPI torchvision.io.VideoReader()  \uff0c\u200b\u5b83\u200b\u5177\u6709\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u6548\u7387\u200b\u5e76\u4e14\u200b\u66f4\u52a0\u200b\u63a5\u8fd1\u200b\u5e95\u5c42\u200b\u5904\u7406\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u5b89\u88c5\u200bffmpeg\u200b\u7136\u540e\u200b\u4ece\u200b\u6e90\u7801\u200b\u91cd\u65b0\u200b\u7f16\u8bd1\u200btorchvision\u200b\u6211\u4eec\u200b\u624d\u80fd\u200b\u6211\u4eec\u200b\u80fd\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u5728\u200b\u4f7f\u7528\u200bVideo\u200b\u76f8\u5173\u200bAPI\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u6700\u597d\u200b\u63d0\u524d\u200b\u5b89\u88c5\u200b\u597d\u200bPyAV\u200b\u8fd9\u4e2a\u200b\u5e93\u200b\u3002</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#826-torchvisionops","title":"8.2.6 torchvision.ops","text":"<p>torchvision.ops \u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u7279\u5b9a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5305\u62ec\u200b\u4f46\u200b\u4e0d\u4ec5\u200b\u9650\u4e8e\u200bNMS\uff0cRoIAlign\uff08MASK R-CNN\u200b\u4e2d\u200b\u5e94\u7528\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\uff09\uff0cRoIPool\uff08Fast R-CNN\u200b\u4e2d\u200b\u7528\u5230\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\uff09\u3002\u200b\u5728\u200b\u5408\u9002\u200b\u7684\u200b\u65f6\u95f4\u200b\u4f7f\u7528\u200b\u53ef\u4ee5\u200b\u5927\u5927\u964d\u4f4e\u200b\u6211\u4eec\u200b\u7684\u200b\u5de5\u4f5c\u91cf\u200b\uff0c\u200b\u907f\u514d\u200b\u91cd\u590d\u200b\u7684\u200b\u9020\u200b\u8f6e\u5b50\u200b\uff0c\u200b\u60f3\u200b\u770b\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u51fd\u6570\u200b\u4ecb\u7ecd\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u8fdb\u884c\u200b\u7ec6\u81f4\u200b\u67e5\u770b\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.2%20%E5%9B%BE%E5%83%8F%20-%20torchvision/#827-torchvisionutils","title":"8.2.7 torchvision.utils","text":"<p>torchvision.utils \u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5c06\u200b\u82e5\u5e72\u200b\u5f20\u200b\u56fe\u7247\u200b\u62fc\u63a5\u200b\u5728\u200b\u4e00\u8d77\u200b\u3001\u200b\u53ef\u89c6\u5316\u200b\u68c0\u6d4b\u200b\u548c\u200b\u5206\u5272\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u5177\u4f53\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\u3002</p> <p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0ctorchvision\u200b\u7684\u200b\u51fa\u73b0\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u89e3\u51b3\u200b\u4e86\u200b\u5e38\u89c1\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u4e00\u4e9b\u200b\u91cd\u590d\u200b\u4e14\u200b\u8017\u65f6\u200b\u7684\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u83b7\u53d6\u200b\u3001\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3001\u200b\u6a21\u578b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7b49\u200b\u65b9\u9762\u200b\u5927\u5927\u964d\u4f4e\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b\u5de5\u4f5c\u96be\u5ea6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u66f4\u52a0\u200b\u5feb\u901f\u200b\u4e0a\u200b\u624b\u200b\u4e00\u4e9b\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4efb\u52a1\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.3%20%E8%A7%86%E9%A2%91%20-%20PyTorchVideo/","title":"8.3 PyTorchVideo\u200b\u7b80\u4ecb","text":"<p>\u200b\u8fd1\u51e0\u5e74\u6765\u200b\uff0c\u200b\u968f\u7740\u200b\u4f20\u64ad\u5a92\u4ecb\u200b\u548c\u200b\u89c6\u9891\u200b\u5e73\u53f0\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u89c6\u9891\u200b\u6b63\u5728\u200b\u53d6\u4ee3\u200b\u56fe\u7247\u200b\u6210\u4e3a\u200b\u4e0b\u4e00\u4ee3\u200b\u7684\u200b\u4e3b\u6d41\u200b\u5a92\u4f53\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u4f7f\u5f97\u200b\u6709\u5173\u200b\u89c6\u9891\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u6b63\u5728\u200b\u83b7\u5f97\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u5173\u6ce8\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6709\u5173\u200b\u89c6\u9891\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u4ecd\u7136\u200b\u6709\u7740\u200b\u8bb8\u591a\u200b\u7f3a\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u8017\u8d39\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6ca1\u6709\u200b\u9ad8\u8d28\u91cf\u200b\u7684\u200b<code>model zoo</code>\uff0c\u200b\u4e0d\u80fd\u200b\u50cf\u200b\u56fe\u7247\u200b\u4e00\u6837\u200b\u8fdb\u884c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u548c\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u3002</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u5904\u7406\u200b\u8f83\u200b\u9ebb\u70e6\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u89c6\u9891\u200b\u5904\u7406\u200b\u5de5\u5177\u200b\u3002</li> <li>\u200b\u968f\u7740\u200b\u591a\u200b\u6a21\u6001\u200b\u8d8a\u6765\u8d8a\u200b\u6d41\u884c\u200b\uff0c\u200b\u4e9f\u9700\u200b\u4e00\u4e2a\u200b\u5de5\u5177\u200b\u6765\u200b\u5904\u7406\u200b\u5176\u4ed6\u200b\u6a21\u6001\u200b\u3002</li> </ul> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u90e8\u7f72\u200b\u4f18\u5316\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0cMeta\u200b\u63a8\u51fa\u200b\u4e86\u200b<code>PyTorchVideo</code>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200b\uff08\u200b\u5305\u542b\u200b\u7ec4\u4ef6\u200b\u5982\u200bFigure 1\u200b\u6240\u793a\u200b\uff09\u3002PyTorchVideo \u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u89c6\u9891\u200b\u7406\u89e3\u200b\u5de5\u4f5c\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200b\u3002PytorchVideo \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u52a0\u901f\u200b\u89c6\u9891\u200b\u7406\u89e3\u200b\u7814\u7a76\u6240\u200b\u9700\u200b\u7684\u200b\u53ef\u200b\u91cd\u7528\u200b\u3001\u200b\u6a21\u5757\u5316\u200b\u548c\u200b\u9ad8\u6548\u200b\u7684\u200b\u7ec4\u4ef6\u200b\u3002PyTorchVideo \u200b\u662f\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5f00\u53d1\u200b\u7684\u200b\uff0c\u200b\u652f\u6301\u200b\u4e0d\u540c\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u89c6\u9891\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u89c6\u9891\u200b\u6a21\u578b\u200b\u3001\u200b\u89c6\u9891\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u89c6\u9891\u200b\u7279\u5b9a\u200b\u8f6c\u6362\u200b\u3002</p> <p></p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.3%20%E8%A7%86%E9%A2%91%20-%20PyTorchVideo/#831-pytorchvideo","title":"8.3.1 PyTorchVideo\u200b\u7684\u200b\u4e3b\u8981\u200b\u90e8\u4ef6\u200b\u548c\u200b\u4eae\u70b9","text":"<p>PytorchVideo \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u52a0\u901f\u200b\u89c6\u9891\u200b\u7406\u89e3\u200b\u7814\u7a76\u6240\u200b\u9700\u200b\u7684\u200b\u6a21\u5757\u5316\u200b\u548c\u200b\u9ad8\u6548\u200b\u7684\u200bAPI\u3002\u200b\u5b83\u200b\u8fd8\u200b\u652f\u6301\u200b\u4e0d\u540c\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u89c6\u9891\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u89c6\u9891\u200b\u6a21\u578b\u200b\u3001\u200b\u89c6\u9891\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u89c6\u9891\u200b\u7279\u5b9a\u200b\u8f6c\u6362\u200b\uff0c\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\uff0cPytorchVideo\u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200bmodel zoo\uff0c\u200b\u4f7f\u5f97\u200b\u4eba\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5404\u79cd\u200b\u5148\u8fdb\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u89c6\u9891\u200b\u6a21\u578b\u200b\u53ca\u5176\u200b\u8bc4\u5224\u200b\u57fa\u51c6\u200b\u3002PyTorchVideo\u200b\u4e3b\u8981\u200b\u4eae\u70b9\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u57fa\u4e8e\u200b PyTorch\uff1a\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6784\u5efa\u200b\u3002\u200b\u4f7f\u200b\u6240\u6709\u200b PyTorch \u200b\u751f\u6001\u7cfb\u7edf\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u4f7f\u7528\u200b\u53d8\u5f97\u200b\u5bb9\u6613\u200b\u3002</li> <li> <p>Model Zoo\uff1aPyTorchVideo\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5305\u542b\u200bI3D\u3001R(2+1)D\u3001SlowFast\u3001X3D\u3001MViT\u200b\u7b49\u200bSOTA\u200b\u6a21\u578b\u200b\u7684\u200b\u9ad8\u8d28\u91cf\u200bmodel zoo\uff08\u200b\u76ee\u524d\u200b\u8fd8\u200b\u5728\u200b\u5feb\u901f\u200b\u6269\u5145\u200b\u4e2d\u200b\uff0c\u200b\u672a\u6765\u200b\u4f1a\u200b\u6709\u200b\u66f4\u200b\u591a\u200bSOTA model\uff09\uff0c\u200b\u5e76\u4e14\u200bPyTorchVideo\u200b\u7684\u200bmodel zoo\u200b\u8c03\u7528\u200b\u4e0e\u200bPyTorch Hub\u200b\u505a\u200b\u4e86\u200b\u6574\u5408\u200b\uff0c\u200b\u5927\u5927\u7b80\u5316\u200b\u6a21\u578b\u200b\u8c03\u7528\u200b\uff0c\u200b\u5177\u4f53\u200b\u7684\u200b\u4e00\u4e9b\u200b\u8c03\u7528\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4e0b\u9762\u200b\u7684\u200b\u3010\u200b\u4f7f\u7528\u200b PyTorchVideo model zoo\u3011\u200b\u90e8\u5206\u200b\u3002</p> </li> <li> <p>\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u548c\u200b\u5e38\u89c1\u200b\u6570\u636e\u200b\uff0cPyTorchVideo\u200b\u652f\u6301\u200bKinetics-400, Something-Something V2, Charades, Ava (v2.2), Epic Kitchen, HMDB51, UCF101, Domsev\u200b\u7b49\u200b\u4e3b\u6d41\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u76f8\u5e94\u200b\u7684\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u652f\u6301\u200brandaug, augmix\u200b\u7b49\u200b\u6570\u636e\u200b\u589e\u5f3a\u200btrick\u3002</p> </li> <li>\u200b\u6a21\u5757\u5316\u200b\u8bbe\u8ba1\u200b\uff1aPyTorchVideo\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u7c7b\u4f3c\u200b\u4e8e\u200btorchvision\uff0c\u200b\u4e5f\u200b\u662f\u200b\u63d0\u4f9b\u200b\u8bb8\u591a\u200b\u6a21\u5757\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u8c03\u7528\u200b\u4fee\u6539\u200b\uff0c\u200b\u5728\u200bPyTorchVideo\u200b\u4e2d\u200b\u5177\u4f53\u6765\u8bf4\u200b\u5305\u62ec\u200bdata, transforms, layer, model, accelerator\u200b\u7b49\u200b\u6a21\u5757\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u8fdb\u884c\u200b\u8c03\u7528\u200b\u548c\u200b\u8bfb\u53d6\u200b\u3002</li> <li>\u200b\u652f\u6301\u200b\u591a\u200b\u6a21\u6001\u200b\uff1aPyTorchVideo\u200b\u73b0\u5728\u200b\u5bf9\u200b\u591a\u200b\u6a21\u6001\u200b\u7684\u200b\u652f\u6301\u200b\u5305\u62ec\u200b\u4e86\u200bvisual\u200b\u548c\u200baudio\uff0c\u200b\u672a\u6765\u200b\u4f1a\u200b\u652f\u6301\u200b\u66f4\u200b\u591a\u200b\u6a21\u6001\u200b\uff0c\u200b\u4e3a\u200b\u591a\u200b\u6a21\u6001\u200b\u6a21\u578b\u200b\u7684\u200b\u53d1\u5c55\u200b\u63d0\u4f9b\u200b\u652f\u6301\u200b\u3002</li> <li>\u200b\u79fb\u52a8\u200b\u7aef\u200b\u90e8\u7f72\u200b\u4f18\u5316\u200b\uff1aPyTorchVideo\u200b\u652f\u6301\u200b\u9488\u5bf9\u200b\u79fb\u52a8\u200b\u7aef\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u7f72\u200b\u4f18\u5316\u200b\uff08\u200b\u4f7f\u7528\u200b\u524d\u8ff0\u200b\u7684\u200bPyTorchVideo/accelerator\u200b\u6a21\u5757\u200b\uff09\uff0c\u200b\u6a21\u578b\u200b\u7ecf\u8fc7\u200bPyTorchVideo\u200b\u4f18\u5316\u200b\u4e86\u200b\u6700\u9ad8\u200b\u8fbe\u200b7\u200b\u500d\u200b\u7684\u200b\u63d0\u901f\u200b\uff0c\u200b\u5e76\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u7b2c\u4e00\u4e2a\u200b\u80fd\u200b\u5b9e\u65f6\u200b\u8dd1\u200b\u5728\u200b\u624b\u673a\u200b\u7aef\u7684\u200bX3D\u200b\u6a21\u578b\u200b\uff08\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u5b9e\u65f6\u200b\u8dd1\u200b\u5728\u200b2018\u200b\u5e74\u200b\u7684\u200b\u4e09\u661f\u200bGalaxy S8\u200b\u4e0a\u200b\uff0c\u200b\u5177\u4f53\u200b\u8bf7\u200b\u89c1\u200bAndroid Demo APP\uff09\u3002</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.3%20%E8%A7%86%E9%A2%91%20-%20PyTorchVideo/#832-pytorchvideo","title":"8.3.2 PyTorchVideo\u200b\u7684\u200b\u5b89\u88c5","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200bpip\u200b\u6765\u200b\u5b89\u88c5\u200bPyTorchVideo\uff1a</p> <pre><code>pip install pytorchvideo\n</code></pre> <p>\u200b\u6ce8\u200b\uff1a</p> <ul> <li>\u200b\u5b89\u88c5\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u7684\u200bpython\u200b\u7248\u672c\u200b &gt;= 3.7</li> <li>PyTorch &gt;= 1.8.0\uff0c\u200b\u5b89\u88c5\u200b\u7684\u200btorchvision\u200b\u4e5f\u200b\u9700\u8981\u200b\u5339\u914d\u200b</li> <li>CUDA &gt;= 10.2</li> <li>ioPath\uff1a\u200b\u5177\u4f53\u60c5\u51b5\u200b</li> <li>fvcore\u200b\u7248\u672c\u200b &gt;= 0.1.4\uff1a\u200b\u5177\u4f53\u60c5\u51b5\u200b</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.3%20%E8%A7%86%E9%A2%91%20-%20PyTorchVideo/#833-model-zoo-benchmark","title":"8.3.3 Model zoo \u200b\u548c\u200b benchmark","text":"<p>\u200b\u5728\u200b\u4e0b\u9762\u200b\u8fd9\u90e8\u5206\u200b\uff0c\u200b\u6211\u200b\u5c06\u200b\u7b80\u5355\u200b\u4ecb\u7ecd\u200b\u4e9b\u200bPyTorchVideo\u200b\u6240\u200b\u63d0\u4f9b\u200b\u7684\u200bModel zoo\u200b\u548c\u200bbenchmark</p> <ul> <li>Kinetics-400</li> </ul> arch depth pretrain frame length x sample rate top 1 top 5 Flops (G) x views Params (M) Model C2D R50 - 8x8 71.46 89.68 25.89 x 3 x 10 24.33 link I3D R50 - 8x8 73.27 90.70 37.53 x 3 x 10 28.04 link Slow R50 - 4x16 72.40 90.18 27.55 x 3 x 10 32.45 link Slow R50 - 8x8 74.58 91.63 54.52 x 3 x 10 32.45 link SlowFast R50 - 4x16 75.34 91.89 36.69 x 3 x 10 34.48 link SlowFast R50 - 8x8 76.94 92.69 65.71 x 3 x 10 34.57 link SlowFast R101 - 8x8 77.90 93.27 127.20 x 3 x 10 62.83 link SlowFast R101 - 16x8 78.70 93.61 215.61 x 3 x 10 53.77 link CSN R101 - 32x2 77.00 92.90 75.62 x 3 x 10 22.21 link R(2+1)D R50 - 16x4 76.01 92.23 76.45 x 3 x 10 28.11 link X3D XS - 4x12 69.12 88.63 0.91 x 3 x 10 3.79 link X3D S - 13x6 73.33 91.27 2.96 x 3 x 10 3.79 link X3D M - 16x5 75.94 92.72 6.72 x 3 x 10 3.79 link X3D L - 16x5 77.44 93.31 26.64 x 3 x 10 6.15 link MViT B - 16x4 78.85 93.85 70.80 x 1 x 5 36.61 link MViT B - 32x3 80.30 94.69 170.37 x 1 x 5 36.61 link <ul> <li>Something-Something V2</li> </ul> arch depth pretrain frame length x sample rate top 1 top 5 Flops (G) x views Params (M) Model Slow R50 Kinetics 400 8x8 60.04 85.19 55.10 x 3 x 1 31.96 link SlowFast R50 Kinetics 400 8x8 61.68 86.92 66.60 x 3 x 1 34.04 link <ul> <li>Charades</li> </ul> arch depth pretrain frame length x sample rate MAP Flops (G) x views Params (M) Model Slow R50 Kinetics 400 8x8 34.72 55.10 x 3 x 10 31.96 link SlowFast R50 Kinetics 400 8x8 37.24 66.60 x 3 x 10 34.00 link <ul> <li>AVA (V2.2)</li> </ul> arch depth pretrain frame length x sample rate MAP Params (M) Model Slow R50 Kinetics 400 4x16 19.5 31.78 link SlowFast R50 Kinetics 400 8x8 24.67 33.82 link"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.3%20%E8%A7%86%E9%A2%91%20-%20PyTorchVideo/#834-pytorchvideo-model-zoo","title":"8.3.4 \u200b\u4f7f\u7528\u200b PyTorchVideo model zoo","text":"<p>PyTorchVideo\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e09\u79cd\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5e76\u4e14\u200b\u7ed9\u200b\u6bcf\u200b\u4e00\u79cd\u200b\u90fd\u200b\u914d\u5907\u200b\u4e86\u200b<code>tutorial</code></p> <ul> <li>TorchHub\uff0c\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u90fd\u200b\u5df2\u7ecf\u200b\u5728\u200bTorchHub\u200b\u5b58\u5728\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u60c5\u51b5\u200b\u6765\u200b\u9009\u62e9\u200b\u9700\u4e0d\u9700\u8981\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5b98\u65b9\u200b\u4e5f\u200b\u7ed9\u51fa\u200b\u4e86\u200bTorchHub\u200b\u4f7f\u7528\u200b\u7684\u200b tutorial \u3002</li> <li>PySlowFast\uff0c\u200b\u4f7f\u7528\u200b PySlowFast workflow \u200b\u53bb\u200b\u8bad\u7ec3\u200b\u6216\u200b\u6d4b\u8bd5\u200bPyTorchVideo models/datasets.</li> <li> <p>PyTorch Lightning\u200b\u5efa\u7acb\u200b\u4e00\u4e2a\u200b\u5de5\u4f5c\u200b\u6d41\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0c\u200b\u70b9\u51fb\u200b\u67e5\u770b\u200b\u5b98\u65b9\u200b tutorial\u3002</p> </li> <li> <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u67e5\u770b\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b \u200b\u8fd9\u91cc\u200b \u200b\u8fdb\u884c\u200b\u5c1d\u8bd5\u200b</p> </li> </ul> <p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0cPyTorchVideo\u200b\u7684\u200b\u4f7f\u7528\u200b\u4e0e\u200btorchvision\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5728\u200b\u6709\u200b\u4e86\u200b\u524d\u9762\u200b\u7684\u200b\u5b66\u4e60\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f88\u5feb\u200b\u4e0a\u200b\u624b\u200bPyTorchVideo\uff0c\u200b\u5177\u4f53\u200b\u7684\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u67e5\u770b\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u6587\u6863\u200b\u548c\u200b\u4e00\u4e9b\u200b\u4f8b\u7a0b\u200b\u6765\u200b\u4e86\u89e3\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\uff1a\u200b\u5b98\u65b9\u200b\u7f51\u5740\u200b</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/","title":"8.4 torchtext\u200b\u7b80\u4ecb","text":"<p>\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u6765\u200b\u4ecb\u7ecd\u200bPyTorch\u200b\u5b98\u65b9\u200b\u7528\u4e8e\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff08NLP\uff09\u200b\u7684\u200b\u5de5\u5177\u5305\u200btorchtext\u3002\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4e5f\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u5927\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u8fd1\u5e74\u6765\u200b\u968f\u7740\u200b\u5927\u89c4\u6a21\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u5e94\u7528\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5728\u200b\u4eba\u673a\u5bf9\u8bdd\u200b\u3001\u200b\u673a\u5668\u7ffb\u8bd1\u200b\u7b49\u200b\u9886\u57df\u200b\u7684\u200b\u53d6\u5f97\u200b\u4e86\u200b\u975e\u5e38\u200b\u597d\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u4e5f\u200b\u4f7f\u5f97\u200bNLP\u200b\u76f8\u5173\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u83b7\u5f97\u200b\u4e86\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u5173\u6ce8\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200bNLP\u200b\u548c\u200bCV\u200b\u5728\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200b\uff0c\u200b\u56e0\u6b64\u200bNLP\u200b\u7684\u200b\u5de5\u5177\u5305\u200btorchtext\u200b\u548c\u200btorchvision\u200b\u7b49\u200bCV\u200b\u76f8\u5173\u200b\u5de5\u5177\u5305\u200b\u4e5f\u200b\u6709\u200b\u4e00\u4e9b\u200b\u529f\u80fd\u200b\u4e0a\u200b\u7684\u200b\u5dee\u5f02\u200b\uff0c\u200b\u5982\u200b\uff1a</p> <ul> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\uff08dataset\uff09\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u4e0d\u540c\u200b</li> <li>\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u5de5\u5177\u200b</li> <li>\u200b\u6ca1\u6709\u200b\u7433\u7405\u6ee1\u76ee\u200b\u7684\u200bmodel zoo</li> </ul> <p>\u200b\u672c\u200b\u8282\u200b\u4ecb\u7ecd\u200b\u53c2\u8003\u200b\u4e86\u200batnlp\u200b\u7684\u200bGithub\uff0c\u200b\u5728\u200b\u6b64\u200b\u81f4\u8c22\u200b\uff01</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/#841-torchtext","title":"8.4.1 torchtext\u200b\u7684\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u90e8\u5206","text":"<p>torchtext\u200b\u53ef\u4ee5\u200b\u65b9\u4fbf\u200b\u7684\u200b\u5bf9\u200b\u6587\u672c\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u4f8b\u5982\u200b\u622a\u65ad\u200b\u8865\u957f\u200b\u3001\u200b\u6784\u5efa\u200b\u8bcd\u8868\u200b\u7b49\u200b\u3002torchtext\u200b\u4e3b\u8981\u200b\u5305\u542b\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff1a</p> <ul> <li>\u200b\u6570\u636e\u5904\u7406\u200b\u5de5\u5177\u200b torchtext.data.functional\u3001torchtext.data.utils</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b torchtext.data.datasets</li> <li>\u200b\u8bcd\u8868\u200b\u5de5\u5177\u200b torchtext.vocab</li> <li>\u200b\u8bc4\u6d4b\u200b\u6307\u6807\u200b torchtext.metrics</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/#842-torchtext","title":"8.4.2 torchtext\u200b\u7684\u200b\u5b89\u88c5","text":"<p>torchtext\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200bpip\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\uff1a</p> <pre><code>pip install torchtext\n</code></pre>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/#843","title":"8.4.3 \u200b\u6784\u5efa\u200b\u6570\u636e\u200b\u96c6","text":"<ul> <li>Field\u200b\u53ca\u5176\u200b\u4f7f\u7528\u200b</li> </ul> <p>Field\u200b\u662f\u200btorchtext\u200b\u4e2d\u200b\u5b9a\u4e49\u6570\u636e\u200b\u7c7b\u578b\u200b\u4ee5\u53ca\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u7684\u200b\u6307\u4ee4\u200b\u3002<code>torchtext</code> \u200b\u8ba4\u4e3a\u200b\u4e00\u4e2a\u200b\u6837\u672c\u200b\u662f\u200b\u7531\u200b\u591a\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\uff08\u200b\u6587\u672c\u200b\u5b57\u200b\u6bb5\u200b\uff0c\u200b\u6807\u7b7e\u200b\u5b57\u200b\u6bb5\u200b\uff09\u200b\u7ec4\u6210\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u5904\u7406\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u6240\u4ee5\u200b\u624d\u200b\u4f1a\u200b\u6709\u200b <code>Field</code> \u200b\u62bd\u8c61\u200b\u3002\u200b\u5b9a\u4e49\u200bField\u200b\u5bf9\u8c61\u200b\u662f\u200b\u4e3a\u4e86\u200b\u660e\u786e\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u4f46\u200b\u5177\u4f53\u200b\u7684\u200b\u5904\u7406\u200b\u5219\u200b\u662f\u200b\u5728\u200bDataset\u200b\u4e2d\u200b\u5b8c\u6210\u200b\u7684\u200b\u3002\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\u6765\u200b\u7b80\u8981\u200b\u8bf4\u660e\u200b\u4e00\u4e0b\u200bField\u200b\u7684\u200b\u4f7f\u7528\u200b\uff1a</p> <pre><code>tokenize = lambda x: x.split()\nTEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, fix_length=200)\nLABEL = data.Field(sequential=False, use_vocab=False)\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <p>\u200b   sequential\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u662f\u5426\u662f\u200b\u987a\u5e8f\u200b\u8868\u793a\u200b\u7684\u200b\uff1b</p> <p>\u200b   tokenize\u200b\u7528\u4e8e\u200b\u8bbe\u7f6e\u200b\u5c06\u200b\u5b57\u7b26\u4e32\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u987a\u5e8f\u200b\u5b9e\u4f8b\u200b\u7684\u200b\u51fd\u6570\u200b</p> <p>\u200b   lower\u200b\u8bbe\u7f6e\u200b\u662f\u5426\u200b\u5c06\u200b\u5b57\u7b26\u4e32\u200b\u5168\u90e8\u200b\u8f6c\u4e3a\u200b\u5c0f\u5199\u200b\uff1b</p> <p>\u200b   fix_length\u200b\u8bbe\u7f6e\u200b\u6b64\u5b57\u6bb5\u200b\u6240\u6709\u200b\u5b9e\u4f8b\u200b\u90fd\u200b\u5c06\u200b\u586b\u5145\u200b\u5230\u200b\u4e00\u4e2a\u200b\u56fa\u5b9a\u200b\u7684\u200b\u957f\u5ea6\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u5904\u7406\u200b\uff1b</p> <p>\u200b   use_vocab\u200b\u8bbe\u7f6e\u200b\u662f\u5426\u200b\u5f15\u5165\u200bVocab object\uff0c\u200b\u5982\u679c\u200b\u4e3a\u200bFalse\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u4fdd\u8bc1\u200b\u4e4b\u540e\u200b\u8f93\u5165\u200bfield\u200b\u4e2d\u200b\u7684\u200bdata\u200b\u90fd\u200b\u662f\u200bnumerical\u200b\u7684\u200b</p> <p>\u200b\u6784\u5efa\u200bField\u200b\u5b8c\u6210\u200b\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u6784\u5efa\u200bdataset\u200b\u4e86\u200b\uff1a</p> <pre><code>from torchtext import data\ndef get_dataset(csv_data, text_field, label_field, test=False):\n    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n                 (\"comment_text\", text_field), (\"toxic\", label_field)]       \n    examples = []\n\n    if test:\n        # \u200b\u5982\u679c\u200b\u4e3a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5219\u200b\u4e0d\u200b\u52a0\u8f7d\u200blabel\n        for text in tqdm(csv_data['comment_text']):\n            examples.append(data.Example.fromlist([None, text, None], fields))\n    else:\n        for text, label in tqdm(zip(csv_data['comment_text'], csv_data['toxic'])):\n            examples.append(data.Example.fromlist([None, text, label], fields))\n    return examples, fields\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u6570\u636e\u200bcsv_data\u200b\u4e2d\u6709\u200b\"comment_text\"\u200b\u548c\u200b\"toxic\"\u200b\u4e24\u5217\u200b\uff0c\u200b\u5206\u522b\u200b\u5bf9\u5e94\u200btext\u200b\u548c\u200blabel\u3002</p> <pre><code>train_data = pd.read_csv('train_toxic_comments.csv')\nvalid_data = pd.read_csv('valid_toxic_comments.csv')\ntest_data = pd.read_csv(\"test_toxic_comments.csv\")\nTEXT = data.Field(sequential=True, tokenize=tokenize, lower=True)\nLABEL = data.Field(sequential=False, use_vocab=False)\n\n# \u200b\u5f97\u5230\u200b\u6784\u5efa\u200bDataset\u200b\u6240\u200b\u9700\u200b\u7684\u200bexamples\u200b\u548c\u200bfields\ntrain_examples, train_fields = get_dataset(train_data, TEXT, LABEL)\nvalid_examples, valid_fields = get_dataset(valid_data, TEXT, LABEL)\ntest_examples, test_fields = get_dataset(test_data, TEXT, None, test=True)\n# \u200b\u6784\u5efa\u200bDataset\u200b\u6570\u636e\u200b\u96c6\u200b\ntrain = data.Dataset(train_examples, train_fields)\nvalid = data.Dataset(valid_examples, valid_fields)\ntest = data.Dataset(test_examples, test_fields)\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5b9a\u4e49\u200bField\u200b\u5bf9\u8c61\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u901a\u8fc7\u200bget_dataset\u200b\u51fd\u6570\u200b\u53ef\u4ee5\u200b\u8bfb\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u6587\u672c\u200b\u548c\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5c06\u200b\u4e8c\u8005\u200b\uff08examples\uff09\u200b\u8fde\u540c\u200bfield\u200b\u4e00\u8d77\u200b\u9001\u5230\u200btorchtext.data.Dataset\u200b\u7c7b\u4e2d\u200b\uff0c\u200b\u5373\u53ef\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6784\u5efa\u200b\u3002\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u53ef\u4ee5\u200b\u770b\u200b\u4e0b\u200b\u8bfb\u5165\u200b\u7684\u200b\u6570\u636e\u200b\u60c5\u51b5\u200b\uff1a</p> <pre><code># \u200b\u68c0\u67e5\u200bkeys\u200b\u662f\u5426\u200b\u6b63\u786e\u200b\nprint(train[0].__dict__.keys())\nprint(test[0].__dict__.keys())\n# \u200b\u62bd\u67e5\u200b\u5185\u5bb9\u200b\u662f\u5426\u200b\u6b63\u786e\u200b\nprint(train[0].comment_text)\n</code></pre> <ul> <li>\u200b\u8bcd\u6c47\u8868\u200b\uff08vocab\uff09</li> </ul> <p>\u200b\u5728\u200bNLP\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u5b57\u7b26\u4e32\u200b\u5f62\u5f0f\u200b\u7684\u200b\u8bcd\u8bed\u200b\uff08word\uff09\u200b\u8f6c\u53d8\u200b\u4e3a\u200b\u6570\u5b57\u200b\u5f62\u5f0f\u200b\u7684\u200b\u5411\u91cf\u200b\u8868\u793a\u200b\uff08embedding\uff09\u200b\u662f\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u4e00\u6b65\u200b\uff0c\u200b\u88ab\u200b\u79f0\u4e3a\u200bWord Embedding\u3002\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u7684\u200b\u57fa\u672c\u200b\u601d\u60f3\u200b\u662f\u200b\u6536\u96c6\u200b\u4e00\u4e2a\u200b\u6bd4\u8f83\u200b\u5927\u200b\u7684\u200b\u8bed\u6599\u5e93\u200b\uff08\u200b\u5c3d\u91cf\u200b\u4e0e\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4efb\u52a1\u200b\u76f8\u5173\u200b\uff09\uff0c\u200b\u5728\u200b\u8bed\u6599\u5e93\u200b\u4e2d\u200b\u4f7f\u7528\u200bword2vec\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6784\u5efa\u200b\u8bcd\u8bed\u200b\u5230\u200b\u5411\u91cf\u200b\uff08\u200b\u6216\u200b\u6570\u5b57\u200b\uff09\u200b\u7684\u200b\u6620\u5c04\u200b\u5173\u7cfb\u200b\uff0c\u200b\u4e4b\u540e\u200b\u5c06\u200b\u8fd9\u200b\u4e00\u200b\u6620\u5c04\u200b\u5173\u7cfb\u200b\u5e94\u7528\u200b\u4e8e\u200b\u5f53\u524d\u200b\u7684\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5c06\u200b\u53e5\u5b50\u200b\u4e2d\u200b\u7684\u200b\u8bcd\u8bed\u200b\u8f6c\u4e3a\u200b\u5411\u91cf\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u5728\u200btorchtext\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bField\u200b\u81ea\u5e26\u200b\u7684\u200bbuild_vocab\u200b\u51fd\u6570\u200b\u5b8c\u6210\u200b\u8bcd\u6c47\u8868\u200b\u6784\u5efa\u200b\u3002</p> <pre><code>TEXT.build_vocab(train)\n</code></pre> <ul> <li>\u200b\u6570\u636e\u200b\u8fed\u4ee3\u200b\u5668\u200b</li> </ul> <p>\u200b\u5176\u5b9e\u200b\u5c31\u662f\u200btorchtext\u200b\u4e2d\u200b\u7684\u200bDataLoader\uff0c\u200b\u770b\u200b\u4e0b\u200b\u4ee3\u7801\u200b\u5c31\u200b\u660e\u767d\u200b\u4e86\u200b\uff1a</p> <pre><code>from torchtext.data import Iterator, BucketIterator\n# \u200b\u82e5\u200b\u53ea\u200b\u9488\u5bf9\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6784\u9020\u200b\u8fed\u4ee3\u200b\u5668\u200b\n# train_iter = data.BucketIterator(dataset=train, batch_size=8, shuffle=True, sort_within_batch=False, repeat=False)\n\n# \u200b\u540c\u65f6\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8fed\u4ee3\u200b\u5668\u200b\u7684\u200b\u6784\u5efa\u200b\ntrain_iter, val_iter = BucketIterator.splits(\n        (train, valid), # \u200b\u6784\u5efa\u200b\u6570\u636e\u200b\u96c6\u6240\u200b\u9700\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\n        batch_sizes=(8, 8),\n        device=-1, # \u200b\u5982\u679c\u200b\u4f7f\u7528\u200bgpu\uff0c\u200b\u6b64\u5904\u200b\u5c06\u200b-1\u200b\u66f4\u6362\u200b\u4e3a\u200bGPU\u200b\u7684\u200b\u7f16\u53f7\u200b\n        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n        sort_within_batch=False\n)\n\ntest_iter = Iterator(test, batch_size=8, device=-1, sort=False, sort_within_batch=False)\n</code></pre> <p>torchtext\u200b\u652f\u6301\u200b\u53ea\u200b\u5bf9\u200b\u4e00\u4e2a\u200bdataset\u200b\u548c\u200b\u540c\u65f6\u200b\u5bf9\u200b\u591a\u4e2a\u200bdataset\u200b\u6784\u5efa\u200b\u6570\u636e\u200b\u8fed\u4ee3\u200b\u5668\u200b\u3002</p> <ul> <li>\u200b\u4f7f\u7528\u200b\u81ea\u5e26\u200b\u6570\u636e\u200b\u96c6\u200b</li> </ul> <p>\u200b\u4e0e\u200btorchvision\u200b\u7c7b\u4f3c\u200b\uff0ctorchtext\u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u82e5\u5e72\u200b\u5e38\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u65b9\u4fbf\u200b\u5feb\u901f\u200b\u8fdb\u884c\u200b\u7b97\u6cd5\u200b\u6d4b\u8bd5\u200b\u3002\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u5bfb\u627e\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/#844-metric","title":"8.4.4 \u200b\u8bc4\u6d4b\u200b\u6307\u6807\u200b\uff08metric\uff09","text":"<p>NLP\u200b\u4e2d\u200b\u90e8\u5206\u200b\u4efb\u52a1\u200b\u7684\u200b\u8bc4\u6d4b\u200b\u4e0d\u662f\u200b\u901a\u8fc7\u200b\u51c6\u786e\u7387\u200b\u7b49\u200b\u6307\u6807\u200b\u5b8c\u6210\u200b\u7684\u200b\uff0c\u200b\u6bd4\u5982\u200b\u673a\u5668\u7ffb\u8bd1\u200b\u4efb\u52a1\u200b\u5e38\u7528\u200bBLEU (bilingual evaluation understudy) score\u200b\u6765\u200b\u8bc4\u4ef7\u200b\u9884\u6d4b\u200b\u6587\u672c\u200b\u548c\u200b\u6807\u7b7e\u200b\u6587\u672c\u200b\u4e4b\u95f4\u200b\u7684\u200b\u76f8\u4f3c\u200b\u7a0b\u5ea6\u200b\u3002torchtext\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8c03\u7528\u200btorchtext.data.metrics.bleu_score\u200b\u6765\u200b\u5feb\u901f\u200b\u5b9e\u73b0\u200bBLEU\uff0c\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code>from torchtext.data.metrics import bleu_score\ncandidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\nreferences_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\nbleu_score(candidate_corpus, references_corpus)\n</code></pre> <pre><code>0.8408964276313782\n</code></pre>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.4%20%E6%96%87%E6%9C%AC%20-%20torchtext/#845","title":"8.4.5 \u200b\u5176\u4ed6","text":"<p>\u200b\u503c\u5f97\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u7531\u4e8e\u200bNLP\u200b\u5e38\u7528\u200b\u7684\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u6bd4\u8f83\u200b\u56fa\u5b9a\u200b\uff0ctorchtext\u200b\u5e76\u200b\u4e0d\u200b\u50cf\u200btorchvision\u200b\u90a3\u6837\u200b\u63d0\u4f9b\u200b\u4e00\u7cfb\u5217\u200b\u5e38\u7528\u200b\u7684\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3002\u200b\u6a21\u578b\u200b\u4e3b\u8981\u200b\u901a\u8fc7\u200btorch.nn\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5757\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6bd4\u5982\u200btorch.nn.LSTM\u3001torch.nn.RNN\u200b\u7b49\u200b\u3002</p> <p>\u200b\u5907\u6ce8\u200b\uff1a</p> <p>\u200b\u5bf9\u4e8e\u200b\u6587\u672c\u200b\u7814\u7a76\u200b\u800c\u8a00\u200b\uff0c\u200b\u5f53\u4e0b\u200bTransformer\u200b\u5df2\u7ecf\u200b\u6210\u4e3a\u200b\u4e86\u200b\u7edd\u5bf9\u200b\u7684\u200b\u4e3b\u6d41\u200b\uff0c\u200b\u56e0\u6b64\u200bPyTorch\u200b\u751f\u6001\u200b\u4e2d\u200b\u7684\u200bHuggingFace\u200b\u7b49\u200b\u5de5\u5177\u5305\u200b\u4e5f\u200b\u53d7\u5230\u200b\u4e86\u200b\u8d8a\u6765\u8d8a\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u5173\u6ce8\u200b\u3002\u200b\u8fd9\u91cc\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\u8bfb\u8005\u200b\u81ea\u884c\u200b\u63a2\u7d22\u200b\u76f8\u5173\u200b\u5185\u5bb9\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5199\u4e0b\u200b\u81ea\u5df1\u200b\u5bf9\u4e8e\u200bHuggingFace\u200b\u7684\u200b\u7b14\u8bb0\u200b\uff0c\u200b\u5982\u679c\u200b\u603b\u7ed3\u200b\u5168\u9762\u200b\u7684\u8bdd\u200b\u6b22\u8fce\u200bpull request\uff0c\u200b\u5145\u5b9e\u200b\u6211\u4eec\u200b\u7684\u200b\u8bfe\u7a0b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u672c\u200b\u8282\u200b\u53c2\u8003\u200b</p> <ul> <li>torchtext\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b</li> <li>atnlp/torchtext-summary</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/","title":"8.5 torchaudio\u200b\u7b80\u4ecb","text":"<p>\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u6765\u200b\u4ecb\u7ecd\u200bPyTorch\u200b\u5b98\u65b9\u200b\u7528\u4e8e\u200b\u8bed\u97f3\u200b\u5904\u7406\u200b\u7684\u200b\u5de5\u5177\u5305\u200btorchaduio\u3002\u200b\u8bed\u97f3\u200b\u7684\u200b\u5904\u7406\u200b\u4e5f\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u5927\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u5305\u62ec\u200b\u8bf4\u8bdd\u200b\u4eba\u200b\u8bc6\u522b\u200b(Speaker Identification)\uff0c\u200b\u8bf4\u8bdd\u200b\u4eba\u200b\u5206\u79bb\u200b(Speaker Diarization)\uff0c\u200b\u97f3\u7d20\u200b\u8bc6\u522b\u200b(Phoneme Recognition)\uff0c\u200b\u8bed\u97f3\u200b\u8bc6\u522b\u200b(Automatic Speech Recognition)\uff0c\u200b\u8bed\u97f3\u200b\u5206\u79bb\u200b(Speech Separation)\uff0c\u200b\u6587\u672c\u200b\u8f6c\u200b\u8bed\u97f3\u200b(TTS)\u200b\u7b49\u200b\u4efb\u52a1\u200b\u3002</p> <p>CV\u200b\u6709\u200btorchvision\uff0cNLP\u200b\u6709\u200btorchtext\uff0c\u200b\u4eba\u4eec\u200b\u5e0c\u671b\u200b\u8bed\u97f3\u200b\u9886\u57df\u200b\u4e2d\u200b\u4e5f\u200b\u80fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5de5\u5177\u5305\u200b\u3002\u200b\u800c\u200b\u8bed\u97f3\u200b\u7684\u200b\u5904\u7406\u200b\u5de5\u5177\u5305\u200b\u5c31\u662f\u200btorchaudio\u3002\u200b\u7531\u4e8e\u200b\u8bed\u97f3\u200b\u4efb\u52a1\u200b\u672c\u8eab\u200b\u7684\u200b\u7279\u6027\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u5176\u200b\u4e0e\u200bNLP\u200b\u548c\u200bCV\u200b\u5728\u200b\u6570\u636e\u5904\u7406\u200b\u3001\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u3001\u200b\u6a21\u578b\u200b\u9a8c\u8bc1\u200b\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8bed\u97f3\u200b\u7684\u200b\u5de5\u5177\u5305\u200btorchaudio\u200b\u548c\u200btorchvision\u200b\u7b49\u200bCV\u200b\u76f8\u5173\u200b\u5de5\u5177\u5305\u200b\u4e5f\u200b\u6709\u200b\u4e00\u4e9b\u200b\u529f\u80fd\u200b\u4e0a\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u672c\u7ae0\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\u7684\u200bI/O</li> <li>\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\u7684\u200b\u9884\u5904\u7406\u200b</li> <li>\u200b\u8bed\u97f3\u200b\u9886\u57df\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b</li> <li>\u200b\u8bed\u97f3\u200b\u9886\u57df\u200b\u7684\u200b\u6a21\u578b\u200b</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#841-torchaduio","title":"8.4.1 torchaduio\u200b\u7684\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u90e8\u5206","text":"<p>torchaduio\u200b\u4e3b\u8981\u200b\u5305\u62ec\u200b\u4ee5\u4e0b\u200b\u51e0\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ul> <li>torchaudio.io\uff1a\u200b\u6709\u5173\u200b\u97f3\u9891\u200b\u7684\u200bI/O</li> <li>torchaudio.backend\uff1a\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u97f3\u9891\u200b\u5904\u7406\u200b\u7684\u200b\u540e\u200b\u7aef\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1asox\uff0csoundfile\u200b\u7b49\u200b</li> <li>torchaudio.functional\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u6570\u636e\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aspectrogram\uff0ccreate_fb_matrix\u200b\u7b49\u200b</li> <li>torchaudio.transforms\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aMFCC\uff0cMelScale\uff0cAmplitudeToDB\u200b\u7b49\u200b</li> <li>torchaudio.datasets\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5982\u200b\uff1aVCTK\uff0cLibriSpeech\uff0cyesno\u200b\u7b49\u200b</li> <li>torchaudio.models\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200b\uff1aWav2Letter\uff0cDeepSpeech\u200b\u7b49\u200b</li> <li>torchaudio.models.decoder\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u89e3\u7801\u5668\u200b\uff0c\u200b\u5982\u200b\uff1aGreedyDecoder\uff0cBeamSearchDecoder\u200b\u7b49\u200b</li> <li>torchaudio.pipelines\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u5904\u7406\u200b\u6d41\u6c34\u7ebf\u200b\uff0c\u200b\u5982\u200b\uff1aSpeechRecognitionPipeline\uff0cSpeakerRecognitionPipeline\u200b\u7b49\u200b</li> <li>torchaudio.sox_effects\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aapply_effects_tensor\uff0capply_effects_file\u200b\u7b49\u200b</li> <li>torchaudio.compliance.kaldi\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u4e0e\u200bKaldi\u200b\u5de5\u5177\u200b\u517c\u5bb9\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aload_kaldi_fst\uff0cload_kaldi_ark\u200b\u7b49\u200b</li> <li>torchaudio.kalid_io\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u4e0e\u200bKaldi\u200b\u5de5\u5177\u200b\u517c\u5bb9\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aread_vec_flt_scp\uff0cread_vec_int_scp\u200b\u7b49\u200b</li> <li>torchaudio.utils\uff1a\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u5de5\u5177\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\uff1aget_audio_backend\uff0cset_audio_backend\u200b\u7b49\u200b</li> </ul>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#842-torchaduio","title":"8.4.2 torchaduio\u200b\u7684\u200b\u5b89\u88c5","text":"<p>\u200b\u4e00\u822c\u200b\u5728\u200b\u5b89\u88c5\u200btorch\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e5f\u200b\u4f1a\u200b\u5b89\u88c5\u200btorchaudio\u3002\u200b\u5047\u5982\u200b\u6211\u4eec\u200b\u7684\u200b\u73af\u5883\u200b\u4e2d\u200b\u6ca1\u6709\u200btorchaudio\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bpip\u200b\u6216\u8005\u200bconda\u200b\u53bb\u200b\u5b89\u88c5\u200b\u5b83\u200b\u3002\u200b\u53ea\u200b\u9700\u8981\u200b\u6267\u884c\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5373\u53ef\u200b\uff1a</p> <p><pre><code>pip install torchaudio # conda install torchaudio\n</code></pre> \u200b\u5728\u200b\u5b89\u88c5\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u7684\u200bPyTorch\u200b\u7248\u672c\u200b\u548c\u200bPython\u200b\u7248\u672c\u200b\u9009\u62e9\u200b\u5bf9\u5e94\u200b\u7684\u200btorchaudio\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u5177\u4f53\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200btorchaudio Compatibility Matrix</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#843-datasets","title":"8.4.3 datasets\u200b\u7684\u200b\u6784\u5efa","text":"<p>torchaudio\u200b\u4e2d\u200b\u5bf9\u4e8e\u200b\u4e00\u4e9b\u200b\u516c\u5171\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4e3b\u8981\u200b\u901a\u8fc7\u200btorchaudio.datasets\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u79c1\u6709\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200btorch.utils.data.Dataset\u200b\u6765\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u8bfb\u53d6\u200b\u548c\u200b\u5904\u7406\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200btorch.utils.data.DataLoader\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002 <pre><code>import torchaudio\nimport torch\n\n# \u200b\u516c\u5171\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6784\u5efa\u200b\nyesno_data = torchaudio.datasets.YESNO('.', download=True)\ndata_loader = torch.utils.data.DataLoader(\n    yesno_data,\n    batch_size=1,\n    shuffle=True,\n    num_workers=4)\n</code></pre></p> <p>torchaudio\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5e38\u7528\u200b\u7684\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u62ec\u200bCMUARCTIC\uff0cCMUDict\uff0cCOMMONVOICE\uff0cDR_VCTK\uff0cFluentSpeechCommands\uff0cGTZAN\uff0cIEMOCAP\uff0cLIBRISPEECH\uff0cLIBRITTS\uff0cLJSPEECH\uff0cLibriLightLimited\uff0cLibriMix\uff0cMUSDB_HQ\uff0cQUESST14\uff0cSPEECHCOMMANDS\uff0cSnips\uff0cTEDLIUM\uff0cVCTK_092\uff0cVoxCeleb1Identification\uff0cVoxCeleb1Verification\uff0cYESNO\u200b\u7b49\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u6765\u200b\u67e5\u770b\u200b\uff1a</p> <p><pre><code>import torchaudio\ndir(torchaudio.datasets)\n</code></pre> <pre><code>'CMUARCTIC','CMUDict','COMMONVOICE','DR_VCTK','FluentSpeechCommands',\n'GTZAN','IEMOCAP','LIBRISPEECH','LIBRITTS','LJSPEECH','LibriLightLimited',\n'LibriMix','MUSDB_HQ','QUESST14','SPEECHCOMMANDS','Snips','TEDLIUM',\n'VCTK_092','VoxCeleb1Identification','VoxCeleb1Verification','YESNO']\n</code></pre></p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#844-modelpipeline","title":"8.4.4 model\u200b\u548c\u200bpipeline\u200b\u7684\u200b\u6784\u5efa","text":"<p>torchaudio.models\u200b\u5305\u542b\u200b\u4e86\u200b\u5e38\u89c1\u200b\u8bed\u97f3\u200b\u4efb\u52a1\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1aWav2Letter\uff0cDeepSpeech\uff0cHuBERTPretrainModel\u200b\u7b49\u200b\u3002torchaudio.pipelines\u200b\u5219\u200b\u662f\u200b\u5c06\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u548c\u200b\u5176\u200b\u5bf9\u5e94\u200b\u7684\u200b\u4efb\u52a1\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6784\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u8bed\u97f3\u200b\u5904\u7406\u200b\u6d41\u6c34\u7ebf\u200b\u3002torchaudio.pipeline\u200b\u76f8\u8f83\u200b\u4e8e\u200btorchvision\u200b\u8fd9\u79cd\u200b\u89c6\u89c9\u200b\u5e93\u200b\u800c\u8a00\u200b\uff0c\u200b\u662f\u200btorchaudio\u200b\u7684\u200b\u7cbe\u534e\u200b\u90e8\u5206\u200b\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u6b64\u200b\u4e5f\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u8fc7\u591a\u200b\u7684\u200b\u9610\u8ff0\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5b98\u65b9\u200b\u7ed9\u51fa\u200b\u7684\u200bPipeline Tutorials\u200b\u548c\u200btorchaudio.pipelines docs\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#845-transformsfunctional","title":"8.4.5 transforms\u200b\u548c\u200bfunctional\u200b\u7684\u200b\u4f7f\u7528","text":"<p>torchaudio.transform\u200b\u6a21\u5757\u200b\u5305\u542b\u200b\u5e38\u89c1\u200b\u7684\u200b\u97f3\u9891\u200b\u5904\u7406\u200b\u548c\u200b\u7279\u5f81\u63d0\u53d6\u200b\u3002torchaudio.functional\u200b\u5219\u200b\u5305\u62ec\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u5e38\u89c1\u200b\u7684\u200b\u97f3\u9891\u200b\u64cd\u4f5c\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u5173\u4e8e\u200btorchaudio.transform\uff0c\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6d41\u7a0b\u56fe\u200b\u4f9b\u200b\u6211\u4eec\u200b\u53c2\u8003\u200b\u5b66\u4e60\u200b\uff1a  torchaudio.transforms\u200b\u7ee7\u627f\u200b\u4e8e\u200btorch.nn.Module\uff0c\u200b\u4f46\u662f\u200b\u4e0d\u540c\u4e8e\u200btorchvision.transforms\uff0ctorchaudio\u200b\u6ca1\u6709\u200bcompose\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u591a\u4e2a\u200btransform\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\u3002\u200b\u56e0\u6b64\u200btorchaudio\u200b\u6784\u5efa\u200btransform pipeline\u200b\u7684\u200b\u5e38\u89c1\u200b\u65b9\u6cd5\u200b\u662f\u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u5757\u200b\u7c7b\u200b\u6216\u200b\u4f7f\u7528\u200btorch.nn.Sequential\u200b\u5c06\u200b\u4ed6\u4eec\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u7136\u540e\u200b\u5c06\u200b\u5176\u200b\u79fb\u52a8\u200b\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5b98\u65b9\u200b\u6240\u200b\u7ed9\u51fa\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff1a <pre><code># Define custom feature extraction pipeline.\n#\n# 1. Resample audio\n# 2. Convert to power spectrogram\n# 3. Apply augmentations\n# 4. Convert to mel-scale\n#\nclass MyPipeline(torch.nn.Module):\n    def __init__(\n        self,\n        input_freq=16000,\n        resample_freq=8000,\n        n_fft=1024,\n        n_mel=256,\n        stretch_factor=0.8,\n    ):\n        super().__init__()\n        self.resample = Resample(orig_freq=input_freq, new_freq=resample_freq)\n        self.spec = Spectrogram(n_fft=n_fft, power=2)\n        self.spec_aug = torch.nn.Sequential(\n            TimeStretch(stretch_factor, fixed_rate=True),\n            FrequencyMasking(freq_mask_param=80),\n            TimeMasking(time_mask_param=80),\n        )\n        self.mel_scale = MelScale(\n            n_mels=n_mel, sample_rate=resample_freq, n_stft=n_fft // 2 + 1)\n\n    def forward(self, waveform: torch.Tensor) -&gt; torch.Tensor:\n        # Resample the input\n        resampled = self.resample(waveform)\n        # Convert to power spectrogram\n        spec = self.spec(resampled)\n        # Apply SpecAugment\n        spec = self.spec_aug(spec)\n        # Convert to mel-scale\n        mel = self.mel_scale(spec)\n        return mel\n</code></pre> torchaudio.transform\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200btorchaudio.transforms\u200b\u8fdb\u4e00\u6b65\u200b\u4e86\u89e3\u200b\u3002</p> <p>torchaudio.functional\u200b\u652f\u6301\u200b\u4e86\u200b\u8bb8\u591a\u200b\u8bed\u97f3\u200b\u7684\u200b\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5173\u4e8e\u200btorchaudio.functional\u200b\u7684\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200btorchaudio.functional\u200b\u8fdb\u4e00\u6b65\u200b\u4e86\u89e3\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#846-compliancekaldi_io","title":"8.4.6 compliance\u200b\u548c\u200bkaldi_io\u200b\u7684\u200b\u4f7f\u7528","text":"<p>Kaldi\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u8bed\u97f3\u200b\u8bc6\u522b\u200b\u7814\u7a76\u200b\u7684\u200b\u5de5\u5177\u7bb1\u200b,\u200b\u7531\u200bCMU\u200b\u5f00\u53d1\u200b,\u200b\u5f00\u6e90\u200b\u514d\u8d39\u200b\u3002\u200b\u5b83\u200b\u5305\u542b\u200b\u4e86\u200b\u6784\u5efa\u200b\u8bed\u97f3\u200b\u8bc6\u522b\u7cfb\u7edf\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5168\u90e8\u200b\u7ec4\u4ef6\u200b,\u200b\u662f\u200b\u8bed\u97f3\u200b\u8bc6\u522b\u200b\u9886\u57df\u200b\u6700\u200b\u6d41\u884c\u200b\u548c\u200b\u5f71\u54cd\u529b\u200b\u6700\u5927\u200b\u7684\u200b\u5f00\u6e90\u200b\u5de5\u5177\u200b\u4e4b\u4e00\u200b\u3002torchaudio\u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u4e0e\u200bKaldi\u200b\u5de5\u5177\u200b\u517c\u5bb9\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u5206\u522b\u200b\u5c5e\u4e8e\u200btorchaduio.compliance.kaldi\uff0ctorchaduio.kaldi_io\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#torchaduiocompliancekaldi","title":"torchaduio.compliance.kaldi","text":"<p>\u200b\u5728\u200btorchaudio.compliance.kaldi\u200b\u4e2d\u200b\uff0ctorchaudio\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u4e09\u79cd\u200b\u65b9\u6cd5\u200b\uff1a - torchaudio.compliance.kaldi.spectrogram\uff1a\u200b\u4ece\u200b\u8bed\u97f3\u200b\u4fe1\u53f7\u200b\u4e2d\u200b\u63d0\u53d6\u200bSpectrogram\u200b\u7279\u5f81\u200b - torchaudio.compliance.kaldi.fbank\uff1a\u200b\u4ece\u200b\u8bed\u97f3\u200b\u4fe1\u53f7\u200b\u4e2d\u200b\u63d0\u53d6\u200bFBank\u200b\u7279\u5f81\u200b - torchaduio.compliance.kaldi.mfcc\uff1a\u200b\u4ece\u200b\u8bed\u97f3\u200b\u4fe1\u53f7\u200b\u4e2d\u200b\u63d0\u53d6\u200bMFCC\u200b\u7279\u5f81\u200b</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#torchaduiokaldi_io","title":"torchaduio.kaldi_io","text":"<p>torchaudio.kaldi_io\u200b\u662f\u200b\u4e00\u4e2a\u200btorchaudio\u200b\u7684\u200b\u5b50\u200b\u6a21\u5757\u200b,\u200b\u7528\u4e8e\u200b\u8bfb\u53d6\u200b\u548c\u200b\u5199\u5165\u200bKaldi\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u3002\u200b\u5f53\u200b\u6211\u4eec\u200b\u8981\u200b\u4f7f\u7528\u200btorchaudio.kaldi_io\u200b\u65f6\u200b,\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u786e\u4fdd\u200bkalid_io\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u4e3b\u8981\u200b\u63a5\u53e3\u200b\u5305\u62ec\u200b\uff1a - torchaudio.kaldi_io.read_vec_int_ark\uff1a\u200b\u4ece\u200bKaldi\u200b\u7684\u200bscp\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u8bfb\u53d6\u200bfloat\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b - torchaudio.kaldi_io.read_vec_flt_scp - torchaudio.kaldi_io.read_vec_flt_ark - torchaudio.kaldi_io.read_mat_scp - torchaudio.kaldi_io.read_mat_ark</p> <p>\u200b\u5177\u4f53\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200btorchaudio.kaldi_io\u200b\u8fdb\u4e00\u6b65\u200b\u4e86\u89e3\u200b\u3002</p>"},{"location":"08-%E7%AC%AC%E5%85%AB%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/8.5%20%E9%9F%B3%E9%A2%91%20-%20torchaudio/#_1","title":"\u603b\u7ed3","text":"<p>\u200b\u672c\u200b\u8282\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u4ecb\u7ecd\u200b\u4e86\u200btorchaudio\u200b\u7684\u200b\u57fa\u672c\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u548c\u200b\u5e38\u7528\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u5982\u679c\u200b\u60f3\u8981\u200b\u8fdb\u4e00\u6b65\u200b\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200btorchaudio\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u3002</p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/","title":"9.1 \u200b\u4f7f\u7528\u200bONNX\u200b\u8fdb\u884c\u200b\u90e8\u7f72\u200b\u5e76\u200b\u63a8\u7406","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u6700\u7ec8\u200b\u76ee\u7684\u200b\u662f\u200b\u8981\u200b\u5b9e\u73b0\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u7f72\u200b\u4ee5\u200b\u65b9\u4fbf\u200b\u6211\u4eec\u200b\u7684\u200b\u751f\u6d3b\u200b\u548c\u200b\u89e3\u51b3\u200b\u4f20\u7edf\u200b\u65b9\u6cd5\u200b\u4e0d\u80fd\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u901a\u5e38\u200b\u4eba\u4eec\u200b\u4f1a\u200b\u5c06\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5728\u200b\u624b\u673a\u200b\u7aef\u200b\u3001\u200b\u5f00\u53d1\u677f\u200b\uff0c\u200b\u5d4c\u5165\u5f0f\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u4f46\u662f\u200b\u8fd9\u4e9b\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u7531\u4e8e\u200b\u6846\u67b6\u200b\u7684\u200b\u89c4\u6a21\u200b\uff0c\u200b\u73af\u5883\u200b\u4f9d\u8d56\u200b\uff0c\u200b\u7b97\u529b\u200b\u7684\u200b\u9650\u5236\u200b\uff0c\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5f97\u5230\u200b\u7684\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u624d\u80fd\u200b\u4f7f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u6210\u529f\u200b\u90e8\u7f72\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002\u200b\u800c\u200b\u7ecf\u8fc7\u200b\u5de5\u4e1a\u754c\u200b\u548c\u200b\u5b66\u672f\u754c\u200b\u591a\u5e74\u200b\u7684\u200b\u63a2\u7d22\u200b\uff0c\u200b\u51fa\u73b0\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200bpipeline\uff1a</p> <p></p> <p>\u200b\u800c\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5c06\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200bONNX \u200b\u683c\u5f0f\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200bONNX Runtime\u200b\u8fd0\u884c\u200b\u5b83\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002\u200b\u901a\u8fc7\u200b\u672c\u8282\u200b\u8bfe\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ol> <li>\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u7684\u200b\u6574\u4f53\u200b\u6d41\u7a0b\u200b</li> <li>\u200b\u4f7f\u7528\u200btorch.onnx\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u683c\u5f0f\u200b\u7684\u200b\u8f6c\u5316\u200b</li> <li>\u200b\u4f7f\u7528\u200bONNX Runtime\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b</li> <li>\u200b\u5b8c\u6574\u200b\u7684\u200b\u5b98\u65b9\u200b\u4ee3\u7801\u200b\u89e3\u91ca\u200b</li> </ol>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#911-onnxonnx-runtime","title":"9.1.1 ONNX\u200b\u548c\u200bONNX Runtime\u200b\u7b80\u4ecb","text":""},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9111-onnx","title":"9.1.1.1 ONNX\u200b\u7b80\u4ecb","text":"<ul> <li>ONNX\u200b\u5b98\u7f51\u200b\uff1ahttps://onnx.ai/</li> <li>ONNX GitHub\uff1ahttps://github.com/onnx/onnx</li> </ul> <p>ONNX( Open Neural Network Exchange) \u200b\u662f\u200b Facebook (\u200b\u73b0\u200bMeta) \u200b\u548c\u200b\u5fae\u8f6f\u200b\u5728\u200b2017\u200b\u5e74\u200b\u5171\u540c\u200b\u53d1\u5e03\u200b\u7684\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6807\u51c6\u200b\u63cf\u8ff0\u200b\u8ba1\u7b97\u200b\u56fe\u200b\u7684\u200b\u4e00\u79cd\u200b\u683c\u5f0f\u200b\u3002ONNX\u200b\u901a\u8fc7\u200b\u5b9a\u4e49\u200b\u4e00\u7ec4\u200b\u4e0e\u200b\u73af\u5883\u200b\u548c\u200b\u5e73\u53f0\u200b\u65e0\u5173\u200b\u7684\u200b\u6807\u51c6\u200b\u683c\u5f0f\u200b\uff0c\u200b\u4f7f\u200bAI\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0d\u540c\u200b\u6846\u67b6\u200b\u548c\u200b\u73af\u5883\u200b\u4e0b\u200b\u4ea4\u4e92\u200b\u4f7f\u7528\u200b\uff0cONNX\u200b\u53ef\u4ee5\u200b\u770b\u4f5c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u548c\u200b\u90e8\u7f72\u200b\u7aef\u7684\u200b\u6865\u6881\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u7f16\u8bd1\u5668\u200b\u7684\u200b\u4e2d\u95f4\u200b\u8bed\u8a00\u200b\u4e00\u6837\u200b\u3002\u200b\u7531\u4e8e\u200b\u5404\u200b\u6846\u67b6\u200b\u517c\u5bb9\u6027\u200b\u4e0d\u200b\u4e00\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u53ea\u7528\u200b ONNX \u200b\u8868\u793a\u200b\u66f4\u200b\u5bb9\u6613\u200b\u90e8\u7f72\u200b\u7684\u200b\u9759\u6001\u200b\u56fe\u200b\u3002\u200b\u786c\u4ef6\u200b\u548c\u200b\u8f6f\u4ef6\u5382\u5546\u200b\u53ea\u200b\u9700\u8981\u200b\u57fa\u4e8e\u200bONNX\u200b\u6807\u51c6\u200b\u4f18\u5316\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\uff0c\u200b\u8ba9\u200b\u6240\u6709\u200b\u517c\u5bb9\u200bONNX\u200b\u6807\u51c6\u200b\u7684\u200b\u6846\u67b6\u200b\u53d7\u76ca\u200b\u3002\u200b\u76ee\u524d\u200b\uff0cONNX\u200b\u4e3b\u8981\u200b\u5173\u6ce8\u200b\u5728\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u65b9\u9762\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u6846\u67b6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f6c\u5316\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u5bb9\u6613\u200b\u7684\u200b\u90e8\u7f72\u200b\u5728\u200b\u517c\u5bb9\u200bONNX\u200b\u7684\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u4e2d\u200b\u3002\u200b\u76ee\u524d\u200b\uff0c\u200b\u5728\u200b\u5fae\u8f6f\u200b\uff0c\u200b\u4e9a\u9a6c\u900a\u200b \uff0cFacebook(\u200b\u73b0\u200bMeta) \u200b\u548c\u200b IBM \u200b\u7b49\u200b\u516c\u53f8\u200b\u548c\u200b\u4f17\u591a\u200b\u5f00\u6e90\u200b\u8d21\u732e\u200b\u7684\u200b\u5171\u540c\u200b\u7ef4\u62a4\u200b\u4e0b\u200b\uff0cONNX \u200b\u5df2\u7ecf\u200b\u5bf9\u63a5\u200b\u4e86\u200b\u4e0b\u56fe\u200b\u7684\u200b\u591a\u79cd\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u548c\u200b\u591a\u79cd\u200b\u63a8\u7406\u200b\u5f15\u64ce\u200b\u3002</p> <p></p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9112-onnx-runtime","title":"9.1.1.2 ONNX Runtime\u200b\u7b80\u4ecb","text":"<ul> <li>ONNX Runtime\u200b\u5b98\u7f51\u200b\uff1ahttps://www.onnxruntime.ai/</li> <li>ONNX Runtime GitHub\uff1ahttps://github.com/microsoft/onnxruntime</li> </ul> <p>ONNX Runtime \u200b\u662f\u200b\u7531\u200b\u5fae\u8f6f\u200b\u7ef4\u62a4\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8de8\u5e73\u53f0\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u63a8\u7406\u200b\u52a0\u901f\u5668\u200b\uff0c\u200b\u5b83\u200b\u76f4\u63a5\u200b\u5bf9\u63a5\u200bONNX\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8bfb\u53d6\u200b.onnx\u200b\u6587\u4ef6\u200b\u5e76\u200b\u5b9e\u73b0\u200b\u63a8\u7406\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u518d\u200b\u628a\u200b .onnx \u200b\u683c\u5f0f\u200b\u7684\u200b\u6587\u4ef6\u200b\u8f6c\u6362\u6210\u200b\u5176\u4ed6\u200b\u683c\u5f0f\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002PyTorch\u200b\u501f\u52a9\u200bONNX Runtime\u200b\u4e5f\u200b\u5b8c\u6210\u200b\u4e86\u200b\u90e8\u7f72\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u200b\u516c\u91cc\u200b\uff0c\u200b\u6784\u5efa\u200b\u4e86\u200b PyTorch --&gt; ONNX --&gt; ONNX Runtime \u200b\u90e8\u7f72\u200b\u6d41\u6c34\u7ebf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200b .onnx \u200b\u6587\u4ef6\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b ONNX Runtime \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9113-onnxonnx-runtime","title":"9.1.1.3 ONNX\u200b\u548c\u200bONNX Runtime\u200b\u7684\u200b\u5b89\u88c5","text":"<p>ONNX\u200b\u548c\u200bONNX Runtime\u200b\u4f5c\u4e3a\u200bpython\u200b\u7684\u200b\u4e00\u4e2a\u5305\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u5305\u200b\u7684\u200b\u5b89\u88c5\u200b\u65b9\u6cd5\u200b\u76f8\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4f7f\u7528\u200bconda\u200b\u6216\u8005\u200bpip\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u8f93\u5165\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5373\u53ef\u200b\uff1a</p> <pre><code># \u200b\u6fc0\u6d3b\u200b\u865a\u62df\u73af\u5883\u200b\nconda activate env_name # env_name\u200b\u6362\u6210\u200b\u73af\u5883\u200b\u540d\u79f0\u200b\n# \u200b\u5b89\u88c5\u200bonnx\npip install onnx \n# \u200b\u5b89\u88c5\u200bonnx runtime\npip install onnxruntime # \u200b\u4f7f\u7528\u200bCPU\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\n# pip install onnxruntime-gpu # \u200b\u4f7f\u7528\u200bGPU\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\n</code></pre> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u9700\u8981\u200b\u6ce8\u610f\u200bONNX\u200b\u548c\u200bONNX Runtime\u200b\u4e4b\u95f4\u200b\u7684\u200b\u9002\u914d\u200b\u5173\u7cfb\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200bONNX Runtime\u200b\u7684\u200bGithub\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\uff0c\u200b\u94fe\u63a5\u200b\u5730\u5740\u200b\u5982\u4e0b\u200b\uff1a</p> <p>ONNX\u200b\u548c\u200bONNX Runtime\u200b\u7684\u200b\u9002\u914d\u200b\u5173\u7cfb\u200b\uff1ahttps://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u60f3\u200b\u4f7f\u7528\u200bGPU\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u5c06\u200b\u5b89\u88c5\u200b\u7684\u200bonnxruntime\u200b\u5378\u8f7d\u200b\uff0c\u200b\u518d\u200b\u5b89\u88c5\u200bonnxruntime-gpu\uff0c\u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u8fd8\u200b\u9700\u8981\u200b\u8003\u8651\u200bONNX Runtime\u200b\u4e0e\u200bCUDA\u200b\u4e4b\u95f4\u200b\u7684\u200b\u9002\u914d\u200b\u5173\u7cfb\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4ee5\u4e0b\u200b\u94fe\u63a5\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\uff1a</p> <p>ONNX Runtime\u200b\u548c\u200bCUDA\u200b\u4e4b\u95f4\u200b\u7684\u200b\u9002\u914d\u200b\u5173\u7cfb\u200b\uff1ahttps://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html</p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#912-onnx","title":"9.1.2 \u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e3a\u200bONNX","text":""},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9121-onnx","title":"9.1.2.1 \u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f","text":"<p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>torch.onnx.export()</code>\u200b\u628a\u200b\u6a21\u578b\u200b\u8f6c\u6362\u6210\u200b ONNX \u200b\u683c\u5f0f\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u6a21\u578b\u200b\u5bfc\u6210\u200bonnx\u200b\u683c\u5f0f\u200b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u8c03\u7528\u200b<code>model.eval()</code>\u200b\u6216\u8005\u200b<code>model.train(False)</code>\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5904\u5728\u200b\u63a8\u7406\u200b\u6a21\u5f0f\u200b\u4e0b\u200b\uff0c\u200b\u907f\u514d\u200b\u56e0\u4e3a\u200b<code>dropout</code>\u200b\u6216\u200b<code>batchnorm</code>\u200b\u7b49\u200b\u8fd0\u7b97\u7b26\u200b\u5728\u200b\u63a8\u7406\u200b\u548c\u200b\u8bad\u7ec3\u200b\u6a21\u5f0f\u200b\u4e0b\u200b\u7684\u200b\u4e0d\u540c\u200b\u4ea7\u751f\u200b\u9519\u8bef\u200b\u3002\uff08\u200b\u56e0\u4e3a\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u504f\u5411\u200b\u5b9e\u6218\u200b\uff0c\u200b\u5bf9\u4e8e\u200bONNX\u200b\u7684\u200b\u5185\u90e8\u200b\u6d41\u7a0b\u200b\u4e0d\u200b\u505a\u200b\u8fc7\u200b\u591a\u200b\u8d58\u8ff0\u200b\uff0c\u200b\u6709\u200b\u5174\u8da3\u200b\u7684\u200b\u540c\u5b66\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u66f4\u6df1\u200b\u4e00\u6b65\u200b\u4e86\u89e3\u200b\uff09</p> <pre><code>import torch.onnx \n# \u200b\u8f6c\u6362\u200b\u7684\u200bonnx\u200b\u683c\u5f0f\u200b\u7684\u200b\u540d\u79f0\u200b\uff0c\u200b\u6587\u4ef6\u200b\u540e\u7f00\u200b\u9700\u4e3a\u200b.onnx\nonnx_file_name = \"xxxxxx.onnx\"\n# \u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8f6c\u6362\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5c06\u200btorch_model\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\nmodel = torch_model\n# \u200b\u52a0\u8f7d\u200b\u6743\u91cd\u200b\uff0c\u200b\u5c06\u200bmodel.pth\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\n# \u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u662f\u200b\u4f7f\u7528\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u51fa\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u53bb\u9664\u200b\u6743\u91cd\u200b\u4e2d\u591a\u200b\u7684\u200bmodule. \u200b\u5177\u4f53\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u89c1\u200b5.4\u200b\u8282\u200b\nmodel = model.load_state_dict(torch.load(\"model.pth\"))\n# \u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u524d\u200b\uff0c\u200b\u5fc5\u987b\u200b\u8c03\u7528\u200bmodel.eval()\u200b\u6216\u8005\u200bmodel.train(False)\nmodel.eval()\n# dummy_input\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u7684\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u4ec5\u200b\u63d0\u4f9b\u200b\u8f93\u5165\u200bshape\u3001type\u200b\u7b49\u200b\u4fe1\u606f\u200b \nbatch_size = 1 # \u200b\u968f\u673a\u200b\u7684\u200b\u53d6\u503c\u200b\uff0c\u200b\u5f53\u200b\u8bbe\u7f6e\u200bdynamic_axes\u200b\u540e\u200b\u5f71\u54cd\u200b\u4e0d\u200b\u5927\u200b\ndummy_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True) \n# \u200b\u8fd9\u7ec4\u200b\u8f93\u5165\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\noutput = model(dummy_input)\n# \u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\ntorch.onnx.export(model,        # \u200b\u6a21\u578b\u200b\u7684\u200b\u540d\u79f0\u200b\n                  dummy_input,   # \u200b\u4e00\u7ec4\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u8f93\u5165\u200b\n                  onnx_file_name,   # \u200b\u6587\u4ef6\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b/\u200b\u540d\u79f0\u200b\n                  export_params=True,        #  \u200b\u5982\u679c\u200b\u6307\u5b9a\u200b\u4e3a\u200bTrue\u200b\u6216\u200b\u9ed8\u8ba4\u200b, \u200b\u53c2\u6570\u200b\u4e5f\u200b\u4f1a\u200b\u88ab\u200b\u5bfc\u51fa\u200b. \u200b\u5982\u679c\u200b\u4f60\u200b\u8981\u200b\u5bfc\u51fa\u200b\u4e00\u4e2a\u200b\u6ca1\u200b\u8bad\u7ec3\u200b\u8fc7\u200b\u7684\u200b\u5c31\u200b\u8bbe\u4e3a\u200b False.\n                  opset_version=10,          # ONNX \u200b\u7b97\u200b\u5b50\u96c6\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u5f53\u524d\u200b\u5df2\u200b\u66f4\u65b0\u200b\u5230\u200b15\n                  do_constant_folding=True,  # \u200b\u662f\u5426\u200b\u6267\u884c\u200b\u5e38\u91cf\u200b\u6298\u53e0\u200b\u4f18\u5316\u200b\n                  input_names = ['input'],   # \u200b\u8f93\u5165\u200b\u6a21\u578b\u200b\u7684\u200b\u5f20\u91cf\u200b\u7684\u200b\u540d\u79f0\u200b\n                  output_names = ['output'], # \u200b\u8f93\u51fa\u200b\u6a21\u578b\u200b\u7684\u200b\u5f20\u91cf\u200b\u7684\u200b\u540d\u79f0\u200b\n                  # dynamic_axes\u200b\u5c06\u200bbatch_size\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6307\u5b9a\u200b\u4e3a\u200b\u52a8\u6001\u200b\uff0c\n                  # \u200b\u540e\u7eed\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u53ef\u4ee5\u200b\u4e0e\u200b\u5bfc\u51fa\u200b\u7684\u200bdummy_input\u200b\u7684\u200bbatch_size\u200b\u4e0d\u540c\u200b\n                  dynamic_axes={'input' : {0 : 'batch_size'},    \n                                'output' : {0 : 'batch_size'}})\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9122-onnx","title":"9.1.2.2 ONNX\u200b\u6a21\u578b\u200b\u7684\u200b\u68c0\u9a8c","text":"<p>\u200b\u5f53\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u8fd0\u884c\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200bONNX \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u3002\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u68c0\u6d4b\u200b\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u662f\u5426\u200b\u53ef\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b<code>onnx.checker.check_model()</code>\u200b\u8fdb\u884c\u200b\u68c0\u9a8c\u200b\uff0c\u200b\u5177\u4f53\u65b9\u6cd5\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>import onnx\n# \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5f02\u5e38\u200b\u5904\u7406\u200b\u7684\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u68c0\u9a8c\u200b\ntry:\n    # \u200b\u5f53\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0d\u53ef\u200b\u7528\u65f6\u200b\uff0c\u200b\u5c06\u4f1a\u200b\u62a5\u51fa\u200b\u5f02\u5e38\u200b\n    onnx.checker.check_model(self.onnx_model)\nexcept onnx.checker.ValidationError as e:\n    print(\"The model is invalid: %s\"%e)\nelse:\n    # \u200b\u6a21\u578b\u200b\u53ef\u7528\u200b\u65f6\u200b\uff0c\u200b\u5c06\u200b\u4e0d\u4f1a\u200b\u62a5\u51fa\u200b\u5f02\u5e38\u200b\uff0c\u200b\u5e76\u4f1a\u200b\u8f93\u51fa\u200b\u201cThe model is valid!\u201d\n    print(\"The model is valid!\")\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#9123-onnx","title":"9.1.2.3 ONNX\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u5728\u200b\u5c06\u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e3a\u200bonnx\u200b\u683c\u5f0f\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6709\u4e2a\u200b\u5de5\u5177\u200b\u53ef\u4ee5\u200b\u50cf\u200bTensorboard\u200b\u4e00\u6837\u200b\u53ef\u89c6\u5316\u200b\u6a21\u578b\u200b\u6765\u200b\u89c2\u5bdf\u200b\u6bcf\u4e2a\u200b\u8282\u70b9\u200b\u7684\u200b\u5c5e\u6027\u200b\u7279\u5f81\u200b\u3002\u200b\u968f\u7740\u200b<code>Netron</code>\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200bonnx\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u3002 - Netron\u200b\u4e0b\u8f7d\u200b\u7f51\u5740\u200b\uff1ahttps://github.com/lutzroeder/netron </p> <p>\u200b\u4f7f\u7528\u200bNetron\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u4ec5\u200b\u80fd\u200b\u770b\u5230\u200b\u6574\u4f53\u200b\u6a21\u578b\u200b\u7684\u200b\u67b6\u6784\u200b\uff0c\u200b\u8fd8\u200b\u80fd\u200b\u770b\u5230\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u8282\u70b9\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200bNetron\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200bsqueezenet\u200b\u4e3a\u4f8b\u200b\u8fdb\u884c\u200b\u4ecb\u7ecd\u200b\u3002\u200b\u4e0b\u9762\u200b\u7b2c\u4e00\u5e45\u200b\u56fe\u200b\u622a\u53d6\u200b\u81ea\u200bsqueezenet\u200b\u7f51\u7edc\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u7f51\u7edc\u200b\u7684\u200b\u6574\u4f53\u200b\u6d41\u7a0b\u200b\u548c\u200b\u8f93\u5165\u200b\u3002</p> <p></p> <p>\u200b\u7b2c\u4e8c\u5e45\u200b\u56fe\u200b\u663e\u793a\u200b\u4e86\u200b\u7b2c\u4e00\u4e2a\u200bConv\u200b\u7684\u200b\u4fe1\u606f\u200b\u5305\u62ec\u200bkernel_size\uff0cstrides\uff0cinput\uff0coutput\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u540c\u7406\u200b\u5f53\u200b\u6211\u4eec\u200b\u70b9\u51fb\u200b\u5176\u4ed6\u200b\u8282\u70b9\u200b\u65f6\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u663e\u793a\u200b\u8be5\u200b\u8282\u70b9\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p></p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#913-onnx-runtime","title":"9.1.3 \u200b\u4f7f\u7528\u200bONNX Runtime\u200b\u8fdb\u884c\u200b\u63a8\u7406","text":"<p>\u200b\u901a\u8fc7\u200b\u4ee5\u4e0a\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200bPyTorch\u200b\u7684\u200b\u6a21\u578b\u200b\u5230\u200bONNX\u200b\u6a21\u578b\u200b\u7684\u200b\u8f6c\u6362\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200bNetron\u200b\u53ef\u89c6\u5316\u200b\u548c\u200b<code>onnx.checker.check_model()</code>\u200b\u68c0\u67e5\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u6b63\u786e\u6027\u200b\u3002\u200b\u5728\u200b\u8fd9\u200b\u4e00\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bONNX Runtime\u200b\u8fd0\u884c\u200b\u4e00\u4e0b\u200b\u8f6c\u5316\u200b\u540e\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u200b\u4e00\u4e0b\u200b\u63a8\u7406\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <pre><code># \u200b\u5bfc\u5165\u200bonnxruntime\nimport onnxruntime\n# \u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u7684\u200bonnx\u200b\u6a21\u578b\u200b\u6587\u4ef6\u540d\u79f0\u200b\nonnx_file_name = \"xxxxxx.onnx\"\n\n# onnxruntime.InferenceSession\u200b\u7528\u4e8e\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b ONNX Runtime \u200b\u63a8\u7406\u200b\u5668\u200b\nort_session = onnxruntime.InferenceSession(onnx_file_name)  \n\n# \u200b\u6784\u5efa\u200b\u5b57\u5178\u200b\u7684\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\uff0c\u200b\u5b57\u5178\u200b\u7684\u200bkey\u200b\u9700\u8981\u200b\u4e0e\u200b\u6211\u4eec\u200b\u6784\u5efa\u200bonnx\u200b\u6a21\u578b\u200b\u65f6\u200b\u7684\u200binput_names\u200b\u76f8\u540c\u200b\n# \u200b\u8f93\u5165\u200b\u7684\u200binput_img \u200b\u4e5f\u200b\u9700\u8981\u200b\u6539\u53d8\u200b\u4e3a\u200bndarray\u200b\u683c\u5f0f\u200b\nort_inputs = {'input': input_img} \n# \u200b\u6211\u4eec\u200b\u66f4\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b,\u200b\u56e0\u4e3a\u200b\u907f\u514d\u200b\u4e86\u200b\u624b\u52a8\u200b\u8f93\u5165\u200bkey\n# ort_inputs = {ort_session.get_inputs()[0].name:input_img}\n\n# run\u200b\u662f\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u4e3a\u200b\u8f93\u51fa\u200b\u5f20\u91cf\u200b\u540d\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bNone\n# \u200b\u7b2c\u4e8c\u4e2a\u200b\u53c2\u6570\u200b\u4e3a\u200b\u6784\u5efa\u200b\u7684\u200b\u8f93\u5165\u200b\u503c\u200b\u7684\u200b\u5b57\u5178\u200b\n# \u200b\u7531\u4e8e\u200b\u8fd4\u56de\u200b\u7684\u200b\u7ed3\u679c\u200b\u88ab\u200b\u5217\u8868\u200b\u5d4c\u5957\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b[0]\u200b\u7684\u200b\u7d22\u5f15\u200b\nort_output = ort_session.run(None,ort_inputs)[0]\n# output = {ort_session.get_outputs()[0].name}\n# ort_output = ort_session.run([output], ort_inputs)[0]\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\u51e0\u4e2a\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u70b9\u200b\uff1a</p> <ol> <li>PyTorch\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u200b\u4e3a\u200btensor\uff0c\u200b\u800c\u200bONNX\u200b\u7684\u200b\u8f93\u5165\u200b\u4e3a\u200barray\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u6216\u8005\u200b\u76f4\u63a5\u200b\u5c06\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u4e3a\u200barray\u200b\u683c\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u4e0b\u9762\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u5f20\u91cf\u200b\u5230\u200barray\u200b\u7684\u200b\u8f6c\u5316\u200b\u3002</li> </ol> <pre><code>def to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n</code></pre> <ol> <li>\u200b\u8f93\u5165\u200b\u7684\u200barray\u200b\u7684\u200bshape\u200b\u5e94\u8be5\u200b\u548c\u200b\u6211\u4eec\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\u7684\u200b<code>dummy_input</code>\u200b\u7684\u200bshape\u200b\u76f8\u540c\u200b\uff0c\u200b\u5982\u679c\u200b\u56fe\u7247\u5927\u5c0f\u200b\u4e0d\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5148\u200b\u8fdb\u884c\u200bresize\u200b\u64cd\u4f5c\u200b\u3002</li> <li>run\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5217\u8868\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u64cd\u4f5c\u200b\u624d\u80fd\u200b\u83b7\u5f97\u200barray\u200b\u683c\u5f0f\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u5728\u200b\u6784\u5efa\u200b\u8f93\u5165\u200b\u7684\u200b\u5b57\u5178\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u5b57\u5178\u200b\u7684\u200bkey\u200b\u5e94\u200b\u4e0e\u200b\u5bfc\u51fa\u200bONNX\u200b\u683c\u5f0f\u200b\u8bbe\u7f6e\u200b\u7684\u200binput_name\u200b\u76f8\u540c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u66f4\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u7b2c\u4e8c\u79cd\u200b\u65b9\u6cd5\u200b\u6784\u5efa\u200b\u8f93\u5165\u200b\u7684\u200b\u5b57\u5178\u200b\u3002</li> </ol>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#914","title":"9.1.4 \u200b\u4ee3\u7801\u200b\u5b9e\u6218","text":"<p>\u200b\u672c\u200b\u90e8\u5206\u200b\u4ee3\u7801\u200b\u9009\u200b\u81ea\u200bPyTorch\u200b\u5b98\u7f51\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8bbf\u95ee\u200b\u94fe\u63a5\u200b\u53ef\u200b\u70b9\u51fb\u200b\u4e0b\u65b9\u200b\u63a8\u8350\u200b\u9605\u8bfb\u200b2\u200b\u8fdb\u884c\u200b\u67e5\u770b\u200b\u548c\u200b\u4f7f\u7528\u200b\u5b98\u7f51\u200b\u63d0\u4f9b\u200b\u7684\u200bColab\u200b\u5bf9\u200bonnx\u200b\u5bfc\u51fa\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u7406\u89e3\u200b\u3002\u200b\u5982\u200b\u6709\u200b\u4fb5\u6743\u200b\uff0c\u200b\u53ef\u200b\u7acb\u5373\u200b\u5220\u9664\u200b</p>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#1","title":"1. \u200b\u5b9a\u4e49\u200b\u8d85\u200b\u5206\u8fa8\u200b\u6a21\u578b","text":"<pre><code># \u200b\u5bfc\u5165\u200b\u76f8\u5173\u200b\u5305\u200b\nimport io\nimport numpy as np\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx\nimport torch.nn as nn\nimport torch.nn.init as init\n\n# \u200b\u5b9a\u4e49\u200b\u8d85\u200b\u5206\u8fa8\u200b\u7f51\u7edc\u200b\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    # \u200b\u6a21\u578b\u200b\u521d\u59cb\u5316\u200b\n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv4.weight)\n\n# \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u6a21\u578b\u200b\ntorch_model = SuperResolutionNet(upscale_factor=3)\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#2-onnx","title":"2. \u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f","text":"<pre><code>model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n# \u200b\u52a0\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u6743\u91cd\u200b\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n# \u200b\u5c06\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u63a8\u7406\u200b\u6a21\u5f0f\u200b\ntorch_model.eval()\n# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# \u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\ntorch.onnx.export(torch_model,               # model being run\n                  x,             # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,   # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  # variable length axes\n                  dynamic_axes={'input' : {0 : 'batch_size'},    \n                                'output' : {0 : 'batch_size'}})\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#3-onnx","title":"3. \u200b\u68c0\u9a8c\u200bONNX\u200b\u6a21\u578b","text":"<pre><code>import onnx\n# \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5f02\u5e38\u200b\u5904\u7406\u200b\u7684\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u68c0\u9a8c\u200b\ntry:\n    # \u200b\u5f53\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0d\u53ef\u200b\u7528\u65f6\u200b\uff0c\u200b\u5c06\u4f1a\u200b\u62a5\u51fa\u200b\u5f02\u5e38\u200b\n    onnx.checker.check_model(\"super_resolution.onnx\")\nexcept onnx.checker.ValidationError as e:\n    print(\"The model is invalid: %s\"%e)\nelse:\n    # \u200b\u6a21\u578b\u200b\u53ef\u7528\u200b\u65f6\u200b\uff0c\u200b\u5c06\u200b\u4e0d\u4f1a\u200b\u62a5\u51fa\u200b\u5f02\u5e38\u200b\uff0c\u200b\u5e76\u4f1a\u200b\u8f93\u51fa\u200b\u201cThe model is valid!\u201d\n    print(\"The model is valid!\")\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#4-onnx-runtime","title":"4. \u200b\u4f7f\u7528\u200bONNX Runtime\u200b\u8fdb\u884c\u200b\u63a8\u7406","text":"<pre><code>import onnxruntime\n\nort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n\n# \u200b\u5c06\u200b\u5f20\u91cf\u200b\u8f6c\u5316\u200b\u4e3a\u200bndarray\u200b\u683c\u5f0f\u200b\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# \u200b\u6784\u5efa\u200b\u8f93\u5165\u200b\u7684\u200b\u5b57\u5178\u200b\u548c\u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nort_outs = ort_session.run(None, ort_inputs)\n\n# \u200b\u6bd4\u8f83\u200b\u4f7f\u7528\u200bPyTorch\u200b\u548c\u200bONNX Runtime\u200b\u5f97\u51fa\u200b\u7684\u200b\u7cbe\u5ea6\u200b\nnp.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n\nprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#5","title":"5. \u200b\u8fdb\u884c\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u5e76\u200b\u53ef\u89c6\u5316","text":"<pre><code>from PIL import Image\nimport torchvision.transforms as transforms\n\n# \u200b\u8bfb\u53d6\u200b\u56fe\u7247\u200b\nimg = Image.open(\"/cat_224x224.jpg\")\n# \u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200bresize\u200b\u64cd\u4f5c\u200b\nresize = transforms.Resize([224, 224])\nimg = resize(img)\n\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\nto_tensor = transforms.ToTensor()\nimg_y = to_tensor(img_y)\nimg_y.unsqueeze_(0)\n# \u200b\u6784\u5efa\u200b\u8f93\u5165\u200b\u7684\u200b\u5b57\u5178\u200b\u5e76\u200b\u5c06\u200bvalue\u200b\u8f6c\u6362\u200b\u4f4d\u200barray\u200b\u683c\u5f0f\u200b\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\nort_outs = ort_session.run(None, ort_inputs)\nimg_out_y = ort_outs[0]\nimg_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n\n# \u200b\u4fdd\u5b58\u200b\u6700\u540e\u200b\u5f97\u5230\u200b\u7684\u200b\u56fe\u7247\u200b\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\n\nfinal_img.save(\"/cat_superres_with_ort.jpg\")\n</code></pre>"},{"location":"09-%E7%AC%AC%E4%B9%9D%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86/#_1","title":"\u53c2\u8003\u8d44\u6599","text":"<ol> <li>miscroft-\u200b\u5c06\u200b PyTorch \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200b ONNX</li> <li>pytorch- EXPORTING A MODEL FROM PYTORCH TO ONNX AND RUNNING IT USING ONNX RUNTIME</li> <li>ONNX tutorials</li> </ol>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.1%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/","title":"10.1 \u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u7b80\u4ecb\u200b\uff08\u200b\u8865\u5145\u200b\u4e2d\u200b\uff09","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","title":"\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b80\u4ecb","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#_2","title":"\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6982\u8ff0","text":"<p>\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6839\u636e\u200b\u6574\u5f20\u200b\u56fe\u50cf\u200b\u5185\u5bb9\u200b\u8fdb\u884c\u200b\u63cf\u8ff0\u200b\uff0c\u200b\u5e76\u200b\u7ed3\u5408\u200b\u76ee\u6807\u200b\u7269\u4f53\u200b\u7684\u200b\u7279\u5f81\u200b\u4fe1\u606f\u200b\uff0c\u200b\u786e\u5b9a\u200b\u8be5\u200b\u7269\u4f53\u200b\u7684\u200b\u7c7b\u522b\u200b\u4e0e\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u4e0d\u540c\u4e8e\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u8f93\u51fa\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u4e3b\u8981\u200b\u7269\u4f53\u200b\u5bf9\u8c61\u200b\u7684\u200b\u7c7b\u6807\u200b\uff0c\u200b\u5728\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u91cc\u200b\u5f80\u5f80\u200b\u542b\u6709\u200b\u591a\u4e2a\u200b\u7269\u4f53\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u4ec5\u200b\u9700\u8981\u200b\u8f93\u51fa\u200b\u8fd9\u4e9b\u200b\u7269\u4f53\u200b\u7684\u200b\u7c7b\u6807\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u9700\u8981\u200b\u8f93\u51fa\u200b\u8fd9\u4e9b\u200b\u7269\u4f53\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u5728\u200b\u8868\u793a\u200b\u4f4d\u7f6e\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u822c\u200b\u91c7\u7528\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u8fb9\u7f18\u200b\u6846\u200b<code>bounding box</code>\u200b\u8fdb\u884c\u200b\u8868\u793a\u200b\uff0c<code>bounding box</code>\u200b\u662f\u200b\u4e00\u7ec4\u200b\u5750\u6807\u503c\u200b\uff0c\u200b\u5e38\u89c1\u200b\u5f62\u5f0f\u200b\u4e3a\u200b<code>(x1,y1,x2,y2)</code>\uff0c\u200b\u5176\u4e2d\u200b<code>x1</code>\u200b\u4ee3\u8868\u200b\u7269\u4f53\u200b\u5de6\u200b\u4e0a\u200b\u6a2a\u5750\u6807\u200b\uff0c<code>y1</code>\u200b\u4ee3\u8868\u200b\u5de6\u200b\u4e0a\u200b\u7eb5\u5750\u6807\u200b\uff0c<code>x2</code>\u200b\u4ee3\u8868\u200b\u7269\u4f53\u200b\u53f3\u200b\u4e0b\u200b\u6a2a\u5750\u6807\u200b\uff0c<code>y2</code>\u200b\u4ee3\u8868\u200b\u7269\u4f53\u200b\u53f3\u200b\u4e0b\u200b\u7eb5\u5750\u6807\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#_3","title":"\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e94\u7528","text":"<p>\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\u4e0d\u540c\u4e8e\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u5355\u4e00\u200b\u8f93\u51fa\u200b\u7269\u4f53\u200b\u7684\u200b\u79cd\u7c7b\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u7269\u4f53\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u548c\u200b\u79cd\u7c7b\u200b\u4e00\u8d77\u200b\u8f93\u51fa\u200b\uff0c\u200b\u8fd9\u200b\u4f7f\u5f97\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5728\u200b\u4e00\u4e9b\u200b\u9886\u57df\u200b\u6709\u7740\u200b\u91cd\u8981\u200b\u7684\u200b\u4f5c\u7528\u200b\uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e38\u7528\u200b\u4e8e\u200b\u4eba\u8138\u200b\u68c0\u6d4b\u200b\u3001\u200b\u667a\u6167\u200b\u4ea4\u901a\u200b\u3001\u200b\u673a\u5668\u4eba\u200b\u3001\u200b\u65e0\u4eba\u9a7e\u9a76\u200b\u3001\u200b\u9065\u611f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u3001\u200b\u5b89\u9632\u200b\u9886\u57df\u200b\u68c0\u6d4b\u200b\u5f02\u5e38\u200b\u3001\u200b\u884c\u4eba\u200b\u8ba1\u6570\u200b\u3001\u200b\u5b89\u5168\u200b\u7cfb\u7edf\u200b\u7b49\u200b\u5404\u5927\u200b\u9886\u57df\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#_4","title":"\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u901a\u5e38\u200b\u6765\u8bf4\u200b\u6bd4\u200b\u56fe\u7247\u200b\u5206\u7c7b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u5c0f\u200b\u5f88\u591a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6bcf\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u6807\u6ce8\u200b\u7684\u200b\u6210\u672c\u200b\u5f88\u200b\u9ad8\u200b\uff0c\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u56fe\u7247\u200b\u5206\u7c7b\u200b\u7684\u200b\u5e38\u89c1\u200b\u6807\u6ce8\u200b\u65b9\u6cd5\u200b\u662f\u200b\u7ed9\u5b9a\u200b\u4e00\u4e2a\u200bCSV\u200b\u6587\u4ef6\u200b\uff08\u200b\u56fe\u7247\u200b\u4e0e\u200b\u6807\u53f7\u200b\u4e00\u4e00\u5bf9\u5e94\u200b\uff09\u200b\u6216\u8005\u200b\u662f\u200b\u7ed9\u5b9a\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u5939\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u5b50\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5bf9\u5e94\u200b\u6807\u53f7\u200b\u7684\u200b\u56fe\u7247\u200b\u653e\u5728\u200b\u5b50\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\uff09\uff0c\u200b\u4f46\u662f\u200b\u5bf9\u4e8e\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6765\u8bf4\u200b\u56e0\u4e3a\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u591a\u4e2a\u200b\u7c7b\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u5c31\u200b\u4e0d\u80fd\u200b\u653e\u5728\u200b\u5b50\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\uff0c\u200b\u6240\u4ee5\u200b\u901a\u5e38\u200b\u6765\u8bf4\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6807\u53f7\u200b\u9700\u8981\u200b\u989d\u5916\u200b\u5b58\u50a8\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u5b58\u50a8\u200b\u683c\u5f0f\u200b\u6709\u200bPASCAL VOC\u200b\u7684\u200b\u683c\u5f0f\u200b\u548c\u200bCOCO\u200b\u7684\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\u3002\u200b\u5047\u8bbe\u200b\u4f7f\u7528\u200b\u6587\u672c\u6587\u4ef6\u200b\u5b58\u50a8\u200b\u7684\u8bdd\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u884c\u200b\u8868\u793a\u200b\u4e00\u4e2a\u200b\u7269\u4f53\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u884c\u200b\u5206\u522b\u200b\u7531\u200b\u56fe\u7247\u200b\u6587\u4ef6\u540d\u200b\uff08\u200b\u56e0\u4e3a\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u6709\u200b\u591a\u4e2a\u200b\u7269\u4f53\u200b\uff0c\u200b\u6240\u4ee5\u200b\u540c\u4e00\u4e2a\u200b\u6587\u4ef6\u540d\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u591a\u6b21\u200b\uff09\u3001\u200b\u7269\u4f53\u200b\u7c7b\u522b\u200b\uff08\u200b\u6807\u53f7\u200b\uff09\u3001\u200b\u8fb9\u7f18\u200b\u6846\u200b\uff08\u200b\u56fe\u7247\u200b\u4e2d\u200b\u7269\u4f53\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff09\u200b\u7ec4\u6210\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u884c\u200b\u4e00\u5171\u200b\u6709\u200b6\uff081+1+4\uff09\u200b\u4e2a\u503c\u200b  \u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e38\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u6709\u200bPASCAL VOC2007, MS COCO</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#coco","title":"COCO\u200b\u6570\u636e\u200b\u96c6","text":"<p>[Dataset url]  [Dataset paper] [Dataset Benchmarks]</p> <p>COCO\u200b\u662f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4e2d\u200b\u6bd4\u8f83\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bImagenet\u200b\u5728\u200b\u56fe\u7247\u200b\u5206\u7c7b\u200b\u4e2d\u200b\u7684\u200b\u5730\u4f4d\u200b\uff0ccoco\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u5fae\u8f6f\u200b\u5f00\u6e90\u200b\u7684\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4e2d\u200b\u5e38\u7528\u200b\u7684\u200b\u5927\u89c4\u6a21\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200bVOC\u200b\u800c\u8a00\u200b\uff0cCOCO\u200b\u6570\u636e\u200b\u96c6\u200b\u6709\u7740\u200b\u5c0f\u200b\u76ee\u6807\u200b\u591a\u200b\u3001\u200b\u5355\u5e45\u200b\u56fe\u7247\u200b\u76ee\u6807\u200b\u591a\u200b\u3001\u200b\u7269\u4f53\u200b\u5927\u591a\u200b\u975e\u200b\u4e2d\u5fc3\u200b\u5206\u5e03\u200b\u3001\u200b\u66f4\u200b\u7b26\u5408\u200b\u65e5\u5e38\u200b\u73af\u5883\u200b\u7684\u200b\u7279\u70b9\u200b\uff0c\u200b\u56e0\u800c\u200bCOCO\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u96be\u5ea6\u200b\u66f4\u5927\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\uff0cCOCO\u200b\u5305\u542b\u200b80\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6bcf\u5e74\u200b\u5927\u8d5b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u5305\u542b\u200b120,000\u200b\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u8d85\u8fc7\u200b40,000\u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u56fe\u7247\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u56fe\u7247\u200b\u5305\u542b\u200b\u591a\u4e2a\u200b\u7269\u4f53\u200b\uff09\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b80\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff1a |\u200b\u6240\u5c5e\u200b\u5927\u200b\u7c7b\u200b|lable| |---|---| |Person#1|person| |Vehicle#8|bicycle,car,motorcycle,airplane,bus,train,truck,boat| |Outdoor#5|traffic light, firhydrant, stop sign, parking meter, bench| |Animal#10|bird,cat, dog,horse, sheep, cow, elephant, bear, zebra, giraffe| |Accessory#5|backpack, umbrella,handbag, tie, suitcase| |Sport#10|frisbee, skis,snowboard, sports ball, kite, baseball bat , baseball glove, skateboard, surfboard, tennisracket| |Kitchen#7|bottle, wine glass,cup, fork, knife, spoon, bowl| |Food#10|banana, apple,sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake| |Furniture#6| chair, couch, potted plant,bed, dining table, toilet| |Electronic#6|tv, laptop, mouse,remote, keyboard, cell phone| |Appliance#5|microwave, oven,toaster, sink, refrigerator| |Indoor#7|book, clock, vase,scissors, teddy bear, hair drier, toothbrus|</p> <p>\u200b\u4e0e\u200bPASCAL VOC\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\u4e0d\u540c\u200b\uff0cCOCO\u200b\u7684\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\u662f\u200b\u4ee5\u200bJson\u200b\u5f62\u5f0f\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5177\u4f53\u200b\u5f62\u5f0f\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bCOCO\u200b\u5b98\u7f51\u200b\u7684\u200bformat data</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#pascal-voc","title":"PASCAL VOC","text":"<p>[Dataset url]  [Dataset paper] [Dataset Benchmarks]</p> <p>Pascal VOC\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7684\u200b\u5e38\u7528\u200b\u7684\u200b\u5927\u89c4\u6a21\u200b\u6570\u636e\u200b\u96c6\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u4ece\u200b05\u200b\u5e74\u200b\u5230\u200b12\u200b\u5e74\u200b\u90fd\u200b\u4f1a\u200b\u4e3e\u529e\u200b\u6bd4\u8d5b\u200b\uff08\u200b\u6bd4\u8d5b\u200b\u4efb\u52a1\u200btask\uff1a \u200b\u5206\u7c7b\u200bClassification \uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200bObject Detection\uff0c\u200b\u8bed\u4e49\u200b\u5206\u5272\u200bClass Segmentation\uff0c\u200b\u5b9e\u4f8b\u200b\u5206\u5272\u200bObject Segmentation\uff0cAction Classification\uff08\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4eba\u4f53\u200b\u52a8\u4f5c\u200b\u7684\u200b\u4e00\u79cd\u200b\u5206\u7c7b\u200b\uff09\uff0cPerson Layout\uff08\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4eba\u4f53\u200b\u5404\u200b\u90e8\u4f4d\u200b\u7684\u200b\u4e00\u79cd\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\uff09\uff09\u3002\u200b\u5f53\u524d\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u6709\u200bVOC2007\u200b\u548c\u200bVOC2012\u200b\u4e24\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u5305\u542b\u200b\u7ea6\u200b10,000\u200b\u5f20\u200b\u5e26\u6709\u200b\u8fb9\u754c\u200b\u6846\u200b\u7684\u200b\u56fe\u7247\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u542b\u6709\u200b20\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002\u200b\u5177\u4f53\u200b\u5305\u62ec\u200b |\u200b\u6240\u5c5e\u200b\u5927\u200b\u7c7b\u200b|label| |---|---| |Person| person| |Animal| bird, cat, cow, dog, horse,sheep| |Vehicle| aeroplane, bicycle, boat,bus, car, motorbike, train| |Indoor|bottle, chair, dining table,potted plant, sofa, tv/monitor|</p> <p>\u200b\u76f8\u8f83\u200b\u4e8e\u200bCOCO\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b80\u200b\u7c7b\u522b\u200b\uff0cPASCAL VOC\u200b\u6570\u636e\u200b\u96c6\u4ec5\u200b\u6709\u200b20\u200b\u7c7b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e5f\u200b\u5e38\u200b\u88ab\u200b\u770b\u4f5c\u200b\u662f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u9886\u57df\u200b\u7684\u200b\u57fa\u51c6\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u5b8c\u540e\u200b\u4f1a\u200b\u6709\u200b\u4e94\u4e2a\u200b\u6587\u4ef6\u5939\u200b\uff1aAnnotations\u3001ImageSets\u3001JPEGImages\u3001SegmentationClass\u3001SegmentationObject annotations\uff1a\u200b\u6570\u636e\u200b\u96c6\u200b\u6807\u7b7e\u200b\u7684\u200b\u5b58\u50a8\u200b\u8def\u5f84\u200b\uff0c\u200b\u901a\u8fc7\u200bXML\u200b\u6587\u4ef6\u683c\u5f0f\u200b\uff0c\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b\u5404\u7c7b\u200b\u4efb\u52a1\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002\u200b\u5176\u4e2d\u200b\u90e8\u5206\u200b\u6807\u7b7e\u200b\u4e3a\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002 JPEGImages\uff1a\u200b\u6570\u636e\u200b\u96c6\u200b\u56fe\u50cf\u200b\u7684\u200b\u5b58\u50a8\u200b\u8def\u5f84\u200b\u3002 \u200b\u5177\u4f53\u200b\u7684\u200b\u6807\u6ce8\u200b\u683c\u5f0f\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200bAnnotation Guidelines</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#dota","title":"DOTA","text":"<p>[Dataset url]  [Dataset paper] [Dataset Benchmarks]</p> <p>DOTA\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u6700\u4e3a\u200b\u91cd\u8981\u200b\u7684\u200b\u9065\u611f\u200b\u822a\u7a7a\u200b\u62cd\u6444\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u6570\u636e\u200b\u6765\u81ea\u200bGoogle Earth\uff0cGF-2\u200b\u548c\u200bJL-1\u200b\u536b\u661f\u200b\uff0cCycloMedia B.V\u3002DOTA\u200b\u5305\u542b\u200b\u7070\u5ea6\u200b\u56fe\u200b\u548c\u200bRGB\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u4ee5\u200bPNG\u200b\u5f62\u5f0f\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5176\u4e2d\u200bRGB\u200b\u6765\u81ea\u200b\u4e8e\u200bGoogle Earth\u200b\u548c\u200bCycloMedia\uff0c\u200b\u7070\u5ea6\u200b\u56fe\u200b\u6765\u81ea\u200bGF-2\u200b\u548c\u200bJL-1\u200b\u536b\u661f\u200b\u3002DOTA\u200b\u73b0\u6709\u200bv1.0\uff0cv1.5\uff0cv2.0\u200b\u4e09\u4e2a\u200b\u7248\u672c\u200b\uff0c\u200b\u5206\u522b\u200b\u652f\u6301\u200b15\uff0c16\uff0c18\u200b\u7c7b\u200b\u7684\u200b\u68c0\u6d4b\u200b\u3002</p> DOTA \u200b\u7248\u672c\u200b \u200b\u5305\u542b\u200b\u7c7b\u522b\u200b DOTA v1.0 plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field and swimming pool DOTA v1.5 plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool and container crane. DOTA v2.0 plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool, container crane, airport and helipad. <p>\u200b\u5173\u4e8e\u200bDOTA\u200b\u7684\u200b\u6807\u6ce8\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200bDOTA Annotation format</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#kitti","title":"KITTI","text":"<p>[Dataset url]  [Dataset paper] [Dataset Benchmarks]</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.2%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/#_5","title":"\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e38\u7528\u200b\u7b97\u6cd5","text":"<p>\u200b\u968f\u7740\u200b\u7b97\u529b\u200b\u7684\u200b\u53d1\u5c55\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7ecf\u5386\u200b\u4e86\u200b\u4ece\u200b\u57fa\u4e8e\u200b\u624b\u5de5\u200b\u8bbe\u8ba1\u200b\u7279\u5f81\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5230\u200b\u57fa\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u63d0\u53d6\u200b\u7279\u5f81\u200b\u7684\u200b\u9636\u6bb5\u200b\u3002\u200b\u5728\u200b\u65e9\u671f\u200b\uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\u5f80\u5f80\u200b\u91c7\u7528\u200b\u624b\u5de5\u200b\u8bbe\u8ba1\u200b\u7279\u5f81\u200b\uff08Haar\u200b\u7279\u5f81\u200b\u3001\u200b\u68af\u5ea6\u200b\u76f4\u65b9\u56fe\u200bHOG\uff09\u200b\u52a0\u200b\u5206\u7c7b\u5668\u200b\uff08SVM\u3001AdaBoost\uff09\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5b9e\u73b0\u200b\u3002\u200b\u968f\u7740\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u51fa\u73b0\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200b\u57fa\u4e8e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\uff0c\u200b\u5305\u62ec\u200bR-CNN\u200b\u7cfb\u5217\u200b,SSD\u200b\u7cfb\u5217\u200b,YOLO\u200b\u7cfb\u5217\u200b\u7b49\u200b\u3002\u200b\u968f\u7740\u200bTransformer\u200b\u5728\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u548c\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u4e5f\u200b\u51fa\u73b0\u200b\u4e86\u200b\u57fa\u4e8e\u200bTransformer\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\uff0c\u200b\u4ee3\u8868\u200b\u5de5\u4f5c\u200b\u6709\u200bDETR\u200b\u7cfb\u5217\u200b\u3002\u200b\u5728\u200b\u672c\u200b\u90e8\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u4ecb\u7ecd\u200b\u57fa\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\u5e76\u200b\u8fdb\u884c\u200b\u90e8\u5206\u200b\u4ee3\u7801\u200b\u7684\u200b\u89e3\u8bfb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u57fa\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\u6309\u7167\u200b\u6709\u65e0\u200b\u4f7f\u7528\u200b\u951a\u70b9\u200b\u6846\u200b\u5206\u4e3a\u200b\u57fa\u4e8e\u200b\u951a\u70b9\u200b\u6846\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\uff08Anchor-based\uff09\uff0c\u200b\u65e0\u951a\u200b\u70b9\u6846\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\uff08Anchor-free\uff09\u200b\u548c\u200b\u7aef\u200b\u5230\u200b\u7aef\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\uff08Anchor-free\uff09\u3002\u200b\u5176\u4e2d\u200b\u7aef\u5230\u200b\u7aef\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6280\u672f\u200b\u4e5f\u200b\u662f\u200b\u5c5e\u4e8e\u200b\u7279\u6b8a\u200b\u7684\u200b\u65e0\u951a\u200b\u70b9\u6846\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u57fa\u4e8e\u200b\u951a\u70b9\u200b\u6846\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u5206\u4e3a\u200b\u5355\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\uff08One-Stage\uff09\u200b\u548c\u200b\u4e24\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\uff08Two-Stage\uff09\uff0c\u200b\u5176\u4e2d\u200b\u5355\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u4ee3\u8868\u4f5c\u200b\u6709\u200bYOLO (You Only Look Once)\u200b\u548c\u200bSSD (Single Shot Detector)\uff0c\u200b\u4e24\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4ee3\u8868\u4f5c\u200b\u6709\u200bR-CNN\uff08R-CNN\uff0cFast RCNN,Faster RCNN\uff0cMask RCNN\uff0cCascade RCNN\uff0cSPPNet\uff09\u200b\u7cfb\u5217\u200b\u3002\u200b\u4e00\u822c\u800c\u8a00\u200b\uff0c\u200b\u4e24\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5177\u6709\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u68c0\u6d4b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u800c\u5355\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u5177\u6709\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002</p> <p>\u200b\u540c\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u65e0\u951a\u200b\u70b9\u6846\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u5206\u4e3a\u200b\u57fa\u4e8e\u200b\u76ee\u6807\u200b\u70b9\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u548c\u200b\u57fa\u4e8e\u200b\u5185\u90e8\u200b\u70b9\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b\u3002 \u200b\u7aef\u5230\u200b\u7aef\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u65b9\u6cd5\u200b</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/10.3%20%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/","title":"10.3 \u200b\u56fe\u50cf\u200b\u5206\u5272\u200b\u7b80\u4ecb\u200b\uff08\u200b\u8865\u5145\u200b\u4e2d\u200b\uff09","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/","title":"\u6587\u7ae0\u200b\u7ed3\u6784","text":"<p>\u200b\u5728\u200bRNN\u200b\u8be6\u89e3\u200b\u53ca\u5176\u200b\u5b9e\u6218\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7b80\u5355\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u4e3a\u4ec0\u4e48\u200b\u9700\u8981\u200bRNN\u200b\u8fd9\u200b\u7c7b\u200b\u6a21\u578b\u200b\u3001RNN\u200b\u7684\u200b\u5177\u4f53\u200b\u601d\u8def\u200b\u3001RNN\u200b\u7684\u200b\u7b80\u5355\u200b\u5b9e\u73b0\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u5728\u200b\u6587\u7ae0\u200b\u7ed3\u5c3e\u200b\u90e8\u5206\u200b\u6211\u4eec\u200b\u63d0\u5230\u200b\u4e86\u200bRNN\u200b\u5b58\u5728\u200b\u7684\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ca\u200b\u4e4b\u540e\u200b\u7684\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1aLSTM\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u672c\u7bc7\u200b\u6587\u7ae0\u200b\u4e3b\u8981\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>LSTM \u200b\u7406\u89e3\u200b\u53ca\u200b\u7b80\u5355\u200b\u5b9e\u73b0\u200b</li> <li>LSTM \u200b\u5b9e\u6218\u200b</li> <li>\u200b\u7ecf\u5178\u200b RNN \u200b\u4e0e\u200b LSTM \u200b\u5bf9\u6bd4\u200b</li> <li>\u200b\u5173\u4e8e\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b</li> </ol>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#lstm","title":"LSTM \u200b\u7406\u89e3","text":"<p>\u200b\u5176\u5b9e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b LSTM \u200b\u4e0e\u200b RNN \u200b\u8bf4\u6210\u200b\u4e24\u4e2a\u200b\u5e76\u200b\u4e0d\u200b\u53ef\u53d6\u200b\uff0c LSTM \u200b\u4f9d\u7136\u200b\u5f52\u5c5e\u4e8e\u200b RNN \u200b\u4e4b\u4e0b\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u65b9\u5f0f\u200b\u6765\u200b\u5904\u7406\u200b\u5e8f\u5217\u200b\u95ee\u9898\u200b\uff0c LSTM \u200b\u5176\u5b9e\u200b\u662f\u200b\u8bbe\u8ba1\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u5757\u200b\u6765\u200b\u53d6\u4ee3\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u7b97\u6cd5\u200b\u3002</p> <p>LSTM(Long Short-Term Memory)\uff0c\u200b\u7ffb\u8bd1\u200b\u8fc7\u6765\u200b\u662f\u200b\u957f\u77ed\u671f\u200b\u8bb0\u5fc6\u6cd5\u200b\uff0c\u200b\u5176\u200b\u6838\u5fc3\u601d\u60f3\u200b\u53ef\u4ee5\u200b\u8bf4\u200b\u975e\u5e38\u200b\u7684\u200b\u7b80\u5355\u200b\uff1a\u200b\u65e2\u7136\u200b RNN \u200b\u53ea\u80fd\u200b\u4fdd\u5b58\u200b\u77ed\u671f\u200b\u7684\u200b\u8bb0\u5fc6\u200b\uff0c\u200b\u90a3\u200b\u6211\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\uff0c\u200b\u4e0d\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u4e86\u200b\u540d\u200b\uff1f\u200b\u56e0\u6b64\u200b\uff0cLSTM\u200b\u63d0\u51fa\u200b\u4e86\u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\u548c\u200b\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\u548c\u200b\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u6765\u200b\u7ef4\u6301\u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\u7684\u200b\u53ef\u9760\u200b\uff0c\u200b\u964d\u4f4e\u200b RNN \u200b\u7684\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u95ee\u9898\u200b\u3002\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u4e0b\u65b9\u200b\u7ed3\u6784\u56fe\u200b\u4e2d\u200b\uff0c\u200b\u6a21\u578b\u200b\u8f93\u5165\u200b\u7531\u200b\u4e24\u4e2a\u200b\u5347\u7ea7\u200b\u5230\u200b\u4e09\u4e2a\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u5f53\u524d\u200b\u8282\u70b9\u200b\u72b6\u6001\u200b \\(\\mathbf{X}_{t}\\)\uff0c\u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\uff1a\\(\\mathbf{C}_{t-1}\\)\uff0c\u200b\u77ed\u671f\u200b\u8bb0\u5fc6\u200b \\(\\mathbf{H}_{t-1}\\)\u3002\u200b\u8f93\u51fa\u200b\u72b6\u6001\u200b\u4f9d\u7136\u200b\u662f\u200b\u4e24\u4e2a\u200b\uff1a\u200b\u8282\u70b9\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b \\(\\mathbf{C}_{t}\\)\uff0c\u200b\u548c\u200b\u8282\u70b9\u200b\u5f53\u524d\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b \\(\\mathbf{H}_{t}\\)\u3002</p> <p></p> <p>\u200b\u90a3\u4e48\u200b\u95ee\u9898\u200b\u6765\u200b\u4e86\u200b\uff0c LSTM \u200b\u662f\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u5bf9\u200b\u957f\u77ed\u200b\u8bb0\u5fc6\u200b\u7684\u200b\u63a7\u5236\u200b\u5462\u200b\uff1f \u200b\u8fd9\u200b\u5c31\u200b\u4e0d\u5f97\u4e0d\u200b\u63d0\u200b\u4f17\u4eba\u200b\u6240\u77e5\u200b\u7684\u200b\u4e09\u4e2a\u200b\u95e8\u200b\uff1a  - \u200b\u9057\u5fd8\u200b\u95e8\u200b\uff1a\u200b\u63a7\u5236\u200b\u4fdd\u7559\u200b\u591a\u5c11\u200b\u4e0a\u200b\u4e00\u200b\u65f6\u523b\u200b\u7684\u200b\u5355\u5143\u200b\u8282\u70b9\u200b\u5230\u200b\u5f53\u524d\u200b\u8282\u70b9\u200b  - \u200b\u8bb0\u5fc6\u200b\u95e8\u200b\uff1a\u200b\u63a7\u5236\u200b\u5c06\u200b\u5f53\u524d\u200b\u65f6\u523b\u200b\u7684\u200b\u591a\u5c11\u200b\u4fe1\u606f\u200b\u8bb0\u5fc6\u200b\u5230\u200b\u8282\u70b9\u200b\u4e2d\u200b  - \u200b\u8f93\u51fa\u200b\u95e8\u200b\uff1a\u200b\u63a7\u5236\u200b\u8f93\u51fa\u200b\u591a\u5c11\u200b\u4fe1\u606f\u200b\u7ed9\u200b\u5f53\u524d\u200b\u8f93\u51fa\u200b</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u5206\u6790\u200b\u4e09\u4e2a\u200b\u95e8\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u4e86\u89e3\u200b \u200b\u95e8\u200b \u200b\u8fd9\u4e00\u200b\u6982\u5ff5\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_2","title":"\u95e8","text":"<p>\u200b\u4ece\u200b\u7b80\u5316\u200b\u56fe\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c \u200b\u95e8\u200b\u7684\u200b\u611f\u89c9\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u7535\u8def\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5f00\u5173\u200b\uff0c\u200b\u5f53\u200b\u5f00\u5173\u200b\u6309\u4e0b\u200b\uff0c\u200b\u4fe1\u606f\u200b\u901a\u8fc7\u200b\uff0c\u200b\u800c\u200b\u5f00\u5173\u200b\u62ac\u8d77\u200b\uff0c\u200b\u4fe1\u606f\u200b\u4e0d\u518d\u200b\u901a\u8fc7\u200b\u3002\u200b\u5b9e\u9645\u200b\u4e5f\u200b\u5982\u6b64\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u95e8\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff0c\u200b\u8f93\u5165\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b\uff0c\u200b\u8f93\u51fa\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u4f4d\u4e8e\u200b [0,1] \u200b\u4e4b\u95f4\u200b\u7684\u200b\u503c\u200b\u3002 \u200b\u6211\u4eec\u200b\u6765\u200b\u8bbe\u8ba1\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u7b80\u5355\u200b\u7684\u200b\u9057\u5fd8\u200b\u95e8\u200b\uff1a\u200b\u6bcf\u6b21\u200b\u5b66\u4e60\u200b\u72b6\u6001\u200b\u4e4b\u540e\u200b\uff0c\u200b\u90fd\u200b\u9057\u5fd8\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5df2\u200b\u5b66\u4e60\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7684\u200b\u9057\u5fd8\u200b\u95e8\u200b\u4e0e\u200b LSTM \u200b\u7684\u200b\u9057\u5fd8\u200b\u95e8\u200b\u65e0\u5173\u200b\uff0c\u200b\u5355\u7eaf\u200b\u7406\u89e3\u200b \u200b\u95e8\u200b \u200b\u8fd9\u4e00\u200b\u6982\u5ff5\u200b\u3002</p> <p><pre><code># \u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b \u200b\u7528\u6765\u200b\u8ba1\u7b97\u200b\u9057\u5fd8\u200b\u591a\u5c11\u200b\ngate_linear = nn.Linear(hidden_size, 1)\n# \u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b \u200b\u7528\u6765\u200b\u5b66\u4e60\u200b\nstudy_linear = nn.Linear(hidden_size, hidden_size)\n# \u200b\u6b64\u523b\u200b h_t \u200b\u662f\u200b\u4e0a\u200b\u4e00\u200b\u65f6\u523b\u200b\u72b6\u6001\u200b\n# \u200b\u8f93\u51fa\u200b\u4e3a\u200b 0 - 1 \u200b\u7684\u200b\u503c\u200b\ngate = gate_linear(h_t)\n# h_t \u200b\u7ecf\u8fc7\u200b study_linear \u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\n_h_t = study_linear(h_t)\n# \u200b\u5728\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u4e4b\u524d\u200b\uff0c\u200b\u7ecf\u8fc7\u200b gate \u200b\u5bfc\u81f4\u200b\u5185\u5bb9\u200b\u53d7\u635f\u200b\uff0c\u200b\u9057\u5fd8\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5b66\u4e60\u200b\u5185\u5bb9\u200b\nh_t = gate * \uff08_h_t\uff09\n</code></pre> \u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5982\u679c\u200b \\(gate\\) \u200b\u503c\u4e3a\u200b 0\uff0c\u200b\u5219\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u5747\u4f1a\u200b\u88ab\u200b\u9057\u5fd8\u200b\uff0c\u200b\u800c\u200b\u5982\u679c\u200b\u503c\u4e3a\u200b1\uff0c\u200b\u5219\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u5219\u200b\u4f1a\u200b\u88ab\u200b\u5b8c\u5168\u200b\u4fdd\u7559\u200b\uff0c\u200b\u800c\u200b <code>gate_linear</code> \u200b\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u7684\u200b\u5f00\u5173\u95e8\u200b\u5c31\u200b\u51fa\u73b0\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\uff0c\\(gate\\) \u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6d6e\u70b9\u200b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u5bf9\u4e8e\u200b \u200b\u4e34\u65f6\u200b\u7ed3\u679c\u200b\u77e9\u9635\u200b\u53d8\u91cf\u200b \\(\\_h\\_t\\) \u200b\u800c\u8a00\u200b\uff0c\u200b\u5176\u200b\u9057\u5fd8\u200b\u63a7\u5236\u200b\u662f\u200b\u5168\u5c40\u200b\u7684\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\uff0c\u200b\u5f53\u200b \\(gate\\) \u200b\u4e3a\u200b 0 \u200b\u65f6\u200b\uff0c \u200b\u5176\u200b\u6700\u7ec8\u200b\u7ed3\u679c\u200b \\(h\\_t\\) \u200b\u4e3a\u200b\u5168\u200b 0 \u200b\u77e9\u9635\u200b\u3002\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u6ce8\u610f\u200b\uff1a LSTM \u200b\u4e2d\u200b\u5e76\u200b\u4e0d\u200b\u91c7\u7528\u200b\u8fd9\u6837\u200b\u7684\u200b\u5927\u200b\u95f8\u95e8\u200b\uff0c\u200b\u800c\u662f\u200b\u91c7\u7528\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u8fdb\u884c\u200b\u5206\u522b\u200b\u63a7\u5236\u200b\u7684\u200b\u5c0f\u200b\u6c34\u9f99\u5934\u200b(\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b <code>nn.Sigmode</code> )</p> <p>\u200b\u800c\u200b\u5728\u200b LSTM \u200b\u4e2d\u200b\uff0c\u200b\u95e8\u200b\u4e3b\u8981\u200b\u4f7f\u7528\u200b \\(Sigmod\\) \u200b\u795e\u7ecf\u7f51\u7edc\u200b(\u200b\u518d\u6b21\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5e76\u975e\u200b\u662f\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u800c\u662f\u200b Sigmod \u200b\u795e\u7ecf\u7f51\u7edc\u200b)\u200b\u6765\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u4e0b\u65b9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\uff1a <pre><code>hidden_size = 5\nsigmoid = nn.Sigmoid()\n# \u200b\u9690\u85cf\u200b\u72b6\u6001\u200b \u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5047\u5b9a\u200b\u5168\u200b 1\nhidden_emb = torch.ones(hidden_size, hidden_size)\n# \u200b\u4e2d\u95f4\u200b\u67d0\u200b\u4e00\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\nmodel = nn.Linear(hidden_size\uff0chidden_size)\n# \u200b\u83b7\u53d6\u200b\u8be5\u5c42\u200b\u8f93\u51fa\u200b,\u200b\u6b64\u65f6\u200b\u5c1a\u672a\u200b\u88ab\u95e8\u200b\u9650\u5236\u200b\nmid_out = model(hidden_emb)\n# \u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u95e8\u200b -- \u200b\u6ce8\u610f\u200b\uff1a\u200b\u5e76\u975e\u200b\u4e00\u5b9a\u200b\u7531\u8be5\u200b\u53d8\u91cf\u200b\u6240\u200b\u63a7\u5236\u200b\n# \u200b\u6bd4\u5982\u200b\uff1a\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u7531\u200b\u4e0a\u200b\u4e00\u200b\u65f6\u523b\u200b\u7684\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b\u63a7\u5236\u200b\n# \u200b\u4ee3\u7801\u200b\u4e3a\u200b\uff1a gate = sigmoid(hidden_emb)\ngate = sigmoid(mid_out) \n# \u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\nfinal_out = gate * mid_out\n</code></pre></p> <p>\u200b\u5728\u200b\u6709\u200b\u4e86\u200b\u5bf9\u200b\u95e8\u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\u540e\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5bf9\u200b\u9057\u5fd8\u200b\u95e8\u200b\u3001\u200b\u8bb0\u5fc6\u200b\u95e8\u200b\u3001\u200b\u8f93\u51fa\u200b\u95e8\u200b\u8fdb\u884c\u200b\u5206\u522b\u200b\u5206\u6790\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_3","title":"\u9057\u5fd8\u200b\u95e8","text":"<p>\u200b\u9057\u5fd8\u200b\u95e8\u200b\u6d89\u53ca\u200b\u90e8\u5206\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a  </p> <p>\u200b\u5176\u4e2d\u200b\uff0c\u200b\u4e0b\u65b9\u200b\u84dd\u8272\u200b\u8868\u793a\u200b\u4e09\u4e2a\u200b\u95e8\u200b\u5171\u7528\u200b\u7684\u200b\u8f93\u5165\u200b\u90e8\u5206\u200b\uff0c\u200b\u5747\u200b\u4e3a\u200b [\\(\\mathbf{h}_{t-1}\\),\\(\\mathbf{X}_{t}\\)],\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7531\u4e8e\u200b\u4e09\u4e2a\u200b\u95e8\u200b\u4e4b\u95f4\u200b\u5e76\u200b\u4e0d\u200b\u5171\u4eab\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\uff0c\u200b\u56e0\u6b64\u200b\u516c\u793a\u200b\u867d\u7136\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u4f46\u662f\u200b\u4e00\u5171\u200b\u8ba1\u7b97\u200b\u4e86\u200b\u4e09\u6b21\u200b\uff0c\u200b\u9057\u5fd8\u200b\u95e8\u200b\u88ab\u200b\u6807\u8bb0\u200b\u4e3a\u200b \\(f_t\\), \u200b\u5217\u51fa\u200b\u9057\u5fd8\u200b\u95e8\u200b\u516c\u5f0f\u200b\u4e3a\u200b\uff1a $$ f_t = \\sigma(\\mathbf{W_f} * [\\mathbf{h}{t-1},\\mathbf{X}{t}]  + \\mathbf{b_f}) $$ \u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u4e3a\u200b\u53d6\u503c\u200b\u8303\u56f4\u200b\u4e3a\u200b [ 0, 1 ] \u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u662f\u200b\u63a7\u5236\u200b\u4e0e\u200b\u4e4b\u200b\u76f8\u4e58\u200b\u7684\u200b\u77e9\u9635\u200b\u7684\u200b\u9057\u5fd8\u200b\u7a0b\u5ea6\u200b\u3002 \u200b\u5c06\u200b \\(f_t\\) \u200b\u4e0e\u200b\u8f93\u5165\u200b\u7684\u200b\u4e0a\u200b\u4e00\u200b\u957f\u671f\u200b\u72b6\u6001\u200b \\(C_{t-1}\\) \u200b\u76f8\u4e58\u200b\uff1a $$ C_t' = f_t * C_{t-1} $$</p> <p>\u200b\u4e00\u90e8\u5206\u200b\u7684\u200b \\(C_{t-1}\\) \u200b\u5c31\u200b\u8fd9\u6837\u200b\u88ab\u200b\u9057\u5fd8\u200b\u4e86\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_4","title":"\u8bb0\u5fc6\u200b\u95e8","text":"<p>\u200b\u8bb0\u5fc6\u200b\u95e8\u200b\u6d89\u53ca\u200b\u90e8\u5206\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a </p> <p>\u200b\u4ece\u56fe\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u8bb0\u5fc6\u200b\u95e8\u4e2d\u200b\u76f8\u4e58\u200b\u7684\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\u5747\u200b\u7531\u200b \\(\\mathbf{h}_{t-1}\\) \u200b\u4e0e\u200b \\(\\mathbf{X}_{t}\\) \u200b\u5f97\u5230\u200b\uff0c \u200b\u5176\u4e2d\u200b\uff0c\u200b\u5de6\u4fa7\u200b\u63a7\u5236\u200b\u8bb0\u5fc6\u200b\u591a\u5c11\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u4e0e\u200b\u9057\u5fd8\u200b\u95e8\u200b\u516c\u5f0f\u200b\u57fa\u672c\u4e00\u81f4\u200b\uff1a $$ i_t = \\sigma(\\mathbf{W_i} * [\\mathbf{h}{t-1},\\mathbf{X}{t}]  + \\mathbf{b_i}) $$ \u200b\u4e0e\u200b\u9057\u5fd8\u200b\u95e8\u200b\u76f8\u901a\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u4e3a\u200b\u53d6\u503c\u200b\u8303\u56f4\u200b\u4e3a\u200b [ 0, 1 ] \u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u662f\u200b\u63a7\u5236\u200b\u4e0e\u200b\u4e4b\u200b\u76f8\u4e58\u200b\u7684\u200b\u77e9\u9635\u200b\u7684\u200b\u8bb0\u5fc6\u200b\u7a0b\u5ea6\u200b\u3002 \u200b\u800c\u200b\u53f3\u4fa7\u200b\uff0c\u200b\u5219\u200b\u66f4\u6362\u200b\u4e86\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u7531\u200b \\(sigmoid\\) \u200b\u53d8\u6210\u200b\u4e86\u200b \\(tanh\\)\uff1a $$ \\tilde{C_t} = \\tanh(\\mathbf{W_c} * [\\mathbf{h}{t-1},\\mathbf{X}{t}]  + \\mathbf{b_c}) $$ \u200b\u8be5\u200b\u516c\u5f0f\u200b\u8d1f\u8d23\u200b\u7684\u200b\u90e8\u5206\u200b\u53ef\u4ee5\u200b\u770b\u505a\u200b\u8d1f\u8d23\u200b\u77ed\u671f\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b\u7684\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53d6\u503c\u200b\u8303\u56f4\u200b\u4e3a\u200b [ -1, 1 ]\u3002</p> <p>\u200b\u6700\u7ec8\u200b\u8bb0\u5fc6\u200b\u95e8\u200b\u66f4\u65b0\u200b\u516c\u5f0f\u200b\u5982\u4e0b\u200b: $$ \\tilde{C_t'}=  i_t * \\tilde{C_t} $$</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8bf4\u200b  \\(\\tilde{C_t'}\\) \u200b\u662f\u200b\u4fdd\u7559\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u5185\u5bb9\u200b\u7684\u200b\u77ed\u671f\u200b\u72b6\u6001\u200b</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_5","title":"\u72b6\u6001\u200b\u66f4\u65b0","text":"<p>\u200b\u5728\u200b\u901a\u8fc7\u200b\u9057\u5fd8\u200b\u95e8\u200b\u83b7\u53d6\u200b\u5230\u200b\u4e86\u200b\u88ab\u200b\u9057\u5fd8\u200b\u4e00\u5b9a\u200b\u5185\u5bb9\u200b\u7684\u200b\u957f\u671f\u200b\u72b6\u6001\u200b \\(C_t'\\) \u200b\u548c\u200b \u200b\u4fdd\u7559\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u5185\u5bb9\u200b\u7684\u200b\u77ed\u671f\u200b\u72b6\u6001\u200b \\(\\tilde{C_t'}\\) \u200b\u4e4b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u52a0\u6cd5\u200b\u76f4\u63a5\u200b\u7ed3\u5408\u200b</p> \\[ C_t =  C_t' + \\tilde{C_t'} \\]"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_6","title":"\u8f93\u51fa\u200b\u95e8","text":"<p>\u200b\u8f93\u51fa\u200b\u95e8\u200b\u662f\u200b\u4e09\u4e2a\u200b\u95e8\u4e2d\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u95e8\u200b\uff0c\u200b\u5f53\u200b\u6570\u636e\u200b\u5230\u8fbe\u200b\u8fd9\u91cc\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u63a7\u5236\u200b\u5c06\u200b\u957f\u671f\u200b\u72b6\u6001\u200b\u4e2d\u200b\u7684\u200b\u5185\u5bb9\u200b \\(C_t\\) \u200b\u4fdd\u5b58\u200b\u4e00\u5b9a\u200b\u5185\u5bb9\u200b\u5230\u200b \\(h_t\\) \u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4e0d\u518d\u200b\u8d58\u8ff0\u200b $$ o_t = \\sigma(\\mathbf{W_o} * [\\mathbf{h}{t-1},\\mathbf{X}{t}]  + \\mathbf{b_o}) $$</p> \\[ h_t = o_t * \\tanh(C_t) \\]"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_7","title":"\u6a21\u578b\u200b\u603b\u7ed3","text":"<p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u6240\u6709\u200b\u516c\u5f0f\u200b\u7684\u200b\u6838\u5fc3\u200b\u90e8\u5206\u200b\u90fd\u200b\u662f\u200b\u5982\u6b64\u200b\u7684\u200b\u76f8\u4f3c\u200b\uff1a $$ \\mathbf{W_c} * [\\mathbf{h}{t-1},\\mathbf{X}{t}]  + \\mathbf{b_c} $$ \u200b\u800c\u200b\u8fd9\u90e8\u5206\u200b\u5176\u5b9e\u200b\u53c8\u200b\u53ea\u662f\u200b\u7b80\u5355\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u6240\u4ee5\u200b LSTM \u200b\u6bd4\u200b RNN \u200b\u9ad8\u7ea7\u200b\u7684\u200b\u5730\u65b9\u200b\u5176\u5b9e\u200b\u5e76\u200b\u4e0d\u200b\u5728\u4e8e\u200b\u67d0\u200b\u4e00\u6761\u200b\u516c\u5f0f\u200b\uff0c\u200b\u800c\u662f\u200b\u5b83\u200b\u8c03\u6574\u200b\u4e86\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6d41\u52a8\u200b\uff0c\u200b\u6309\u7167\u200b\u4e00\u5b9a\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u8fdb\u884c\u200b\u878d\u5408\u200b\uff0c\u200b\u5f31\u5316\u200b\u4e86\u200b\u957f\u8ddd\u79bb\u200b\u4e0b\u200b\u7684\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\u603b\u7684\u6765\u770b\u200b\uff0cLSTM \u200b\u5176\u5b9e\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u5347\u7ea7\u200b\u7248\u672c\u200b\u7684\u200b\u7684\u200b RNN\uff0c\u200b\u4ed6\u200b\u989d\u5916\u200b\u521d\u59cb\u5316\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u72b6\u6001\u200b \\(C\\)\uff0c \u200b\u7528\u6765\u200b\u4fdd\u5b58\u200b\u957f\u671f\u200b\u7684\u200b\u8bb0\u5fc6\u200b\uff0c\u200b\u63a7\u5236\u200b\u8fdc\u8ddd\u79bb\u200b\u4e0a\u200b\u7684\u200b\u53c2\u6570\u200b\u6743\u91cd\u200b\u3002\u200b\u800c\u200b\u8f93\u51fa\u200b\u4e5f\u200b\u57fa\u672c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u6b64\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#lstm_1","title":"LSTM \u200b\u5b9e\u6218","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_8","title":"\u5b9e\u9a8c\u200b\u8bf4\u660e","text":"<p>\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u4e0b\u8f7d\u200b\u3002\u200b\u5728\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5171\u8ba1\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u5bf9\u6bd4\u200b\u4e86\u200b\u4ed6\u4eec\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u4e09\u4e2a\u200b\u6a21\u578b\u200b\u5206\u522b\u200b\u662f\u200b\uff1a\u200b\u7531\u200b\u6211\u200b\u5b8c\u5168\u200b\u4f7f\u7528\u200b <code>nn.Linear</code> \u200b\u5b9e\u73b0\u200b\u7684\u200b LSTM \u200b\u6a21\u578b\u200b\u3001 \u200b\u4f7f\u7528\u200b <code>nn.LSTM</code> \u200b\u4e3a\u200b\u57fa\u7840\u200b\u7684\u200b LSTM \u200b\u6a21\u578b\u200b\u548c\u200b\u4f7f\u7528\u200b <code>nn.RNN</code> \u200b\u4e3a\u200b\u57fa\u7840\u200b\u5b9e\u73b0\u200b\u7684\u200b RNN \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u5b9e\u9a8c\u200b\u6570\u636e\u200b\u96c6\u200b\u91c7\u7528\u200b IMDB \u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u4e3b\u8981\u200b\u7531\u200b\u7535\u5f71\u200b\u8bc4\u8bba\u200b\u6784\u6210\u200b\uff0c\u200b\u957f\u5ea6\u200b\u4e0d\u5747\u200b\uff0c\u200b\u4f46\u662f\u200b\u957f\u5ea6\u200b\u5728\u200b 1000 \u200b\u5de6\u53f3\u200b\u7684\u200b\u6570\u636e\u200b\u5c5e\u4e8e\u200b\u5e38\u89c1\u200b\u6570\u636e\u200b\u3002\u200b\u6570\u636e\u200b\u96c6\u200b\u6837\u672c\u200b\u5747\u8861\u200b\uff0c\u200b\u6570\u200b\u5171\u8ba1\u200b 50000 \u200b\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5404\u6709\u200b 25000 \u200b\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u540c\u65f6\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u6b63\u8d1f\u200b\u6bd4\u4f8b\u200b\u5747\u200b\u4e3a\u200b 1:1\u3002</p> <p>\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u5bf9\u200b RNN \u200b\u7684\u200b\u4e86\u89e3\u200b\uff0c\u200b\u8fd9\u6837\u200b\u7684\u200b\u957f\u5ea6\u200b\u662f\u200b\u5f88\u200b\u96be\u200b\u5b66\u4e60\u200b\u5230\u200b\u6709\u6548\u200b\u7684\u200b\u77e5\u8bc6\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5f88\u200b\u9002\u5408\u200b\u6bd4\u8f83\u200b RNN \u200b\u4e0e\u200b LSTM \u200b\u4e4b\u95f4\u200b\u7684\u200b\u533a\u522b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u4ee3\u7801\u200b\u590d\u73b0\u200b\uff0c\u200b\u5728\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\u501f\u52a9\u200b\u4e86\u200b <code>torchtext</code> \u200b\u6765\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u53ca\u200b\u52a0\u8f7d\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8bc1\u660e\u200b\u6a21\u578b\u200b\u771f\u7684\u200b\u6709\u200b\u5b66\u4e60\u200b\u5230\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5bf9\u6bd4\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u90e8\u5206\u200b\u53c2\u6570\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u90e8\u5206\u200b\u533a\u522b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u672c\u5730\u200b\u8c03\u6574\u200b\u5230\u200b\u540c\u4e00\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u7ec6\u81f4\u200b\u7684\u200b\u5bf9\u6bd4\u200b\u5b9e\u9a8c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_9","title":"\u6a21\u578b\u200b\u5b9e\u73b0","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u5206\u6790\u200b\u4e00\u4e0b\u200b\u7531\u200b\u6211\u200b\u5b9e\u73b0\u200b\u7684\u200b LSTM \u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u4ee5\u6b64\u200b\u4e86\u89e3\u200b LSTM \u200b\u6a21\u578b\u200b\u3002(ps:\u200b\u4e2a\u4eba\u200b\u80fd\u529b\u200b\u6709\u9650\u200b,\u200b\u6ca1\u200b\u80fd\u200b\u5b9e\u73b0\u200b <code>num_layers</code> \u200b\u548c\u200b <code>Bi-LSTM</code> \u200b\u4e24\u4e2a\u200b\u7279\u70b9\u200b\uff0c\u200b\u6b64\u5916\u200b\u53ef\u80fd\u200b\u5b9e\u73b0\u200b\u5b58\u5728\u200b\u5176\u4ed6\u200b\u95ee\u9898\u200b\uff0c\u200b\u6b22\u8fce\u200b\u7ed9\u4e88\u200b\u53cd\u9988\u200b) <pre><code># \u200b\u5b9a\u4e49\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        \"\"\"\n        args:\n            input_size: \u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\n            hidden_size: \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b\n            num_classes: \u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b 0 \u200b\u6216\u8005\u200b 1\n        \"\"\"\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.fc_i = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_f = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_g = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_o = nn.Linear(input_size + hidden_size, hidden_size)\n        self.sigmoid = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n        self.fc_out = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        # \u200b\u521d\u59cb\u5316\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b -- \u200b\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\n        h_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)\n        # \u200b\u521d\u59cb\u5316\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b -- \u200b\u957f\u671f\u200b\u8bb0\u5fc6\u200b\n        c_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)\n        # \u200b\u8f93\u5165\u200b\u4e0e\u200b\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\u76f8\u200b\u62fc\u63a5\u200b\n        combined = torch.cat((x, h_t), dim=2)\n        # \u200b\u8bb0\u5fc6\u200b\u95e8\u200b -- \u200b\u8f93\u51fa\u200b\u77e9\u9635\u200b\u5185\u5bb9\u200b\u4e3a\u200b 0-1 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u6570\u5b57\u200b\n        i_t = self.sigmoid(self.fc_i(combined))\n        # \u200b\u9057\u5fd8\u200b\u95e8\u200b -- \u200b\u8f93\u51fa\u200b\u77e9\u9635\u200b\u5185\u5bb9\u200b\u4e3a\u200b 0-1 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u6570\u5b57\u200b\n        f_t = self.sigmoid(self.fc_f(combined))\n        #\n        g_t = self.tanh(self.fc_g(combined))\n        #  \u200b\u8f93\u51fa\u200b\u95e8\u200b -- \u200b\u8f93\u51fa\u200b\u77e9\u9635\u200b\u5185\u5bb9\u200b\u4e3a\u200b 0-1 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u6570\u5b57\u200b\n        o_t = self.sigmoid(self.fc_o(combined))\n        # \u200b\u957f\u671f\u200b\u72b6\u6001\u200b =  \u200b\u9057\u5fd8\u200b\u95e8\u200b * \u200b\u4e0a\u200b\u4e00\u200b\u65f6\u523b\u200b\u7684\u200b\u957f\u671f\u200b\u72b6\u6001\u200b + \u200b\u8bb0\u5fc6\u200b\u95e8\u200b* \u200b\u5f53\u524d\u200b\u8bb0\u5fc6\u200b\u72b6\u6001\u200b\n        c_t = f_t * c_t + i_t * g_t\n        # \u200b\u9690\u85cf\u200b\u72b6\u6001\u200b = \u200b\u8f93\u51fa\u200b\u95e8\u200b * \u200b\u957f\u671f\u200b\u72b6\u6001\u200b\n        h_t = o_t * self.tanh(c_t)\n        # \u200b\u964d\u7ef4\u200b\u64cd\u4f5c\u200b \n        h_t = F.avg_pool2d(h_t, (h_t.shape[1],1)).squeeze()\n        # \n        out = self.fc_out(h_t)\n        return out \n</code></pre></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_10","title":"\u8d85\u200b\u53c2\u6570\u200b\u53ca\u200b\u53c2\u6570\u200b\u8bf4\u660e","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#mylstm-nnlstm","title":"MyLSTM \u200b\u4e0e\u200b nn.LSTM","text":"\u540d\u79f0\u200b \u200b\u503c\u200b learning_rate 0.001 batch_size 32 epoch 6(3) input_size 64 hidden_size 128 num_classes 2 <p>\u200b\u6b64\u65f6\u200b\uff1a MyLSTM \u200b\u53c2\u200b\u6570\u91cf\u200b: 99074 nn.LSTM \u200b\u53c2\u200b\u6570\u91cf\u200b: 99328</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u200b\u5b9e\u73b0\u200b\u7684\u200b MyLSTM \u200b\u4e0e\u200b nn.LSTM \u200b\u6709\u200b 254 \u200b\u7684\u200b\u53c2\u6570\u200b\u5dee\u200b\uff0c\u200b\u6211\u200b\u672c\u4eba\u200b\u5e76\u200b\u6ca1\u80fd\u200b\u5206\u6790\u200b\u51fa\u6765\u200b\u5dee\u522b\u200b\u3002 <code>nn.LSTM</code> \u200b\u5728\u200b\u5b9e\u9a8c\u200b\u65f6\u5927\u200b\u6982\u7387\u200b\u6bd4\u200b\u6211\u200b\u7684\u200b MyLSTM \u200b\u8fed\u4ee3\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5bb9\u6613\u200b\u8f83\u200b\u65e9\u200b\u7684\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5c06\u200b\u5176\u200b\u8bad\u7ec3\u200b epoch \u200b\u780d\u200b\u534a\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b MyLSTM \u200b\u4f7f\u7528\u200b 6 epoch \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u800c\u200b <code>nn.LSTM</code> \u200b\u4f7f\u7528\u200b 3 epoch \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4e24\u8005\u200b\u53ef\u4ee5\u200b\u8fbe\u5230\u200b\u57fa\u672c\u200b\u76f8\u8fd1\u200b\u7684\u200b\u6548\u679c\u200b</p> <p>\u200b\u53e6\u5916\u200b\u5728\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e2d\u200b <code>nn.LSTM</code> \u200b\u540e\u9762\u200b\u52a0\u200b\u4e86\u200b\u4e00\u4e2a\u200b <code>nn.Linear</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u4e8c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u53c2\u200b\u6570\u91cf\u200b\u4e3a\u200b 258\uff0c \u200b\u6240\u4ee5\u200b MyLSTM \u200b\u548c\u200b LSTM \u200b\u76f8\u5dee\u200b\u53c2\u6570\u200b\u603b\u91cf\u200b\u4e3a\u200b 512\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#nnrnn","title":"nn.RNN","text":"\u540d\u79f0\u200b \u200b\u503c\u200b learning_rate 0.0001 batch_size 32 epoch 12-18 input_size 64 hidden_size 128 num_classes 2 <p>\u200b\u6b64\u65f6\u200b\uff1a nn.RNN \u200b\u53c2\u200b\u6570\u91cf\u200b: 25090</p> <p>\u200b\u7531\u4e8e\u200b\u5b9e\u9a8c\u200b\u6837\u672c\u200b\u957f\u5ea6\u200b\u5728\u200b 1000 \u200b\u4e0a\u4e0b\u200b\uff0c RNN \u200b\u663e\u793a\u200b\u51fa\u6765\u200b\u4e86\u200b\u6781\u5927\u200b\u7684\u200b\u4e0d\u7a33\u5b9a\u6027\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff0c \u200b\u76f8\u8f83\u200b\u4e8e\u200b LSTM \u200b\u66f4\u200b\u5bb9\u6613\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u3001\u200b\u8bad\u7ec3\u200b epoch \u200b\u66f4\u200b\u591a\u200b\u3001\u200b\u5b66\u4e60\u200b\u7387\u200b\u9700\u8981\u200b\u8c03\u4f4e\u200b\u7b49\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u5c3d\u7ba1\u5982\u6b64\u200b\u4f9d\u7136\u200b\u4e0d\u80fd\u200b\u4fdd\u8bc1\u200b\u7a33\u5b9a\u200b\u7684\u200b\u826f\u597d\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4e3e\u4f8b\u6765\u8bf4\u200b\uff0c\u200b\u67d0\u200b\u5b66\u751f\u200b\u5b66\u4e60\u200b\u9605\u8bfb\u200b\u7406\u89e3\u200b\uff0c\u200b\u8981\u6c42\u200b\u6839\u636e\u200b\u6587\u7ae0\u5185\u5bb9\u200b\u56de\u7b54\u200b\u6587\u7ae0\u200b\u7684\u200b\u60c5\u611f\u200b\u503e\u5411\u200b\uff0c\u200b\u4f46\u662f\u200b\u5b66\u751f\u200b\u53ea\u200b\u559c\u6b22\u200b\u770b\u200b\u6700\u540e\u200b\u4e00\u53e5\u200b\u8bdd\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u90fd\u200b\u6839\u636e\u200b\u6700\u540e\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u6765\u200b\u56de\u7b54\u200b\u95ee\u9898\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4ed6\u200b\u57fa\u672c\u4e0a\u200b\u662f\u200b\u7b49\u4e8e\u200b\u778e\u731c\u200b\u7684\u200b\uff0c\u200b\u53ea\u80fd\u200b\u5b66\u5230\u200b\u4e00\u70b9\u200b\u6d45\u8584\u200b\u7684\u200b\u77e5\u8bc6\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_11","title":"\u5b9e\u9a8c\u200b\u7ed3\u679c","text":"MyLSTM nn.LSTM nn.RNN 0.86 0.80 0.67"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/LSTM%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98/#_12","title":"\u5173\u4e8e\u200b\u68af\u5ea6\u200b\u95ee\u9898","text":"<ul> <li> <p>RNN\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u603b\u200b\u7684\u200b\u68af\u5ea6\u200b\u662f\u200b\u4e0d\u4f1a\u200b\u6d88\u5931\u200b\u7684\u200b\u3002\u200b\u5373\u4fbf\u200b\u68af\u5ea6\u200b\u8d8a\u4f20\u8d8a\u200b\u5f31\u200b\uff0c\u200b\u90a3\u200b\u4e5f\u200b\u662f\u200b\u8fdc\u5904\u200b\u7684\u200b\u68af\u5ea6\u200b\u9010\u6e10\u200b\u6d88\u5931\u200b\uff0c\u200b\u800c\u200b\u8fd1\u8ddd\u79bb\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e0d\u4f1a\u200b\u6d88\u5931\u200b\uff0c\u200b\u56e0\u6b64\u200b\uff0c\u200b\u68af\u5ea6\u200b\u603b\u548c\u200b\u4e0d\u4f1a\u200b\u6d88\u5931\u200b\u3002RNN \u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u7684\u200b\u771f\u6b63\u200b\u542b\u4e49\u200b\u662f\u200b\uff1a\u200b\u68af\u5ea6\u200b\u88ab\u200b\u8fd1\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u6240\u200b\u4e3b\u5bfc\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u6a21\u578b\u200b\u96be\u4ee5\u200b\u5b66\u5230\u200b\u8fdc\u8ddd\u79bb\u200b\u7684\u200b\u4f9d\u8d56\u200b\u5173\u7cfb\u200b\u3002</p> </li> <li> <p>LSTM \u200b\u4e0a\u200b\u6709\u200b\u591a\u6761\u200b\u4fe1\u606f\u6d41\u200b\u8def\u5f84\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff0c\u200b\u5143\u7d20\u200b\u76f8\u52a0\u200b\u7684\u200b\u8def\u5f84\u200b\u7684\u200b\u68af\u5ea6\u200b\u6d41\u662f\u200b\u6700\u200b\u7a33\u5b9a\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u5176\u4ed6\u200b\u8def\u5f84\u200b\u4e0a\u200b\u4e0e\u200b\u57fa\u672c\u200b\u7684\u200b RNN \u200b\u76f8\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f9d\u7136\u200b\u5b58\u5728\u200b\u53cd\u590d\u200b\u76f8\u4e58\u200b\u95ee\u9898\u200b\u3002</p> </li> <li> <p>LSTM \u200b\u521a\u521a\u200b\u63d0\u51fa\u200b\u65f6\u200b\u4e0d\u200b\u5b58\u5728\u200b\u9057\u5fd8\u200b\u95e8\u200b\u3002\u200b\u8fd9\u65f6\u5019\u200b\u5386\u53f2\u6570\u636e\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u6761\u200b\u8def\u5f84\u200b\u4e0a\u200b\u65e0\u635f\u200b\u7684\u200b\u4f20\u9012\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u89c6\u4e3a\u200b\u4e00\u6761\u200b \u200b\u9ad8\u901f\u516c\u8def\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b ResNet \u200b\u4e2d\u200b\u7684\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</p> </li> <li> <p>\u200b\u4f46\u662f\u200b\u5176\u4ed6\u200b\u8def\u5f84\u200b\u4e0a\u200b\uff0c LSTM \u200b\u4e0e\u200b RNN \u200b\u5e76\u200b\u65e0\u592a\u591a\u200b\u533a\u522b\u200b\uff0c\u200b\u4f9d\u7136\u200b\u4f1a\u200b\u7206\u70b8\u200b\u6216\u8005\u200b\u6d88\u5931\u200b\u3002\u200b\u7531\u4e8e\u200b\u603b\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b = \u200b\u5404\u4e2a\u200b\u8def\u5f84\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u4e4b\u200b\u548c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53ea\u8981\u200b\u6709\u200b\u4e00\u6761\u200b\u8def\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u6ca1\u6709\u200b\u6d88\u5931\u200b\uff0c\u200b\u603b\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u5c31\u200b\u4e0d\u4f1a\u200b\u6d88\u5931\u200b\u3002\u200b\u53ef\u4ee5\u200b\u8bf4\u200b\uff0cLSTM \u200b\u901a\u8fc7\u200b\u8fd9\u200b\u4e00\u6761\u200b\u8def\u200b\u62ef\u6551\u200b\u4e86\u200b\u603b\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u3002</p> </li> <li> <p>\u200b\u540c\u6837\u200b\uff0c\u200b\u603b\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b = \u200b\u5404\u4e2a\u200b\u8def\u5f84\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b\u4e4b\u200b\u548c\u200b\uff0c\u200b\u867d\u7136\u200b\u9ad8\u901f\u200b\u8def\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u6d41\u200b\u6bd4\u8f83\u7a33\u5b9a\u200b\uff0c\u200b\u4f46\u662f\u200b\u5176\u4ed6\u200b\u8def\u4e0a\u200b\u4f9d\u7136\u200b\u5b58\u5728\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u95ee\u9898\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u603b\u200b\u7684\u200b\u8fdc\u8ddd\u79bb\u200b\u68af\u5ea6\u200b = \u200b\u6b63\u5e38\u200b\u68af\u5ea6\u200b + \u200b\u7206\u70b8\u200b\u68af\u5ea6\u200b = \u200b\u7206\u70b8\u200b\u68af\u5ea6\u200b\uff0c\u200b\u56e0\u6b64\u200b LSTM \u200b\u4f9d\u7136\u200b\u5b58\u5728\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u95ee\u9898\u200b\u3002 \u200b\u4f46\u662f\u200b\u7531\u4e8e\u200b LSTM \u200b\u7684\u200b\u9053\u8def\u200b\u76f8\u6bd4\u200b\u7ecf\u5178\u200b RNN \u200b\u6765\u8bf4\u200b\u975e\u5e38\u200b\u5d0e\u5c96\u200b\uff0c \u200b\u5b58\u5728\u200b\u591a\u6b21\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u56e0\u6b64\u200b LSTM \u200b\u53d1\u751f\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u7684\u200b\u6982\u7387\u200b\u8981\u200b\u5c0f\u5f97\u591a\u200b\u3002\u200b\u5b9e\u8df5\u200b\u4e2d\u200b\u901a\u5e38\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u526a\u88c1\u200b\u6765\u200b\u4f18\u5316\u200b\u95ee\u9898\u200b\u3002</p> </li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/","title":"\u6587\u7ae0\u200b\u7ed3\u6784","text":"<p>\u200b\u63d0\u53ca\u200b RNN\uff0c\u200b\u7edd\u5927\u90e8\u5206\u200b\u4eba\u200b\u90fd\u200b\u77e5\u9053\u200b\u4ed6\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5e8f\u5217\u200b\u4efb\u52a1\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u4f1a\u200b\u63d0\u53ca\u200b\u4ed6\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u65f6\u5e8f\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f46\u662f\u200b\uff0c\u200b\u4e3a\u4ec0\u4e48\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u65f6\u5e8f\u200b\u7684\u200b\u4fe1\u606f\u200b\uff1f\u200b\u4e3a\u4ec0\u4e48\u200b\u8bf4\u200b RNN \u200b\u4fdd\u5b58\u200b\u4e86\u200b\u65f6\u5e8f\u200b\u7684\u200b\u4fe1\u606f\u200b\uff1fRNN\u200b\u53c8\u200b\u5b58\u5728\u200b\u54ea\u4e9b\u200b\u95ee\u9898\u200b\uff1f \u200b\u672c\u7bc7\u200b\u5185\u5bb9\u200b\u5c06\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u987a\u5e8f\u200b\u9010\u6b65\u200b\u5e26\u200b\u4f60\u200b\u6478\u6e05\u200b RNN \u200b\u7684\u200b\u7ec6\u8282\u200b\u4e4b\u200b\u5904\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6765\u200b\u5b8c\u6210\u200b\u4e00\u4e2a\u200b\u81ea\u5df1\u200b\u7684\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u3002</p> <ol> <li>\u200b\u4e3a\u4ec0\u4e48\u200b\u9700\u8981\u200b RNN\uff1f</li> <li>RNN \u200b\u7406\u89e3\u200b\u53ca\u5176\u200b\u7b80\u5355\u200b\u5b9e\u73b0\u200b\u3002</li> <li>\u200b\u7528\u200b RNN \u200b\u5b8c\u6210\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u3002</li> <li>RNN \u200b\u5b58\u5728\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</li> </ol>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/#rnn","title":"\u4e3a\u4ec0\u4e48\u200b\u9700\u8981\u200b RNN\uff1f","text":"<p>\u200b\u5728\u200b\u73b0\u5b9e\u751f\u6d3b\u200b\u7684\u200b\u4e16\u754c\u200b\u4e2d\u200b\uff0c\u200b\u6709\u200b\u5f88\u591a\u200b\u7684\u200b\u5185\u5bb9\u200b\u662f\u200b\u6709\u7740\u200b\u524d\u540e\u200b\u5173\u7cfb\u200b\u7684\u200b\uff0c\u200b\u6bd4\u5982\u200b\u4f60\u200b\u6240\u200b\u9605\u8bfb\u200b\u7684\u200b\u8fd9\u6bb5\u200b\u6587\u5b57\u200b\uff0c\u200b\u4ed6\u200b\u5e76\u200b\u4e0d\u662f\u200b\u6beb\u65e0\u200b\u7406\u7531\u200b\u7684\u200b\u968f\u673a\u200b\u7ec4\u5408\u200b\uff0c\u200b\u800c\u662f\u200b\u6211\u200b\u6784\u601d\u200b\u4e4b\u540e\u200b\u6309\u200b\u987a\u5e8f\u200b\u5199\u4e0b\u200b\u7684\u200b\u4e00\u4e2a\u4e2a\u200b\u6587\u5b57\u200b\u3002\u200b\u9664\u4e86\u200b\u6587\u5b57\u200b\u4e4b\u5916\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4eba\u200b\u7684\u200b\u53d1\u97f3\u200b\u3001\u200b\u7269\u54c1\u200b\u7684\u200b\u4ef7\u683c\u200b\u7684\u200b\u66f2\u7ebf\u200b\u3001\u200b\u6e29\u5ea6\u200b\u53d8\u5316\u200b\u7b49\u7b49\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u6709\u7740\u200b\u524d\u540e\u200b\u987a\u5e8f\u200b\u5b58\u5728\u200b\u7684\u200b\u3002</p> <p>\u200b\u5f88\u200b\u660e\u663e\u200b\uff0c\u200b\u5f53\u200b\u77e5\u9053\u200b\u4e86\u200b\u524d\u9762\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u540e\u9762\u200b\u7684\u200b\u4fe1\u606f\u200b\u8fdb\u884c\u200b\u5408\u7406\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c\u200b\u524d\u200b\u5341\u5929\u200b\u6e29\u5ea6\u200b\u90fd\u200b\u53ea\u6709\u200b20\u200b\u5ea6\u200b\uff0c\u200b\u660e\u5929\u200b\u7684\u200b\u6e29\u5ea6\u200b\u65e0\u8bba\u5982\u4f55\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u96f6\u4e0b\u200b\uff1b\u200b\u8fd9\u4e2a\u200b\u5546\u54c1\u200b\u4e00\u5e74\u200b\u6765\u200b\u4ef7\u683c\u200b\u90fd\u200b\u5728\u200b30\u200b\u5de6\u53f3\u200b\u6d6e\u52a8\u200b\uff0c\u200b\u660e\u5929\u200b\u6211\u200b\u53bb\u200b\u4e70\u200b\u4ed6\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u51c6\u5907\u200b40\u200b\u5c31\u200b\u8db3\u591f\u200b\u4e86\u200b\uff1b\u200b\u8001\u5e08\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u8868\u626c\u200b\u4e86\u200b\u4f60\u200b\uff0c\u200b\u7d27\u8ddf\u7740\u200b\u8bf4\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4f46\u662f\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u77e5\u9053\u200b\u4ed6\u200b\u7684\u200b\u5185\u5bb9\u200b\u8981\u200b\u5f00\u59cb\u200b\u8f6c\u6298\u200b\u4e86\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u9690\u85cf\u200b\u5728\u200b\u65e5\u5e38\u751f\u6d3b\u200b\u4e2d\u200b\u7684\u200b\u5e8f\u5217\u200b\u4fe1\u606f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5df2\u7ecf\u200b\u77e5\u9053\u200b\u4e86\u200b\u524d\u9762\u200b\u53d1\u751f\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6240\u4ee5\u200b\u624d\u200b\u53ef\u4ee5\u200b\u63a8\u7406\u200b\u540e\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4f20\u7edf\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u6765\u200b\u5904\u7406\u200b\u5e8f\u5217\u200b\u95ee\u9898\u200b\u5417\u200b\uff1f\u200b\u6309\u7167\u200b\u57fa\u672c\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u6a21\u578b\u200b\u65b9\u6848\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5e94\u8be5\u200b\u662f\u200b\u8fd9\u6837\u200b\u7684\u200b\uff1a\u200b\u5c06\u200b\u5e8f\u5217\u200b\u8f93\u5165\u200b\u56fa\u5b9a\u200b\u6210\u200b\u4e00\u4e2a\u200b \\(d\\) \u200b\u7ef4\u200b\u5411\u91cf\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u9001\u5165\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5f62\u200b\u5982\u200b\u516c\u5f0f\u200b\uff1a $$  \\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}{xh} + \\mathbf{b}{h}) $$  \u200b\u516c\u5f0f\u200b\u4e2d\u200b\uff0c \\(\\phi\\) \u200b\u8868\u793a\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) \u200b\u8868\u793a\u200b\u4e00\u7ec4\u200b\u5c0f\u6279\u91cf\u200b\u6837\u672c\u200b\uff0c\u200b\u5176\u4e2d\u200b \\(n\\) \u200b\u662f\u200b\u6837\u672c\u200b\u5927\u5c0f\u200b\uff0c \\(d\\) \u200b\u8868\u793a\u200b\u8f93\u5165\u200b\u7684\u200b\u7279\u5f81\u200b\u7ef4\u5ea6\u200b\u3002\uff1a\\(\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}\\)\u200b\u8868\u793a\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\uff0c\\(d \\in \\mathbb{R}^{1 \\times h}\\)\u200b\u8868\u793a\u200b\u6a21\u578b\u200b\u504f\u7f6e\u200b\u3002\u200b\u6700\u540e\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u9690\u85cf\u200b\u5c42\u200b\u8f93\u5165\u200b\uff1a\\(\\mathbf{H} \\in \\mathbb{R}^{n \\times h}\\)\uff0c\u200b\u5176\u4e2d\u200b \\(h\\) \u200b\u8868\u793a\u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u7d27\u63a5\u7740\u200b\uff0c\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\uff1a $$ \\mathbf{O} = \\mathbf{H}\\mathbf{W}_{hq} + \\mathbf{b}_q $$ \u200b\u5176\u4e2d\u200b\uff0c\\(\\mathbf{O} \\in \\mathbb{R}^{n \\times q}\\) \u200b\u4e3a\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u53d8\u91cf\u200b\uff0c\\(q\\) \u200b\u8868\u793a\u200b\u8f93\u51fa\u200b\u5c42\u200b\u5411\u91cf\u200b\uff0c\u200b\u7531\u4e8e\u200b\u672c\u6b21\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u90a3\u200b\u8fd9\u91cc\u200b \\(q\\) \u200b\u5c31\u200b\u8868\u793a\u200b\u6587\u672c\u200b\u7c7b\u522b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b \\(\\mathbf{Softmax(O)}\\) \u200b\u6765\u200b\u8fdb\u884c\u200b\u6982\u7387\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\uff0c\u200b\u4e0a\u9762\u200b\u7684\u200b\u6d41\u7a0b\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u660e\u663e\u200b\u7684\u200b\u524d\u7f6e\u6761\u4ef6\u200b\uff1a\u200b\u56fa\u5b9a\u200b\u6210\u200b \\(d\\) \u200b\u7ef4\u200b\u5411\u91cf\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u4f20\u7edf\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff0c\u200b\u662f\u200b\u4e0d\u80fd\u200b\u5bf9\u200b\u53d8\u957f\u200b\u5e8f\u5217\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u7684\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5728\u200b\u5e8f\u5217\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u5e8f\u5217\u200b\u957f\u77ed\u200b\u5f88\u200b\u660e\u663e\u200b\u662f\u200b\u5e76\u4e0d\u76f8\u540c\u200b\u7684\u200b\uff0c\u200b\u4e0d\u4ec5\u200b\u9700\u8981\u200b\u7528\u200b\u4e00\u5929\u200b\u7684\u200b\u6570\u636e\u200b\u9884\u6d4b\u200b\u660e\u5929\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u62ff\u200b\u4e00\u5e74\u200b\u7684\u200b\u6570\u636e\u200b\u9884\u6d4b\u200b\u660e\u5929\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u5728\u200b\u8fd9\u6837\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5982\u679c\u200b\u8fd8\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u4f20\u7edf\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff0c\u200b\u5c31\u200b\u4f1a\u200b\u9762\u4e34\u200b\u7740\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u95ee\u9898\u200b\uff1a\u200b\u5982\u4f55\u200b\u5c06\u200b\u4e00\u5929\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e0e\u200b\u4e00\u5e74\u200b\u7684\u200b\u5185\u5bb9\u200b\u53d8\u5316\u200b\u6210\u200b\u76f8\u540c\u200b\u7684\u200b \\(d\\) \u200b\u7ef4\u200b\u5411\u91cf\u200b\uff1f</p> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u5e8f\u5217\u200b\u4fe1\u606f\u200b\u53ef\u80fd\u200b\u8fd8\u6709\u200b\u53e6\u5916\u200b\u4e00\u4e2a\u200b\u60c5\u51b5\u200b\uff1a\u200b\u67d0\u4e9b\u200b\u4fe1\u606f\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u5728\u200b\u5e8f\u5217\u200b\u7684\u200b\u4e0d\u540c\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u867d\u7136\u200b\u4fe1\u606f\u200b\u51fa\u73b0\u200b\u5728\u200b\u4e0d\u540c\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u4f46\u662f\u200b\u4ed6\u200b\u53ef\u80fd\u200b\u8868\u8fbe\u200b\u51fa\u200b\u4e86\u200b\u76f8\u540c\u200b\u7684\u200b\u542b\u4e49\u200b\u3002</p> <p>\u200b\u4e3e\u4f8b\u6765\u8bf4\u200b\uff1a\u200b\u5f53\u200b\u6211\u4eec\u200b\u548c\u200b\u8001\u5e08\u200b\u8c08\u8bdd\u200b\u65f6\u200b\uff0c\u200b\u5982\u679c\u200b\u4ed6\u200b\u8868\u626c\u200b\u4e86\u200b\u6211\u4eec\u200b\u534a\u5c0f\u65f6\u200b\uff0c\u200b\u7136\u540e\u200b\u8bf4\u200b\uff1a\"\u200b\u4f46\u662f\u200b...\"\uff0c\u200b\u6211\u4eec\u200b\u5f80\u5f80\u200b\u662f\u200b\u4e0d\u200b\u62c5\u5fc3\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4ed6\u200b\u53ef\u80fd\u200b\u53ea\u662f\u200b\u4e3a\u4e86\u200b\u6307\u51fa\u200b\u4e00\u4e9b\u200b\u5c0f\u200b\u95ee\u9898\u200b\u3002\u200b\u5982\u679c\u200b\u4ed6\u200b\u521a\u521a\u200b\u8868\u626c\u200b\u4e86\u200b\u4e00\u53e5\u200b\u8bdd\u200b\uff0c\u200b\u7d27\u63a5\u7740\u200b\u5c31\u200b\u8bf4\u200b\u201c\u200b\u4f46\u662f\u200b\u201d\uff0c\u200b\u90a3\u200b\u6211\u4eec\u200b\u5c31\u200b\u5fc5\u987b\u200b\u505a\u597d\u200b\u9762\u5bf9\u200b\u534a\u5c0f\u65f6\u200b\u7684\u200b\u72c2\u98ce\u66b4\u96e8\u200b\u3002\u200b\u8fd8\u6709\u200b\u53e6\u5916\u200b\u4e00\u79cd\u200b\u53ef\u80fd\u200b\uff0c\u200b\u8001\u5e08\u200b\u53ef\u80fd\u200b\u8fde\u7eed\u200b\u6279\u8bc4\u200b\u4f60\u200b\u5f88\u200b\u4e45\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u201c\u200b\u4f46\u662f\u200b\u201d\u200b\u8f6c\u6298\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u4f1a\u200b\u5728\u200b\u8fd9\u65f6\u5019\u200b\u5982\u91ca\u91cd\u8d1f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u77e5\u9053\u200b\u8fd9\u573a\u200b\u8c08\u8bdd\u200b\u5c31\u200b\u5feb\u8981\u200b\u7ed3\u675f\u200b\u4e86\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u6839\u636e\u200b\u524d\u200b\u6587\u200b(\u200b\u8868\u626c\u200b\u7684\u200b\u5185\u5bb9\u200b\u548c\u200b\u65f6\u95f4\u200b)\uff0c\u200b\u5728\u200b\u8001\u5e08\u200b\u8bf4\u51fa\u200b\"\u200b\u4f46\u662f\u200b\"\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6240\u200b\u4f5c\u51fa\u200b\u7684\u200b\u5224\u65ad\u200b\u3002</p> <p>\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f7f\u7528\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u672c\u8eab\u200b\u4f3c\u4e4e\u200b\u96be\u4ee5\u89e3\u51b3\u200b\uff0c\u200b\u4f46\u662f\u200b\u6240\u5e78\u200b\uff0cRNN \u200b\u4ece\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5e38\u89c4\u200b\u7684\u200b\u601d\u8def\u200b\u51fa\u53d1\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff1a\u200b\u8bb0\u4f4f\u200b\u4e4b\u524d\u200b\u770b\u5230\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5e76\u200b\u7ed3\u5408\u200b\u5f53\u524d\u200b\u770b\u5230\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6765\u200b\u9884\u6d4b\u200b\u4e4b\u540e\u200b\u53ef\u80fd\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/#rnn_1","title":"RNN \u200b\u7406\u89e3\u200b\u53ca\u5176\u200b\u7b80\u5355\u200b\u5b9e\u73b0","text":"<p>\u200b\u6839\u636e\u200b\u5f00\u7bc7\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u76f8\u4fe1\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u53ef\u4ee5\u200b\u7b80\u5355\u200b\u7684\u200b\u7406\u89e3\u200b\u4e3a\u4ec0\u4e48\u200b\u4f20\u7edf\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u65e0\u6cd5\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u89e3\u51b3\u200b\u5e8f\u5217\u200b\u4fe1\u606f\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u7406\u89e3\u200b\uff0cRNN \u200b\u5982\u4f55\u200b\u8bb0\u5fc6\u200b\u4e4b\u524d\u200b\u7684\u200b\u5185\u5bb9\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u5148\u200b\u653e\u51fa\u200b RNN \u200b\u7684\u200b\u516c\u5f0f\u200b\uff0c\u200b\u8bf7\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u5bf9\u6bd4\u200b\uff1a $$ \\mathbf{H}t = \\phi(\\mathbf{X}_t \\mathbf{W}{xh} + \\mathbf{H}{t-1} \\mathbf{W}{hh}  + \\mathbf{b}_h). $$ \u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u4e0e\u200b\u4e0a\u200b\u4e00\u4e2a\u200b\u516c\u5f0f\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6700\u200b\u660e\u663e\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\u591a\u200b\u4e86\u200b\u4e00\u4e2a\u200b \\(\\mathbf{H}_{t-1} \\mathbf{W}_{hh}\\) \uff0c\u200b\u4ece\u200b\u516c\u5f0f\u200b\u4e0a\u200b\u4f3c\u4e4e\u200b\u5f88\u200b\u597d\u200b\u7406\u89e3\u200b\uff0c\\(\\mathbf{H}_{t-1}\\) \u200b\u8868\u793a\u200b\u7740\u200b\u524d\u200b\u4e00\u200b\u65f6\u523b\u200b\u7684\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b\uff0c\u200b\u8868\u793a\u200b\u7684\u200b\u662f\u200b\u4e4b\u524d\u200b\u770b\u5230\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u7136\u540e\u200b\u52a0\u4e0a\u200b\u5f53\u524d\u200b\u65f6\u523b\u200b\u7684\u200b\u8f93\u5165\u200b \\(\\mathbf{X}_t\\)\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8f93\u51fa\u200b\u5f53\u524d\u200b\u65f6\u523b\u200b\u7684\u200b\u9690\u85cf\u200b\u7ed3\u679c\u200b \\(\\mathbf{H}_{t}\\)\u3002\u200b\u5728\u200b\u5f97\u5230\u200b\u9690\u85cf\u200b\u7ed3\u679c\u200b\u540e\u200b\uff0c\u200b\u5b83\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u7528\u4e8e\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u7684\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8fed\u4ee3\u200b\u8fc7\u7a0b\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u968f\u65f6\u200b\u7ec8\u6b62\u200b\uff0c\u200b\u5982\u679c\u200b\u5c06\u200b\u5f97\u5230\u200b\u7684\u200b\u9690\u85cf\u200b\u7ed3\u679c\u200b\u7528\u4e8e\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4fbf\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\uff0c\u200b\u516c\u5f0f\u200b\u8868\u8fbe\u200b\u4e3a\u200b\uff1a $$ \\mathbf{O} = \\phi(\\mathbf{H}{t} \\mathbf{W}{hq}  + \\mathbf{b}_q). $$</p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u516c\u5f0f\u200b\u56db\u200b\u4e0e\u200b\u516c\u5f0f\u200b\u4e8c\u200b\u6781\u5176\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u4ec5\u200b\u6709\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b \\(\\mathbf{H}\\) \u200b\u7565\u6709\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u6b64\u65f6\u200b\uff0c\u200b\u6839\u636e\u200b\u4ee5\u4e0a\u200b\u516c\u5f0f\u200b\u53ca\u5176\u200b\u7406\u89e3\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u53ef\u4ee5\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b RNN \u200b\u6a21\u578b\u200b\u4e86\u200b\uff1a <pre><code>class RNNDemo(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CharRNNClassify, self).__init__()\n\n        self.hidden_size = hidden_size\n        # \u200b\u8ba1\u7b97\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b H\n        # \u200b\u56e0\u4e3a\u200b\u8981\u200b\u7528\u200b\u4ee5\u4e0b\u200b\u4e00\u6b21\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u5e94\u8be5\u200b\u662f\u200b hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        # \u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b O\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    def forward(self, input, hidden):\n        # \u200b\u5c06\u200b X \u200b\u548c\u200b Ht-1 \u200b\u5408\u5e76\u200b\n        combined = torch.cat((input, hidden), 1)\n        # \u200b\u8ba1\u7b97\u200b Ht\n        hidden = self.i2h(combined)\n        # \u200b\u8ba1\u7b97\u200b\u5f53\u524d\u60c5\u51b5\u200b\u4e0b\u200b\u7684\u200b\u8f93\u51fa\u200b\n        output = self.i2o(combined)\n        # \u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4f7f\u7528\u200b softmax \u200b\u8fdb\u884c\u200b\u6982\u7387\u200b\u9884\u6d4b\u200b\n        output = self.softmax(output)\n        # \u200b\u8fd4\u56de\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b \u200b\u548c\u200b \u200b\u5f53\u524d\u200b\u7684\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b\n        return output, hidden\n    def initHidden(self):\n        # \u200b\u907f\u514d\u200b\u968f\u673a\u200b\u751f\u6210\u200b\u7684\u200b H0 \u200b\u5e72\u6270\u200b\u540e\u7eed\u200b\u7ed3\u679c\u200b\n        return torch.zeros(1, self.hidden_size)\n</code></pre> \u200b\u8f85\u52a9\u200b\u4ee3\u7801\u200b\u7406\u89e3\u200b\uff1a\u200b\u6839\u636e\u200b\u516c\u5f0f\u200b\u4e09\u200b\u53ef\u77e5\u200b\uff0c\\(\\mathbf{W}_{xh}\\) \u200b\u548c\u200b \\(\\mathbf{W}_{hh}\\) \u200b\u5728\u200b\u8f93\u5165\u200b\u9636\u6bb5\u200b\u65f6\u200b\u4e24\u8005\u200b\u4e92\u4e0d\u200b\u5f71\u54cd\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b <code>self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</code> \u200b\u4e2d\u200b\u5bf9\u200b\u8f93\u5165\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u6269\u5bb9\u200b\uff0c\u200b\u524d\u200b <code>input_size</code> \u200b\u4e0e\u200b\u516c\u5f0f\u200b \\(\\mathbf{W}_{xh}\\) \u200b\u5bf9\u5e94\u200b\uff0c\u200b\u800c\u200b\u540e\u9762\u200b\u7684\u200b <code>hidden_size</code> \u200b\u5219\u200b\u662f\u200b\u548c\u200b \\(\\mathbf{W}_{hh}\\) \u200b\u5bf9\u5e94\u200b\u3002</p> <p>\u200b\u9605\u8bfb\u200b\u4ee3\u7801\u200b\u4e4b\u540e\u200b\uff0c\u200b\u8bf7\u200b\u6839\u636e\u200b\u4ee3\u7801\u200b\u548c\u200b\u516c\u5f0f\u200b\uff0c\u200b\u6765\u200b\u56de\u5fc6\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u63d0\u51fa\u200b\u7684\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u901a\u8fc7\u200b\u56de\u7b54\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u5206\u6790\u200b RNN\u3002</p> <p>\u200b\u7b2c\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u662f\u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\uff0c\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u5176\u200b\u5411\u91cf\u200b\u8868\u793a\u200b\uff1a</p> <p>\u200b\u4ece\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0cRNN \u200b\u5e76\u200b\u4e0d\u200b\u8981\u6c42\u200b\u4e0d\u540c\u200b\u7684\u200b\u5e8f\u5217\u200b\u8868\u793a\u200b\u6210\u200b\u76f8\u540c\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u800c\u662f\u200b\u8981\u6c42\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u503c\u200b\uff0c\u200b\u8868\u793a\u200b\u6210\u4e3a\u200b\u76f8\u540c\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5728\u200b \\(t\\) \u200b\u65f6\u523b\u200b\u8f93\u5165\u200b\u7684\u200b\u503c\u200b\u89c6\u4e3a\u200b\u7684\u200b \\(\\mathbf{X}_t\\)\uff0c\u200b\u5e76\u4e14\u200b\u7ed3\u5408\u200b\u4e4b\u524d\u200b\u65f6\u523b\u200b\u8f93\u5165\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u5f97\u6765\u200b\u7684\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b \\(\\mathbf{H}_{t-1}\\)\uff0c\u200b\u5f97\u5230\u200b\u5f53\u524d\u200b\u65f6\u523b\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u8fd9\u6837\u200b\u65e0\u8bba\u200b\u5e8f\u5217\u200b\u5b9e\u9645\u200b\u957f\u5ea6\u200b\u5982\u4f55\u200b\uff0c\u200b\u6211\u4eec\u200b\u968f\u65f6\u200b\u53ef\u4ee5\u200b\u5728\u200b\u60f3\u8981\u200b\u4e2d\u65ad\u200b\u7684\u200b\u65f6\u5019\u200b\u5c06\u200b\u9690\u85cf\u200b\u72b6\u6001\u200b\u8f6c\u53d8\u6210\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u751a\u81f3\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8f93\u5165\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002 \u3010\u200b\u56fe\u7247\u200b\uff0c\u200b\u5f85\u200b\u8865\u5145\u200b\u3011</p> <p>\u200b\u7b2c\u4e8c\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u67d0\u4e9b\u200b\u4fe1\u606f\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u5728\u200b\u5e8f\u5217\u200b\u7684\u200b\u4e0d\u540c\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u4f46\u662f\u200b\u5176\u200b\u8868\u8fbe\u200b\u7684\u200b\u542b\u4e49\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\uff1a</p> <p>\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u5355\u72ec\u200b\u67e5\u770b\u200b\u516c\u5f0f\u200b\u4e0e\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u4e0d\u592a\u597d\u200b\u7406\u89e3\u200b\uff0c\u200b\u4f46\u662f\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5f97\u5230\u200b\u4e00\u5b9a\u200b\u7684\u200b\u7075\u611f\u200b\u3002</p> <p>\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5177\u6709\u200b\u5e73\u79fb\u200b\u7b49\u200b\u53d8\u6027\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\u8f93\u5165\u200b\u7684\u200b \\(\\mathbf{X}\\) \u200b\u4e0d\u4f1a\u200b\u56e0\u4e3a\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u53d8\u5316\u200b\u800c\u200b\u5bfc\u81f4\u200b\u8f93\u51fa\u200b\u7684\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8fd9\u200b\u5f97\u76ca\u4e8e\u200b\u5377\u79ef\u200b\u6838\u200b\u4f7f\u7528\u200b\u4e86\u200b\u53c2\u6570\u200b\u5171\u4eab\u200b\uff0c\u200b\u65e0\u8bba\u200b\u56fe\u7247\u200b\u54ea\u4e2a\u200b\u4f4d\u7f6e\u200b\u8fdb\u884c\u200b\u8f93\u5165\u200b\uff0c\u200b\u53ea\u8981\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u53c2\u6570\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u8f93\u5165\u200b\u503c\u200b\u5c31\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u5176\u200b\u7ed3\u679c\u200b\u5c31\u200b\u4e0d\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\u3002</p> <p>\u200b\u626d\u200b\u56de\u5934\u200b\u6765\u770b\u200b RNN \u200b\u4e2d\u200b\uff0c\u200b\u5176\u200b \\(\\mathbf{X}\\)  \u200b\u4e0e\u200b \\(\\mathbf{H}\\) \u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u4e00\u76f4\u200b\u662f\u200b\u4e00\u4e2a\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b \\(\\mathbf{W}_{xh}\\) \u200b\u548c\u200b \\(\\mathbf{W}_{hh}\\) \u200b\u662f\u200b\u53c2\u6570\u200b\u5171\u4eab\u200b\u7684\u200b\uff0c\u200b\u90a3\u4e48\u200b\u65e0\u8bba\u200b\u4ece\u200b\u5e8f\u5217\u200b\u7684\u200b\u54ea\u4e2a\u200b\u4f4d\u7f6e\u200b\u8fdb\u884c\u200b\u8f93\u5165\u200b\uff0c\u200b\u53ea\u8981\u200b\u8f93\u5165\u200b\u5185\u5bb9\u200b\u5b8c\u5168\u200b\u4e00\u6837\u200b\uff0c\u200b\u5176\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u5b8c\u5168\u200b\u4e00\u6837\u200b\u7684\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u200b\u7406\u89e3\u200b\u4e86\u200b RNN \u200b\u6765\u9f99\u53bb\u8109\u200b\u4e4b\u540e\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5f00\u59cb\u200b\u4ece\u200b RNN \u200b\u7684\u200b\u5728\u200b\u5b9e\u9645\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u66f4\u200b\u6df1\u5165\u200b\u7684\u200b\u5206\u6790\u200b\u3002(\u200b\u6ce8\u200b\uff1a\u200b\u8be5\u6837\u200b\u4f8b\u200b\u6e90\u81ea\u200b Torch \u200b\u5b98\u65b9\u200b\u6559\u7a0b\u200b)\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/#rnn_2","title":"RNN \u200b\u5b8c\u6210\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4efb\u52a1","text":"<p>\u200b\u5b8c\u6210\u200b\u4e00\u4e2a\u200b\u57fa\u672c\u200b\u7684\u200b\u7b97\u6cd5\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u6d41\u7a0b\u200b\uff1a\u200b\u6570\u636e\u5206\u6790\u200b\u3001\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u3001\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u3001\u200b\u5b9a\u4e49\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\u3001\u200b\u6267\u884c\u200b\u8bad\u7ec3\u200b\u3001\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3001\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u6458\u53d6\u200b\u5b98\u65b9\u200b\u6559\u7a0b\u200b\u4e2d\u200b\u90e8\u5206\u200b\u5173\u952e\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u76f4\u63a5\u200b\u4e0b\u8f7d\u200b\u5b98\u65b9\u200b notebook\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002\u200b\u8bad\u7ec3\u200b\u6240\u7528\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200b\u8fd9\u91cc\u200b\u3002</p> <p>\u200b\u5728\u200b notebook \u200b\u7b2c\u4e00\u4e2a\u200b\u53ef\u200b\u6267\u884c\u200b cell \u200b\u4e2d\u200b\uff0c\u200b\u9996\u5148\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u53ef\u7528\u200b\u5b57\u7b26\u200b <code>all_letters</code> \u200b\u548c\u200b \u200b\u53ef\u7528\u200b\u5b57\u7b26\u200b\u6570\u91cf\u200b <code>n_letters</code> \u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u8f6c\u4e3a\u200b ASCII \u200b\u7f16\u7801\u200b\uff0c\u200b\u4f7f\u7528\u200b  Dict l\u200b\u7c7b\u578b\u200b\u8fdb\u884c\u200b\u5b58\u50a8\u200b\u3002\u200b\u5176\u200b\u4fdd\u5b58\u200b\u683c\u5f0f\u200b\u4e3a\u200b\uff1a<code>{language: [names ...]}</code>\u3002</p> <p>\u200b\u5728\u200b\u7b2c\u4e09\u4e2a\u200b cell \u200b\u4e2d\u200b\uff0c\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4e3b\u8981\u200b\u76ee\u7684\u200b\u662f\u200b\u5c06\u200b\u7531\u200b \\(n\\) \u200b\u4e2a\u5b57\u7b26\u200b\u7ec4\u6210\u200b\u7684\u200b\u53e5\u5b50\u200b\u53d8\u6210\u200b\u4e00\u4e2a\u200b \\(n \\times d\\) \u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u5176\u4e2d\u200b \\(d\\) \u200b\u8868\u793a\u200b\u5b57\u7b26\u200b\u7279\u5f81\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b One-Hot \u200b\u7f16\u7801\u200b\u3002\u200b\u7531\u4e8e\u200b One-Hot \u200b\u7f16\u7801\u200b\u5f62\u4e3a\u200b \\(1 \\times n\\_letters\\)\uff0c\u200b\u5219\u200b\u6700\u7ec8\u200b\u5f62\u72b6\u200b\u4e3a\u200b \\(n \\times 1 \\times n\\_letters\\)\u3002</p> <p>\u200b\u7b2c\u56db\u4e2a\u200b Cell \u200b\u4e2d\u200b\uff0c\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u57fa\u672c\u200b\u7684\u200b RNN \u200b\u6a21\u578b\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u4e0a\u65b9\u200b\u4ee3\u7801\u200b\u4e00\u81f4\u200b\uff0c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b\u4e3a\u200b128\u3002\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u7b2c\u4e94\u4e2a\u200b\u548c\u200b\u7b2c\u516d\u4e2a\u200b\u53ef\u200b\u6267\u884c\u200b Cell\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b RNN \u200b\u8fdb\u884c\u200b\u7b80\u5355\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u7b80\u5355\u200b\u7684\u200b\u8bb2\u89e3\u200b\u7b2c\u516d\u4e2a\u200b Cell\uff0c\u200b\u7b2c\u516d\u4e2a\u200b Cell \u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a <pre><code># \u200b\u5c06\u200b Albert \u200b\u8f6c\u4e3a\u200b 6 * 1 * n_letters \u200b\u7684\u200b Tensor\ninput = lineToTensor('Albert')\n# \u200b\u8bbe\u7f6e\u200b h0 \u200b\u5168\u96f6\u200b\u7684\u200b\u539f\u56e0\u200b\u5728\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u8fc7\u200b\nhidden = torch.zeros(1, n_hidden)\n# \u200b\u83b7\u53d6\u200b output \u200b\u548c\u200b h1\noutput, next_hidden = rnn(input[0], hidden)\nprint(output)\n</code></pre> \u200b\u7b2c\u4e09\u884c\u200b\u4e2d\u200b <code>rnn(input[0], hidden)</code>\uff0c\u200b\u8f93\u5165\u200b\u4e86\u200b\u9996\u5b57\u6bcd\u200b \u200b\u4e5f\u200b\u5c31\u662f\u200b 'A' \u200b\u7684\u200b One-Hot \u200b\u7f16\u7801\u200b\uff0c\u200b\u8f93\u51fa\u200b <code>output</code> \u200b\u662f\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u5b57\u7b26\u200b\u53ef\u80fd\u200b\u662f\u200b\u4ec0\u4e48\u200b\u7684\u200b\u6982\u7387\u200b\uff0c\u200b\u800c\u200b <code>next-hidden</code> \u200b\u5219\u200b\u662f\u200b \u200b\u7528\u4e8e\u200b\u642d\u914d\u200b <code>input[1]</code> \u200b\u8fdb\u884c\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u8f93\u5165\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8df3\u8fc7\u200b\u4e03\u516b\u200b\u4e5d\u4e09\u4e2a\u200b Cell \u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u518d\u200b\u5bf9\u200b train \u200b\u6240\u5728\u200b\u7684\u200b Cell \u200b\u8fdb\u884c\u200b\u5206\u6790\u200b\uff0c\u200b\u4e0b\u9762\u200b\u662f\u200b\u76f8\u5173\u200b\u4ee3\u7801\u200b: <pre><code># \u200b\u8bbe\u7f6e\u200b\u5b66\u4e60\u200b\u7387\u200b\nlearning_rate = 0.005 \n# \u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u4e2d\u200b\uff0c categor_tensor \u200b\u8868\u793a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u7528\u4ee5\u200b\u8ba1\u7b97\u200b loss\n# line_tensor \u200b\u662f\u200b\u7531\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u6240\u200b\u8f6c\u53d8\u200b\u7684\u200b tensor, shape: n * 1 * n_letter\ndef train(category_tensor, line_tensor):\n    # \u200b\u8bbe\u7f6e\u200b H0\n    hidden = rnn.initHidden()\n    # \u200b\u68af\u5ea6\u200b\u6e05\u96f6\u200b\n    rnn.zero_grad()\n    # \u200b\u5c06\u200b\u5b57\u7b26\u200b\u6328\u4e2a\u200b\u8f93\u5165\u200b\n    for i in range(line_tensor.size()[0]):\n        output, hidden = rnn(line_tensor[i], hidden)\n    # \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\n    loss = criterion(output, category_tensor)\n    # \u200b\u68af\u5ea6\u200b\u56de\u4f20\u200b\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    # \u200b\u8fd9\u91cc\u200b\u5176\u5b9e\u200b\u662f\u200b\u4e00\u4e2a\u200b\u624b\u52a8\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b optimizer\n    for p in rnn.parameters():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return output, loss.item()\n</code></pre> \u200b\u7ed3\u5408\u200b\u6ce8\u91ca\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u89c2\u5bdf\u200b\u4ee3\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u53d8\u200b\u957f\u200b\u5e8f\u5217\u200b\uff0c\u200b\u5728\u200b\u8f93\u5165\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5b57\u7b26\u200b\u4e4b\u524d\u200b\uff0c\u200b\u90fd\u200b\u4f7f\u7528\u200b <code>hidden</code> \u200b\u4f5c\u4e3a\u200b\u8f93\u51fa\u200b\u7528\u4e8e\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u5c06\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u5e26\u5165\u200b\u4e0b\u200b\u4e00\u8f6e\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u53bb\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5b57\u7b26\u200b\u8f93\u5165\u200b\u7ed3\u675f\u200b\u540e\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>outpt</code> \u200b\u4f5c\u4e3a\u200b\u8f93\u51fa\u200b\uff0c\u200b\u8fdb\u884c\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u5728\u200b <code>train</code> \u200b\u4e2d\u200b\u7684\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5bf9\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5355\u72ec\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u800c\u200b\u5b9e\u9645\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u8981\u200b\u5bf9\u200b\u591a\u4e2a\u200b\u53e5\u5b50\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u91c7\u7528\u200b\u968f\u673a\u200b\u91c7\u6837\u6cd5\u200b\uff0c\u200b\u4ece\u200b\u5168\u90e8\u200b\u6570\u636e\u200b\u4e2d\u200b\u968f\u673a\u200b\u63d0\u53d6\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u7ed3\u679c\u200b\uff1a <pre><code>import time\nimport math\n# \u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\nn_iters = 100000\n# \u200b\u8f93\u51fa\u200b\u9891\u7387\u200b\nprint_every = 5000\n# loss\u200b\u8ba1\u7b97\u200b\u9891\u7387\u200b\nplot_every = 1000\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n# \u200b\u5f00\u59cb\u200b\u65f6\u95f4\u200b\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    # \u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u91c7\u6837\u200b\u63d0\u53d6\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u53ca\u5176\u200b\u6807\u7b7e\u200b\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    # \u200b\u8bad\u7ec3\u200b\n    output, loss = train(category_tensor, line_tensor)\n    # \u200b\u8ba1\u7b97\u200bloss\n    current_loss += loss\n\n    # Print iter number, loss, name and guess\n    # \u200b\u8f93\u51fa\u200b\u9636\u6bb5\u200b\u7ed3\u679c\u200b\n    if iter % print_every == 0:\n        guess, guess_i = categoryFromOutput(output)\n        correct = '\u2713' if guess == category else '\u2717 (%s)' % category\n        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n\n    # Add current loss avg to list of losses\n    # \u200b\u4fdd\u5b58\u200b\u9636\u6bb5\u6027\u200b loss\n    if iter % plot_every == 0:\n        all_losses.append(current_loss / plot_every)\n        current_loss = 0\n</code></pre> \u200b\u5230\u200b\u8fd9\u91cc\u200b\uff0cRNN \u200b\u8bad\u7ec3\u200b\u7684\u200b\u4ee3\u7801\u200b\u53ef\u4ee5\u200b\u8bf4\u200b\u8bb2\u89e3\u200b\u7ed3\u675f\u200b\u4e86\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u4ee5\u53ca\u200b\u63a8\u7406\u200b\u5728\u200b\u7406\u89e3\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e0a\u200b\u5e76\u200b\u4e0d\u7b97\u200b\u96be\u4e8b\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e0d\u518d\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\u518d\u200b\u6839\u636e\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u5bf9\u200b RNN \u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u5206\u6790\u200b\u3002\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5728\u200b\u672c\u6b21\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u8bcd\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8bcd\u200b\u7684\u200b\u957f\u77ed\u4e0d\u4e00\u200b\u5e76\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b RNN \u200b\u7684\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u800c\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5219\u200b\u662f\u200b\u5b57\u7b26\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5b57\u7b26\u200b\u90fd\u200b\u6784\u6210\u200b\u4e86\u200b\u76f8\u540c\u200b\u7684\u200b\u5411\u91cf\u200b\uff1a \\(1 \\times d\\) \uff0c\u200b\u8fd9\u200b\u4f7f\u5f97\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e5f\u200b\u6bd4\u8f83\u200b\u7684\u200b\u7edf\u4e00\u200b\u3002</p> <p>\u200b\u518d\u200b\u7b80\u5355\u200b\u7684\u200b\u4e3e\u4e00\u53cd\u4e09\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7ed3\u5408\u200b\u4e4b\u524d\u200b\u6240\u5b66\u200b\u7684\u200b word2vec\u3001Glovec \u200b\u7b49\u200b\u6a21\u578b\u200b\u5c06\u200b\u8bcd\u8bed\u200b\u8f6c\u4e3a\u200b\u5411\u91cf\u200b\uff0c\u200b\u5c06\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u8f6c\u4e3a\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8bcd\u200b\u8f6c\u4e3a\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u503c\u200b\uff0c\u200b\u8fd9\u6837\u7684\u8bdd\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u8fdb\u884c\u200b\u6587\u672c\u200b\u5206\u7c7b\u200b\u4e86\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/#rnn_3","title":"RNN \u200b\u5b58\u5728\u200b\u7684\u200b\u95ee\u9898","text":"<p>\u200b\u524d\u9762\u200b\u8bb2\u89e3\u200b\u4e86\u200b RNN \u200b\u662f\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u7b80\u5355\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65e0\u6cd5\u200b\u5904\u7406\u200b\u5e8f\u5217\u200b\u95ee\u9898\u200b\u7684\u200b\uff0c\u200b\u4f46\u662f\u200b RNN \u200b\u662f\u5426\u200b\u5c31\u200b\u5b8c\u7f8e\u65e0\u7f3a\u200b\uff1f\u200b\u80fd\u200b\u5e94\u7528\u200b\u4e8e\u200b\u5168\u90e8\u200b\u7684\u200b\u5e8f\u5217\u200b\u4efb\u52a1\u200b\u4e86\u200b\u5462\u200b\uff1f\u200b\u7b54\u6848\u200b\u5f53\u7136\u200b\u662f\u200b\u5426\u5b9a\u200b\u7684\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u7531\u4e8e\u200b RNN \u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u7f3a\u9677\u200b\uff1a\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u4e0e\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u3002</p> <p>\u200b\u91cd\u65b0\u200b\u5ba1\u67e5\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u516c\u5f0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u8f7b\u677e\u200b\u7684\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5728\u200b\u5e8f\u5217\u200b\u8fbe\u5230\u200b\u672b\u5c3e\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u624d\u200b\u9700\u8981\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u4e0e\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u56de\u4f20\u200b\uff0c\u200b\u6b64\u65f6\u200b\u5c06\u200b \\(\\mathbf{H}_t\\) \u200b\u5c55\u5f00\u200b\uff0c\u200b\u5176\u200b\u5185\u90e8\u200b\u5b58\u5728\u200b \\(\\mathbf{W}_{hh} \\times \\mathbf{H}_{t-1}\\)\u3002\u200b\u800c\u200b\u5c06\u200b \\(\\mathbf{H}_{t-1}\\) \u200b\u5c55\u5f00\u200b\uff0c\u200b\u4e5f\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b \\(\\mathbf{W}_{hh}\\)\uff0c\u200b\u90a3\u4e48\u200b\u5f88\u200b\u660e\u663e\u200b \u200b\u5982\u679c\u200b \\(\\mathbf{W}_{hh}\\) \u200b\u5927\u4e8e\u200b 1\uff0c\u200b\u5728\u200b\u7ecf\u8fc7\u200b \\(t\\) \u200b\u6b21\u200b\u8fde\u4e58\u200b\u4e4b\u540e\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\uff0c\u200b\u5982\u679c\u200b  \\(\\mathbf{W}_{hh}\\) \u200b\u5c0f\u4e8e\u200b 1\uff0c\u200b\u5728\u200b\u7ecf\u8fc7\u200b \\(t\\) \u200b\u6b21\u200b\u8fde\u4e58\u200b\u4e4b\u540e\u200b\u53c8\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u3002\u200b\u540c\u7406\u200b\uff0c\u200b\u5728\u200b \\(\\mathbf{W}_{xh}\\) \u200b\u4e0a\u200b\uff0c\u200b\u4e5f\u200b\u5b58\u5728\u200b\u8fd9\u6837\u200b\u7684\u200b\u4f9d\u8d56\u200b\u5173\u7cfb\u200b\uff0c\u200b\u4e5f\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u6216\u8005\u200b\u6d88\u5931\u200b\u3002</p> <p>\u200b\u68af\u5ea6\u200b\u7684\u200b\u6d88\u5931\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u601d\u8003\u200b\uff1a\u200b\u524d\u65b9\u200b\u8282\u70b9\u200b\u7684\u200b\u9690\u85cf\u200b\u4fe1\u606f\u200b\u603b\u548c\u200b\u5360\u200b\u4e86\u200b 0.5\uff0c\u200b\u800c\u200b\u5f53\u524d\u200b\u8282\u70b9\u200b\u9690\u85cf\u200b\u4fe1\u606f\u200b\u5360\u200b\u4e86\u200b 0.5\uff0c\u200b\u968f\u7740\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u7684\u200b\u589e\u52a0\u200b\uff0c\u200b\u4f4d\u4e8e\u200b\u5f00\u59cb\u200b\u90e8\u5206\u200b\u8282\u70b9\u200b\u7684\u200b\u9690\u85cf\u200b\u4fe1\u606f\u200b\u5360\u200b\u6bd4\u200b\u51e0\u4e4e\u200b\u662f\u200b\u6307\u6570\u200b\u578b\u200b\u4e0b\u964d\u200b\uff0c\u200b\u4ece\u200b 1/2 \u200b\u5230\u200b 1/4 \u200b\u5230\u200b 1/8\uff0c\u200b\u4ee5\u6b64\u7c7b\u63a8\u200b\u3002\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4e0b\u56fe\u200b\u5c1d\u8bd5\u200b\u7406\u89e3\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b RNN \u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u5c31\u200b\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6bd4\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u66f4\u200b\u5173\u6ce8\u200b\u7684\u200b\u96be\u9898\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u53ea\u80fd\u200b\u8bb0\u5f97\u200b\u79bb\u200b\u81ea\u5df1\u200b\u6700\u8fd1\u200b\u7684\u200b\u51e0\u4e2a\u200b\u8282\u70b9\u200b\u7684\u200b\u72b6\u6001\u200b\uff0c\u200b\u800c\u200b\u5f88\u200b\u96be\u200b\u8003\u8651\u200b\u66f4\u200b\u8fdc\u200b\u4e00\u6b65\u200b\u7684\u200b\u72b6\u6001\u200b\uff0c\u200b\u65e0\u6cd5\u200b\u66f4\u597d\u200b\u7684\u200b\u5173\u8054\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b \u200b\u68af\u5ea6\u200b\u526a\u88c1\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4fdd\u8bc1\u200b\u68af\u5ea6\u200b\u4f20\u9012\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4e0d\u200b\u5927\u4e8e\u200b1\uff0c\u200b\u4ece\u800c\u200b\u4f7f\u5f97\u200b\u68af\u5ea6\u200b\u4e0d\u4f1a\u200b\u7206\u70b8\u200b\uff0c\u200b\u4f46\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u963b\u6b62\u200b\u68af\u5ea6\u200b\u7684\u200b\u6d88\u5931\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u4e0b\u200b\u4e00\u7bc7\u200b\uff0c\u200b\u9488\u5bf9\u200b RNN \u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u7684\u200b LSTM\u3001GRU \u200b\u7b49\u200b\u7b97\u6cd5\u200b</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","title":"ResNet\u200b\u6e90\u7801\u200b\u89e3\u8bfb","text":"<p>\u200b\u672c\u6587\u200b\u5bf9\u200b\u6b8b\u5dee\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08ResNet\uff09\u200b\u7684\u200b\u6e90\u7801\u200b\u8fdb\u884c\u200b\u89e3\u8bfb\u200b\u3002\u200b\u6b8b\u5dee\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u7531\u200b\u5fae\u8f6f\u200b\u7814\u7a76\u9662\u200b\u7684\u200b\u4f55\u607a\u660e\u200b\u3001\u200b\u5f20\u7965\u96e8\u200b\u3001\u200b\u4efb\u5c11\u537f\u200b\u3001\u200b\u5b59\u5251\u200b\u7b49\u200b\u4eba\u200b\u63d0\u51fa\u200b\u7684\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u4e3b\u8981\u200b\u8d21\u732e\u200b\u662f\u200b\u53d1\u73b0\u200b\u4e86\u200b\u5728\u200b\u589e\u52a0\u200b\u7f51\u7edc\u5c42\u200b\u6570\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200e\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200b\u7cbe\u5ea6\u200b(Training accuracy)\u200e\u200b\u9010\u6e10\u200b\u8d8b\u4e8e\u200b\u9971\u548c\u200b\uff0c\u200b\u7ee7\u7eed\u200b\u589e\u52a0\u200b\u5c42\u6570\u200b\uff0ctraining accuracy \u200b\u5c31\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e0b\u964d\u200b\u7684\u200b\u73b0\u8c61\u200b\uff0c\u200b\u800c\u200b\u8fd9\u79cd\u200b\u4e0b\u964d\u200b\u4e0d\u662f\u200b\u7531\u8fc7\u200b\u62df\u5408\u200b\u9020\u6210\u200b\u7684\u200b\u3002\u200b\u4ed6\u4eec\u200b\u5c06\u200b\u8fd9\u200b\u4e00\u200b\u73b0\u8c61\u200b\u79f0\u4e4b\u4e3a\u200b\u201c\u200b\u9000\u5316\u200b\u73b0\u8c61\u200b\uff08Degradation\uff09\u201d\uff0c\u200b\u5e76\u200b\u9488\u5bf9\u200b\u9000\u5316\u200b\u73b0\u8c61\u200b\u53d1\u660e\u200b\u4e86\u200b \u201c\u200b\u5feb\u6377\u200b\u8fde\u63a5\u200b\uff08Shortcut connection\uff09\u201d\uff0c\u200b\u6781\u5927\u200b\u7684\u200b\u6d88\u9664\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u8fc7\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u56f0\u96be\u200b\u95ee\u9898\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u201c\u200b\u6df1\u5ea6\u200b\u201d\u200b\u9996\u6b21\u200b\u7a81\u7834\u200b\u4e86\u200b100\u200b\u5c42\u200b\u3001\u200b\u6700\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u751a\u81f3\u200b\u8d85\u8fc7\u200b\u4e86\u200b1000\u200b\u5c42\u200b\u3002\uff08\u200b\u5728\u200b\u6b64\u200b\uff0c\u200b\u5411\u200b\u5df2\u6545\u200b\u7684\u200b\u5b59\u5251\u200b\u535a\u58eb\u200b\u8868\u793a\u200b\u5d07\u9ad8\u200b\u7684\u200b\u656c\u610f\u200b\uff09</p> <p>\u200b\u901a\u8fc7\u200b\u672c\u6587\u200b\u4f60\u200b\u5c06\u200b\u5b66\u4e60\u200b\u5230\u200b\uff1a</p> <ul> <li> <p>\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b/\u200b\u7206\u70b8\u200b\u7684\u200b\u7b80\u4ecb\u200b</p> </li> <li> <p>\u200b\u4ee3\u7801\u200b\u91cc\u9762\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8bbe\u8ba1\u200bBasicBlock\u200b\u548c\u200bBottleneck\u200b\u4e24\u79cd\u200b\u7ed3\u6784\u200b</p> </li> <li> <p>\u200b\u4ee3\u7801\u200b\u91cc\u9762\u200b\u7684\u200bexpansion\u200b\u4f5c\u7528\u200b</p> </li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#1","title":"1 \u200b\u57fa\u672c\u200b\u4ecb\u7ecd","text":"<p>\u200b\u968f\u7740\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u4eba\u4eec\u200b\u53d1\u73b0\u200b\u591a\u5c42\u200b\u5377\u79ef\u200b\u6216\u8005\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u5927\u4e8e\u200b\u5355\u5c42\u200b\u5377\u79ef\u200b\u6216\u8005\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u3002\u200b\u4e8e\u662f\u200b\u5f88\u591a\u200b\u4eba\u200b\u6f5c\u610f\u8bc6\u200b\u8ba4\u4e3a\u200b\u7f51\u7edc\u200b\u7684\u200b\u5c42\u6570\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u5176\u200b\u6548\u679c\u200b\u5c31\u200b\u4f1a\u200b\u8d8a\u200b\u597d\u200b\u3002\u200b\u4f46\u662f\u200b\u5f53\u65f6\u200b\u5fae\u8f6f\u200b\u7814\u7a76\u9662\u200b\u7684\u200b\u4f55\u607a\u660e\u200b\u3001\u200b\u5f20\u7965\u96e8\u200b\u3001\u200b\u4efb\u5c11\u537f\u200b\u3001\u200b\u5b59\u5251\u200b\u7b49\u200b\u4eba\u200b\u53d1\u73b0\u200b\u52a0\u6df1\u200b\u7f51\u7edc\u200b\u7684\u200b\u6df1\u5ea6\u200b\u540e\u200b\uff0c\u200b\u6574\u4e2a\u200b\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u53cd\u800c\u200b\u53d8\u5dee\u200b\u4e86\u200b\u8bb8\u591a\u200b\u3002\u200b\u4ed6\u4eec\u200b\u8ba4\u4e3a\u200b\u5f88\u6df1\u200b\u7684\u200b\u7f51\u7edc\u200b\u65e0\u6cd5\u200b\u8bad\u7ec3\u200b\u7684\u200b\u539f\u56e0\u200b\u53ef\u80fd\u200b\u662f\u200b\u7f51\u7edc\u200b\u5728\u200b\u4fe1\u606f\u200b\u4f20\u9012\u200b\u7684\u200b\u65f6\u5019\u200b\u6216\u591a\u6216\u5c11\u200b\u4f1a\u200b\u5b58\u5728\u200b\u4fe1\u606f\u200b\u4e22\u5931\u200b\uff0c\u200b\u635f\u8017\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u6216\u8005\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u73b0\u8c61\u200b\u3002\u200b\u9488\u5bf9\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u63d0\u51fa\u200b\u4e86\u200bResNet\u200b\u4ee5\u200b\u671f\u671b\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0cResNet\u200b\u7684\u200b\u51fa\u73b0\u200b\u4e5f\u200b\u8ba9\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9010\u6e10\u200b\u771f\u6b63\u200b\u8d70\u5411\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002ResNet\u200b\u6700\u5927\u200b\u7684\u200b\u8d21\u732e\u200b\u5728\u4e8e\u200b\u6dfb\u52a0\u200b\u4e86\u200bshortcut connection\u200b\u5c06\u200b\u8f93\u5165\u200b\u76f4\u63a5\u200b\u8fde\u63a5\u200b\u5230\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u7f13\u89e3\u200b\u4e86\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u5e76\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u8be6\u7ec6\u200b\u7684\u200b\u89e3\u91ca\u4e00\u4e0b\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u3002</p> <p>\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u548c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u7684\u200b\u6839\u6e90\u200b\u4e3b\u8981\u200b\u662f\u56e0\u4e3a\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ed3\u6784\u200b\u4ee5\u53ca\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u76ee\u524d\u200b\u4f18\u5316\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u601d\u60f3\u200b\uff0c\u200b\u5373\u200b\u6839\u636e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u7684\u200b\u8bef\u5dee\u200b\u901a\u8fc7\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u6307\u5bfc\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u6743\u503c\u200b\u7684\u200b\u66f4\u65b0\u200b\u3002\u200b\u8bef\u5dee\u200b\u68af\u5ea6\u200b\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u7684\u200b\u65b9\u5411\u200b\u548c\u200b\u6570\u91cf\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4ee5\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5411\u200b\u548c\u200b\u5408\u9002\u200b\u7684\u200b\u91cf\u200b\u66f4\u65b0\u200b\u7f51\u7edc\u200b\u6743\u91cd\u200b\u3002 \u200b\u5728\u200b\u6df1\u5c42\u200b\u7f51\u7edc\u200b\u6216\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u8bef\u5dee\u200b\u68af\u5ea6\u200b\u53ef\u200b\u5728\u200b\u66f4\u65b0\u200b\u4e2d\u200b\u7d2f\u79ef\u200b\uff0c\u200b\u53d8\u6210\u200b\u975e\u5e38\u200b\u5927\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u5bfc\u81f4\u200b\u7f51\u7edc\u200b\u6743\u91cd\u200b\u7684\u200b\u5927\u5e45\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5e76\u200b\u56e0\u6b64\u200b\u4f7f\u200b\u7f51\u7edc\u200b\u53d8\u5f97\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\u3002\u200b\u5728\u200b\u6781\u7aef\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6743\u91cd\u200b\u7684\u200b\u503c\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u5927\u200b\uff0c\u200b\u4ee5\u81f3\u4e8e\u200b\u6ea2\u51fa\u200b\uff0c\u200b\u5bfc\u81f4\u200b NaN \u200b\u503c\u200b\u3002 \u200b\u7f51\u7edc\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u503c\u200b\u5927\u4e8e\u200b 1.0\uff09\u200b\u91cd\u590d\u200b\u76f8\u4e58\u200b\u5bfc\u81f4\u200b\u7684\u200b\u6307\u6570\u200b\u7ea7\u200b\u589e\u957f\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u3002 \u200b\u5728\u200b\u6df1\u5ea6\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u4f1a\u200b\u5f15\u8d77\u200b\u7f51\u7edc\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\uff0c\u200b\u6700\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u65e0\u6cd5\u200b\u4ece\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\uff0c\u200b\u800c\u200b\u6700\u574f\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u51fa\u73b0\u200b\u65e0\u6cd5\u200b\u518d\u200b\u66f4\u65b0\u200b\u7684\u200b NaN \u200b\u6743\u91cd\u200b\u503c\u200b\u3002</p> <p>\u200b\u800c\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u68af\u5ea6\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c \u200b\u7f51\u7edc\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u503c\u200b\u5c0f\u4e8e\u200b 1.0\uff09\u200b\u91cd\u590d\u200b\u76f8\u4e58\u200b\u5bfc\u81f4\u200b\u7684\u200b\u6307\u6570\u200b\u7ea7\u200b\u53d8\u5c0f\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u68af\u5ea6\u200b\u6d88\u5931\u200b\u3002\u200b\u5728\u200b\u6700\u574f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5b8c\u5168\u200b\u505c\u6b62\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u8fdb\u4e00\u6b65\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f20\u7edf\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b(\u200b\u5982\u200b\u53cc\u66f2\u200b\u6b63\u5207\u200b\u51fd\u6570\u200b)\u200b\u5177\u6709\u200b\u8303\u56f4\u200b(0,1)\u200b\u5185\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u901a\u8fc7\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002\u200b\u8fd9\u6837\u200b\u505a\u200b\u7684\u200b\u6548\u679c\u200b\u662f\u200b\uff0c\u200b\u7528\u200b\u8fd9\u4e9b\u200b\u5c0f\u200b\u6570\u5b57\u200b\u7684\u200bn\u200b\u4e58\u4ee5\u200bn\u200b\u6765\u200b\u8ba1\u7b97\u200bn\u200b\u5c42\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u201c\u200b\u524d\u7aef\u200b\u201d\u200b\u5c42\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u68af\u5ea6\u200b(\u200b\u8bef\u5dee\u200b\u4fe1\u53f7\u200b)\u200b\u968f\u200bn\u200b\u5448\u200b\u6307\u6570\u200b\u9012\u51cf\u200b\uff0c\u200b\u800c\u200b\u524d\u7aef\u200b\u5c42\u200b\u7684\u200b\u8bad\u7ec3\u200b\u975e\u5e38\u200b\u7f13\u6162\u200b\u3002\u200b\u6700\u7ec8\u200b\u5bfc\u81f4\u200b\u66f4\u65b0\u200b\u505c\u6ede\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#2","title":"2 \u200b\u6e90\u7801\u200b\u89e3\u8bfb","text":"<p>\u200b\u4e3a\u4e86\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u5bf9\u200bResNet\u200b\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200btorchvision\u200b\u7684\u200bResNet\u200b\u6e90\u7801\u200b\u8fdb\u884c\u200b\u89e3\u8bfb\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#21","title":"2.1 \u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u5c01\u88c5","text":"<p>\u200b\u5728\u200b\u4ee3\u7801\u200b\u7684\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9996\u5148\u200b\u5c01\u88c5\u200b\u4e86\u200b3x3\u200b\u548c\u200b1x1\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u589e\u52a0\u200b\u4ee3\u7801\u200b\u7684\u200b\u53ef\u8bfb\u6027\u200b\u3002\u200b\u9664\u4e86\u200b\u8fd9\u79cd\u200b\u4ee3\u7801\u200b\u5199\u6cd5\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u5728\u200b\u5f00\u59cb\u200b\u4e5f\u200b\u4f1a\u200b\u5c06\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5c42\u200b\u548c\u200bBN\u200b\u5c42\u200b\u5c01\u88c5\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u540c\u6837\u200b\u662f\u200b\u4e3a\u4e86\u200b\u589e\u52a0\u200b\u4ee3\u7801\u200b\u7684\u200b\u53ef\u8bfb\u6027\u200b\u3002</p> <pre><code>def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -&gt; nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -&gt; nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#22","title":"2.2 \u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u7684\u200b\u8bbe\u8ba1","text":"<p>ResNet\u200b\u7f51\u7edc\u200b\u662f\u200b\u7531\u200b\u5f88\u591a\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u5806\u53e0\u200b\u8d77\u6765\u200b\u7684\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4fdd\u8bc1\u200b\u4ee3\u7801\u200b\u5177\u6709\u200b\u53ef\u8bfb\u6027\u200b\u548c\u200b\u53ef\u6269\u5c55\u6027\u200b\uff0cResNet\u200b\u5728\u200b\u8bbe\u8ba1\u200b\u65f6\u200b\u91c7\u7528\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u9488\u5bf9\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200bResNet\uff0c\u200b\u4e66\u5199\u200b\u4e86\u200bBasicBlock\u200b\u548c\u200bBottleNeck\u200b\u4e24\u4e2a\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u3002\u200b\u8fd9\u79cd\u200b\u6a21\u5757\u5316\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u5728\u200b\u73b0\u5728\u200b\u8bb8\u591a\u200b\u5e38\u89c1\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ecf\u5e38\u200b\u770b\u5230\u200b\u3002</p> <p>ResNet\u200b\u5e38\u89c1\u200b\u7684\u200b\u5927\u5c0f\u200b\u6709\u200b\u4e0b\u56fe\u200b\u7684\u200bResNet-18\uff0cResNet-34\uff0cResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\uff0c\u200b\u5176\u4e2d\u200b\u7f51\u7edc\u200b\u540e\u9762\u200b\u7684\u200b\u6570\u5b57\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u7f51\u7edc\u200b\u7684\u200b\u5c42\u6570\u200b\u3002</p> <p></p> <p>\u200b\u4e3a\u4e86\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u66f4\u597d\u200b\u7684\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ee5\u200bResNet101\u200b\u4e3a\u4f8b\u200b\u3002</p> layer_name \u200b\u6b21\u6570\u200b conv1 \u200b\u5377\u79ef\u200b1\u200b\u6b21\u200b conv2_x \u200b\u5377\u79ef\u200b3 x 3 = 9\u200b\u6b21\u200b conv3_x \u200b\u5377\u79ef\u200b4 x 3 = 12\u200b\u6b21\u200b conv4_x \u200b\u5377\u79ef\u200b23 x 3 = 69\u200b\u6b21\u200b conv5_x \u200b\u5377\u79ef\u200b3 x 3 = 9\u200b\u6b21\u200b fc average pool 1\u200b\u6b21\u200b \u200b\u5408\u8ba1\u200b 1 + 9 + 12 + 69 + 9 + 1 = 101\u200b\u6b21\u200b <p>\u200b\u89c2\u5bdf\u200b\u4e0a\u9762\u200b\u5404\u4e2a\u200bResNet\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u5185\u200b\uff0c\u200b\u6570\u636e\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e0d\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u4f46\u662f\u200bResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u5185\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200bchannel\u200b\u6570\u76ee\u200b\u4e0d\u200b\u4e00\u6837\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7684\u200bchannel\u200b\u6269\u5927\u200b\u4e3a\u200b\u8f93\u5165\u200bchannel\u200b\u7684\u200b4\u200b\u500d\u200b\uff0c\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u5377\u79ef\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e5f\u200b\u53d8\u6362\u200b\u4e3a\u200b1\uff0c3\uff0c1\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u57fa\u4e8e\u200b\u8fd9\u4e2a\u200b\u53d1\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u7684\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u5f53\u4f5c\u200b\u4e00\u7c7b\u200b\uff0cResNet-50\u3001ResNet-101\u200b\u548c\u200bResNet-152\u200b\u8fd9\u4e09\u7c7b\u200b\u7f51\u7edc\u200b\u7684\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u5f53\u4f5c\u200b\u4e00\u7c7b\u200b\u3002\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0ctorchvision\u200b\u7684\u200b\u6e90\u7801\u200b\u4e5f\u200b\u662f\u200b\u57fa\u4e8e\u200b\u8fd9\u79cd\u200b\u8bbe\u8ba1\u200b\u601d\u60f3\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u56fe\u200b\u7684\u200bBasicBlock\uff08\u200b\u5de6\u200b\uff09\u200b\u548c\u200bBottleNeck\uff08\u200b\u53f3\u200b\uff09\u200b\u6a21\u5757\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4e3a\u4e86\u200b\u63a7\u5236\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u76ee\u200b\u7684\u200b\u53d8\u5316\u200b\uff0c\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u7ef4\u5ea6\u200b\u4e5f\u200b\u901a\u8fc7\u200bexpansion\u200b\u8fdb\u884c\u200b\u63a7\u5236\u200b\uff0c\u200b\u4e24\u4e2a\u200bblock\u200b\u7c7b\u200b\u8f93\u5165\u200b\u4e00\u4e2a\u200b\u901a\u9053\u200b\u4e3a\u200bin_planes\u200b\u7ef4\u7684\u5ea6\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200bplanes*block.expansion\u200b\u7ef4\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u5176\u4e2d\u200bplanes\u200b\u7684\u200b\u6570\u76ee\u200b\u5927\u5c0f\u200b\u7b49\u4e8e\u200bin_planes\u3002\u200b\u9664\u6b64\u4ee5\u5916\u200b\uff0c\u200b\u4ee3\u7801\u200b\u53f3\u4fa7\u200b\u7684\u200b\u66f2\u7ebf\u200b\u5c31\u662f\u200b\u672c\u6587\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200bshortcut\u200b\u652f\u8def\u200b\uff0c\u200b\u8be5\u652f\u200b\u8def\u4e0a\u200b\u7684\u200bdownsample\u200b\u64cd\u4f5c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5bf9\u200bshortcut\u200b\u652f\u8def\u200b\u8fdb\u884c\u200b\u5927\u5c0f\u200b\u6216\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u8c03\u6574\u200b\uff0c\u200b\u4ee5\u200b\u5e0c\u671b\u200b\u6267\u884c\u200b\u76f8\u52a0\u200b\u64cd\u4f5c\u200b</p> <p></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#221-shortcut-connection","title":"2.2.1 Shortcut Connection.","text":"<p>\u200b\u8fd9\u91cc\u200b\u518d\u200b\u5206\u6790\u200b\u4e00\u4e0b\u200bshortcut connection\uff1a</p> <p></p> <p>shortcut connection\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u201c\u200b\u6284\u8fd1\u200b\u9053\u200b\u201d\uff0c\u200b\u5b83\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5176\u4e00\u200b\u4e3a\u200b\u540c\u7b49\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u5373\u200b\u8f93\u5165\u8f93\u51fa\u200b\u76f4\u63a5\u200b\u76f8\u52a0\u200b\uff08\u200b\u5373\u200b\u4e0a\u200b\u56fe\u200b\u4e2d\u200b\u7684\u200bF(x) + x\uff09\uff0c\u200b\u53e6\u200b\u4e00\u79cd\u200b\u4e3a\u200b\u4e0d\u540c\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u8fd9\u65f6\u5019\u200b\u5c31\u200b\u9700\u8981\u200b\u7ed9\u200bx\u200b\u8865\u5145\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u6765\u200b\u5339\u914d\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\u4e0b\u9762\u200b\u8fd9\u4e2a\u200b\u56fe\u200b\uff1a</p> <p></p> <p>\u200b\u5de6\u200b\uff1aVGG-19\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u53c2\u8003\u200b\u3002 \u200b\u4e2d\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u6709\u200b34\u200b\u4e2a\u200b\u53c2\u6570\u200b\u5c42\u200b\u7684\u200b\u666e\u901a\u200b\u7f51\u7edc\u200b\u3002 \u200b\u53f3\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u6709\u200b34\u200b\u4e2a\u200b\u53c2\u6570\u200b\u5c42\u200b\u7684\u200b\u6b8b\u5dee\u200b\u7f51\u7edc\u200b\uff08\u200b\u5373\u200bresnet34\uff09</p> <p>\u200b\u5728\u200b\u4e0a\u200b\u56fe\u200b\u6700\u200b\u53f3\u4fa7\u200b\u7684\u200b\u8def\u5f84\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u660e\u663e\u200b\u7684\u200b\u770b\u5230\u200bshortcut connection\u200b\u52a0\u5165\u200b\u4e86\u200b\u7f51\u7edc\u200b\u4e4b\u4e2d\u200b\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u56fe\u4e2d\u200b\u4e5f\u200b\u5f88\u200b\u660e\u663e\u200b\u7684\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5b9e\u7ebf\u200b\u90e8\u5206\u200b\u5c31\u662f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5355\u7eaf\u200b\u7684\u200bF(x)+x\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u800c\u200b\u865a\u7ebf\u200b\u90e8\u5206\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u5c42\u200b\u7684\u200bstride\u200b\u662f\u200b2\uff08\u200b\u90a3\u4e2a\u200b/2\u200b\u7684\u200b\u610f\u601d\u200b\u5c31\u662f\u200bstride\u200b\u662f\u200b2\uff09\uff1b\u200b\u540c\u65f6\u200b\u6ce8\u610f\u200b\u5230\u200b\u6df1\u5ea6\u200b\u4e5f\u200b\u53d1\u751f\u200b\u4e86\u200b\u53d8\u6362\u200b\uff0cchannel\u200b\u6570\u76ee\u200b\u589e\u52a0\u4e00\u500d\u200b\uff08\u200b\u6269\u5927\u200b\u4e24\u500d\u200b\uff09\uff0c\u200b\u8fd9\u6837\u200bF(x)\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\u6bd4\u200bx\u200b\u5c0f\u200b\u4e00\u534a\u200b\uff0c\u200b\u539a\u5ea6\u200b\u6bd4\u200bx\u200b\u5927\u200b\u4e00\u500d\u200b\u3002\u200b\u5728\u200b\u8fd9\u6837\u200b\u7684\u200bshortcut connection\u200b\u4e2d\u200b\uff0c\u200b\u5c31\u200b\u9700\u8981\u200b\u8865\u5145\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u6765\u200b\u589e\u52a0\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u5728\u200bResNet\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4f7f\u7528\u200b\u4e86\u200b1 x 1\u200b\u7684\u200b\u5377\u79ef\u200b\u6838\u6765\u200b\u8fbe\u5230\u200b\u8fd9\u4e2a\u200b\u76ee\u7684\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53c8\u200b\u63d0\u5230\u200b\u8bf4\u200b\uff1a\u201c\u2026\u2026where both designs have similar time complexity.\u201d \u200b\u65e2\u7136\u200bBasicBlock\u200b\u548c\u200bBottleneck\u200b\u4e8c\u8005\u200b\u7684\u200b\u65f6\u95f4\u200b\u590d\u6742\u5ea6\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4e3a\u4ec0\u4e48\u200b\u8fd8\u8981\u200b\u989d\u5916\u200b\u8bbe\u8ba1\u200b\u4e00\u4e2a\u200bBottleneck\u200b\u7ed3\u6784\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6839\u636e\u200b\u524d\u9762\u200b\u7684\u200b\u53d9\u8ff0\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\uff0cBasicBlock\u200b\u7ed3\u6784\u200b\u6bd4\u200b\u4f20\u7edf\u200b\u7684\u200b\u5377\u79ef\u200b\u7ed3\u6784\u200b\u591a\u200b\u4e86\u200b\u4e00\u4e2a\u200bshortcut\u200b\u652f\u8def\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4f20\u9012\u200b\u4f4e\u5c42\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u7f51\u7edc\u200b\u80fd\u591f\u200b\u8bad\u7ec3\u200b\u5730\u200b\u5f88\u200b\u6df1\u200b\u3002\u200b\u800c\u200bBottleNeck\u200b\u5148\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b1x1\u200b\u7684\u200b\u5377\u79ef\u200b\u51cf\u5c11\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u4e2d\u95f4\u200b\u5377\u79ef\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u51cf\u5c11\u200b\u4e3a\u200b1/4\uff1b\u200b\u4e2d\u95f4\u200b\u7684\u200b\u666e\u901a\u200b\u5377\u79ef\u200b\u505a\u200b\u5b8c\u200b\u5377\u79ef\u200b\u540e\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\u7b49\u4e8e\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u6570\u200b\uff1b\u200b\u7b2c\u4e09\u4e2a\u200b\u5377\u79ef\u200b\u7528\u4e8e\u200b\u6062\u590d\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u5f97\u200bBottleNeck\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6570\u200b\u7b49\u4e8e\u200bBottleNeck\u200b\u7684\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u6570\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b1x1\u200b\u5377\u79ef\u200b\u6709\u6548\u200b\u5730\u200b\u51cf\u5c11\u200b\u4e86\u200b\u5377\u79ef\u200b\u7684\u200b\u53c2\u6570\u200b\u4e2a\u6570\u200b\u548c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\uff0c\u200b\u540c\u65f6\u200b\u51cf\u5c11\u200b\u4e86\u200b\u4e2d\u95f4\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u4f7f\u200b\u5355\u4e2a\u200bBlock\u200b\u6d88\u8017\u200b\u7684\u200b\u663e\u5b58\u200b\u66f4\u200b\u5c11\u200b\uff0c\u200b\u5728\u200b\u8f83\u200b\u6df1\u200b\u7684\u200b\u7f51\u7edc\u200b\u4e2d\u200bBottleNeck\u200b\u4f1a\u200b\u5728\u200b\u53c2\u6570\u200b\u4e0a\u200b\u66f4\u52a0\u200b\u8282\u7ea6\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u6709\u5229\u4e8e\u200b\u6784\u5efa\u200b\u5c42\u6570\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7f51\u7edc\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u80fd\u200b\u4fdd\u6301\u200b\u6027\u80fd\u200b\u7684\u200b\u63d0\u5347\u200b\u3002\u200b\u6240\u4ee5\u200bresnet50, resnet101\u200b\u548c\u200bresnet152\u200b\u4f7f\u7528\u200b\u4e86\u200b\u53e6\u5916\u200b\u8bbe\u8ba1\u200b\u7684\u200bBottleNeck\u200b\u7ed3\u6784\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#222-basicblock","title":"2.2.2 BasicBlock","text":"<p>BasicBlock\u200b\u6a21\u5757\u200b\u7528\u6765\u200b\u6784\u5efa\u200bresnet18\u200b\u548c\u200bresnet34</p> <pre><code>class BasicBlock(nn.Module):\n    expansion: int = 1\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n        if dilation &gt; 1:\n            raise NotImplementedError(\"Dilation &gt; 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x  # x  \u200b\u7ed9\u200b\u81ea\u5df1\u200b\u5148\u200b\u5907\u4efd\u200b\u4e00\u4efd\u200b\n\n        out = self.conv1(x)  # \u200b\u5bf9\u200bx\u200b\u505a\u200b\u5377\u79ef\u200b \n        out = self.bn1(out)  # \u200b\u5bf9\u200bx\u200b\u5f52\u4e00\u5316\u200b \n        out = self.relu(out)  # \u200b\u5bf9\u200bx\u200b\u7528\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\n\n        out = self.conv2(out)  # \u200b\u5bf9\u200bx\u200b\u505a\u200b\u5377\u79ef\u200b\n        out = self.bn2(out)  # \u200b\u5f52\u4e00\u5316\u200b\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity  # \u200b\u8fdb\u884c\u200bdownsample\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#223-bottleneck","title":"2.2.3 BottleNeck","text":"<p>BottleNeck\u200b\u6a21\u5757\u200b\u7528\u6765\u200b\u6784\u5efa\u200bresnet50\uff0cresnet101\u200b\u548c\u200bresnet152</p> <pre><code>class Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4  # \u200b\u5bf9\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u8fdb\u884c\u200b\u500d\u589e\u200b\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        # Bottleneckd forward\u200b\u51fd\u6570\u200b\u548c\u200bBasicBlock\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4e0d\u518d\u200b\u989d\u5916\u200b\u6ce8\u91ca\u200b\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u518d\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u4e2d\u200bexpansion\u200b\u7684\u200b\u4f5c\u7528\u200b\u505a\u200b\u4e00\u4e2a\u200b\u8bf4\u660e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\u4e0b\u9762\u200b\u8fd9\u200b\u5f20\u56fe\u200b\u3002</p> <p></p> <p>expansion\u200b\u7b80\u5355\u200b\u6765\u8bf4\u200b\u5c31\u662f\u200b\u5bf9\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u7684\u200b\u500d\u200b\u4e58\u200b\u3002\u200b\u5728\u200bBasicBlock\u200b\u548c\u200bBottleNeck\u200b\u4e2d\u200b\uff0c\u201c__init__\u201d\u200b\u51fd\u6570\u200b\u4e2d\u6709\u200b\u4e09\u4e2a\u200b\u6bd4\u8f83\u200b\u5173\u952e\u200b\u7684\u200b\u53c2\u6570\u200b\uff1ainplanes,planes\u200b\u548c\u200bstride\uff0c\u200b\u8fd9\u200b\u4e09\u8005\u200b\u5206\u522b\u200b\u8868\u793a\u200b\u8f93\u5165\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u548c\u200b\u6b65\u5e45\u200b\u3002\u200b\u5728\u200b\u4e24\u4e2a\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c__init__\u200b\u4f20\u5165\u200b\u7684\u200bplanes\u200b\u90fd\u200b\u662f\u200b64,128,156,512\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u89c2\u5bdf\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u683c\u200b\uff0c\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u5bf9\u4e8e\u200bResNet-50\uff0cResNet-101\u200b\u548c\u200bResNet-152\u200b\u800c\u8a00\u200b\uff0c\u200b\u5b83\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u5e94\u8be5\u200b\u4e3a\u200b256,512,1024,2048\u200b\u624d\u200b\u5bf9\u200b\u3002\u200b\u56e0\u6b64\u200b\u5728\u200b\u8fd9\u91cc\u200b\u8bbe\u7f6e\u200bexpansion=4\uff0c\u200b\u5bf9\u5e94\u200b\u4e0a\u9762\u200bBottleNeck\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u7684\u200b30\u200b\u884c\u200b\u548c\u200b31\u200b\u884c\u200b\uff0c\u200b\u5c06\u200b\u6bcf\u200b\u4e00\u4e2a\u200bplanes\u200b\u90fd\u200b\u4e58\u200b\u4e0a\u200b\u8fd9\u4e2a\u200bexpansion\uff0c\u200b\u5c31\u200b\u5f97\u5230\u200b\u4e86\u200b\u9700\u8981\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff1b\u200b\u800c\u200b\u5bf9\u4e8e\u200bResNet-18\u200b\u548c\u200bResNet-34\u200b\u800c\u8a00\u200b\uff0c\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u548c\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u6ca1\u6709\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u56e0\u6b64\u200bexpansion\u200b\u4e5f\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b1\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#23","title":"2.3 \u200b\u7f51\u7edc\u200b\u6574\u4f53\u200b\u7ed3\u6784","text":"<p>\u200b\u5728\u200b\u5b9a\u4e49\u200b\u597d\u200b\u6700\u200b\u57fa\u672c\u200b\u7684\u200bBottlenneck\u200b\u548c\u200bBasicBlock\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u6784\u5efa\u200bResNet\u200b\u7f51\u7edc\u200b\u4e86\u200b\u3002</p> <pre><code>class ResNet(nn.Module):\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]], # \u200b\u9009\u62e9\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\n        layers: List[int], # \u200b\u6bcf\u200b\u4e00\u5c42\u200bblock\u200b\u7684\u200b\u6570\u76ee\u200b\u6784\u6210\u200b -&gt; [3,4,6,3]\n        num_classes: int = 1000, # \u200b\u5206\u7c7b\u200b\u6570\u76ee\u200b\n        zero_init_residual: bool = False, # \u200b\u521d\u59cb\u5316\u200b\n\n        #######\u200b\u5176\u4ed6\u200b\u5377\u79ef\u200b\u6784\u6210\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200bResNet\u200b\u65e0\u5173\u200b######\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        #########################################\n\n        norm_layer: Optional[Callable[..., nn.Module]] = None, # norm\u200b\u5c42\u200b\n    ) -&gt; None:\n        super().__init__()\n        _log_api_usage_once(self)\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64 # \u200b\u8f93\u5165\u200b\u901a\u9053\u200b\n\n        #######\u200b\u5176\u4ed6\u200b\u5377\u79ef\u200b\u6784\u6210\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200bResNet\u200b\u65e0\u5173\u200b######\n        self.dilation = 1 # \u200b\u7a7a\u6d1e\u200b\u5377\u79ef\u200b\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n        #########################################\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        # \u200b\u901a\u8fc7\u200b_make_layer\u200b\u5e26\u5230\u200b\u5c42\u6b21\u5316\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u6548\u679c\u200b\n        self.layer1 = self._make_layer(block, 64, layers[0])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv2_x\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv3_x\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv4_x\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])  # \u200b\u5bf9\u5e94\u200b\u7740\u200bconv5_x\n        # \u200b\u5206\u7c7b\u200b\u5934\u200b\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        # \u200b\u6a21\u578b\u200b\u521d\u59cb\u5316\u200b\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n    # \u200b\u5c42\u6b21\u5316\u200b\u8bbe\u8ba1\u200b\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]], # \u200b\u57fa\u672c\u200b\u6784\u6210\u200b\u6a21\u5757\u200b\u9009\u62e9\u200b\n        planes: int,  # \u200b\u8f93\u5165\u200b\u7684\u200b\u901a\u9053\u200b\n        blocks: int, # \u200b\u6a21\u5757\u200b\u6570\u76ee\u200b\n        stride: int = 1, # \u200b\u6b65\u957f\u200b\n        dilate: bool = False, # \u200b\u7a7a\u6d1e\u200b\u5377\u79ef\u200b\uff0c\u200b\u4e0e\u200b\u672c\u6587\u200b\u65e0\u5173\u200b\n    ) -&gt; nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None # \u200b\u662f\u5426\u200b\u91c7\u7528\u200b\u4e0b\u200b\u91c7\u6837\u200b\n        ####################\u200b\u65e0\u5173\u200b#####################\n        previous_dilation = self.dilation \n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        #############################################\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        # \u200b\u4f7f\u7528\u200blayers\u200b\u5b58\u50a8\u200b\u6bcf\u4e2a\u200blayer\n        layers = []\n        layers.append(\n            block(\n                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    dilation=self.dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n        # \u200b\u5c06\u200blayers\u200b\u901a\u8fc7\u200bnn.Sequential\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u7f51\u7edc\u200b\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -&gt; Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)  # conv1   x shape [1 64 112 112]\n        x = self.bn1(x)   # \u200b\u5f52\u4e00\u5316\u200b\u5904\u7406\u200b   \n        x = self.relu(x)  # \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\n        x = self.maxpool(x)  # conv2_x\u200b\u7684\u200b3x3 maxpool        x shape [1 64 56 56]\n\n        x = self.layer1(x) # layer 1\n        x = self.layer2(x) # layer 2\n        x = self.layer3(x) # layer 3\n        x = self.layer4(x) # layer 4\n\n        x = self.avgpool(x) # \u200b\u81ea\u200b\u9002\u5e94\u200b\u6c60\u5316\u200b\n        x = torch.flatten(x, 1) \n        x = self.fc(x) # \u200b\u5206\u7c7b\u200b\n\n        return x\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self._forward_impl(x) \n</code></pre> <p>\u200b\u89c2\u5bdf\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u96be\u200b\u770b\u5230\u200b\uff0c\u200b\u9996\u5148\u200b\u662f\u200b\u4e00\u4e2a\u200b7 x 7\u200b\u7684\u200b\u5377\u79ef\u200b\u4f5c\u7528\u200b\u5728\u200b\u8f93\u5165\u200b\u7684\u200b3\u200b\u7ef4\u200b\u56fe\u7247\u200b\u4e0a\u200b\uff0c\u200b\u5e76\u200b\u8f93\u5165\u200b\u4e00\u4e2a\u200b64\u200b\u7ef4\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff08\u200b\u5373\u200bself.inplanes\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\uff09\uff0c\u200b\u901a\u8fc7\u200bBatchNorm\u200b\u5c42\u200b\uff0cReLU\u200b\u5c42\u200b\uff0cMaxPool\u200b\u5c42\u200b\uff1b\u200b\u7136\u540e\u200b\u7ecf\u8fc7\u200b_make_layer()\u200b\u51fd\u6570\u200b\u6784\u5efa\u200b\u7684\u200b4\u200b\u5c42\u200blayer\uff0c\u200b\u6700\u540e\u200b\u7ecf\u8fc7\u200b\u4e00\u4e2a\u200bAveragePooling\u200b\u5c42\u200b\uff0c\u200b\u518d\u200b\u7ecf\u8fc7\u200b\u4e00\u4e2a\u200bfc\u200b\u5c42\u200b\u5f97\u5230\u200b\u5206\u7c7b\u200b\u8f93\u51fa\u200b\u3002\u200b\u5728\u200b\u7f51\u7edc\u200b\u642d\u5efa\u200b\u8d77\u6765\u200b\u540e\u200b\uff0c\u200b\u8fd8\u200b\u5bf9\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b(Conv2d\u3001BatchNorm2d\u3001last BN)\u200b\u8fdb\u884c\u200b\u4e86\u200b\u521d\u59cb\u5316\u200b\u3002</p> <p>\u200b\u800c\u200b\u5bf9\u4e8e\u200b_make_layer\u200b\u51fd\u6570\u200b\uff0c\u200b\u4e00\u4e2a\u200b_make_layer()\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200blayer\u200b\u5c42\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200blayer\u200b\u5c42\u200b\u662f\u200b\u4e0a\u8ff0\u200b\u4e24\u79cd\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u7684\u200b\u5806\u53e0\u200b\u3002\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u4e2d\u200bblock\u200b\u4ee3\u8868\u200b\u8be5\u200blayer\u200b\u5806\u53e0\u200b\u6a21\u5757\u200b\u7684\u200b\u7c7b\u578b\u200b\uff0c\u200b\u53ef\u200b\u9009\u200bBasicBlock\u200b\u6216\u8005\u200bBottleNeck\uff1bblocks\u200b\u4ee3\u8868\u200b\u8be5\u200blayer\u200b\u4e2d\u200b\u5806\u53e0\u200b\u7684\u200bblock\u200b\u7684\u200b\u6570\u76ee\u200b\uff1bplanes\u200b\u4e0e\u200b\u8be5\u200blayer\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u200b\u6709\u5173\u200b\uff0c\u200b\u6ce8\u610f\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u4e3a\u200bplanes * block.expansion\u3002\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0c _make_layer()\u200b\u662f\u200b\u7528\u6765\u200b\u751f\u6210\u200b\u6b8b\u5dee\u200b\u5757\u200b\u7684\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u7275\u626f\u200b\u5230\u200b\u5b83\u200b\u7684\u200b\u7b2c\u56db\u4e2a\u200b\u53c2\u6570\u200b\uff1astride\uff0c\u200b\u5373\u200b\u5377\u79ef\u200b\u6b65\u5e45\u200b\u3002\u200b\u8be5\u200b\u51fd\u6570\u200b\u4e2d\u200b\u9996\u5148\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u5982\u679c\u200bstride\u200b\u4e0d\u200b\u7b49\u4e8e\u200b1\u200b\u6216\u8005\u200b\u7ef4\u5ea6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff08\u200b\u5373\u200b\u8f93\u5165\u200b\u901a\u9053\u200b\u4e0d\u200b\u6ee1\u8db3\u200b\u5bf9\u5e94\u200b\u5173\u7cfb\u200b\uff09\u200b\u7684\u200b\u65f6\u5019\u200b\u7684\u200bdownsample\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200bBN\u200b\u64cd\u4f5c\u200b\u3002\u200b\u63a5\u7740\u200b\u5bf9\u200binplanes\u200b\u548c\u200bplanes\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u7684\u200b\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u6b21\u200bdownsample \uff0c\u200b\u5373\u5c06\u200b\u5e26\u200bdownsample\u200b\u7684\u200bblock\u200b\u6dfb\u52a0\u200b\u81f3\u200blayers\u3002\u200b\u8fd9\u6837\u200b\u4fdd\u8bc1\u200b\u4e86\u200bx\u200b\u548c\u200bout\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u5faa\u73af\u200b\u6dfb\u52a0\u200b\u4e86\u200b\u6307\u5b9a\u200b\u4e2a\u6570\u200b\u7684\u200bBlock\uff0c\u200b\u7531\u4e8e\u200bx\u200b\u5df2\u7ecf\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\u4e86\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6dfb\u52a0\u200b\u7684\u200b\u5176\u4ed6\u200b\u7684\u200bBlock\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e0d\u7528\u200b\u964d\u7ef4\u200b\u4e86\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5faa\u73af\u200b\u6dfb\u52a0\u200b\u4e0d\u200b\u542b\u200bDownsample\u200b\u7684\u200bBlock\u3002\u200b\u6b63\u5982\u200b\u4e0b\u9762\u200b\u4ee3\u7801\u200b\u6240\u793a\u200b</p> <pre><code>if stride != 1 or self.inplanes != planes * block.expansion:\n    downsample = nn.Sequential(\n        conv1x1(self.inplanes, planes * block.expansion, stride),\n        norm_layer(planes * block.expansion),\n    )\n</code></pre> <p>\u200b\u5f53\u200b\u4e00\u4e2a\u200blayer\u200b\u5305\u542b\u200b\u591a\u4e2a\u200bblock\u200b\u65f6\u200b\uff0c\u200b\u662f\u200b\u901a\u8fc7\u200b\u5411\u200blayers\u200b\u5217\u8868\u200b\u4e2d\u200b\u4f9d\u6b21\u200b\u52a0\u5165\u200b\u6bcf\u4e2a\u200bblock\uff0c\u200b\u6765\u200b\u5b9e\u73b0\u200bblock\u200b\u7684\u200b\u5806\u53e0\u200b\u7684\u200b\u3002\u200b\u7b2c\u4e00\u4e2a\u200bblock\u200b\u9700\u8981\u200b\u7279\u6b8a\u200b\u5904\u7406\u200b\uff0c\u200b\u8be5\u200bblock\u200b\u4f9d\u636e\u200b\u4f20\u5165\u200b\u7684\u200bself.inplanes, planes\u200b\u4ee5\u53ca\u200bstride\u200b\u5224\u65ad\u200b\uff0c\u200b\u53ef\u80fd\u200b\u542b\u6709\u200bdownsample\u200b\u652f\u8def\u200b\uff1b\u200b\u8fd9\u4e2a\u200bblock\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u662f\u200bplanes*block.expansion\u3002\u200b\u7d27\u63a5\u7740\u200b\u4fbf\u200b\u628a\u200bself.inplanes\u200b\u66f4\u65b0\u200b\u4e3a\u6b64\u200b\u503c\u200b\u4f5c\u4e3a\u200b\u540e\u7eed\u200bblock\u200b\u7684\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u540e\u9762\u200b\u7684\u200bblock\u200b\u7684\u200bstride\u200b\u4e3a\u200b\u9ed8\u8ba4\u503c\u200b1\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8f93\u5165\u200b\u4e3a\u200bself.inplanes\uff0c\u200b\u8f93\u51fa\u200b\u4e3a\u200bplanes*block.expansion\uff0c\u200b\u800c\u200bself.inplanes = planes * block.expansion\uff0c\u200b\u56e0\u6b64\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u7279\u5f81\u200b\u56fe\u200b\u5927\u5c0f\u200b\u6216\u8005\u200b\u5c3a\u5bf8\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200bdownsample\u200b\u64cd\u4f5c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ResNet%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#3","title":"3 \u200b\u603b\u7ed3","text":"<p>\u200b\u4e0e\u200b\u666e\u901a\u200b\u7684\u200b\u7f51\u7edc\u200b\u76f8\u6bd4\u200b\uff0cResNet\u200b\u6700\u5927\u200b\u7684\u200b\u4f18\u52bf\u200b\u5c31\u662f\u200b\u5f15\u5165\u200b\u4e86\u200bShortcut\u200b\u8fd9\u4e2a\u200b\u652f\u8def\u200b\uff0c\u200b\u8ba9\u200b\u67d0\u200b\u4e00\u5c42\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8fde\u63a5\u200b\u5230\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u540e\u9762\u200b\u7684\u200b\u5c42\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5b66\u4e60\u200b\u6b8b\u5dee\u200b\u3002\u200b\u4f20\u7edf\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u6216\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u5728\u200b\u4fe1\u606f\u200b\u4f20\u9012\u200b\u65f6\u200b\uff0c\u200b\u6216\u591a\u6216\u5c11\u200b\u4f1a\u200b\u5b58\u5728\u200b\u4fe1\u606f\u200b\u4e22\u5931\u200b\u3001\u200b\u635f\u8017\u200b\u7b49\u200b\u95ee\u9898\u200b\u3002ResNet \u200b\u5728\u200b\u67d0\u79cd\u7a0b\u5ea6\u200b\u4e0a\u200b\u89e3\u51b3\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u5c06\u200b\u8f93\u5165\u200b\u4fe1\u606f\u200b\u7ed5\u9053\u200b\u4f20\u5230\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4fdd\u62a4\u200b\u4fe1\u606f\u200b\u7684\u200b\u5b8c\u6574\u6027\u200b\uff0c\u200b\u6574\u4e2a\u200b\u7f51\u7edc\u200b\u5219\u200b\u53ea\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\u8f93\u5165\u200b\u3001\u200b\u8f93\u51fa\u200b\u5dee\u522b\u200b\u7684\u200b\u90a3\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u7b80\u5316\u200b\u5b66\u4e60\u200b\u76ee\u6807\u200b\u548c\u200b\u96be\u5ea6\u200b\u3002</p> <p>ResNet\u200b\u7684\u200b\u51fa\u73b0\u200b\uff0c\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u89e3\u51b3\u200b\u4e86\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u968f\u200b\u6df1\u5ea6\u200b\u7684\u200b\u589e\u52a0\u200b\uff0c\u200b\u4f46\u662f\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u5374\u200b\u53d8\u5dee\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u7528\u200b\u4f5c\u8005\u200b\u7684\u8bdd\u200b\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b: \u201cOur deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\u201d\u3002\u200b\u539f\u59cb\u200b\u7684\u200bResNet\u200b\u5bf9\u4e8e\u200b\u8bad\u7ec3\u200b\u5377\u79ef\u200b\u795e\u7ecf\u200b\u7f51\u8def\u200b\u505a\u51fa\u200b\u4e86\u200b\u5f88\u5927\u200b\u7684\u200b\u8d21\u732e\u200b\uff0c\u200b\u4f46\u662f\u200b\u540c\u6837\u200b\u4e5f\u200b\u6709\u7740\u200b\u8bb8\u591a\u200b\u53ef\u4ee5\u200b\u6539\u8fdb\u200b\u7684\u200b\u5730\u65b9\u200b\u3002\u200b\u968f\u7740\u200b\u65f6\u4ee3\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u539f\u7248\u200b\u7684\u200bResNet\u200b\u5728\u200b\u4e00\u6b21\u200b\u53c8\u200b\u4e00\u6b21\u200b\u7684\u200b\u7814\u7a76\u200b\u4e2d\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e30\u5bcc\u200b\u548c\u200b\u5b8c\u5584\u200b\uff0c\u200b\u884d\u751f\u200b\u51fa\u200b\u4e86\u200b\u4e30\u5bcc\u200b\u7684\u200b\u6539\u8fdb\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200bResNeXt\u3002\u200b\u5b83\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u4ecb\u4e8e\u200b\u666e\u901a\u200b\u5377\u79ef\u200b\u6838\u200b\u6df1\u5ea6\u200b\u53ef\u200b\u5206\u79bb\u200b\u5377\u79ef\u200b\u7684\u200b\u8fd9\u79cd\u200b\u7b56\u7565\u200b\uff1a\u200b\u5206\u7ec4\u200b\u5377\u79ef\u200b\u3002\u200b\u901a\u8fc7\u200b\u63a7\u5236\u200b\u5206\u7ec4\u200b\u7684\u200b\u6570\u91cf\u200b\uff08\u200b\u57fa\u6570\u200b\uff09\u200b\u6765\u200b\u8fbe\u5230\u200b\u4e24\u79cd\u200b\u7b56\u7565\u200b\u7684\u200b\u5e73\u8861\u200b\u3002\u200b\u5206\u7ec4\u200b\u5377\u79ef\u200b\u7684\u200b\u601d\u60f3\u200b\u662f\u200b\u6e90\u81ea\u200bInception\uff0c\u200b\u4e0d\u540c\u4e8e\u200bInception\u200b\u7684\u200b\u9700\u8981\u200b\u4eba\u5de5\u200b\u8bbe\u8ba1\u200b\u6bcf\u4e2a\u200b\u5206\u652f\u200b\uff0cResNeXt\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5206\u652f\u200b\u7684\u200b\u62d3\u6251\u200b\u7ed3\u6784\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002\u200b\u6700\u540e\u200b\u518d\u200b\u7ed3\u5408\u200b\u6b8b\u5dee\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5f97\u5230\u200b\u7684\u200b\u4fbf\u662f\u200b\u6700\u7ec8\u200b\u7684\u200bResNeXt\u3002</p> <p>\u200b\u9664\u6b64\u4e4b\u5916\u200b\uff0cResNet\u200b\u8fd8\u6709\u200b\u5176\u5b83\u200b\u53d8\u4f53\u200b\u5982\u200bWider ResNet\uff0cDarkNet53\u200b\u7b49\u200b\u3002\u200b\u5b83\u4eec\u200b\u7684\u200b\u6539\u8fdb\u200b\u76f8\u5bf9\u200b\u8f83\u5927\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200bDarkNet53\uff0c\u200b\u5b83\u200b\u548c\u200bResNet\u200b\u5df2\u7ecf\u200b\u6709\u200b\u5f88\u5927\u200b\u4e0d\u540c\u200b\u4e86\u200b\uff0c\u200b\u53ea\u662f\u200b\u4f7f\u7528\u200b\u5230\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u4ece\u800c\u200b\u590d\u7528\u200b\u7279\u5f81\u200b\u800c\u5df2\u200b\u3002\u200b\u603b\u800c\u8a00\u4e4b\u200b\uff0cResNet\u200b\u4f5c\u4e3a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cc\u7a0b\u7891\u5f0f\u200b\u7684\u200b\u6a21\u578b\u200b\u4e00\u76f4\u200b\u5728\u200b\u5404\u4e2a\u9886\u57df\u200b\u88ab\u200b\u5e94\u7528\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5b66\u4e60\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u5f88\u200b\u6709\u200b\u5fc5\u8981\u200b\u3002\u200b\u6beb\u4e0d\u200b\u5938\u5f20\u200b\u7684\u200b\u8bf4\u200b\uff0cResNet\u200b\u503c\u5f97\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u5b66\u4e60\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u5b66\u5b50\u200b\u8ba4\u771f\u200b\u7814\u7a76\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/","title":"Swin Transformer\u200b\u89e3\u8bfb","text":"<p> [Swin-T] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. ICCV 2021. [paper] [code] \u200b\u89e3\u200b\u8bfb\u8005\u200b\uff1a\u200b\u6c88\u8c6a\u200b\uff0c\u200b\u590d\u65e6\u5927\u5b66\u200b\u535a\u58eb\u200b\uff0cDatawhale\u200b\u6210\u5458\u200b </p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_1","title":"\u524d\u8a00","text":"<p>\u300aSwin Transformer: Hierarchical Vision Transformer using Shifted Windows\u300b\u200b\u4f5c\u4e3a\u200b2021 ICCV\u200b\u6700\u4f73\u200b\u8bba\u6587\u200b\uff0c\u200b\u5c60\u699c\u200b\u4e86\u200b\u5404\u5927\u200bCV\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6027\u80fd\u200b\u4f18\u4e8e\u200bDeiT\u3001ViT\u200b\u548c\u200bEfficientNet\u200b\u7b49\u200b\u4e3b\u5e72\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u66ff\u4ee3\u200b\u7ecf\u5178\u200b\u7684\u200bCNN\u200b\u67b6\u6784\u200b\uff0c\u200b\u6210\u4e3a\u200b\u4e86\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u901a\u7528\u200b\u7684\u200bbackbone\u3002\u200b\u5b83\u200b\u57fa\u4e8e\u200b\u4e86\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u601d\u60f3\u200b\uff0c\u200b\u521b\u65b0\u6027\u200b\u7684\u200b\u5f15\u5165\u200b\u4e86\u200b\u6ed1\u52a8\u200b\u7a97\u53e3\u200b\u673a\u5236\u200b\uff0c\u200b\u8ba9\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5b66\u4e60\u200b\u5230\u200b\u8de8\u200b\u7a97\u53e3\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u3002\u200b\u540c\u65f6\u200b\u901a\u8fc7\u200b\u4e0b\u200b\u91c7\u6837\u200b\u5c42\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u8d85\u200b\u5206\u8fa8\u7387\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u8282\u7701\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u4ee5\u53ca\u200b\u80fd\u591f\u200b\u5173\u6ce8\u200b\u5168\u5c40\u200b\u548c\u200b\u5c40\u90e8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u800c\u200b\u672c\u6587\u200b\u5c06\u200b\u4ece\u200b\u539f\u7406\u200b\u548c\u200b\u4ee3\u7801\u200b\u89d2\u5ea6\u200b\u8be6\u7ec6\u200b\u89e3\u6790\u200bSwin Transformer\u200b\u7684\u200b\u67b6\u6784\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\u5c06\u200b Transformer \u200b\u4ece\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u9886\u57df\u200b\u5e94\u7528\u200b\u5230\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u4e3b\u8981\u200b\u6709\u200b\u4e24\u5927\u200b\u6311\u6218\u200b\uff1a</p> <ul> <li>\u200b\u89c6\u89c9\u200b\u5b9e\u4f53\u200b\u7684\u200b\u65b9\u5dee\u200b\u8f83\u5927\u200b\uff0c\u200b\u4f8b\u5982\u200b\u540c\u4e00\u4e2a\u200b\u7269\u4f53\u200b\uff0c\u200b\u62cd\u6444\u89d2\u5ea6\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4e8c\u8fdb\u5236\u200b\u540e\u200b\u7684\u200b\u56fe\u7247\u200b\u5c31\u200b\u4f1a\u200b\u5177\u6709\u200b\u5f88\u5927\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002\u200b\u540c\u65f6\u200b\u5728\u200b\u4e0d\u540c\u200b\u573a\u666f\u200b\u4e0b\u200b\u89c6\u89c9\u200b Transformer \u200b\u6027\u80fd\u200b\u672a\u5fc5\u200b\u5f88\u200b\u597d\u200b\u3002</li> <li>\u200b\u56fe\u50cf\u200b\u5206\u8fa8\u7387\u200b\u9ad8\u200b\uff0c\u200b\u50cf\u7d20\u70b9\u200b\u591a\u200b\uff0c\u200b\u5982\u679c\u200b\u91c7\u7528\u200bViT\u200b\u6a21\u578b\u200b\uff0c\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u4f1a\u200b\u4e0e\u200b\u50cf\u7d20\u200b\u7684\u200b\u5e73\u65b9\u200b\u6210\u6b63\u6bd4\u200b\u3002</li> </ul> <p>\u200b\u9488\u5bf9\u200b\u4e0a\u8ff0\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u57fa\u4e8e\u200b\u6ed1\u52a8\u200b\u7a97\u53e3\u200b\u673a\u5236\u200b\uff0c\u200b\u5177\u6709\u200b\u5c42\u7ea7\u200b\u8bbe\u8ba1\u200b\uff08\u200b\u4e0b\u200b\u91c7\u6837\u200b\u5c42\u200b\uff09 \u200b\u7684\u200b Swin Transformer\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u6ed1\u7a97\u200b\u64cd\u4f5c\u200b\u5305\u62ec\u200b\u4e0d\u200b\u91cd\u53e0\u200b\u7684\u200b local window\uff0c\u200b\u548c\u200b\u91cd\u53e0\u200b\u7684\u200b cross-window\u3002\u200b\u5c06\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u9650\u5236\u200b\u5728\u200b\u4e00\u4e2a\u200b\u7a97\u53e3\u200b\uff08window size\u200b\u56fa\u5b9a\u200b\uff09\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u65b9\u9762\u200b\u80fd\u200b\u5f15\u5165\u200b CNN \u200b\u5377\u79ef\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5c40\u90e8\u6027\u200b\uff0c\u200b\u53e6\u4e00\u65b9\u9762\u200b\u80fd\u200b\u5927\u5e45\u5ea6\u200b\u8282\u7701\u200b\u8ba1\u7b97\u200b\u91cf\u200b\uff0c\u200b\u5b83\u200b\u53ea\u200b\u548c\u200b\u7a97\u53e3\u200b\u6570\u91cf\u200b\u6210\u200b\u7ebf\u6027\u5173\u7cfb\u200b\u3002\u200b\u901a\u8fc7\u200b\u4e0b\u200b\u91c7\u6837\u200b\u7684\u200b\u5c42\u7ea7\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u80fd\u591f\u200b\u9010\u6e10\u200b\u589e\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\uff0c\u200b\u4ece\u800c\u200b\u4f7f\u5f97\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u4e5f\u200b\u80fd\u591f\u200b\u6ce8\u610f\u200b\u5230\u200b\u5168\u5c40\u200b\u7684\u200b\u7279\u5f81\u200b\u3002</p> <p></p> <p>\u200b\u5728\u200b\u8bba\u6587\u200b\u7684\u200b\u6700\u540e\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4e5f\u200b\u901a\u8fc7\u200b\u5927\u91cf\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u8bc1\u660e\u200bSwin Transformer\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u4ee5\u524d\u200b\u7684\u200bSOTA\u200b\u6a21\u578b\u200b\u5747\u200b\u6709\u200b\u63d0\u9ad8\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u5728\u200bADE20K\u200b\u6570\u636e\u200b\u548c\u200bCOCO\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\u3002\u200b\u4e5f\u200b\u8bc1\u660e\u200b\u4e86\u200bSwin Transformer\u200b\u53ef\u4ee5\u200b\u4f5c\u4e3a\u200b\u4e00\u79cd\u200b\u901a\u7528\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u88ab\u200b\u4f7f\u7528\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_2","title":"\u6a21\u578b\u200b\u7ed3\u6784","text":"<p>\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u91c7\u53d6\u200b\u5c42\u6b21\u5316\u200b\u7684\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u4e00\u5171\u200b\u5305\u542b\u200b 4 \u200b\u4e2a\u200b Stage\uff0c\u200b\u9664\u200b\u7b2c\u4e00\u4e2a\u200b stage \u200b\u5916\u200b\uff0c\u200b\u6bcf\u4e2a\u200b stage \u200b\u90fd\u200b\u4f1a\u200b\u5148\u200b\u901a\u8fc7\u200b Patch Merging \u200b\u5c42\u200b\u7f29\u5c0f\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e0b\u200b\u91c7\u6837\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u50cf\u200b CNN \u200b\u4e00\u6837\u200b\u9010\u5c42\u200b\u6269\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u83b7\u53d6\u200b\u5230\u200b\u5168\u5c40\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4ee5\u200b\u8bba\u6587\u200b\u7684\u200b\u89d2\u5ea6\u200b\uff1a</p> <ul> <li>\u200b\u5728\u200b\u8f93\u5165\u200b\u5f00\u59cb\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u505a\u200b\u4e86\u200b\u4e00\u4e2a\u200b<code>Patch Partition</code>\uff0c\u200b\u5373\u200bViT\u200b\u4e2d\u200b<code>Patch Embedding</code>\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u901a\u8fc7\u200b Patch_size \u200b\u4e3a\u200b4\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u5c06\u200b\u56fe\u7247\u200b\u5207\u6210\u200b\u4e00\u4e2a\u4e2a\u200b Patch \uff0c\u200b\u5e76\u200b\u5d4c\u5165\u200b\u5230\u200b<code>Embedding</code>\uff0c\u200b\u5c06\u200b embedding_size \u200b\u8f6c\u53d8\u200b\u4e3a\u200b48\uff08\u200b\u53ef\u4ee5\u200b\u5c06\u200b CV \u200b\u4e2d\u200b\u56fe\u7247\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u7406\u89e3\u200b\u4e3a\u200bNLP\u200b\u4e2d\u200btoken\u200b\u7684\u200b\u8bcd\u200b\u5d4c\u5165\u200b\u957f\u5ea6\u200b\uff09\u3002</li> <li>\u200b\u968f\u540e\u200b\u5728\u200b\u7b2c\u4e00\u4e2a\u200bStage\u200b\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200b<code>Linear Embedding</code>\u200b\u8c03\u6574\u200b\u901a\u9053\u200b\u6570\u4e3a\u200bC\u3002</li> <li>\u200b\u5728\u200b\u6bcf\u4e2a\u200b Stage \u200b\u91cc\u200b\uff08\u200b\u9664\u200b\u7b2c\u4e00\u4e2a\u200b Stage \uff09\uff0c\u200b\u5747\u200b\u7531\u200b<code>Patch Merging</code>\u200b\u548c\u200b\u591a\u4e2a\u200b<code>Swin Transformer Block</code>\u200b\u7ec4\u6210\u200b\u3002</li> <li>\u200b\u5176\u4e2d\u200b<code>Patch Merging</code>\u200b\u6a21\u5757\u200b\u4e3b\u8981\u200b\u5728\u200b\u6bcf\u4e2a\u200b Stage \u200b\u4e00\u200b\u5f00\u59cb\u200b\u964d\u4f4e\u200b\u56fe\u7247\u200b\u5206\u8fa8\u7387\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e0b\u200b\u91c7\u6837\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</li> <li>\u200b\u800c\u200b<code>Swin Transformer Block</code>\u200b\u5177\u4f53\u200b\u7ed3\u6784\u200b\u5982\u200b\u53f3\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u4e3b\u8981\u200b\u662f\u200b<code>LayerNorm</code>\uff0c<code>Window Attention</code> \uff0c<code>Shifted Window Attention</code>\u200b\u548c\u200b<code>MLP</code>\u200b\u7ec4\u6210\u200b \u3002</li> </ul> <p>\u200b\u4ece\u200b\u4ee3\u7801\u200b\u7684\u200b\u89d2\u5ea6\u200b\uff1a</p> <p>\u200b\u5728\u200b\u5fae\u8f6f\u200b\u4e9a\u6d32\u200b\u7814\u7a76\u9662\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u662f\u200b\u5c06\u200b<code>Patch Merging</code>\u200b\u4f5c\u4e3a\u200b\u6bcf\u4e2a\u200b Stage  \u200b\u6700\u540e\u200b\u7ed3\u675f\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8f93\u5165\u200b\u5148\u200b\u8fdb\u884c\u200b<code>Swin Transformer Block</code>\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u518d\u4e0b\u200b\u91c7\u6837\u200b\u3002\u200b\u800c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b Stage \u200b\u4e0d\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u4e0b\u200b\u91c7\u6837\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4e4b\u95f4\u200b\u901a\u8fc7\u200b\u540e\u7eed\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u4e0e\u200b target label \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u3002</p> <p></p> <pre><code># window_size=7 \n# input_batch_image.shape=[128,3,224,224]\nclass SwinTransformer(nn.Module):\n    def __init__(...):\n        super().__init__()\n        ...\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(...)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x) # Patch Partition\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # Batch_size Windows_num Channels\n        x = self.avgpool(x.transpose(1, 2))  # Batch_size Channels 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x) # self.head =&gt; Linear(in=Channels,out=Classification_num)\n        return x\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\u6709\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\u5904\u7406\u200b\u65b9\u6cd5\u200b\u4e0e\u200b ViT \u200b\u4e0d\u540c\u200b\uff1a</p> <ul> <li>ViT \u200b\u5728\u200b\u8f93\u5165\u200b\u4f1a\u200b\u7ed9\u200b embedding \u200b\u8fdb\u884c\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u3002\u200b\u800c\u200b Swin-T \u200b\u8fd9\u91cc\u200b\u5219\u200b\u662f\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u53ef\u9009\u9879\u200b\uff08<code>self.ape</code>\uff09\uff0cSwin-T \u200b\u662f\u200b\u5728\u200b\u8ba1\u7b97\u200b Attention \u200b\u7684\u200b\u65f6\u5019\u200b\u505a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff0c\u200b\u6211\u200b\u8ba4\u4e3a\u200b\u8fd9\u662f\u200b\u8fd9\u7bc7\u200b\u8bba\u6587\u200b\u8bbe\u8ba1\u200b\u6700\u200b\u5de7\u5999\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</li> <li>ViT \u200b\u4f1a\u200b\u5355\u72ec\u200b\u52a0\u4e0a\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u7684\u200b token\u3002\u200b\u800c\u200b Swin-T \u200b\u5219\u200b\u662f\u200b\u76f4\u63a5\u200b\u505a\u200b\u5e73\u5747\u200b\uff08avgpool\uff09\uff0c\u200b\u8f93\u51fa\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6709\u70b9\u200b\u7c7b\u4f3c\u200b CNN \u200b\u6700\u540e\u200b\u7684\u200b\u5168\u5c40\u200b\u5e73\u5747\u200b\u6c60\u5316\u5c42\u200b\u3002</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#patch-embedding","title":"Patch Embedding","text":"<p>\u200b\u5728\u200b\u8f93\u5165\u200b\u8fdb\u200b Block \u200b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u56fe\u7247\u200b\u5207\u6210\u200b\u4e00\u4e2a\u4e2a\u200b patch\uff0c\u200b\u7136\u540e\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\u3002</p> <p>\u200b\u5177\u4f53\u505a\u6cd5\u200b\u662f\u200b\u5bf9\u200b\u539f\u59cb\u200b\u56fe\u7247\u200b\u88c1\u6210\u200b\u4e00\u4e2a\u4e2a\u200b <code>window_size * window_size</code> \u200b\u7684\u200b\u7a97\u53e3\u200b\u5927\u5c0f\u200b\uff0c\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e8c\u7ef4\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b stride\uff0ckernel_size \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b window_size \u200b\u5927\u5c0f\u200b\u3002\u200b\u8bbe\u5b9a\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u6765\u200b\u786e\u5b9a\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002\u200b\u6700\u540e\u200b\u5c06\u200b H,W \u200b\u7ef4\u5ea6\u200b\u5c55\u5f00\u200b\uff0c\u200b\u5e76\u200b\u79fb\u52a8\u200b\u5230\u200b\u7b2c\u4e00\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u8bba\u6587\u200b\u4e2d\u200b\u8f93\u51fa\u200b\u901a\u9053\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b48\uff0c\u200b\u4f46\u662f\u200b\u4ee3\u7801\u200b\u4e2d\u4e3a\u200b96\uff0c\u200b\u4ee5\u4e0b\u200b\u6211\u4eec\u200b\u5747\u200b\u4ee5\u200b\u4ee3\u7801\u200b\u4e3a\u51c6\u200b\u3002</p> <p>Batch_size=128</p> <p></p> <pre><code>import torch\nimport torch.nn as nn\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size) # -&gt; (img_size, img_size)\n        patch_size = to_2tuple(patch_size) # -&gt; (patch_size, patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        # \u200b\u5047\u8bbe\u200b\u91c7\u53d6\u200b\u9ed8\u8ba4\u200b\u53c2\u6570\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200bembedding_size\u200b\u662f\u200b96\uff0c\u200b\u4f46\u662f\u200b\u4ee3\u7801\u200b\u4e2d\u4e3a\u200b48.\u200b\u6211\u4eec\u200b\u4ee5\u200b\u4ee3\u7801\u200b\u4e3a\u51c6\u200b\n        x = self.proj(x) # \u200b\u51fa\u6765\u200b\u7684\u200b\u662f\u200b(N, 96, 224/4, 224/4) \n        x = torch.flatten(x, 2) # \u200b\u628a\u200bHW\u200b\u7ef4\u200b\u5c55\u5f00\u200b\uff0c(N, 96, 56*56)\n        x = torch.transpose(x, 1, 2)  # \u200b\u628a\u200b\u901a\u9053\u200b\u7ef4\u200b\u653e\u5230\u200b\u6700\u540e\u200b (N, 56*56, 96)\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#patch-merging","title":"Patch Merging","text":"<p>\u200b\u8be5\u200b\u6a21\u5757\u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u5728\u200b\u6bcf\u4e2a\u200b Stage \u200b\u5f00\u59cb\u200b\u524d\u200b\u505a\u200b\u964d\u200b\u91c7\u6837\u200b\uff0c\u200b\u7528\u4e8e\u200b\u7f29\u5c0f\u200b\u5206\u8fa8\u7387\u200b\uff0c\u200b\u8c03\u6574\u200b\u901a\u9053\u200b\u6570\u200b\u8fdb\u800c\u200b\u5f62\u6210\u200b\u5c42\u6b21\u5316\u200b\u7684\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u80fd\u200b\u8282\u7701\u200b\u4e00\u5b9a\u200b\u8fd0\u7b97\u91cf\u200b\u3002</p> <p>\u200b\u5728\u200b CNN \u200b\u4e2d\u200b\uff0c\u200b\u5219\u200b\u662f\u200b\u5728\u200b\u6bcf\u4e2a\u200b Stage \u200b\u5f00\u59cb\u200b\u524d\u7528\u200b<code>stride=2</code>\u200b\u7684\u200b\u5377\u79ef\u200b/\u200b\u6c60\u5316\u5c42\u200b\u6765\u200b\u964d\u4f4e\u200b\u5206\u8fa8\u7387\u200b\u3002</p> <p>\u200b\u6bcf\u6b21\u200b\u964d\u200b\u91c7\u6837\u200b\u662f\u200b\u4e24\u500d\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u884c\u200b\u65b9\u5411\u200b\u548c\u200b\u5217\u200b\u65b9\u5411\u200b\u4e0a\u200b\uff0c\u200b\u95f4\u9694\u200b 2 \u200b\u9009\u53d6\u200b\u5143\u7d20\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\u62fc\u63a5\u200b\u5728\u200b\u4e00\u8d77\u200b\u4f5c\u4e3a\u200b\u4e00\u200b\u6574\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u6700\u540e\u200b\u5c55\u5f00\u200b\u3002\u200b\u6b64\u65f6\u200b\u901a\u9053\u200b\u7ef4\u5ea6\u200b\u4f1a\u200b\u53d8\u6210\u200b\u539f\u5148\u200b\u7684\u200b 4 \u200b\u500d\u200b\uff08\u200b\u56e0\u4e3a\u200b H,W \u200b\u5404\u200b\u7f29\u5c0f\u200b 2 \u200b\u500d\u200b\uff09\uff0c\u200b\u6b64\u65f6\u200b\u518d\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u518d\u200b\u8c03\u6574\u200b\u901a\u9053\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b\u539f\u6765\u200b\u7684\u200b\u4e24\u500d\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e2a\u200b\u793a\u610f\u56fe\u200b\uff08\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b N=1, H=W=8, C=1\uff0c\u200b\u4e0d\u200b\u5305\u542b\u200b\u6700\u540e\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u8c03\u6574\u200b\uff09</p> <p></p> <p></p> <pre><code>class PatchMerging(nn.Module):\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#window-partitionreverse","title":"Window Partition/Reverse","text":"<p><code>window partition</code>\u200b\u51fd\u6570\u200b\u662f\u200b\u7528\u4e8e\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u5212\u5206\u200b\u7a97\u53e3\u200b\uff0c\u200b\u6307\u5b9a\u200b\u7a97\u53e3\u200b\u5927\u5c0f\u200b\u3002\u200b\u5c06\u200b\u539f\u672c\u200b\u7684\u200b\u5f20\u91cf\u200b\u4ece\u200b <code>N H W C</code>, \u200b\u5212\u5206\u200b\u6210\u200b <code>num_windows*B, window_size, window_size, C</code>\uff0c\u200b\u5176\u4e2d\u200b <code>num_windows = H*W / window_size*window_size</code>\uff0c\u200b\u5373\u200b\u7a97\u53e3\u200b\u7684\u200b\u4e2a\u6570\u200b\u3002\u200b\u800c\u200b<code>window reverse</code>\u200b\u51fd\u6570\u200b\u5219\u200b\u662f\u200b\u5bf9\u5e94\u200b\u7684\u200b\u9006\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u51fd\u6570\u200b\u4f1a\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b<code>Window Attention</code>\u200b\u7528\u5230\u200b\u3002</p> <p></p> <pre><code>def window_partition(x, window_size):\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\ndef window_reverse(windows, window_size, H, W):\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#window-attention","title":"Window Attention","text":"<p>\u200b\u4f20\u7edf\u200b\u7684\u200b Transformer \u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b\u5168\u5c40\u200b\u6765\u200b\u8ba1\u7b97\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8ba1\u7b97\u200b\u590d\u6742\u5ea6\u200b\u5341\u5206\u200b\u9ad8\u200b\u3002\u200b\u800c\u200b Swin Transformer \u200b\u5219\u200b\u5c06\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\u8ba1\u7b97\u200b\u9650\u5236\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7a97\u53e3\u200b\u5185\u200b\uff0c\u200b\u8fdb\u800c\u200b\u51cf\u5c11\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u5148\u200b\u7b80\u5355\u200b\u770b\u200b\u4e0b\u200b\u516c\u5f0f\u200b</p> <p>$$ Attention(Q,K,V)=Softmax(\\frac{{QK}^T}{\\sqrt d}+B)V $$ \u200b\u4e3b\u8981\u200b\u533a\u522b\u200b\u662f\u200b\u5728\u200b\u539f\u59cb\u200b\u8ba1\u7b97\u200b Attention \u200b\u7684\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200b Q,K \u200b\u65f6\u200b\u52a0\u5165\u200b\u4e86\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u3002</p> <p></p> <pre><code>class WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads # nH\n        head_dim = dim // num_heads # \u200b\u6bcf\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\u5bf9\u5e94\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # \u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b\uff082*(Wh-1) * 2*(Ww-1), nH\uff09\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53d8\u91cf\u200b\uff0c\u200b\u7528\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n        # \u200b\u76f8\u5173\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b...\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_3","title":"\u76f8\u5173\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u76f4\u89c2\u200b\u7406\u89e3","text":"<p>Q,K,V.shape=[numWindwos*B, num_heads, window_size*window_size, head_dim]</p> <ul> <li>window_size*window_size \u200b\u5373\u200b NLP \u200b\u4e2d\u200b<code>token</code>\u200b\u7684\u200b\u4e2a\u6570\u200b</li> <li>\\(head\\_dim=\\frac{Embedding\\_dim}{num\\_heads}\\) \u200b\u5373\u200b NLP \u200b\u4e2d\u200b<code>token</code>\u200b\u7684\u200b\u8bcd\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b</li> </ul> <p>\\({QK}^T\\)\u200b\u8ba1\u7b97\u51fa\u6765\u200b\u7684\u200b<code>Attention</code>\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[numWindows*B, num_heads, Q_tokens, K_tokens]</code></p> <ul> <li>\u200b\u5176\u4e2d\u200bQ_tokens=K_tokens=window_size*window_size</li> </ul> <p>\u200b\u4ee5\u200b<code>window_size=2</code>\u200b\u4e3a\u4f8b\u200b\uff1a</p> <p></p> <p>\u200b\u56e0\u6b64\u200b\uff1a\\({QK}^T=\\left[\\begin{array}{cccc}a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \\\\ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44}\\end{array}\\right]\\)</p> <ul> <li>\u200b\u7b2c\u200b \\(i\\) \u200b\u884c\u200b\u8868\u793a\u200b\u7b2c\u200b \\(i\\) \u200b\u4e2a\u200b token \u200b\u7684\u200b<code>query</code>\u200b\u5bf9\u200b\u6240\u6709\u200btoken\u200b\u7684\u200b<code>key</code>\u200b\u7684\u200battention\u3002</li> <li>\u200b\u5bf9\u4e8e\u200b Attention \u200b\u5f20\u91cf\u200b\u6765\u8bf4\u200b\uff0c\u200b\u4ee5\u200b\u4e0d\u540c\u200b\u5143\u7d20\u200b\u4e3a\u200b\u539f\u70b9\u200b\uff0c\u200b\u5176\u4ed6\u200b\u5143\u7d20\u200b\u7684\u200b\u5750\u6807\u200b\u4e5f\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\uff0c</li> </ul> <p></p> <p>\u200b\u6240\u4ee5\u200b\\({QK}^T\u200b\u7684\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b=\\left[\\begin{array}{cccc}(0,0) &amp; (0,-1) &amp; (-1,0) &amp; (-1,-1) \\\\ (0,1) &amp; (0,0) &amp; (-1,1) &amp; (-1,0) \\\\ (1,0) &amp; (1,-1) &amp; (0,0) &amp; (0,-1) \\\\ (1,1) &amp; (1,0) &amp; (0,1) &amp; (0,0)\\end{array}\\right]\\)</p> <p>\u200b\u7531\u4e8e\u200b\u6700\u7ec8\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u4e00\u7ef4\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5750\u6807\u200b <code>x+y</code> \u200b\u4ee3\u66ff\u200b\u4e8c\u7ef4\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5750\u6807\u200b<code>(x,y)</code>\uff0c\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b (1,2) (2,1) \u200b\u4e24\u4e2a\u200b\u5750\u6807\u200b\u8f6c\u4e3a\u200b\u4e00\u7ef4\u200b\u65f6\u5747\u200b\u4e3a\u200b3\uff0c\u200b\u6211\u4eec\u200b\u4e4b\u540e\u200b\u5bf9\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u7ebf\u6027\u53d8\u6362\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u80fd\u200b\u901a\u8fc7\u200b\u4e00\u7ef4\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5750\u6807\u200b\u552f\u4e00\u200b\u6620\u5c04\u200b\u5230\u200b\u4e00\u4e2a\u4e8c\u7ef4\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5750\u6807\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee3\u7801\u200b\u90e8\u5206\u200b\u8fdb\u884c\u200b\u7406\u89e3\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_4","title":"\u76f8\u5173\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u4ee3\u7801\u200b\u8be6\u89e3","text":"<p>\u200b\u9996\u5148\u200b\u6211\u4eec\u200b\u5229\u7528\u200b<code>torch.arange</code>\u200b\u548c\u200b<code>torch.meshgrid</code>\u200b\u51fd\u6570\u200b\u751f\u6210\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5750\u6807\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ee5\u200b<code>windowsize=2</code>\u200b\u4e3a\u200b\u4f8b\u5b50\u200b</p> <pre><code>coords_h = torch.arange(self.window_size[0])\ncoords_w = torch.arange(self.window_size[1])\ncoords = torch.meshgrid([coords_h, coords_w]) # -&gt; 2*(wh, ww)\n\"\"\"\n  (tensor([[0, 0],\n           [1, 1]]), \n   tensor([[0, 1],\n           [0, 1]]))\n\"\"\"\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5806\u53e0\u200b\u8d77\u6765\u200b\uff0c\u200b\u5c55\u5f00\u200b\u4e3a\u200b\u4e00\u4e2a\u4e8c\u7ef4\u200b\u5411\u91cf\u200b</p> <pre><code>coords = torch.stack(coords)  # 2, Wh, Ww\ncoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n\"\"\"\ntensor([[0, 0, 1, 1],\n        [0, 1, 0, 1]])\n\"\"\"\n</code></pre> <p>\u200b\u5229\u7528\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b\uff0c\u200b\u5206\u522b\u200b\u5728\u200b\u7b2c\u4e00\u200b\u7ef4\u200b\uff0c\u200b\u7b2c\u4e8c\u200b\u7ef4\u200b\uff0c\u200b\u63d2\u5165\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u8fdb\u884c\u200b\u5e7f\u64ad\u200b\u76f8\u51cf\u200b\uff0c\u200b\u5f97\u5230\u200b <code>2, wh*ww, wh*ww</code>\u200b\u7684\u200b\u5f20\u91cf\u200b</p> <pre><code>relative_coords_first = coords_flatten[:, :, None]  # 2, wh*ww, 1\nrelative_coords_second = coords_flatten[:, None, :] # 2, 1, wh*ww\nrelative_coords = relative_coords_first - relative_coords_second # \u200b\u6700\u7ec8\u200b\u5f97\u5230\u200b 2, wh*ww, wh*ww \u200b\u5f62\u72b6\u200b\u7684\u200b\u5f20\u91cf\u200b\n</code></pre> <p></p> <p>\u200b\u56e0\u4e3a\u200b\u91c7\u53d6\u200b\u7684\u200b\u662f\u200b\u76f8\u51cf\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5f97\u5230\u200b\u7684\u200b\u7d22\u5f15\u200b\u662f\u4ece\u200b\u8d1f\u6570\u200b\u5f00\u59cb\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u52a0\u4e0a\u200b\u504f\u79fb\u91cf\u200b\uff0c\u200b\u8ba9\u200b\u5176\u200b\u4ece\u200b 0 \u200b\u5f00\u59cb\u200b\u3002</p> <pre><code>relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2\nrelative_coords[:, :, 0] += self.window_size[0] - 1\nrelative_coords[:, :, 1] += self.window_size[1] - 1\n</code></pre> <p>\u200b\u540e\u7eed\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5176\u200b\u5c55\u5f00\u200b\u6210\u200b\u4e00\u7ef4\u200b\u504f\u79fb\u91cf\u200b\u3002\u200b\u800c\u200b\u5bf9\u4e8e\u200b (1\uff0c2\uff09\u200b\u548c\u200b\uff082\uff0c1\uff09\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5750\u6807\u200b\u3002\u200b\u5728\u200b\u4e8c\u7ef4\u200b\u4e0a\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\uff0c\u200b\u4f46\u662f\u200b\u901a\u8fc7\u200b\u5c06\u200b x,y \u200b\u5750\u6807\u200b\u76f8\u52a0\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7ef4\u200b\u504f\u79fb\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4ed6\u200b\u7684\u200b\u504f\u79fb\u91cf\u200b\u662f\u200b\u76f8\u7b49\u200b\u7684\u200b\u3002</p> <p> </p> <p>\u200b\u6240\u4ee5\u200b\u6700\u540e\u200b\u6211\u4eec\u200b\u5bf9\u200b\u5176\u4e2d\u200b\u505a\u200b\u4e86\u200b\u4e2a\u200b\u4e58\u6cd5\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u533a\u5206\u200b</p> <pre><code>relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n</code></pre> <p></p> <p>\u200b\u7136\u540e\u200b\u518d\u200b\u6700\u540e\u200b\u4e00\u7ef4\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u6c42\u548c\u200b\uff0c\u200b\u5c55\u5f00\u200b\u6210\u200b\u4e00\u4e2a\u200b\u4e00\u7ef4\u200b\u5750\u6807\u200b\uff0c\u200b\u5e76\u200b\u6ce8\u518c\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u4e0d\u200b\u53c2\u4e0e\u200b\u7f51\u7edc\u200b\u5b66\u4e60\u200b\u7684\u200b\u53d8\u91cf\u200b</p> <pre><code>relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\nself.register_buffer(\"relative_position_index\", relative_position_index)\n</code></pre> <p>\u200b\u4e4b\u524d\u200b\u8ba1\u7b97\u200b\u7684\u200b\u662f\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b\uff0c\u200b\u5e76\u200b\u4e0d\u662f\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u504f\u7f6e\u200b\u53c2\u6570\u200b\u3002\u200b\u771f\u6b63\u200b\u4f7f\u7528\u200b\u5230\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\\(\\hat B\\)\u200b\u662f\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>relative position bias table</code>\u200b\u8868\u91cc\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8868\u200b\u7684\u200b\u957f\u5ea6\u200b\u662f\u200b\u7b49\u4e8e\u200b (2M\u22121) \u00d7 (2M\u22121) (\u200b\u5728\u200b\u4e8c\u7ef4\u200b\u4f4d\u7f6e\u200b\u5750\u6807\u200b\u4e2d\u200b\u7ebf\u6027\u200b\u53d8\u5316\u200b\u4e58\u4ee5\u200b2M-1\u200b\u5bfc\u81f4\u200b)\u200b\u7684\u200b\u3002\u200b\u90a3\u4e48\u200b\u4e0a\u8ff0\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u504f\u6267\u200b\u53c2\u6570\u200bB\u200b\u662f\u200b\u6839\u636e\u200b\u4e0a\u9762\u200b\u7684\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b\u8868\u200b\u6839\u636e\u200b\u67e5\u200b<code>relative position bias table</code>\u200b\u8868\u200b\u5f97\u5230\u200b\u7684\u200b\u3002</p> <p></p> <p>\u200b\u63a5\u7740\u200b\u6211\u4eec\u200b\u770b\u524d\u200b\u5411\u200b\u4ee3\u7801\u200b</p> <pre><code>def forward(self, x, mask=None):\n    \"\"\"\n    Args:\n        x: input features with shape of (num_windows*B, N, C)\n        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n    \"\"\"\n    B_, N, C = x.shape\n\n    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n    q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n    q = q * self.scale\n    attn = (q @ k.transpose(-2, -1))\n\n    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n    attn = attn + relative_position_bias.unsqueeze(0) # (1, num_heads, windowsize, windowsize)\n\n    if mask is not None: # \u200b\u4e0b\u6587\u200b\u4f1a\u200b\u5206\u6790\u200b\u5230\u200b\n        ...\n    else:\n        attn = self.softmax(attn)\n\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n</code></pre> <ul> <li>\u200b\u9996\u5148\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[numWindows*B, window_size * window_size, C]</code></li> <li>\u200b\u7136\u540e\u200b\u7ecf\u8fc7\u200b<code>self.qkv</code>\u200b\u8fd9\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u540e\u200b\uff0c\u200b\u8fdb\u884c\u200b reshape\uff0c\u200b\u8c03\u6574\u200b\u8f74\u200b\u7684\u200b\u987a\u5e8f\u200b\uff0c\u200b\u5f97\u5230\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[3, numWindows*B, num_heads, window_size*window_size, c//num_heads]</code>\uff0c\u200b\u5e76\u200b\u5206\u914d\u200b\u7ed9\u200b<code>q,k,v</code>\u3002</li> <li>\u200b\u6839\u636e\u200b\u516c\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5bf9\u200b<code>q</code>\u200b\u4e58\u4ee5\u200b\u4e00\u4e2a\u200b<code>scale</code>\u200b\u7f29\u653e\u200b\u7cfb\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u4e0e\u200b<code>k</code>\uff08\u200b\u4e3a\u4e86\u200b\u6ee1\u8db3\u200b\u77e9\u9635\u200b\u4e58\u200b\u8981\u6c42\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b\u6700\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u8c03\u6362\u200b\uff09\u200b\u8fdb\u884c\u200b\u76f8\u4e58\u200b\u3002\u200b\u5f97\u5230\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[numWindows*B, num_heads, window_size*window_size, window_size*window_size]</code>\u200b\u7684\u200b<code>attn</code>\u200b\u5f20\u91cf\u200b</li> <li>\u200b\u4e4b\u524d\u200b\u6211\u4eec\u200b\u9488\u5bf9\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(2*window_size-1*2*window_size-1, numHeads)</code>\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53d8\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u7528\u200b\u8ba1\u7b97\u200b\u5f97\u5230\u200b\u7684\u200b\u76f8\u5bf9\u200b\u7f16\u7801\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b<code>self.relative_position_index.vew(-1)</code>\u200b\u9009\u53d6\u200b\uff0c\u200b\u5f97\u5230\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(window_size*window_size, window_size*window_size, numHeads)</code>\u200b\u7684\u200b\u7f16\u7801\u200b\uff0c\u200b\u518d\u200bpermute(2,0,1)\u200b\u540e\u52a0\u200b\u5230\u200b<code>attn</code>\u200b\u5f20\u91cf\u200b\u4e0a\u200b</li> <li>\u200b\u6682\u200b\u4e0d\u200b\u8003\u8651\u200b mask \u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5269\u4e0b\u200b\u5c31\u662f\u200b\u8ddf\u200b transformer \u200b\u4e00\u6837\u200b\u7684\u200b softmax\uff0cdropout\uff0c\u200b\u4e0e\u200b<code>V</code>\u200b\u77e9\u9635\u200b\u4e58\u200b\uff0c\u200b\u518d\u200b\u7ecf\u8fc7\u200b\u4e00\u5c42\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u548c\u200b dropout</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#shifted-window-attention","title":"Shifted Window Attention","text":"<p>\u200b\u524d\u9762\u200b\u7684\u200b Window Attention \u200b\u662f\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7a97\u53e3\u200b\u4e0b\u200b\u8ba1\u7b97\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u66f4\u597d\u200b\u7684\u200b\u548c\u200b\u5176\u4ed6\u200b window \u200b\u8fdb\u884c\u200b\u4fe1\u606f\u200b\u4ea4\u4e92\u200b\uff0cSwin Transformer \u200b\u8fd8\u200b\u5f15\u5165\u200b\u4e86\u200b shifted window \u200b\u64cd\u4f5c\u200b\u3002</p> <p></p> <p>\u200b\u5de6\u8fb9\u200b\u662f\u200b\u6ca1\u6709\u200b\u91cd\u53e0\u200b\u7684\u200b Window Attention\uff0c\u200b\u800c\u200b\u53f3\u8fb9\u200b\u5219\u200b\u662f\u200b\u5c06\u200b\u7a97\u53e3\u200b\u8fdb\u884c\u200b\u79fb\u4f4d\u200b\u7684\u200b Shift Window Attention\u3002\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u79fb\u4f4d\u200b\u540e\u200b\u7684\u200b\u7a97\u53e3\u200b\u5305\u542b\u200b\u4e86\u200b\u539f\u672c\u200b\u76f8\u90bb\u200b\u7a97\u53e3\u200b\u7684\u200b\u5143\u7d20\u200b\u3002\u200b\u4f46\u200b\u8fd9\u200b\u4e5f\u200b\u5f15\u5165\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u95ee\u9898\u200b\uff0c\u200b\u5373\u200b window \u200b\u7684\u200b\u4e2a\u6570\u200b\u7ffb\u500d\u200b\u4e86\u200b\uff0c\u200b\u7531\u200b\u539f\u672c\u200b\u56db\u4e2a\u200b\u7a97\u53e3\u200b\u53d8\u6210\u200b\u4e86\u200b 9 \u200b\u4e2a\u200b\u7a97\u53e3\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u4ee3\u7801\u200b\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u662f\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u7279\u5f81\u200b\u56fe\u200b\u79fb\u4f4d\u200b\uff0c\u200b\u5e76\u200b\u7ed9\u200b Attention \u200b\u8bbe\u7f6e\u200b mask \u200b\u6765\u200b\u95f4\u63a5\u200b\u5b9e\u73b0\u200b\u7684\u200b\u3002\u200b\u80fd\u200b\u5728\u200b\u4fdd\u6301\u200b\u539f\u6709\u200b\u7684\u200b window \u200b\u4e2a\u6570\u200b\u4e0b\u200b\uff0c\u200b\u6700\u540e\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u7b49\u4ef7\u200b\u3002 </p> <p></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_5","title":"\u7279\u5f81\u200b\u56fe\u200b\u79fb\u4f4d\u200b\u64cd\u4f5c","text":"<p>\u200b\u4ee3\u7801\u200b\u91cc\u200b\u5bf9\u200b\u7279\u5f81\u200b\u56fe\u200b\u79fb\u4f4d\u200b\u662f\u200b\u901a\u8fc7\u200b<code>torch.roll</code>\u200b\u6765\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff0c\u200b\u4e0b\u9762\u200b\u662f\u200b\u793a\u610f\u56fe\u200b</p> <p></p> <p>\u200b\u5982\u679c\u200b\u9700\u8981\u200b<code>reverse cyclic shift</code>\u200b\u7684\u8bdd\u200b\u53ea\u200b\u9700\u200b\u628a\u200b\u53c2\u6570\u200b<code>shifts</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6b63\u200b\u6570\u503c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#attention-mask","title":"Attention Mask","text":"<p>\u200b\u8fd9\u662f\u200b Swin Transformer \u200b\u7684\u200b\u7cbe\u534e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b\u5408\u7406\u200b\u7684\u200b mask\uff0c\u200b\u8ba9\u200b<code>Shifted Window Attention</code>\u200b\u5728\u200b\u4e0e\u200b<code>Window Attention</code>\u200b\u76f8\u540c\u200b\u7684\u200b\u7a97\u53e3\u200b\u4e2a\u6570\u200b\u4e0b\u200b\uff0c\u200b\u8fbe\u5230\u200b\u7b49\u4ef7\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\u6211\u4eec\u200b\u5bf9\u200b Shift Window \u200b\u540e\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7a97\u53e3\u200b\u90fd\u200b\u7ed9\u200b\u4e0a\u200b index\uff0c\u200b\u5e76\u4e14\u200b\u505a\u200b\u4e00\u4e2a\u200b<code>roll</code>\u200b\u64cd\u4f5c\u200b\uff08window_size=2, shift_size=1\uff09</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5728\u200b\u8ba1\u7b97\u200b Attention \u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u8ba9\u200b\u5177\u6709\u200b\u76f8\u540c\u200b index QK \u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u5ffd\u7565\u200b\u4e0d\u540c\u200b index QK \u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u3002\u200b\u6700\u540e\u200b\u6b63\u786e\u200b\u7684\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b</p> <p></p> <p>\u200b\u800c\u200b\u8981\u200b\u60f3\u200b\u5728\u200b\u539f\u59cb\u200b\u56db\u4e2a\u200b\u7a97\u53e3\u200b\u4e0b\u200b\u5f97\u5230\u200b\u6b63\u786e\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5fc5\u987b\u200b\u7ed9\u200b Attention \u200b\u7684\u200b\u7ed3\u679c\u200b\u52a0\u5165\u200b\u4e00\u4e2a\u200b mask\uff08\u200b\u5982\u4e0a\u56fe\u200b\u6700\u200b\u53f3\u8fb9\u200b\u6240\u793a\u200b\uff09\u200b\u76f8\u5173\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>if self.shift_size &gt; 0:\n    # calculate attention mask for SW-MSA\n    H, W = self.input_resolution\n    img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n    h_slices = (slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None))\n    w_slices = (slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None))\n    cnt = 0\n    for h in h_slices:\n        for w in w_slices:\n            img_mask[:, h, w, :] = cnt\n            cnt += 1\n\n    mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n    mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\nelse:\n    attn_mask = None\n</code></pre> <p>\u200b\u4ee5\u4e0a\u200b\u56fe\u200b\u7684\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u6211\u4eec\u200b\u7528\u200b\u8fd9\u6bb5\u200b\u4ee3\u7801\u200b\u4f1a\u200b\u5f97\u5230\u200b\u8fd9\u6837\u200b\u7684\u200b\u4e00\u4e2a\u200b mask</p> <pre><code>tensor([[[[[   0.,    0.,    0.,    0.],\n           [   0.,    0.,    0.,    0.],\n           [   0.,    0.,    0.,    0.],\n           [   0.,    0.,    0.,    0.]]],\n\n\n         [[[   0., -100.,    0., -100.],\n           [-100.,    0., -100.,    0.],\n           [   0., -100.,    0., -100.],\n           [-100.,    0., -100.,    0.]]],\n\n\n         [[[   0.,    0., -100., -100.],\n           [   0.,    0., -100., -100.],\n           [-100., -100.,    0.,    0.],\n           [-100., -100.,    0.,    0.]]],\n\n\n         [[[   0., -100., -100., -100.],\n           [-100.,    0., -100., -100.],\n           [-100., -100.,    0., -100.],\n           [-100., -100., -100.,    0.]]]]])\n</code></pre> <p>\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b window attention \u200b\u6a21\u5757\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4ee3\u7801\u200b\u91cc\u200b\uff0c\u200b\u5305\u542b\u200b\u8fd9\u4e48\u200b\u4e00\u6bb5\u200b</p> <pre><code>if mask is not None:\n    nW = mask.shape[0] # \u200b\u4e00\u5f20\u200b\u56fe\u200b\u88ab\u200b\u5206\u4e3a\u200b\u591a\u5c11\u200b\u4e2a\u200bwindows eg:[4,49,49]\n    attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) # torch.Size([128, 4, 12, 49, 49]) torch.Size([1, 4, 1, 49, 49])\n    attn = attn.view(-1, self.num_heads, N, N)\n    attn = self.softmax(attn)\nelse:\n    attn = self.softmax(attn)\n</code></pre> <p>\u200b\u5c06\u200b mask \u200b\u52a0\u200b\u5230\u200b attention \u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\uff0c\u200b\u5e76\u200b\u8fdb\u884c\u200b softmax\u3002mask \u200b\u7684\u200b\u503c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b - 100\uff0csoftmax \u200b\u540e\u200b\u5c31\u200b\u4f1a\u200b\u5ffd\u7565\u200b\u6389\u200b\u5bf9\u5e94\u200b\u7684\u200b\u503c\u200b\u3002\u200b\u5173\u4e8e\u200bMask\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u5728\u200b\u5b98\u65b9\u200b\u4ee3\u7801\u200b\u5e93\u4e2d\u200b\u7684\u200bissue38\u200b\u4e5f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8ba8\u8bba\u200b:--&gt;The Question about the mask of window attention #38</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#w-msamsa","title":"W-MSA\u200b\u548c\u200bMSA\u200b\u7684\u200b\u590d\u6742\u5ea6\u200b\u5bf9\u6bd4","text":"<p>\u200b\u5728\u200b\u539f\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u63d0\u51fa\u200b\u7684\u200b\u57fa\u4e8e\u200b\u6ed1\u52a8\u200b\u7a97\u53e3\u200b\u64cd\u4f5c\u200b\u7684\u200b <code>W-MSA</code> \u200b\u80fd\u200b\u5927\u5e45\u5ea6\u200b\u51cf\u5c11\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u3002\u200b\u90a3\u4e48\u200b\u4e24\u8005\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u548c\u200b\u7b97\u6cd5\u200b\u590d\u6742\u5ea6\u200b\u5927\u6982\u200b\u662f\u200b\u5982\u4f55\u200b\u7684\u200b\u5462\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u4e00\u4e0b\u200b\u4e24\u4e2a\u200b\u516c\u5f0f\u200b\u8fdb\u884c\u200b\u5bf9\u6bd4\u200b\u3002 $$ \\begin{aligned} &amp;\\Omega(M S A)=4 h w C^{2}+2(h w)^{2} C \\ &amp;\\Omega(W-M S A)=4 h w C^{2}+2 M^{2} h w C \\end{aligned} $$</p> <ul> <li>h\uff1afeature map\u200b\u7684\u200b\u9ad8\u5ea6\u200b</li> <li>w\uff1afeature map\u200b\u7684\u200b\u5bbd\u5ea6\u200b</li> <li>C\uff1afeature map\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\uff08\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u79f0\u4e3a\u200bembedding size\u200b\u7684\u200b\u5927\u5c0f\u200b\uff09</li> <li>M\uff1awindow_size\u200b\u7684\u200b\u5927\u5c0f\u200b</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#msa","title":"MSA\u200b\u6a21\u5757\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf","text":"<p>\u200b\u9996\u5148\u200b\u5bf9\u4e8e\u200b<code>feature map</code>\u200b\u4e2d\u200b\u6bcf\u200b\u4e00\u4e2a\u200b<code>token</code>\uff08\u200b\u4e00\u5171\u200b\u6709\u200b \\(hw\\) \u200b\u4e2a\u200btoken\uff0c\u200b\u901a\u9053\u200b\u6570\u4e3a\u200bC\uff09\uff0c\u200b\u8bb0\u200b\u4f5c\u200b\\(X^{h w \\times C}\\)\uff0c\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u4e09\u6b21\u200b\u7ebf\u6027\u53d8\u6362\u200b \\(W_q,W_k,W_v\\) \uff0c\u200b\u4ea7\u751f\u200b\u5bf9\u5e94\u200b\u7684\u200b<code>q,k,v</code>\u200b\u5411\u91cf\u200b\uff0c\u200b\u8bb0\u200b\u4f5c\u200b \\(Q^{h w \\times C},K^{h w \\times C},V^{h w \\times C}\\) \uff08\u200b\u901a\u9053\u200b\u6570\u4e3a\u200bC\uff09\u3002 $$ X^{h w \\times C} \\cdot W_{q}^{C \\times C}=Q^{h w \\times C} \\ X^{h w \\times C} \\cdot W_{k}^{C \\times C}=K^{h w \\times C} \\ X^{h w \\times C} \\cdot W_{v}^{C \\times C}=V^{h w \\times C} \\ $$ \u200b\u6839\u636e\u200b\u77e9\u9635\u200b\u8fd0\u7b97\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u516c\u5f0f\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u8fd0\u7b97\u91cf\u200b\u4e3a\u200b \\(3hwC \\times C\\) \uff0c\u200b\u5373\u200b\u4e3a\u200b \\(3hwC^2\\) \u3002 $$ Q^{h w \\times C} \\cdot K^T=A^{h w \\times hw} \\ \\Lambda^{h w \\times h w}=Softmax(\\frac{A^{h w \\times hw}}{\\sqrt(d)}+B) \\ \\Lambda^{h w \\times h w} \\cdot V^{h w \\times C}=Y^{h w \\times C} $$ \u200b\u5ffd\u7565\u200b\u9664\u4ee5\u200b\\(\\sqrt d\\) \u200b\u4ee5\u53ca\u200bsoftmax\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\uff0c\u200b\u6839\u636e\u200b\u6839\u636e\u200b\u77e9\u9635\u200b\u8fd0\u7b97\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u516c\u5f0f\u200b\u53ef\u200b\u5f97\u200b \\(hwC \\times hw + hw^2 \\times C\\)  \uff0c\u200b\u5373\u200b\u4e3a\u200b \\(2(hw^2)C\\) \u3002 $$ Y^{h w \\times C} \\cdot W_O^{C \\times C}=O^{h w \\times C} $$ \u200b\u6700\u7ec8\u200b\u518d\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200bLinear\u200b\u5c42\u200b\u8f93\u51fa\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u4e3a\u200b \\(hwC^2\\) \u3002\u200b\u56e0\u6b64\u200b\u6574\u4f53\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u4e3a\u200b \\(4 h w C^{2}+2(h w)^{2} C\\)\u200b \u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#w-msa","title":"W-MSA\u200b\u6a21\u5757\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf","text":"<p>\u200b\u5bf9\u4e8e\u200bW-MSA\u200b\u6a21\u5757\u200b\uff0c\u200b\u9996\u5148\u200b\u4f1a\u200b\u5c06\u200b<code>feature map</code>\u200b\u6839\u636e\u200b<code>window_size</code>\u200b\u5206\u6210\u200b \\(\\frac{hw}{M^2}\\) \u200b\u7684\u200b\u7a97\u53e3\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7a97\u53e3\u200b\u7684\u200b\u5bbd\u200b\u9ad8\u5747\u200b\u4e3a\u200b\\(M\\)\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7a97\u53e3\u200b\u8fdb\u884c\u200bMSA\u200b\u7684\u200b\u8fd0\u7b97\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u4e0a\u9762\u200bMSA\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u516c\u5f0f\u200b\uff0c\u200b\u5c06\u200b \\(h=M\uff0cw=M\\) \u200b\u5e26\u5165\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u7a97\u53e3\u200b\u7684\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u4e3a\u200b \\(4 M^2 C^{2}+2M^{4} C\\)  \u3002</p> <p>\u200b\u53c8\u200b\u56e0\u4e3a\u200b\u6709\u200b \\(\\frac{hw}{M^2}\\) \u200b\u4e2a\u200b\u7a97\u53e3\u200b\uff0c\u200b\u5219\u200b\uff1a $$ \\frac{hw}{M^2} \\times\\left(4M^2 C^2+2M^{4} C\\right)=4 h w C^{2}+2 M^{2} h w C $$ \u200b\u5047\u8bbe\u200b<code>feature map</code>\u200b\u7684\u200b\\(h=w=112\uff0cM=7\uff0cC=128\\)\uff0c\u200b\u91c7\u7528\u200bW-MSA\u200b\u6a21\u5757\u200b\u4f1a\u200b\u6bd4\u200bMSA\u200b\u6a21\u5757\u200b\u8282\u7701\u200b\u7ea6\u200b40124743680 FLOPs\uff1a $$ 2(h w)^{2} C-2 M^{2} h w C=2 \\times 112^{4} \\times 128-2 \\times 7^{2} \\times 112^{2} \\times 128=40124743680 $$</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Swin-Transformer%E8%A7%A3%E8%AF%BB/#_6","title":"\u6574\u4f53\u200b\u6d41\u7a0b\u56fe","text":"<p>\u200b\u53c2\u8003\u200b\u535a\u5ba2\u200b\uff1a</p> <p>https://zhuanlan.zhihu.com/p/367111046</p> <p>\u200b\u8054\u7cfb\u65b9\u5f0f\u200b\uff1a</p> <ul> <li> <p>\u200b\u4e2a\u4eba\u200b\u77e5\u4e4e\u200b\uff1ahttps://www.zhihu.com/people/shenhao-63</p> </li> <li> <p>Github\uff1ahttps://github.com/shenhao-stu</p> </li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/","title":"Transformer \u200b\u89e3\u8bfb","text":"<p> [Transformer] Attention Is All You Need Ashish Vaswani\uff0cNoam Shazeer\uff0cNiki Parmar\uff0cJakob Uszkoreit\uff0cLlion Jones\uff0cAidan N. Gomez\uff0c\u0141ukasz Kaiser\uff0cIllia Polosukhin. NIPS 2017. [paper] [tensorflow] [pytorch] \u200b\u89e3\u200b\u8bfb\u8005\u200b\uff1a\u200b\u90b9\u96e8\u8861\u200b\uff0c\u200b\u5bf9\u5916\u7ecf\u6d4e\u8d38\u6613\u5927\u5b66\u200b\u672c\u79d1\u751f\u200b\uff0c\u200b\u725b\u5fd7\u5eb7\u200b\uff0c\u200b\u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66\u200b\u672c\u79d1\u751f\u200b </p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_1","title":"\u524d\u8a00","text":"<p>\u200bAttention \uff08\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u673a\u5236\u200b\u53ca\u5176\u200b\u53d8\u4f53\u200b\u5df2\u7ecf\u200b\u6210\u4e3a\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u7684\u200b\u4e00\u79cd\u200b\u57fa\u672c\u200b\u7f51\u7edc\u200b\u67b6\u6784\u200b\uff0c\u200b\u751a\u81f3\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u53d6\u4ee3\u200b\u4e86\u200b RNN \uff08\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u7684\u200b\u4e3b\u6d41\u200b\u5730\u4f4d\u200b\uff0c2018\u200b\u5e74\u200b\u53d6\u5f97\u200b\u4e86\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u591a\u9879\u200b\u4efb\u52a1\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u7684\u200b BERT \u200b\u6a21\u578b\u200b\u4fbf\u662f\u200b\u57fa\u4e8e\u200b attention \u200b\u673a\u5236\u200b\u642d\u5efa\u200b\u3002\u200b\u4f46\u200b\u81ea\u200b2014\u200b\u5e74\u200b attention \u200b\u673a\u5236\u200b\u88ab\u200b\u63d0\u51fa\u200b\u81f3\u200b2017\u200b\u5e74\u200b\uff0c\u200b\u867d\u7136\u200b\u88ab\u200b\u5e7f\u6cdb\u5e94\u7528\u200b\u5230\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u5404\u200b\u9886\u57df\u200b\u4e2d\u200b\uff0cattention \u200b\u673a\u5236\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u662f\u200b\u4f5c\u4e3a\u200b CNN\uff08\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u3001RNN \u200b\u7684\u200b\u7ec4\u4ef6\u200b\u5b58\u5728\u200b\uff0c\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u7684\u200b\u5404\u79cd\u200b\u4efb\u52a1\u200b\u4ecd\u7136\u200b\u662f\u200b\u4ee5\u200b RNN \u200b\u53ca\u5176\u200b\u53d8\u4f53\u200b\u5982\u200b LSTM\uff08\u200b\u957f\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\u9012\u5f52\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u4e3a\u4e3b\u200b\u3002</p> <p>\u200b\u200b\u4f46\u662f\u200b RNN\u3001LSTM \u200b\u867d\u7136\u200b\u5728\u200b\u5904\u7406\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u7684\u200b\u5e8f\u5217\u200b\u5efa\u6a21\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u5f97\u5929\u72ec\u539a\u200b\uff0c\u200b\u5374\u200b\u4e5f\u200b\u6709\u7740\u200b\u96be\u4ee5\u200b\u5ffd\u89c6\u200b\u7684\u200b\u7f3a\u9677\u200b\uff1a</p> <p>\u200b1. RNN \u200b\u4e3a\u200b\u5355\u5411\u200b\u4f9d\u5e8f\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5e8f\u5217\u200b\u9700\u8981\u200b\u4f9d\u6b21\u200b\u8f93\u5165\u200b\u3001\u200b\u4e32\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u9650\u5236\u200b\u4e86\u200b\u8ba1\u7b97\u673a\u200b\u7684\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u80fd\u529b\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u65f6\u95f4\u200b\u6210\u672c\u200b\u8fc7\u9ad8\u200b\u3002</p> <p>\u200b2. RNN \u200b\u96be\u4ee5\u200b\u6355\u6349\u200b\u957f\u671f\u200b\u4f9d\u8d56\u200b\u95ee\u9898\u200b\uff0c\u200b\u5373\u200b\u5bf9\u4e8e\u200b\u6781\u957f\u200b\u5e8f\u5217\u200b\uff0cRNN \u200b\u96be\u4ee5\u200b\u6355\u6349\u200b\u8fdc\u8ddd\u79bb\u200b\u8f93\u5165\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002\u200b\u867d\u7136\u200b LSTM \u200b\u901a\u8fc7\u200b\u95e8\u200b\u673a\u5236\u200b\u5bf9\u6b64\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u4f18\u5316\u200b\uff0c\u200b\u4f46\u200b RNN \u200b\u5bf9\u200b\u957f\u671f\u200b\u4f9d\u8d56\u200b\u95ee\u9898\u200b\u7684\u200b\u6355\u6349\u200b\u80fd\u529b\u200b\u4f9d\u65e7\u200b\u662f\u200b\u4e0d\u5982\u4eba\u610f\u200b\u7684\u200b\u3002</p> <p>\u200b\u200b\u9488\u5bf9\u200b\u4e0a\u8ff0\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff0c2017\u200b\u5e74\u200b\uff0cVaswani \u200b\u7b49\u200b\u4eba\u200b\u53d1\u8868\u200b\u4e86\u200b\u8bba\u6587\u200b\u300aAttention Is All You Need\u300b\uff0c\u200b\u629b\u5f03\u200b\u4e86\u200b\u4f20\u7edf\u200b\u7684\u200b CNN\u3001RNN \u200b\u67b6\u6784\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u5168\u65b0\u200b\u7684\u200b\u5b8c\u5168\u200b\u57fa\u4e8e\u200b attention \u200b\u673a\u5236\u200b\u7684\u200b\u6a21\u578b\u200b\u2014\u2014Transformer\uff0c\u200b\u89e3\u51b3\u200b\u4e86\u200b\u4e0a\u8ff0\u200b\u95ee\u9898\u200b\uff0c\u200b\u5728\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u65f6\u95f4\u200b\u6210\u672c\u200b\u4e0b\u200b\u53d6\u5f97\u200b\u4e86\u200b\u591a\u4e2a\u200b\u4efb\u52a1\u200b\u7684\u200b the-state-of-art \u200b\u6548\u679c\u200b\uff0c\u200b\u5e76\u200b\u4e3a\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u65b0\u200b\u7684\u200b\u601d\u8def\u200b\u3002\u200b\u81ea\u6b64\u200b\uff0cattention \u200b\u673a\u5236\u200b\u8fdb\u5165\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u7684\u200b\u4e3b\u6d41\u200b\u67b6\u6784\u200b\uff0c\u200b\u4f17\u591a\u200b\u6027\u80fd\u200b\u5353\u8d8a\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u90fd\u200b\u57fa\u4e8e\u200b Transformer \u200b\u67b6\u6784\u200b\u63d0\u51fa\u200b\uff0c\u200b\u4f8b\u5982\u200b BERT\u3001OpenAI GPT \u200b\u7b49\u200b\u3002</p> <p>\u200b\u200b\u672c\u6587\u200b\u5c06\u200b\u4ece\u200b\u6a21\u578b\u200b\u539f\u7406\u200b\u53ca\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e0a\u200b\u8bb2\u89e3\u200b\u8be5\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u7740\u91cd\u200b\u4ecb\u7ecd\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u3002\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u7531\u4e8e\u200b Transformer \u200b\u6e90\u4ee3\u7801\u200b\u4f7f\u7528\u200b TensorFlow \u200b\u642d\u5efa\u200b\uff0c\u200b\u6b64\u5904\u200b\u9009\u62e9\u200b\u4e86\u200b\u54c8\u4f5b\u5927\u5b66\u200b harvardnlp \u200b\u56e2\u961f\u200b\u57fa\u4e8e\u200b Pytorch \u200b\u6846\u67b6\u200b\u5f00\u53d1\u200b\u7684\u200b Annotated Transformer \u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\uff0c\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u4e86\u89e3\u200b Transformer \u200b\u7684\u200b\u5b9e\u73b0\u200b\u7ec6\u8282\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_2","title":"\u6574\u4f53\u200b\u67b6\u6784","text":"<p>\u200b   Transformer \u200b\u662f\u200b\u9488\u5bf9\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u7684\u200b Seq2Seq\uff08\u200b\u5e8f\u5217\u200b\u5230\u200b\u5e8f\u5217\u200b\uff09\u200b\u4efb\u52a1\u200b\u5f00\u53d1\u200b\u7684\u200b\uff0c\u200b\u6574\u4f53\u200b\u4e0a\u200b\u6cbf\u7528\u200b\u4e86\u200b Seq2Seq \u200b\u6a21\u578b\u200b\u7684\u200b Encoder-Decoder\uff08\u200b\u7f16\u7801\u5668\u200b-\u200b\u89e3\u7801\u5668\u200b\uff09\u200b\u7ed3\u6784\u200b\uff0c\u200b\u6574\u4f53\u200b\u67b6\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b   Transformer \u200b\u7531\u200b\u4e00\u4e2a\u200b Encoder\uff0c\u200b\u4e00\u4e2a\u200b Decoder \u200b\u5916\u52a0\u200b\u4e00\u4e2a\u200b Softmax \u200b\u5206\u7c7b\u5668\u200b\u4e0e\u200b\u4e24\u5c42\u200b\u7f16\u7801\u200b\u5c42\u200b\u6784\u6210\u200b\u3002\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u5de6\u4fa7\u200b\u65b9\u6846\u200b\u4e3a\u200b Encoder\uff0c\u200b\u53f3\u4fa7\u200b\u65b9\u6846\u200b\u4e3a\u200b Decoder\u3002</p> <p>\u200b   \u200b\u7531\u4e8e\u200b\u662f\u200b\u4e00\u4e2a\u200b Seq2Seq \u200b\u4efb\u52a1\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0cTransformer \u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bed\u6599\u200b\u4e3a\u200b\u82e5\u5e72\u4e2a\u200b\u53e5\u200b\u5bf9\u200b\uff0c\u200b\u5177\u4f53\u200b\u5b50\u200b\u4efb\u52a1\u200b\u53ef\u4ee5\u200b\u662f\u200b\u673a\u5668\u7ffb\u8bd1\u200b\u3001\u200b\u9605\u8bfb\u200b\u7406\u89e3\u200b\u3001\u200b\u673a\u5668\u200b\u5bf9\u8bdd\u200b\u7b49\u200b\u3002\u200b\u5728\u200b\u539f\u200b\u8bba\u6587\u200b\u4e2d\u662f\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u82f1\u8bed\u200b\u4e0e\u200b\u5fb7\u8bed\u200b\u7684\u200b\u673a\u5668\u7ffb\u8bd1\u200b\u4efb\u52a1\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u53e5\u200b\u5bf9\u200b\u4f1a\u200b\u88ab\u200b\u5212\u5206\u200b\u4e3a\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u548c\u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\uff0c\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u5c06\u200b\u4ece\u200b\u5de6\u4fa7\u200b\u901a\u8fc7\u200b\u7f16\u7801\u200b\u5c42\u200b\u8fdb\u5165\u200b Encoder\uff0c\u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u5c06\u200b\u4ece\u200b\u53f3\u4fa7\u200b\u901a\u8fc7\u200b\u7f16\u7801\u200b\u5c42\u200b\u8fdb\u5165\u200b Decoder\u3002Encoder \u200b\u7684\u200b\u4e3b\u8981\u200b\u4efb\u52a1\u200b\u662f\u200b\u5bf9\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u8fdb\u884c\u200b\u7f16\u7801\u200b\u518d\u200b\u8f93\u51fa\u200b\u7ed9\u200b Decoder\uff0cDecoder \u200b\u518d\u200b\u6839\u636e\u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u7684\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u4e0e\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u518d\u200b\u7ecf\u8fc7\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\u548c\u200b Softmax \u200b\u5206\u7c7b\u5668\u200b\u5373\u53ef\u200b\u8f93\u51fa\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u679c\u200b\u6982\u7387\u200b\uff0c\u200b\u6574\u4f53\u200b\u903b\u8f91\u200b\u5982\u4e0b\u200b\u56fe\u200b\uff1a</p> <p>\u200b   \u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u5b9e\u73b0\u200b\u4e3a\u200b\u4e00\u4e2a\u200b Encoder-Decoder\u200b\u67b6\u6784\u200b\uff1a</p> <pre><code>class EncoderDecoder(nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many\n    other models.\n    \"\"\"\n\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        # \u200b\u7f16\u7801\u5668\u200b\n        self.decoder = decoder\n        # \u200b\u89e3\u7801\u5668\u200b\n        self.src_embed = src_embed\n        # \u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u7684\u200b\u7f16\u7801\u200b\u51fd\u6570\u200b\n        self.tgt_embed = tgt_embed\n        # \u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u7684\u200b\u7f16\u7801\u200b\u51fd\u6570\u200b\n        self.generator = generator\n        # \u200b\u7ebf\u6027\u200b\u5c42\u200b+softmax\u200b\u5206\u7c7b\u200b\u5c42\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5206\u7c7b\u200b\u6982\u7387\u200b\uff0c\u200b\u5373\u200b\u67b6\u6784\u56fe\u200b\u4e2d\u200b\u7684\u200b\u6700\u9ad8\u5c42\u200b\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        # \u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\u51fd\u6570\u200b\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        # src \u200b\u4e3a\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\n        # src_mask \u200b\u4e3a\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u7684\u200b\u906e\u853d\u200b\u7b26\u53f7\u200b\n        return self.encoder(self.src_embed(src), src_mask)\n        # \u200b\u5148\u200b\u7f16\u7801\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\uff0c\u200b\u518d\u200b\u548c\u200b\u906e\u853d\u200b\u7b26\u53f7\u200b\u4e00\u8d77\u200b\u4f20\u5165\u200b\u7f16\u7801\u5668\u200b\u4e2d\u200b\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n        # \u200b\u89e3\u7801\u200b\u51fd\u6570\u200b\u7c7b\u4f3c\u200b\u7f16\u7801\u200b\u51fd\u6570\u200b,memory\u200b\u4e3a\u200b\u8bb0\u5fc6\u200b\uff0c\u200b\u5176\u5b9e\u200b\u5c31\u662f\u200b\u7f16\u7801\u5668\u200b\u7684\u200b\u8f93\u51fa\u200b\n</code></pre> <p>\u200b   \u200b\u5728\u200b\u8be5\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u51fd\u6570\u200b\u5373\u5148\u200b\u901a\u8fc7\u200b Encoder \u200b\u7f16\u7801\u200b\uff0c\u200b\u518d\u200b\u8f93\u51fa\u200b\u7ed9\u200b Decoder \u200b\u89e3\u7801\u200b\u3002\u200b\u6700\u7ec8\u200b\u7684\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\u5e76\u672a\u200b\u5728\u200b\u8be5\u7c7b\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b   \u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u9010\u4e2a\u200b\u4ecb\u7ecd\u200b Transformer \u200b\u7684\u200b\u5b9e\u73b0\u200b\u7ec6\u8282\u200b\u548c\u200b\u539f\u7406\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#attention","title":"Attention","text":"<p>\u200bAttention \u200b\u673a\u5236\u200b\u662f\u200b Transformer \u200b\u7684\u200b\u6838\u5fc3\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u8981\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200bAttention \u200b\u673a\u5236\u200b\u7684\u200b\u601d\u60f3\u200b\u4e0e\u200b\u64cd\u4f5c\u200b\u9700\u8981\u200b\u8f83\u200b\u591a\u200b\u7684\u200b\u7bc7\u5e45\u200b\u4e0e\u200b\u7b14\u58a8\u200b\uff0c\u200b\u6b64\u5904\u200b\u6211\u4eec\u200b\u4ec5\u200b\u7b80\u8981\u200b\u6982\u8ff0\u200b attention \u200b\u673a\u5236\u200b\u7684\u200b\u601d\u60f3\u200b\u548c\u200b\u5927\u81f4\u200b\u8ba1\u7b97\u65b9\u6cd5\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u8bf7\u200b\u5927\u5bb6\u200b\u5177\u4f53\u200b\u67e5\u9605\u200b\u76f8\u5173\u200b\u8d44\u6599\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff1aUnderstanding Attention In Deep Learning (NLP)\u3001Attention? Attention!\u200b\u7b49\u200b\u3002\u200b\u5728\u4e0b\u6587\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u4f55\u4e3a\u200bAttention\u3001self-attention \u200b\u548c\u200b Multi-Head attention \u200b\u4e09\u4e2a\u200b\u65b9\u9762\u200b\u9010\u6b65\u200b\u4ecb\u7ecd\u200b Transformer \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200bAttention \u200b\u673a\u5236\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#attention_1","title":"\u4f55\u4e3a\u200b Attention","text":"<p>\u200bAttention \u200b\u673a\u5236\u200b\u6700\u5148\u200b\u6e90\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\uff0c\u200b\u5176\u200b\u6838\u5fc3\u601d\u60f3\u200b\u4e3a\u200b\u5f53\u200b\u6211\u4eec\u200b\u5173\u6ce8\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f80\u5f80\u200b\u65e0\u9700\u200b\u770b\u6e05\u695a\u200b\u5168\u90e8\u5185\u5bb9\u200b\u800c\u4ec5\u200b\u5c06\u200b\u6ce8\u610f\u529b\u200b\u96c6\u4e2d\u200b\u5728\u200b\u91cd\u70b9\u200b\u90e8\u5206\u200b\u5373\u53ef\u200b\u3002\u200b\u800c\u200b\u5728\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u9886\u57df\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f80\u5f80\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u91cd\u70b9\u200b\u6ce8\u610f\u529b\u200b\u96c6\u4e2d\u200b\u5728\u200b\u4e00\u4e2a\u200b\u6216\u200b\u51e0\u4e2a\u200b token\uff0c\u200b\u4ece\u800c\u200b\u53d6\u5f97\u200b\u66f4\u200b\u9ad8\u6548\u200b\u9ad8\u8d28\u200b\u7684\u200b\u8ba1\u7b97\u200b\u6548\u679c\u200b\u3002</p> <p>\u200bAttention \u200b\u673a\u5236\u200b\u7684\u200b\u7279\u70b9\u200b\u662f\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u200b Query (\u200b\u67e5\u8be2\u200b\u503c\u200b)\u200b\u4e0e\u200bKey(\u200b\u952e\u503c\u200b)\u200b\u7684\u200b\u76f8\u5173\u6027\u200b\u4e3a\u200b\u771f\u503c\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\uff0c\u200b\u4ece\u800c\u200b\u62df\u5408\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u8bcd\u540c\u200b\u5176\u4ed6\u200b\u8bcd\u200b\u7684\u200b\u76f8\u5173\u200b\u5173\u7cfb\u200b\u3002\u200b\u5176\u200b\u5927\u81f4\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u901a\u8fc7\u200b\u8f93\u5165\u200b\u4e0e\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\uff0c\u200b\u5f97\u5230\u200b\u67e5\u8be2\u200b\u503c\u200b\\(q\\)\uff0c\u200b\u952e\u503c\u200b\\(k\\)\uff0c\u200b\u771f\u503c\u200b\\(v\\)\u3002\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\uff0c\\(q\\) \u200b\u662f\u200b\u8ba1\u7b97\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u53e5\u5b50\u200b\uff08\u200b\u6216\u200b\u8bcd\u7ec4\u200b\uff09\uff0c\\(v\\) \u200b\u4e3a\u200b\u5f85\u200b\u8ba1\u7b97\u200b\u53e5\u5b50\u200b\uff0c\\(k\\) \u200b\u4e3a\u200b\u5f85\u200b\u8ba1\u7b97\u200b\u53e5\u5b50\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u8bcd\u200b\uff08\u200b\u5373\u200b \\(v\\) \u200b\u7684\u200b\u6bcf\u4e2a\u200b\u8bcd\u200b\uff09\u200b\u7684\u200b\u5bf9\u5e94\u200b\u952e\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\\(v\\) \u200b\u4e0e\u200b \\(k\\) \u200b\u7684\u200b\u77e9\u9635\u200b\u7ef4\u5ea6\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\uff0c\\(q\\)\u200b\u7684\u200b\u77e9\u9635\u200b\u7ef4\u5ea6\u200b\u5219\u200b\u53ef\u4ee5\u200b\u4e0d\u540c\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u6ee1\u8db3\u200b \\(q\\) \u200b\u80fd\u200b\u548c\u200b\\(k^T\\)\u200b\u6ee1\u8db3\u200b\u77e9\u9635\u200b\u76f8\u4e58\u200b\u7684\u200b\u6761\u4ef6\u200b\u5373\u53ef\u200b\u3002</li> <li>\u200b\u5bf9\u200b \\(q\\) \u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b \\(q_i\\) ,\u200b\u5bf9\u200b \\(q_i\\) \u200b\u4e0e\u200b \\(k\\) \u200b\u505a\u70b9\u79ef\u200b\u5e76\u200b\u8fdb\u884c\u200b softmax\uff0c\u200b\u5f97\u5230\u200b\u4e00\u7ec4\u200b\u5411\u91cf\u200b\uff0c\u200b\u8be5\u200b\u5411\u91cf\u200b\u63ed\u793a\u200b\u4e86\u200b \\(q_i\\) \u200b\u5bf9\u200b\u6574\u4e2a\u200b\u53e5\u5b50\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5927\u5c0f\u200b\u3002</li> <li>\u200b\u4ee5\u4e0a\u200b\u4e00\u6b65\u200b\u8f93\u51fa\u200b\u5411\u91cf\u200b\u4f5c\u4e3a\u200b\u6743\u91cd\u200b\uff0c\u200b\u5bf9\u200b \\(v\\) \u200b\u8fdb\u884c\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\uff0c\u200b\u5c06\u200b \\(q\\) \u200b\u7684\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u5f97\u5230\u200b\u7684\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\u7ed3\u679c\u200b\u62fc\u63a5\u200b\u5f97\u5230\u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u3002</li> </ol> <p>\u200b\u200b\u5176\u4e2d\u200b\uff0cq\uff0ck\uff0cv \u200b\u5206\u522b\u200b\u662f\u200b\u7531\u200b\u8f93\u5165\u200b\u4e0e\u200b\u4e09\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u505a\u79ef\u200b\u5f97\u5230\u200b\u7684\u200b\uff1a</p> <p>\u200b\u200b\u5728\u200b\u5b9e\u9645\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4e3a\u200b\u63d0\u9ad8\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u901f\u5ea6\u200b\uff0c\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b \\(q\\)\u3001\\(k\\)\u3001\\(v\\) \u200b\u62fc\u63a5\u200b\u800c\u6210\u200b\u7684\u200b\u77e9\u9635\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u8ba1\u7b97\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u200b\u5177\u4f53\u200b\u5230\u200b Transformer \u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u8ba1\u7b97\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a $$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ \u200b\u200b\u5176\u4e2d\u200b\uff0c\\(d_k\\) \u200b\u4e3a\u952e\u200b\u5411\u91cf\u200b\\(k\\)\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u9664\u4ee5\u200b\u6839\u53f7\u200b\\(d_k\\)\u200b\u4e3b\u8981\u200b\u4f5c\u7528\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u4e00\u4e2a\u200b\u7a33\u5b9a\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u8ba1\u7b97\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\u56fe\u200b\uff1a</p> <p>\u200b   Attention \u200b\u673a\u5236\u200b\u7684\u200b\u57fa\u672c\u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>def attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1) # \u200b\u83b7\u53d6\u200b\u952e\u200b\u5411\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u952e\u200b\u5411\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u548c\u200b\u503c\u200b\u5411\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u76f8\u540c\u200b\uff0c\u200b\u5373\u200b\u7ecf\u8fc7\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    # \u200b\u8ba1\u7b97\u200bQ\u200b\u4e0e\u200bK\u200b\u7684\u200b\u5185\u79ef\u200b\u5e76\u200b\u9664\u4ee5\u200b\u6839\u53f7\u200bdk\n    # transpose \u200b\u5373\u200b\u5bf9\u200b K \u200b\u8fdb\u884c\u200b\u4e86\u200b\u8f6c\u7f6e\u200b\uff0c\u200b\u4f7f\u7528\u200b-2\u200b\u548c\u200b-1\u200b\u662f\u56e0\u4e3a\u200b\u5728\u200b\u540e\u7eed\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u4e2d\u200b\u8f93\u5165\u200b\u5411\u91cf\u200b\u4f1a\u200b\u8fbe\u5230\u200b\u56db\u7ef4\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u5373\u53ef\u200b\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n        # masker_fill\u200b\u4e3a\u200b\u906e\u853d\u200b\uff0c\u200b\u5373\u200b\u57fa\u4e8e\u200b\u4e00\u4e2a\u200b\u5e03\u5c14\u503c\u200b\u7684\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u5bf9\u200b\u77e9\u9635\u200b\u8fdb\u884c\u200b\u906e\u853d\u200b\n        # \u200b\u6b64\u5904\u200b\u540c\u200b\u4e0a\u9762\u200b\u7684\u200bsubsequent_mask\u200b\u51fd\u6570\u200b\u7ed3\u5408\u200b\uff0c\u200b\u6b64\u5904\u200b\u7684\u200bmask\u200b\u5373\u200b\u4e3a\u200b\u8be5\u200b\u51fd\u6570\u200b\u7684\u200b\u8f93\u51fa\u200b\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n        # \u200b\u91c7\u6837\u200b\n    # \u200b\u6ce8\u610f\u200b\u6700\u540e\u200b\u8ba1\u7b97\u200b\u52a0\u6743\u200b\u503c\u200b\u662f\u200b\u4e0d\u200b\u9700\u8981\u200b\u8f6c\u7f6e\u200b\u7684\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u8ba1\u7b97\u200b\u8fd4\u56de\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b length*length\uff0c\u200b\u503c\u200b\u53c2\u6570\u200b\u4e3a\u200blength*dk\uff0c\u200b\u76f4\u63a5\u200b\u5185\u79ef\u200b\u5373\u53ef\u200b\n    return torch.matmul(p_attn, value), p_attn\n    # \u200b\u6839\u636e\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u5bf9\u200bvalue\u200b\u8fdb\u884c\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#self-attention","title":"Self-Attention","text":"<p>\u200b\u200b\u4ece\u200b\u4e0a\u200b\u5bf9\u200b Attention \u200b\u673a\u5236\u200b\u539f\u7406\u200b\u7684\u200b\u53d9\u8ff0\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0cAttention \u200b\u673a\u5236\u200b\u7684\u200b\u672c\u8d28\u200b\u662f\u200b\u5bf9\u200b\u4e24\u6bb5\u200b\u5e8f\u5217\u200b\u7684\u200b\u5143\u7d20\u200b\u4f9d\u6b21\u200b\u8fdb\u884c\u200b\u76f8\u4f3c\u200b\u5ea6\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5bfb\u627e\u200b\u51fa\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u5bf9\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u7684\u200b\u76f8\u5173\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u57fa\u4e8e\u200b\u76f8\u5173\u5ea6\u200b\u8fdb\u884c\u200b\u52a0\u6743\u200b\uff0c\u200b\u5373\u200b\u5206\u914d\u200b\u6ce8\u610f\u529b\u200b\u3002\u200b\u800c\u200b\u8fd9\u200b\u4e24\u6bb5\u200b\u5e8f\u5217\u200b\u5373\u200b\u662f\u200b\u6211\u4eec\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b \\(Q\\)\u3001\\(K\\)\u3001\\(V\\) \u200b\u7684\u200b\u6765\u6e90\u200b\u3002</p> <p>\u200b\u200b\u5728\u200b\u7ecf\u5178\u200b\u7684\u200b Attention \u200b\u673a\u5236\u200b\u4e2d\u200b\uff0c\\(Q\\) \u200b\u5f80\u5f80\u200b\u6765\u81ea\u200b\u4e8e\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\uff0c\\(K\\) \u200b\u4e0e\u200b \\(V\\) \u200b\u6765\u81ea\u200b\u4e8e\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\uff0c\u200b\u90fd\u200b\u901a\u8fc7\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u8ba1\u7b97\u200b\u5f97\u5230\u200b\uff0c\u200b\u4ece\u800c\u200b\u53ef\u4ee5\u200b\u62df\u5408\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5e8f\u5217\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002\u200b\u4f8b\u5982\u200b\u5728\u200b Transformer \u200b\u7684\u200b Decoder \u200b\u7ed3\u6784\u200b\u4e2d\u200b\uff0c\\(Q\\) \u200b\u6765\u81ea\u200b\u4e8e\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\\(K\\) \u200b\u4e0e\u200b \\(V\\) \u200b\u6765\u81ea\u200b\u4e8e\u200b Decoder \u200b\u7684\u200b\u8f93\u5165\u200b\uff0c\u200b\u4ece\u800c\u200b\u62df\u5408\u200b\u4e86\u200b\u7f16\u7801\u200b\u4fe1\u606f\u200b\u4e0e\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\uff0c\u200b\u4fbf\u4e8e\u200b\u7efc\u5408\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u4fe1\u606f\u200b\u5b9e\u73b0\u200b\u672a\u6765\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u200b\u4f46\u200b\u5728\u200b Transformer \u200b\u7684\u200b Encoder \u200b\u7ed3\u6784\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Attention \u200b\u673a\u5236\u200b\u7684\u200b\u53d8\u79cd\u200b\u2014\u2014 self-attention \uff08\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u673a\u5236\u200b\u3002\u200b\u6240\u8c13\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff0c\u200b\u5373\u200b\u662f\u200b\u8ba1\u7b97\u200b\u672c\u8eab\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u90fd\u200b\u5176\u4ed6\u200b\u5143\u7d20\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5206\u5e03\u200b\uff0c\u200b\u5373\u200b\u5728\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\\(Q\\)\u3001\\(K\\)\u3001\\(V\\) \u200b\u90fd\u200b\u7531\u200b\u540c\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u901a\u8fc7\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u8ba1\u7b97\u200b\u5f97\u5230\u200b\u3002\u200b\u5728\u200b Encoder \u200b\u4e2d\u200b\uff0c\\(Q\\)\u3001\\(K\\)\u3001\\(V\\) \u200b\u5206\u522b\u200b\u662f\u200b\u8f93\u5165\u200b\u5bf9\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b \\(W_q\\)\u3001\\(W_k\\)\u3001\\(W_v\\) \u200b\u505a\u79ef\u200b\u5f97\u5230\u200b\uff0c\u200b\u4ece\u800c\u200b\u62df\u5408\u200b\u8f93\u5165\u200b\u8bed\u53e5\u200b\u4e2d\u200b\u6bcf\u200b\u4e00\u4e2a\u200b token \u200b\u5bf9\u200b\u5176\u4ed6\u200b\u6240\u6709\u200b token \u200b\u7684\u200b\u5173\u7cfb\u200b\u3002</p> <p>\u200b\u200b\u4f8b\u5982\u200b\uff0c\u200b\u901a\u8fc7\u200b Encoder \u200b\u4e2d\u200b\u7684\u200b self-attention \u200b\u5c42\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u62df\u5408\u200b\u4e0b\u9762\u200b\u8bed\u53e5\u200b\u4e2d\u200b it \u200b\u5bf9\u200b\u5176\u4ed6\u200b token \u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5206\u5e03\u200b\u5982\u56fe\u200b\uff1a</p> <p>\u200b\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0cself-attention \u200b\u673a\u5236\u200b\u5176\u5b9e\u200b\u662f\u200b\u901a\u8fc7\u200b\u7ed9\u200b \\(Q\\)\u3001\\(K\\)\u3001\\(V\\) \u200b\u7684\u200b\u8f93\u5165\u200b\u4f20\u5165\u200b\u540c\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff1a</p> <pre><code>x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n# \u200b\u4f20\u5165\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u4f5c\u4e3a\u200bsublayer,\u200b\u8f93\u51fa\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u8ba1\u7b97\u200b\u5e76\u200b\u8fdb\u884c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\n</code></pre> <p>\u200b\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u662f\u200b Encoder \u200b\u5c42\u200b\u7684\u200b\u90e8\u5206\u200b\u5b9e\u73b0\u200b\uff0cself_attn \u200b\u5373\u200b\u662f\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u4f20\u5165\u200b\u7684\u200b\u4e09\u4e2a\u200b\u53c2\u6570\u200b\u90fd\u200b\u662f\u200b \\(x\\)\uff0c\u200b\u5206\u522b\u200b\u662f\u200b \\(Q\\)\u3001\\(K\\)\u3001\\(V\\) \u200b\u7684\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\uff0c\u200b\u4ece\u800c\u200b \\(Q\\)\u3001\\(K\\)\u3001$$ \u200b\u5747\u200b\u6765\u6e90\u4e8e\u200b\u540c\u4e00\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5219\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\u62df\u5408\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u200bAttention \u200b\u673a\u5236\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u5e76\u884c\u200b\u5316\u200b\u4e0e\u200b\u957f\u671f\u200b\u4f9d\u8d56\u200b\u5173\u7cfb\u200b\u62df\u5408\u200b\uff0c\u200b\u4f46\u200b\u4e00\u6b21\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u53ea\u80fd\u200b\u62df\u5408\u200b\u4e00\u79cd\u200b\u76f8\u5173\u200b\u5173\u7cfb\u200b\uff0c\u200b\u5355\u4e00\u200b\u7684\u200b Attention \u200b\u673a\u5236\u200b\u5f88\u96be\u200b\u5168\u9762\u200b\u62df\u5408\u200b\u8bed\u53e5\u200b\u5e8f\u5217\u200b\u91cc\u200b\u7684\u200b\u76f8\u5173\u200b\u5173\u7cfb\u200b\u3002\u200b\u56e0\u6b64\u200b Transformer \u200b\u4f7f\u7528\u200b\u4e86\u200b Multi-Head attention \u200b\u673a\u5236\u200b\uff0c\u200b\u5373\u200b\u540c\u65f6\u200b\u5bf9\u200b\u4e00\u4e2a\u200b\u8bed\u6599\u200b\u8fdb\u884c\u200b\u591a\u6b21\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u90fd\u200b\u80fd\u200b\u62df\u5408\u200b\u4e0d\u540c\u200b\u7684\u200b\u5173\u7cfb\u200b\uff0c\u200b\u5c06\u200b\u6700\u540e\u200b\u7684\u200b\u591a\u6b21\u200b\u7ed3\u679c\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\u4f5c\u4e3a\u200b\u6700\u540e\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5373\u53ef\u200b\u66f4\u200b\u5168\u9762\u200b\u6df1\u5165\u200b\u5730\u200b\u62df\u5408\u200b\u8bed\u8a00\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b   \u200b\u5728\u200b\u539f\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4e5f\u200b\u901a\u8fc7\u200b\u5b9e\u9a8c\u200b\u8bc1\u5b9e\u200b\uff0c\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\u80fd\u591f\u200b\u62df\u5408\u200b\u8bed\u53e5\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\uff1a</p> <p>\u200b\u200b\u4e0a\u5c42\u200b\u4e0e\u200b\u4e0b\u5c42\u200b\u5206\u522b\u200b\u662f\u200b\u4e24\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\u5bf9\u200b\u540c\u200b\u4e00\u6bb5\u200b\u8bed\u53e5\u200b\u5e8f\u5217\u200b\u8fdb\u884c\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\uff0c\u200b\u80fd\u591f\u200b\u62df\u5408\u200b\u4e0d\u540c\u200b\u5c42\u6b21\u200b\u7684\u200b\u76f8\u5173\u200b\u4fe1\u606f\u200b\u3002\u200b\u901a\u8fc7\u200b\u591a\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\u540c\u65f6\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u80fd\u591f\u200b\u66f4\u200b\u5168\u9762\u200b\u5730\u200b\u62df\u5408\u200b\u8bed\u53e5\u200b\u5173\u7cfb\u200b\u3002</p> <p>\u200bMulti-Head attention \u200b\u7684\u200b\u6574\u4f53\u200b\u8ba1\u7b97\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u6240\u8c13\u200b\u7684\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u5176\u5b9e\u200b\u5c31\u662f\u200b\u5c06\u200b\u539f\u59cb\u200b\u7684\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u8fdb\u884c\u200b\u591a\u7ec4\u200b\u7684\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5904\u7406\u200b\uff1b\u200b\u7136\u540e\u200b\u518d\u200b\u5c06\u200b\u6bcf\u200b\u4e00\u7ec4\u200b\u5f97\u5230\u200b\u7684\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u679c\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\uff0c\u200b\u518d\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0c\u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002\u200b\u6211\u4eec\u200b\u7528\u200b\u516c\u5f0f\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4e3a\u200b\uff1a $$ \\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\     \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i) $$</p> <p>\u200b\u5176\u200b\u6700\u200b\u76f4\u89c2\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u5e76\u200b\u4e0d\u200b\u590d\u6742\u200b\uff0c\u200b\u5373\u200b n \u200b\u4e2a\u5934\u200b\u5c31\u200b\u6709\u200b n \u200b\u7ec4\u200b3\u200b\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u7ec4\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u4ece\u800c\u200b\u901a\u8fc7\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b n \u200b\u4e2a\u200b\u7ed3\u679c\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\u8f93\u51fa\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e0a\u8ff0\u200b\u5b9e\u73b0\u200b\u590d\u6742\u5ea6\u200b\u8f83\u200b\u9ad8\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u77e9\u9635\u200b\u8fd0\u7b97\u200b\u5de7\u5999\u200b\u5730\u200b\u5b9e\u73b0\u200b\u5e76\u884c\u200b\u7684\u200b\u591a\u5934\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6574\u4f53\u200b\u8ba1\u7b97\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\uff08\u200b\u6ce8\u200b\uff1a\u200b\u7531\u4e8e\u200b\u6b64\u5904\u200b\u4f7f\u7528\u200b\u4e86\u200b\u77e9\u9635\u200b\u8fd0\u7b97\u200b\u6765\u200b\u5b9e\u73b0\u200b\u591a\u5934\u200b\u5e76\u884c\u200b\uff0c\u200b\u5185\u90e8\u200b\u903b\u8f91\u200b\u76f8\u5bf9\u200b\u590d\u6742\u200b\uff0c\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u914c\u60c5\u200b\u9605\u8bfb\u200b\uff09\uff1a</p> <pre><code>class MultiHeadedAttention(nn.Module):\n    # \u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u64cd\u4f5c\u200b\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # \u200b\u8fd9\u91cc\u200b\u7684\u200b d_model \u200b\u5373\u4e3a\u200b\u8be5\u5c42\u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u662f\u200b n \u200b\u4e2a\u5934\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u5e94\u5f53\u200b\u80fd\u591f\u200b\u6574\u9664\u200b\u5934\u6570\u200b\n        # \u200b\u65ad\u8a00\u200b\uff0c\u200b\u63a7\u5236\u200bh\u200b\u603b\u662f\u200b\u6574\u9664\u200b\u4e8e\u200bd_model\uff0c\u200b\u5982\u679c\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u4e0d\u200b\u6ee1\u8db3\u200b\u5c06\u200b\u62a5\u9519\u200b\n        # \u200b\u8fd9\u91cc\u200b\u5047\u8bbe\u200b d_v = d_k\uff0c\u200b\u5176\u5b9e\u200b\u662f\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u6700\u540e\u200b\u7ebf\u6027\u200b\u5c42\u200b\u7684\u200b\u5904\u7406\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u5047\u8bbe\u200b\uff0c\u200b\u628a\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u62bd\u51fa\u6765\u200b\u5355\u72ec\u200b\u521d\u59cb\u5316\u200b\u5373\u53ef\u200b\n        self.d_k = d_model // h\n        # \u200b\u6bcf\u4e2a\u200b\u5934\u8981\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ef4\u5ea6\u200b\n        self.h = h\n        # \u200b\u5934\u6570\u200b\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        # \u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u521d\u59cb\u5316\u200b\u4e86\u200b4\u200b\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\uff0c\u200b\u524d\u200b\u4e09\u4e2a\u200b\u5206\u522b\u200b\u662f\u200b\u4e09\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u6bcf\u4e2a\u200b\u5934\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u662f\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff0c\u200b\u8fd9\u6837\u200b\u64cd\u4f5c\u200b\u7684\u200b\u524d\u63d0\u200b\u662f\u200b\u4e0a\u8ff0\u200b\u5047\u8bbe\u200b\n        # \u200b\u8fd9\u91cc\u200b\u901a\u8fc7\u200b\u4e09\u4e2a\u200b\u7ec4\u5408\u200b\u77e9\u9635\u200b\u6765\u200b\u4ee3\u66ff\u200b\u4e86\u200bn\u200b\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u7684\u200b\u7ec4\u5408\u200b\uff0c\u200b\u5176\u200b\u903b\u8f91\u200b\u5728\u4e8e\u200b\u77e9\u9635\u200b\u5185\u79ef\u200b\u518d\u200b\u62fc\u63a5\u200b\u5176\u5b9e\u200b\u7b49\u540c\u4e8e\u200b\u62fc\u63a5\u200b\u77e9\u9635\u200b\u518d\u200b\u5185\u79ef\u200b\uff0c\u200b\u4e0d\u200b\u7406\u89e3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u81ea\u884c\u200b\u6a21\u62df\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\u5176\u5b9e\u200b\u76f8\u5f53\u4e8e\u200bn\u200b\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u7684\u200b\u62fc\u63a5\u200b\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        # \u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\n        nbatches = query.size(0)\n\n        # 1) \u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u901a\u8fc7\u200b\u7ebf\u6027\u200b\u5c42\u200b\u5373\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u5f97\u5230\u200b\u6620\u5c04\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\n        # \u200b\u8fd9\u91cc\u200b\u8f93\u5165\u200b\u7ecf\u8fc7\u200b\u7ebf\u6027\u200b\u5c42\u200b\u4e4b\u540e\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b nbatches*length*d_model\uff0c\u200b\u56e0\u4e3a\u200b\u8981\u200b\u8fdb\u5165\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u9700\u8981\u200b\u628a\u200b\u4e0d\u540c\u200b\u5934\u200b\u7684\u200b\u8f93\u5165\u200b\u62c6\u5f00\u200b\uff0c\u200b\u5373\u5c06\u200b\u8f93\u51fa\u200b\u5c55\u5f00\u200b\u4e3a\u200b nbatches*length*n_head*d_k\uff0c\u200b\u7136\u540e\u200b\u5c06\u200blength\u200b\u548c\u200bn_head\u200b\u7ef4\u5ea6\u200b\u4e92\u6362\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u4e2d\u200b\u6211\u4eec\u200b\u662f\u200b\u53d6\u200b\u4e86\u200b\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u53c2\u4e0e\u200b\u8ba1\u7b97\u200b\n        query, key, value = [\n            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n        # \u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u5148\u200b\u6309\u200bn_batches*-1*n_head*d_k\u200b\u5c55\u5f00\u200b\u518d\u200b\u4e92\u6362\u200b1\u30012\u200b\u7ef4\u5ea6\u200b\u800c\u200b\u4e0d\u662f\u200b\u76f4\u63a5\u200b\u6309\u200b\u6ce8\u610f\u529b\u200b\u8f93\u5165\u200b\u5c55\u5f00\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200bview\u200b\u7684\u200b\u5c55\u5f00\u200b\u65b9\u5f0f\u200b\u662f\u200b\u76f4\u63a5\u200b\u628a\u200b\u8f93\u5165\u200b\u5168\u90e8\u200b\u6392\u5f00\u200b\uff0c\u200b\u7136\u540e\u200b\u6309\u200b\u8981\u6c42\u200b\u6784\u9020\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u53ea\u6709\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\u80fd\u591f\u200b\u5b9e\u73b0\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u5934\u200b\u5bf9\u5e94\u200b\u90e8\u5206\u200b\u53d6\u51fa\u200b\u6765\u200b\u7684\u200b\u76ee\u6807\u200b\n\n        # 2) \u200b\u8fdb\u884c\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout\n        )\n        # x \u200b\u4e3a\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\u7ed3\u679c\u200b\uff0cattn\u200b\u4e3a\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5206\u6570\u200b\n\n        # 3) \u200b\u5c06\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u62fc\u63a5\u200b\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u6700\u540e\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\n        # \u200b\u6ce8\u610f\u529b\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bn_batches*n_head*length*d_k\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u8f93\u5165\u200b\u4e3a\u200bn_batches*length*d_model\uff0c\u200b\u6240\u4ee5\u200b\u76f4\u63a5\u200b\u9488\u5bf9\u200b\u4e0a\u8ff0\u200b\u8f6c\u6362\u200b\u64cd\u4f5c\u200b\u9006\u200b\u64cd\u4f5c\u200b\u5373\u53ef\u200b\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(nbatches, -1, self.h * self.d_k)\n        )\n        # contiguous \u200b\u51fd\u6570\u200b\u7528\u4e8e\u200b\u91cd\u65b0\u200b\u5f00\u8f9f\u200b\u4e00\u5757\u200b\u65b0\u200b\u5185\u5b58\u200b\u5b58\u50a8\u200b\uff0c\u200b\u56e0\u4e3a\u200bPytorch\u200b\u8bbe\u7f6e\u200b\u5148\u200btranspose\u200b\u518d\u200bview\u200b\u4f1a\u200b\u62a5\u9519\u200b\uff0c\u200b\u56e0\u4e3a\u200bview\u200b\u76f4\u63a5\u200b\u57fa\u4e8e\u200b\u5e95\u5c42\u200b\u5b58\u50a8\u200b\u5f97\u5230\u200b\uff0c\u200b\u7136\u800c\u200btranspose\u200b\u5e76\u200b\u4e0d\u4f1a\u200b\u6539\u53d8\u200b\u5e95\u5c42\u200b\u5b58\u50a8\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u989d\u5916\u200b\u5b58\u50a8\u200b\n\n        del query\n        del key\n        del value\n        # \u200b\u6700\u540e\u200b\u7ecf\u8fc7\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u5373\u53ef\u200b\n        return self.linears[-1](x)\n</code></pre> <p>\u200b\u200b\u5728\u200b Pytorch \u200b\u4e2d\u200b\uff0c\u200b\u5176\u5b9e\u200b\u63d0\u4f9b\u200b\u4e86\u200b Multi-Head Attention \u200b\u673a\u5236\u200b\u7684\u200b API\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e0b\u5217\u200b\u4ee3\u7801\u200b\u76f4\u63a5\u200b\u6784\u9020\u200b\u5e76\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff1a</p> <pre><code>multihead_attn = nn.MultiheadAttention(embed_dim , num_heads)\n# \u200b\u6784\u9020\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\n# embed_dim :\u200b\u8f93\u51fa\u200b\u8bcd\u200b\u5411\u91cf\u200b\u957f\u5ea6\u200b\uff1bnum_heads :\u200b\u5934\u6570\u200b\n# \u200b\u53ef\u200b\u9009\u200b\u53c2\u6570\u200b\uff1a\n# drop_out: \u200b\u5728\u200b\u91c7\u6837\u200b\u5c42\u4e2d\u200bdrop_out\u200b\u7684\u200b\u6982\u7387\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b0\uff1bbias\uff1a\u200b\u7ebf\u6027\u200b\u5c42\u200b\u662f\u5426\u200b\u8ba1\u7b97\u200b\u504f\u7f6e\u200b\n# add_bias_kv\uff1a\u200b\u662f\u5426\u200b\u5c06\u200b\u504f\u7f6e\u200b\u6dfb\u52a0\u200b\u5230\u200b K \u200b\u548c\u200b V \u200b\u4e2d\u200b\uff1badd_zero_attn\uff1a\u200b\u662f\u5426\u200b\u4e3a\u200b K \u200b\u548c\u200b V \u200b\u518d\u200b\u6dfb\u52a0\u200b\u4e00\u4e32\u200b\u4e3a\u200b0\u200b\u7684\u200b\u5e8f\u5217\u200b\n# kdim\uff1aK \u200b\u7684\u200b\u603b\u5171\u200bfeature\u200b\u6570\u200b\uff1bvdim\uff1aV \u200b\u7684\u200b\u603b\u5171\u200bfeature\u200b\u6570\u200b\uff1bbatch_first\uff1a\u200b\u662f\u5426\u200b\u5c06\u200b\u8f93\u5165\u200b\u8c03\u6574\u200b\u4e3a\u200b(batch,seq,feature)\nattn_output, attn_output_weights = multihead_attn(query, key, value)\n# \u200b\u524d\u5411\u200b\u8ba1\u7b97\u200b\n# \u200b\u8f93\u51fa\u200b\uff1a\n# attn_output\uff1a\u200b\u5f62\u200b\u5982\u200b(N,L,E)\u200b\u7684\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\uff0cN\u200b\u4e3a\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff0cL\u200b\u4e3a\u200b\u76ee\u6807\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\uff0cE\u200b\u4e3a\u200bembed_dim\n# attn_output_weights\uff1a\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u5206\u6570\u200b\uff0c\u200b\u4ec5\u5f53\u200bneed_weights=True\u200b\u65f6\u200b\u8fd4\u56de\u200b\n# query\u3001key\u3001value \u200b\u5206\u522b\u200b\u662f\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u7684\u200b\u4e09\u4e2a\u200b\u8f93\u5165\u200b\u77e9\u9635\u200b\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#encoder","title":"Encoder","text":"<p>\u200b\u200b\u5982\u56fe\u6240\u793a\u200b\uff0cEncoder \u200b\u7531\u200b N \u200b\u4e2a\u200b\uff08\u200b\u8bba\u6587\u200b\u4e2d\u53d6\u200b N = 6\uff09EncoderLayer \u200b\u7ec4\u6210\u200b\uff0c\u200b\u6bcf\u4e2a\u200b EncoderLayer \u200b\u53c8\u200b\u7531\u200b\u4e24\u4e2a\u200b sublayer \uff08\u200b\u5b50\u5c42\u200b\uff09\u200b\u7ec4\u6210\u200b\u3002\u200b\u5728\u4e0b\u6587\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200b layer \u200b\u662f\u200b\u4e00\u4e2a\u200b EncoderLayer\uff0c\u200b\u4ee3\u7801\u200b\u5728\u200b\u6700\u540e\u200b\u53c8\u200b\u989d\u5916\u200b\u52a0\u5165\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u5316\u200b\u5c42\u200b\u8fdb\u884c\u200b\u6807\u51c6\u5316\u200b\u64cd\u4f5c\u200b\uff1a</p> <pre><code>class Encoder(nn.Module):\n    \"Core encoder is a stack of N layers\"\n    # \u200b\u7f16\u7801\u5668\u200b\u5176\u5b9e\u200b\u662f\u200b\u7531\u200bN\u200b\u5c42\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b+\u200b\u6807\u51c6\u5316\u200b\u5c42\u200b\u6784\u6210\u200b\u7684\u200b\n\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        # clones \u200b\u51fd\u6570\u200b\u4e3a\u200b\u4f5c\u8005\u200b\u5b9a\u4e49\u200b\u7684\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u5c06\u200b layer \u200b\u5c42\u200b\u590d\u5236\u200b N \u200b\u6b21\u200b\u7684\u200b\u529f\u80fd\u200b\n        self.norm = LayerNorm(layer.size)\n        # \u200b\u6807\u51c6\u5316\u200b\u5c42\u200b\uff0c\u200b\u53c2\u6570\u200b\u4e3a\u200b\u7279\u5f81\u200b\u6570\u200b\n\n    def forward(self, x, mask):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n</code></pre> <p>\u200b1. \u200b\u7b2c\u4e00\u90e8\u200b\u5206\u4e3a\u200b\u4e00\u4e2a\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u3002Transformer \u200b\u7684\u200b\u6700\u5927\u200b\u7279\u70b9\u200b\u5373\u200b\u662f\u200b\u629b\u5f03\u200b\u4e86\u200b\u4f20\u7edf\u200b\u7684\u200b CNN\u3001RNN \u200b\u67b6\u6784\u200b\uff0c\u200b\u5b8c\u5168\u200b\u4f7f\u7528\u200b attention \u200b\u673a\u5236\u200b\u642d\u5efa\u200b\u3002\u200b\u5728\u200b EncoderLayer \u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b Multi-Head self-attention\uff08\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u5c42\u200b\uff0c\u200b\u7f16\u7801\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u7684\u200b\u76f8\u5173\u200b\u5173\u7cfb\u200b\u3002\u200b\u901a\u8fc7\u200b attention \u200b\u673a\u5236\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u5e76\u884c\u200b\u5316\u200b\u4e0e\u200b\u957f\u671f\u200b\u4f9d\u8d56\u200b\u5173\u7cfb\u200b\u7684\u200b\u62df\u5408\u200b\u3002\u200b\u5173\u4e8e\u200b Multi-Head self-attention \u200b\u5c06\u200b\u5728\u200b\u4e4b\u540e\u200b\u7684\u200b\u677f\u5757\u200b\u8be6\u8ff0\u200b\u5176\u200b\u7279\u70b9\u200b\u4e0e\u200b\u5b9e\u73b0\u200b\u3002\u200b\u5728\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u540e\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7ecf\u8fc7\u200b\u6807\u51c6\u5316\u200b\u5c42\u200b\u8fdb\u5165\u200b\u4e0b\u200b\u4e00\u4e2a\u200b sublayer\u3002</p> <pre><code>class EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        # \u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\n        self.feed_forward = feed_forward\n        # \u200b\u524d\u9988\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        # \u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b\u5728\u200b\u4e4b\u540e\u200b\u5b9a\u4e49\u200b\n        # \u200b\u4e24\u5c42\u200b\u5206\u522b\u200b\u7528\u4e8e\u200b\u8fde\u63a5\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u548c\u200b\u524d\u9988\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        # \u200b\u4f20\u5165\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u4f5c\u4e3a\u200bsublayer,\u200b\u8f93\u51fa\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u8ba1\u7b97\u200b\u5e76\u200b\u8fdb\u884c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\n        return self.sublayer[1](x, self.feed_forward)\n</code></pre> <p>\u200b2. \u200b\u7b2c\u4e8c\u200b\u90e8\u5206\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u79f0\u4e3a\u200b\u201cposition-wise fully connected feed-forward network\u201d\uff0c\u200b\u5b9e\u9645\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b+\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b+ dropout + \u200b\u7ebf\u6027\u200b\u5c42\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u3002</p> <pre><code>class PositionwiseFeedForward(nn.Module):\n    # \u200b\u5373\u200b\u67b6\u6784\u56fe\u200b\u4e2d\u200b\u7684\u200bFeedForward\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n        # \u200b\u5b9e\u5219\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b+\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b+\u200b\u91c7\u6837\u200b+\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#decoder","title":"Decoder","text":"<p>\u200bDecoder \u200b\u4e0e\u200b Encoder \u200b\u7684\u200b\u7ec4\u6210\u200b\u975e\u5e38\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u540c\u6837\u200b\u662f\u200b\u7531\u200bN\u200b\u4e2a\u200bDecoderLayer\u200b\u7ec4\u6210\u200b\uff0cDecoderLayer \u200b\u4e0e\u200b EncoderLayer \u200b\u7684\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b\uff1a</p> <p>\u200b1. EncoderLayer\u200b\u7531\u200b\u4e24\u4e2a\u200bsublayer\u200b\u7ec4\u6210\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u4e00\u4e2a\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u4e0e\u200b\u4e00\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u5c42\u200b\u3002DecoderLayer \u200b\u5219\u200b\u5728\u200b EncoderLayer \u200b\u7684\u200b\u4e24\u4e2a\u200b sublayer \u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u589e\u52a0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5e26\u200b\u63a9\u7801\u200b\u7684\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u3002\u200b\u5982\u56fe\u200b\uff0c\u200b\u6700\u200b\u4e0b\u65b9\u200b\u7684\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u540c\u200b EncoderLayer\u200b\u7ed3\u6784\u200b\u76f8\u540c\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u5219\u200b\u662f\u200b\u589e\u52a0\u200b\u7684\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b\u4f7f\u7528\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u548c\u200b\u4e0b\u65b9\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\u8fdb\u884c\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u3002\u200b\u6700\u200b\u4e0a\u65b9\u200b\u7684\u200b\u5168\u200b\u8fde\u63a5\u200b\u7f51\u7edc\u200b\u4e5f\u200b\u540c\u200b EncoderLayer \u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b2. EncoderLayer \u200b\u7684\u200b\u8f93\u5165\u200b\u4ec5\u200b\u6765\u81ea\u200b\u4e8e\u200b\u7f16\u7801\u200b\u4e4b\u540e\u200b\u7684\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\uff08\u200b\u6216\u200b\u4e0a\u200b\u4e00\u4e2a\u200b EncoderLayer \u200b\u7684\u200b\u8f93\u51fa\u200b\uff09\uff0cDecoderLayer \u200b\u7684\u200b\u8f93\u5165\u200b\u9664\u200b\u6765\u81ea\u200b\u4e8e\u200b\u7f16\u7801\u200b\u4e4b\u540e\u200b\u7684\u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u5916\u200b\uff0c\u200b\u8fd8\u200b\u5305\u62ec\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200bDecoderLayer \u200b\u7684\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>class DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        # \u200b\u8bed\u6599\u200b\u6ce8\u610f\u529b\u200b\uff0c\u200b\u5373\u56fe\u200b\u4e2d\u200bDecoder\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u7f51\u7edc\u200b\uff0c\u200b\u62df\u5408\u200b\u7f16\u7801\u5668\u200b\u8f93\u51fa\u200b\u548c\u200b\u4e0a\u200b\u4e00\u4e2a\u200b\u5b50\u5c42\u200b\u8f93\u51fa\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n        # \u200b\u56e0\u4e3a\u200b\u89e3\u7801\u5668\u200b\u6709\u200b\u4e09\u4e2a\u200b\u5b50\u5c42\u200b\uff0c\u200b\u6240\u4ee5\u200b\u9700\u8981\u200b\u4e09\u4e2a\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u5c42\u200b\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        # \u200b\u7531\u6b64\u53ef\u89c1\u200b\u89e3\u7801\u5668\u200b\u4e2d\u200b\u4e24\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5b50\u5c42\u200b\u7684\u200b\u533a\u522b\u200b\n        # \u200b\u7b2c\u4e00\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5b50\u5c42\u200b\u540c\u200b\u7f16\u7801\u5668\u200b\u4e2d\u200b\u76f8\u540c\u200b\uff0c\u200b\u662f\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u4f20\u5165\u200b\u7684\u200b\u4e09\u4e2a\u200b\u53c2\u6570\u5747\u200b\u901a\u8fc7\u200b\u8f93\u5165\u200b\u8bed\u6599\u200bx\u200b\u5f97\u5230\u200b\n        # \u200b\u7b2c\u4e8c\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5b50\u5c42\u200b\u4e3a\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u4f20\u5165\u200b\u7684\u200b\u4e09\u4e2a\u200b\u53c2\u6570\u200b\u5206\u522b\u200b\u7531\u200b\u8f93\u5165\u200b\u8bed\u6599\u200bx\u200b\u4e0e\u200b\u8bb0\u5fc6\u200b\uff08\u200b\u5373\u200b\u7f16\u7801\u5668\u200b\u8f93\u51fa\u200b\uff09m\u200b\u5f97\u5230\u200b\n        return self.sublayer[2](x, self.feed_forward)\n</code></pre> <p>\u200b   \u200b\u6ce8\u610f\u200b\u6b64\u5904\u200b\u4e3a\u4e86\u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u4e0e\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\uff0c\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u65f6\u200b\uff0c\u200b\u7b2c\u4e8c\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5b50\u5c42\u200b\u5c06\u200b\u4f20\u5165\u200b memory\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u3002Decoder \u200b\u7684\u200b\u6574\u4f53\u200b\u5b9e\u73b0\u200b\u540c\u200b Encoder \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6b64\u5904\u200b\u5c31\u200b\u4e0d\u518d\u200b\u8d58\u8ff0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_3","title":"\u6b8b\u5dee\u200b\u8fde\u63a5","text":"<p>\u200b\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b\u6a21\u578b\u200b\u9000\u5316\u200b\uff0cTransformer \u200b\u91c7\u7528\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u7684\u200b\u601d\u60f3\u200b\u6765\u200b\u8fde\u63a5\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u5b50\u5c42\u200b\u3002\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5373\u4e0b\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0d\u4ec5\u200b\u662f\u200b\u4e0a\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u8fd8\u200b\u5305\u62ec\u200b\u4e0a\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u5141\u8bb8\u200b\u6700\u5e95\u5c42\u200b\u4fe1\u606f\u200b\u76f4\u63a5\u200b\u4f20\u5230\u200b\u6700\u9ad8\u5c42\u200b\uff0c\u200b\u8ba9\u200b\u9ad8\u5c42\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u6b8b\u5dee\u200b\u7684\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b Encoder \u200b\u4e2d\u200b\uff0c\u200b\u5728\u200b\u7b2c\u4e00\u4e2a\u200b\u5b50\u5c42\u200b\uff0c\u200b\u8f93\u5165\u200b\u8fdb\u5165\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u7684\u200b\u540c\u65f6\u200b\u4f1a\u200b\u76f4\u63a5\u200b\u4f20\u9012\u200b\u5230\u200b\u8be5\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u7136\u540e\u200b\u8be5\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u4f1a\u200b\u4e0e\u200b\u539f\u200b\u8f93\u5165\u200b\u76f8\u52a0\u200b\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200b\u6807\u51c6\u5316\u200b\u3002\u200b\u5728\u200b\u7b2c\u4e8c\u4e2a\u200b\u5b50\u5c42\u200b\u4e5f\u200b\u662f\u200b\u4e00\u6837\u200b\u3002\u200b\u5373\u200b\uff1a $$ \\rm output = LayerNorm(x + Sublayer(x)) $$ \u200bLayerNorm \u200b\u4e3a\u200b\u8be5\u200b\u5c42\u200b\u7684\u200b\u6807\u51c6\u5316\u200b\u64cd\u4f5c\u200b\uff0cSublayer \u200b\u4e3a\u200b\u8be5\u5b50\u5c42\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff08\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u6216\u5168\u200b\u8fde\u63a5\u200b\u8ba1\u7b97\u200b\uff09\u3002\u200b\u6e90\u7801\u200b\u901a\u8fc7\u200b\u6784\u9020\u200b\u4e00\u4e2a\u200b SublayerConnection \u200b\u7c7b\u6765\u200b\u5b9e\u73b0\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff1a</p> <pre><code>class SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n        # \u200b\u91c7\u6837\u200b\u5c42\u200b\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n        # \u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5728\u200b\u8be5\u5c42\u200b\u5b9e\u73b0\u200b\u4e86\u200bnorm\u200b\u548c\u200badd\n</code></pre> <p>\u200b\u200b\u56de\u5230\u200b EncoderLayer \u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u4f5c\u8005\u200b\u5728\u200b\u8be5\u5c42\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e24\u4e2a\u200b SublayerConnection \u200b\u5bf9\u8c61\u200b\uff0c\u200b\u5206\u522b\u200b\u5b9e\u73b0\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u5230\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u3001\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u5230\u200b\u8f93\u51fa\u200b\u7684\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff1a</p> <pre><code>class EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        # \u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u5c42\u200b\n        # \u200b\u4e24\u5c42\u200b\u5206\u522b\u200b\u7528\u4e8e\u200b\u8fde\u63a5\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u548c\u200b\u524d\u9988\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        # \u200b\u4f20\u5165\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u4f5c\u4e3a\u200bsublayer,\u200b\u8f93\u51fa\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u8ba1\u7b97\u200b\u5e76\u200b\u8fdb\u884c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\n        return self.sublayer[1](x, self.feed_forward)\n</code></pre> <p>\u200bDecoderLayer \u200b\u4e2d\u200b\u4e5f\u200b\u540c\u6837\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u8be5\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u7531\u4e8e\u200b\u6709\u200b\u4e09\u4e2a\u200b sublayer\uff0c\u200b\u6240\u4ee5\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e09\u4e2a\u200b SublayerConnection \u200b\u5bf9\u8c61\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#mask","title":"Mask","text":"<p>\u200bTransformer \u200b\u662f\u200b\u4e00\u4e2a\u200b\u81ea\u200b\u56de\u5f52\u200b\u6a21\u578b\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\uff0c\u200b\u5176\u200b\u5c06\u200b\u5229\u7528\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u4f9d\u5e8f\u200b\u5bf9\u200b\u8f93\u51fa\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u8bed\u6599\u200b\u7684\u200b\u53e5\u200b\u5bf9\u200b\u4e3a\u200b\uff1a1. \\ \u200b\u6211\u7231\u4f60\u200b \\\uff1b2. \\ I like you \\\u3002\u200b\u5219\u200b Encoder \u200b\u83b7\u53d6\u200b\u7684\u200b\u8f93\u5165\u200b\u5c06\u4f1a\u200b\u662f\u200b\u53e5\u200b 1 \u200b\u6574\u4f53\u200b\uff0c\u200b\u5e76\u200b\u8f93\u51fa\u200b\u53e5\u200b 1 \u200b\u7684\u200b\u7f16\u7801\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f46\u200b Decoder \u200b\u7684\u200b\u8f93\u5165\u200b\u5e76\u200b\u4e0d\u200b\u4e00\u200b\u5f00\u59cb\u200b\u5c31\u662f\u200b\u53e5\u200b 2 \u200b\u6574\u4f53\u200b\uff0c\u200b\u800c\u662f\u200b\u5148\u200b\u8f93\u5165\u200b\u8d77\u59cb\u200b\u7b26\u200b\\\uff0cDecoder \u200b\u6839\u636e\u200b\\ \u200b\u4e0e\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u9884\u6d4b\u200b I\uff0c\u200b\u518d\u200b\u8f93\u5165\u200b\\ I\uff0cDecoder \u200b\u6839\u636e\u200b\u8f93\u5165\u200b\u548c\u200b Encoder \u200b\u7684\u200b\u8f93\u51fa\u200b\u9884\u6d4b\u200b like\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u81ea\u200b\u56de\u5f52\u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u5bf9\u200b\u8f93\u5165\u200b\u8fdb\u884c\u200b mask\uff08\u200b\u906e\u853d\u200b\uff09\uff0c\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u6a21\u578b\u200b\u4e0d\u4f1a\u200b\u4f7f\u7528\u200b\u672a\u6765\u200b\u4fe1\u606f\u200b\u9884\u6d4b\u200b\u5f53\u4e0b\u200b\u3002 <p>\u200b\u200b\u5173\u4e8e\u200b\u81ea\u200b\u56de\u5f52\u200b\u6a21\u578b\u200b\u4e0e\u200b\u81ea\u200b\u7f16\u7801\u200b\u6a21\u578b\u200b\u7684\u200b\u7ec6\u8282\u200b\uff0c\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u4e0b\u6765\u200b\u67e5\u9605\u200b\u66f4\u200b\u591a\u200b\u8d44\u6599\u200b\uff0c\u200b\u5728\u200b\u6b64\u200b\u63d0\u4f9b\u200b\u90e8\u5206\u200b\u94fe\u63a5\u200b\u4f9b\u200b\u8bfb\u8005\u200b\u53c2\u8003\u200b\u3002\u200b\u535a\u5ba2\u200b\uff1a\u200b\u81ea\u200b\u56de\u5f52\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b VS \u200b\u81ea\u200b\u7f16\u7801\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u3001\u200b\u9884\u200b\u8bad\u7ec3\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u6574\u7406\u200b\uff1b\u200b\u8bba\u6587\u200b\uff1a\u200b\u57fa\u4e8e\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6280\u672f\u200b\u7814\u7a76\u200b\u7efc\u8ff0\u200b\u7b49\u200b\u3002</p> <p>\u200b\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b Transformer \u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5efa\u7acb\u200b\u4e00\u4e2a\u200b mask \u200b\u51fd\u6570\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u5f53\u4e0b\u200b\u9884\u6d4b\u200b\u7684\u200b\u65f6\u95f4\u200b\u9636\u6bb5\u200b\u5bf9\u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u8fdb\u884c\u200b mask\uff0c\u200b\u88ab\u200b mask \u200b\u7684\u200b\u4fe1\u606f\u200b\u5c31\u200b\u4e0d\u4f1a\u200b\u88ab\u200b\u6a21\u578b\u200b\u5f97\u77e5\u200b\uff0c\u200b\u4ece\u800c\u200b\u4fdd\u8bc1\u200b\u4e86\u200b\u6a21\u578b\u200b\u53ea\u200b\u4f7f\u7528\u200b\u5386\u53f2\u200b\u4fe1\u606f\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002mask \u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u6cd5\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>def subsequent_mask(size):\n    \"Mask out subsequent positions.\"\n    attn_shape = (1, size, size)\n    # \u200b\u6ce8\u610f\u529b\u200b\u8bed\u6599\u200b\u7684\u200b\u5f62\u72b6\u200b\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n        torch.uint8\n    )\n    # triu\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u4e0a\u200b\u4e09\u89d2\u200b\u77e9\u9635\u200b\uff0cdiagonal=1\u200b\u63a7\u5236\u200b\u4e0d\u200b\u4fdd\u7559\u200b\u4e3b\u200b\u5bf9\u89d2\u7ebf\u200b\n    '''\n    \u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a\n    a = [[1,2,3],\n         [4,5,6],\n         [7,8,9]\n    ]\n    triu(a, diagonal=1)\u200b\u8fd4\u56de\u200b\uff1a\n    [[0,2,3],\n     [0,0,6],\n     [0,0,0]]\n     \u200b\u53ef\u89c1\u200b\u901a\u8fc7\u200b\u751f\u6210\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u4e0a\u200b\u4e09\u89d2\u200b\u77e9\u9635\u200b\uff0c\u200b\u518d\u53d6\u200b\u5176\u4e2d\u200b\u4e3a\u200b0\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u6211\u4eec\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u672a\u6765\u200b\u4fe1\u606f\u200b\u906e\u853d\u200b\n    '''\n    return subsequent_mask == 0\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_4","title":"\u4f4d\u7f6e\u200b\u7f16\u7801","text":"<p>\u200bAttention \u200b\u673a\u5236\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u5e76\u884c\u8ba1\u7b97\u200b\uff0c\u200b\u4f46\u200b\u540c\u65f6\u200b\uff0c\u200b\u5176\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e5f\u200b\u5bfc\u81f4\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u4e22\u5931\u200b\u3002\u200b\u5728\u200b RNN\u3001LSTM \u200b\u4e2d\u200b\uff0c\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u4f1a\u200b\u6cbf\u7740\u200b\u8bed\u53e5\u200b\u672c\u8eab\u200b\u7684\u200b\u987a\u5e8f\u200b\u88ab\u200b\u4f9d\u6b21\u200b\u9012\u5f52\u200b\u5904\u7406\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u7684\u200b\u987a\u5e8f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6781\u5176\u91cd\u8981\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u548c\u200b\u81ea\u7136\u8bed\u8a00\u200b\u7684\u200b\u672c\u8eab\u200b\u7279\u6027\u200b\u975e\u5e38\u200b\u543b\u5408\u200b\u3002\u200b\u4f46\u200b\u4ece\u200b\u4e0a\u200b\u6587\u5bf9\u200b Attention \u200b\u673a\u5236\u200b\u7684\u200b\u5206\u6790\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5728\u200b Attention \u200b\u673a\u5236\u200b\u7684\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b token\uff0c\u200b\u5176\u4ed6\u200b\u5404\u4e2a\u200b\u4f4d\u7f6e\u200b\u5bf9\u200b\u5176\u200b\u6765\u8bf4\u200b\u90fd\u200b\u662f\u200b\u5e73\u7b49\u200b\u7684\u200b\uff0c\u200b\u5373\u200b\u201c\u200b\u6211\u200b\u559c\u6b22\u200b\u4f60\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u4f60\u200b\u559c\u6b22\u200b\u6211\u200b\u201d\u200b\u5728\u200b Attention \u200b\u673a\u5236\u200b\u770b\u6765\u200b\u662f\u200b\u5b8c\u5168\u76f8\u540c\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u65e0\u7591\u200b\u8fd9\u662f\u200b Attention \u200b\u673a\u5236\u200b\u5b58\u5728\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u95ee\u9898\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u200b\u4f7f\u7528\u200b\u5e8f\u5217\u200b\u987a\u5e8f\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4fdd\u7559\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0cTransformer \u200b\u91c7\u7528\u200b\u4e86\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u673a\u5236\u200b\uff0c\u200b\u8be5\u200b\u673a\u5236\u200b\u4e5f\u200b\u5728\u200b\u4e4b\u540e\u200b\u88ab\u200b\u591a\u79cd\u200b\u6a21\u578b\u200b\u6cbf\u7528\u200b\u3002</p> <p>\u200b\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff0c\u200b\u5373\u200b\u6839\u636e\u200b\u5e8f\u5217\u200b\u4e2d\u200b token \u200b\u7684\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u7f16\u7801\u200b\uff0c\u200b\u518d\u200b\u5c06\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u52a0\u5165\u200b\u8bcd\u200b\u5411\u91cf\u200b\u7f16\u7801\u200b\u4e2d\u200b\u3002\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6709\u200b\u5f88\u591a\u200b\uff0cTransformer \u200b\u4f7f\u7528\u200b\u4e86\u200b\u6b63\u200b\u4f59\u5f26\u200b\u51fd\u6570\u200b\u6765\u200b\u8fdb\u884c\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff0c\u200b\u5176\u200b\u7f16\u7801\u65b9\u5f0f\u200b\u4e3a\u200b\uff1a $$ PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\\ PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}}) $$ \u200b\u200b\u4e0a\u5f0f\u200b\u4e2d\u200b\uff0cpos \u200b\u4e3a\u200b token \u200b\u5728\u200b\u53e5\u5b50\u200b\u4e2d\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c2i \u200b\u548c\u200b 2i+1 \u200b\u5219\u200b\u662f\u200b\u6307\u793a\u200b\u4e86\u200b token \u200b\u662f\u200b\u5947\u6570\u200b\u4f4d\u7f6e\u200b\u8fd8\u662f\u200b\u5076\u6570\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u4ece\u200b\u4e0a\u5f0f\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\u5bf9\u4e8e\u200b\u5947\u6570\u200b\u4f4d\u7f6e\u200b\u7684\u200b token \u200b\u548c\u200b\u5076\u6570\u200b\u4f4d\u7f6e\u200b\u7684\u200b token\uff0cTransformer \u200b\u91c7\u7528\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u7f16\u7801\u200b\u3002\u200b\u6211\u4eec\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b\u6765\u200b\u8bf4\u660e\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\uff1a\u200b\u5047\u5982\u200b\u6211\u4eec\u200b\u8f93\u5165\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200b\u957f\u5ea6\u200b\u4e3a\u200b 4 \u200b\u7684\u200b\u53e5\u5b50\u200b\"I like to code\"\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u4e0b\u9762\u200b\u7684\u200b\u8bcd\u200b\u5411\u91cf\u200b\u77e9\u9635\u200b\\(\\rm x\\)\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u200b\u4e00\u884c\u200b\u4ee3\u8868\u200b\u7684\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u8bcd\u200b\u5411\u91cf\u200b\uff0c\\(\\rm x_0=[0.1,0.2,0.3,0.4]\\)\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5c31\u662f\u200b\u201cI\u201d\u200b\u7684\u200b\u8bcd\u200b\u5411\u91cf\u200b\uff0c\u200b\u5b83\u200b\u7684\u200bpos\u200b\u5c31\u662f\u200b\u4e3a\u200b0\uff0c\u200b\u4ee5\u6b64\u7c7b\u63a8\u200b\uff0c\u200b\u7b2c\u4e8c\u884c\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u201clike\u201d\u200b\u7684\u200b\u8bcd\u200b\u5411\u91cf\u200b\uff0c\u200b\u5b83\u200b\u7684\u200bpos\u200b\u5c31\u662f\u200b1\uff1a $$ \\rm x = \\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\ 0.3 &amp; 0.4 &amp; 0.5 &amp; 0.6 \\ 0.4 &amp; 0.5 &amp; 0.6 &amp; 0.7 \\end{bmatrix} $$ \u200b\u200b\u5219\u200b\u7ecf\u8fc7\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u540e\u200b\u7684\u200b\u8bcd\u200b\u5411\u91cf\u200b\u4e3a\u200b\uff1a $$ \\rm x_{PE} = \\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\ 0.3 &amp; 0.4 &amp; 0.5 &amp; 0.6 \\ 0.4 &amp; 0.5 &amp; 0.6 &amp; 0.7 \\end{bmatrix} + \\begin{bmatrix} \\sin(\\frac{0}{10000^0}) &amp; \\cos(\\frac{0}{10000^0}) &amp; \\sin(\\frac{0}{10000^{2/4}}) &amp; \\cos(\\frac{0}{10000^{2/4}}) \\ \\sin(\\frac{1}{10000^0}) &amp; \\cos(\\frac{1}{10000^0}) &amp; \\sin(\\frac{1}{10000^{2/4}}) &amp; \\cos(\\frac{1}{10000^{2/4}}) \\ \\sin(\\frac{2}{10000^0}) &amp; \\cos(\\frac{2}{10000^0}) &amp; \\sin(\\frac{2}{10000^{2/4}}) &amp; \\cos(\\frac{2}{10000^{2/4}}) \\ \\sin(\\frac{3}{10000^0}) &amp; \\cos(\\frac{3}{10000^0}) &amp; \\sin(\\frac{3}{10000^{2/4}}) &amp; \\cos(\\frac{3}{10000^{2/4}}) \\end{bmatrix} = \\begin{bmatrix} 0.1 &amp; 1.2 &amp; 0.3 &amp; 1.4 \\ 1.041 &amp; 0.84 &amp; 0.41 &amp; 1.49 \\ 1.209 &amp; -0.016 &amp; 0.52 &amp; 1.59 \\ 0.541 &amp; -0.489 &amp; 0.895 &amp; 1.655 \\end{bmatrix} $$ \u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u7684\u200b\u4ee3\u7801\u200b\u6765\u200b\u83b7\u53d6\u200b\u4e0a\u8ff0\u200b\u4f8b\u5b50\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff1a <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\ndef PositionEncoding(seq_len, d_model, n=10000):\n    P = np.zeros((seq_len, d_model))\n    for k in range(seq_len):\n        for i in np.arange(int(d_model/2)):\n            denominator = np.power(n, 2*i/d_model)\n            P[k, 2*i] = np.sin(k/denominator)\n            P[k, 2*i+1] = np.cos(k/denominator)\n    return P\n\nP = PositionEncoding(seq_len=4, d_model=4, n=100)\nprint(P)\n</code></pre> <pre><code>[[ 0.          1.          0.          1.        ]\n [ 0.84147098  0.54030231  0.09983342  0.99500417]\n [ 0.90929743 -0.41614684  0.19866933  0.98006658]\n [ 0.14112001 -0.9899925   0.29552021  0.95533649]]\n</code></pre> \u200b\u8fd9\u6837\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u4e3b\u8981\u200b\u6709\u200b\u4e24\u4e2a\u200b\u597d\u5904\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u200b PE \u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u6bd4\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u91cc\u9762\u200b\u6240\u6709\u200b\u53e5\u5b50\u200b\u66f4\u957f\u200b\u7684\u200b\u53e5\u5b50\u200b\uff0c\u200b\u5047\u8bbe\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u91cc\u9762\u200b\u6700\u957f\u200b\u7684\u200b\u53e5\u5b50\u200b\u662f\u200b\u6709\u200b 20 \u200b\u4e2a\u200b\u5355\u8bcd\u200b\uff0c\u200b\u7a81\u7136\u200b\u6765\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u957f\u5ea6\u200b\u4e3a\u200b 21 \u200b\u7684\u200b\u53e5\u5b50\u200b\uff0c\u200b\u5219\u200b\u4f7f\u7528\u200b\u516c\u5f0f\u200b\u8ba1\u7b97\u200b\u7684\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u7b2c\u200b 21 \u200b\u4f4d\u200b\u7684\u200b Embedding\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5bb9\u6613\u200b\u5730\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u56fa\u5b9a\u200b\u957f\u5ea6\u200b\u7684\u200b\u95f4\u8ddd\u200b k\uff0cPE(pos+k) \u200b\u53ef\u4ee5\u200b\u7528\u200b PE(pos) \u200b\u8ba1\u7b97\u200b\u5f97\u5230\u200b\u3002\u200b\u56e0\u4e3a\u200b Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)\u3002</li> </ol> <p>\u200b\u200b\u5173\u4e8e\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff0c\u200b\u6709\u200b\u8bb8\u591a\u200b\u5b66\u8005\u200b\u4ece\u200b\u6570\u5b66\u200b\u7684\u200b\u89d2\u5ea6\u200b\u8bc1\u660e\u200b\u4e86\u200b\u8be5\u200b\u7f16\u7801\u65b9\u5f0f\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u5176\u4ed6\u200b\u66f4\u200b\u7b80\u5355\u200b\u3001\u200b\u76f4\u89c2\u200b\u7684\u200b\u7f16\u7801\u65b9\u5f0f\u200b\u7684\u200b\u4f18\u8d8a\u6027\u200b\u4e0e\u200b\u5fc5\u8981\u6027\u200b\uff0c\u200b\u7531\u4e8e\u200b\u672c\u6587\u200b\u91cd\u70b9\u200b\u5728\u4e8e\u200b\u4ee3\u7801\u200b\u7684\u200b\u89e3\u6790\u200b\uff0c\u200b\u6b64\u5904\u200b\u4e0d\u518d\u200b\u8d58\u8ff0\u200b\uff0c\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u67e5\u9605\u200b\u76f8\u5173\u200b\u8d44\u6599\u200b\uff0c\u200b\u5982\u200b\u535a\u5ba2\u200b\uff1aTransformer Architecture: The Positional Encoding\u3001A Gentle Introduction to Positional Encoding in Transformer Models \u200b\u7b49\u200b\u3002</p> <p>\u200b\u200b\u7f16\u7801\u200b\u7ed3\u679c\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        # \u200b\u4f4d\u7f6e\u200b\u77e9\u9635\u200b\uff0c\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b0\n        position = torch.arange(0, max_len).unsqueeze(1)\n        # \u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200bPOS\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        # \u200b\u6b64\u5904\u200b\u5bf9\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200b\u6307\u6570\u200b\u8ba1\u7b97\u200b\u53d6\u200b\u4e86\u200b\u4e2a\u200b\u5bf9\u6570\u200b\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # \u200b\u57fa\u4e8e\u200b\u516c\u5f0f\u200b\u8ba1\u7b97\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n        # \u200b\u5b9a\u4e49\u200b\u4e00\u7ec4\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u4e0d\u4f1a\u200b\u6539\u53d8\u200b\u7684\u200b\u53c2\u6570\u200bpe\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n        # \u200b\u5411\u200b\u8bcd\u200b\u5411\u91cf\u200b\u4e2d\u200b\u52a0\u5165\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\n        return self.dropout(x)\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_5","title":"\u6700\u7ec8\u200b\u5efa\u6a21","text":"<p>\u200b\u200b\u5728\u200b\u5b8c\u6210\u200b\u4e0a\u8ff0\u200b\u5b9e\u73b0\u200b\u540e\u200b\uff0c\u200b\u518d\u200b\u5c06\u200b\u5176\u200b\u5404\u4e2a\u200b\u7ec4\u4ef6\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\u5373\u53ef\u200b\uff0c\u200b\u6b64\u5904\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u5efa\u7acb\u200b\u5b8c\u6574\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>def make_model(\n    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    # \u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    # \u200b\u524d\u9988\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\n    position = PositionalEncoding(d_model, dropout)\n    # \u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u5c42\u200b\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        # \u200b\u7f16\u7801\u5668\u200b\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        # \u200b\u89e3\u7801\u5668\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200battn\u200b\u4e3a\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u7b2c\u4e8c\u4e2a\u200battn\u200b\u4e3a\u200b\u52a0\u5165\u200b\u7f16\u7801\u5668\u200b\u8f93\u51fa\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        # \u200b\u8f93\u5165\u200b\u8bed\u6599\u200b\u7f16\u7801\u200b\u5c42\u200b\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        # \u200b\u8f93\u51fa\u200b\u8bed\u6599\u200b\u7f16\u7801\u200b\u5c42\u200b\n        Generator(d_model, tgt_vocab),\n        # \u200b\u5206\u7c7b\u200b\u5c42\u200b\n    )\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            nn.init.xavier_uniform_(p)\n            # \u200b\u521d\u59cb\u5316\u200b\u53c2\u6570\u200b\n    return model\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_6","title":"\u8bad\u7ec3","text":"<p>\u200b\u200b\u57fa\u4e8e\u200b Pytorch \u200b\u5b9e\u73b0\u200b\u7684\u200b Transformer \u200b\u7684\u200b\u8bad\u7ec3\u200b\u5927\u81f4\u200b\u6cbf\u7528\u200b\u4e86\u200b Pytorch \u200b\u7684\u200b\u6846\u67b6\u200b\uff0c\u200b\u6574\u4f53\u200b\u6d41\u7a0b\u200b\u540c\u200b\u4f7f\u7528\u200b Pytorch \u200b\u5efa\u7acb\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u6d41\u7a0b\u200b\u5927\u81f4\u76f8\u540c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u66f4\u200b\u5e95\u5c42\u200b\u7684\u200b\u7ec6\u8282\u200b\u81ea\u5b9a\u4e49\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#traning-loop","title":"Traning Loop","text":"<p>\u200b\u200b\u4f5c\u8005\u200b\u5e76\u200b\u6ca1\u6709\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b Pytorch \u200b\u63d0\u4f9b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\uff0c\u200b\u800c\u662f\u200b\u81ea\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u8bb0\u5f55\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u7c7b\u200b\uff1a</p> <pre><code>class TrainState:\n    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n\n    step: int = 0  # Steps in the current epoch\n    accum_step: int = 0  # Number of gradient accumulation steps\n    samples: int = 0  # total # of examples used\n    tokens: int = 0  # total # of tokens processed\n</code></pre> <p>\u200b   \u200b\u63a5\u7740\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u8be5\u7c7b\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u8fd0\u884c\u200b\u51fd\u6570\u200b\uff1a</p> <pre><code>def run_epoch(\n    data_iter,# \u200b\u6570\u636e\u200b\u96c6\u200b\n    model,# \u200b\u6a21\u578b\u200b\n    loss_compute,# \u200b\u635f\u5931\u200b\u8ba1\u7b97\u200b\u51fd\u6570\u200b\n    optimizer,# \u200b\u4f18\u5316\u200b\u5668\u200b\n    scheduler,# \u200b\u8c03\u5ea6\u200b\u5668\u200b\n    mode=\"train\",# \u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6216\u200b\u6d4b\u8bd5\u200b\n    accum_iter=1,# \u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\u7684\u200b\u8f6e\u6570\u200b\n    train_state=TrainState(),\n):\n    \"\"\"Train a single epoch\"\"\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    n_accum = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(\n            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n        )# \u200b\u6a21\u578b\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\n        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n        # \u200b\u8ba1\u7b97\u200b\u5f53\u4e0b\u200b\u7684\u200b\u6a21\u578b\u200b\u635f\u5931\u200b\n        # loss_node = loss_node / accum_iter\n        if mode == \"train\" or mode == \"train+log\":\n            loss_node.backward()\n            # \u200b\u8fd4\u4e61\u200b\u4f20\u64ad\u200b\n            train_state.step += 1\n            train_state.samples += batch.src.shape[0]\n            train_state.tokens += batch.ntokens\n            if i % accum_iter == 0:\n                # \u200b\u6bcf\u9694\u200b accum_iter \u200b\u8f6e\u200b\u4f18\u5316\u200b\u4e00\u6b21\u200b\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                n_accum += 1\n                train_state.accum_step += 1\n            scheduler.step()\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n        tokens += batch.ntokens\n        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n            lr = optimizer.param_groups[0][\"lr\"]\n            elapsed = time.time() - start\n            print(\n                (\n                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n                )\n                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n            )\n            start = time.time()\n            tokens = 0\n        del loss\n        del loss_node\n    return total_loss / total_tokens, train_state\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_7","title":"\u4f18\u5316\u200b\u5668","text":"<p>\u200b\u200b\u5728\u200b Transformer \u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b Adam \u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u8be5\u200b\u4f18\u5316\u200b\u5668\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u8ba1\u7b97\u200b\u57fa\u4e8e\u200b\u4e0b\u200b\u5f0f\u200b\uff1a $$ \\rm learning rate=d^{-0.5}_{model} * min(step_num^{-0.5}, step_num * warmup_steps^{-1.5}) $$ \u200b\u200b\u4e3a\u4f55\u200b\u9009\u62e9\u200b\u8be5\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8be5\u200b\u4f18\u5316\u200b\u5668\u6709\u200b\u4ec0\u4e48\u200b\u4f18\u52bf\u200b\uff0c\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u67e5\u9605\u200b\u76f8\u5173\u200b\u8d44\u6599\u200b\u5982\u200b\uff1a\u3010Adam\u3011\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\u6d45\u6790\u200b\u6765\u200b\u6df1\u5165\u200b\u63a2\u7a76\u200b\u5176\u200b\u5185\u90e8\u200b\u6570\u5b66\u539f\u7406\u200b\uff0c\u200b\u6b64\u5904\u200b\u4e0d\u518d\u200b\u8d58\u8ff0\u200b\u3002\u200b\u57fa\u4e8e\u200b\u4e0a\u200b\u5f0f\u200b\uff0c\u200b\u5176\u200b\u4f18\u5316\u200b\u5668\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>def rate(step, model_size, factor, warmup):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )\n</code></pre> <p>\u200bPytorch \u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5404\u79cd\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u8c03\u7528\u200b API\uff0c\u200b\u4e5f\u200b\u5305\u62ec\u200b Adam\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8c03\u7528\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_8","title":"\u6b63\u5219\u200b\u5316","text":"<p>\u200b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4f7f\u7528\u200b\u4e86\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u6765\u200b\u5b9e\u73b0\u200b\u6b63\u5219\u200b\u5316\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b KL \u200b\u6563\u5ea6\u200b\u6765\u200b\u8ba1\u7b97\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\uff0c\u200b\u6b64\u5904\u200b\u540c\u6837\u200b\u4e0d\u518d\u200b\u8d58\u8ff0\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u3001KL \u200b\u6563\u5ea6\u200b\u7684\u200b\u539f\u7406\u200b\uff0c\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\u53ef\u4ee5\u200b\u53c2\u9605\u200b\u4e0b\u5217\u200b\u535a\u5ba2\u200b\uff1a\u3010\u200b\u6b63\u5219\u200b\u5316\u200b\u3011Label Smoothing\u200b\u8be6\u89e3\u200b\u3001Kullback-Leibler(KL)\u200b\u6563\u5ea6\u200b\u4ecb\u7ecd\u200b\u3002\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>class LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(reduction=\"sum\")# \u200b\u4f7f\u7528\u200b KL \u200b\u6563\u5ea6\u200b\u8ba1\u7b97\u200b\n        self.padding_idx = padding_idx # \u200b\u906e\u63a9\u200b\u90e8\u5206\u200bindex\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() &gt; 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, true_dist.clone().detach())\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_9","title":"\u603b\u7ed3","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_10","title":"\u4f18\u70b9","text":"<p>\u200bTransformer \u200b\u4f5c\u4e3a\u200b NLP \u200b\u53d1\u5c55\u53f2\u200b\u4e0a\u200b\u91cc\u7a0b\u7891\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u662f\u200b\u5177\u6709\u200b\u8f83\u5927\u200b\u521b\u65b0\u6027\u200b\u548c\u200b\u6307\u5bfc\u6027\u200b\u7684\u200b\u3002\u200b\u5176\u200b\u521b\u9020\u6027\u5730\u200b\u629b\u5f03\u200b\u4e86\u200b\u6cbf\u7528\u200b\u4e86\u200b\u51e0\u5341\u5e74\u200b\u7684\u200b CNN\u3001RNN \u200b\u67b6\u6784\u200b\uff0c\u200b\u5b8c\u5168\u200b\u4f7f\u7528\u200b Attention \u200b\u673a\u5236\u200b\u6765\u200b\u642d\u5efa\u200b\u7f51\u7edc\u200b\u5e76\u200b\u53d6\u5f97\u200b\u4e86\u200b\u826f\u597d\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u5e2e\u52a9\u200b Attention \u200b\u673a\u5236\u200b\u7ad9\u4e0a\u200b\u4e86\u200b\u65f6\u4ee3\u200b\u7684\u200b\u821e\u53f0\u200b\u3002\u200b\u800c\u8bba\u200b\u53ca\u200b\u6a21\u578b\u200b\u672c\u8eab\u200b\uff0cAttention \u200b\u673a\u5236\u200b\u7684\u200b\u4f7f\u7528\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u6709\u6548\u200b\u6355\u6349\u200b\u957f\u8ddd\u79bb\u200b\u76f8\u5173\u6027\u200b\uff0c\u200b\u89e3\u51b3\u200b\u4e86\u200b NLP \u200b\u9886\u57df\u200b\u68d8\u624b\u200b\u7684\u200b\u957f\u8ddd\u200b\u4f9d\u8d56\u200b\u95ee\u9898\u200b\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u629b\u5f03\u200b\u4e86\u200b RNN \u200b\u67b6\u6784\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u5145\u5206\u200b\u5b9e\u73b0\u200b\u5e76\u884c\u200b\u5316\u200b\uff0c\u200b\u63d0\u5347\u200b\u4e86\u200b\u6a21\u578b\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_11","title":"\u7f3a\u70b9","text":"<p>\u200b\u200b\u4e0d\u53ef\u5426\u8ba4\u200b\uff0cTransformer \u200b\u4e5f\u200b\u5b58\u5728\u200b\u8bf8\u591a\u200b\u7f3a\u9677\u200b\u3002\u200b\u6700\u200b\u660e\u663e\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\uff0c\u200b\u63d0\u51fa\u200b\u8be5\u200b\u6a21\u578b\u200b\u7684\u200b\u8bba\u6587\u200b\u540d\u4e3a\u200b\u300aAttention Is All You Need\u300b\uff0c\u200b\u4f46\u200b\u4e8b\u5b9e\u4e0a\u200b\u6211\u4eec\u200b\u771f\u7684\u200b\u4ec5\u4ec5\u200b\u53ea\u200b\u9700\u8981\u200b Attention \u200b\u5417\u200b\uff1f\u200b\u7c97\u66b4\u200b\u7684\u200b\u629b\u5f03\u200b CNN \u200b\u4e0e\u200b RNN \u200b\u867d\u7136\u200b\u975e\u5e38\u200b\u70ab\u6280\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u4f7f\u200b\u6a21\u578b\u200b\u4e27\u5931\u200b\u4e86\u200b\u6355\u6349\u200b\u5c40\u90e8\u200b\u7279\u5f81\u200b\u7684\u200b\u80fd\u529b\u200b\uff0cRNN + CNN + Attention \u200b\u4e5f\u8bb8\u200b\u80fd\u591f\u200b\u5e26\u6765\u200b\u66f4\u597d\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u5176\u6b21\u200b\uff0cAttention \u200b\u673a\u5236\u200b\u5931\u53bb\u200b\u4e86\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u867d\u7136\u200b Transformer \u200b\u4f7f\u7528\u200b\u4e86\u200b Positional Encoding \u200b\u6765\u200b\u8865\u5145\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f46\u200b\u53ea\u662f\u200b\u6743\u5b9c\u4e4b\u8ba1\u200b\uff0c\u200b\u6ca1\u6709\u200b\u6539\u53d8\u200b\u5176\u200b\u7ed3\u6784\u200b\u4e0a\u200b\u7684\u200b\u56fa\u6709\u200b\u7f3a\u9677\u200b\u3002\u200b\u6700\u540e\u200b\uff0cAttention \u200b\u673a\u5236\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u5e9e\u5927\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u95e8\u69db\u200b\u4e0e\u200b\u6210\u672c\u200b\u4e5f\u200b\u6709\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u63d0\u9ad8\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6b63\u662f\u200b\u5728\u200b Transformer \u200b\u63d0\u51fa\u200b\u4e4b\u540e\u200b\uff0c\u200b\u5982\u200b BERT\u3001XLNet \u200b\u7b49\u200b\u57fa\u4e8e\u200b Transformer \u200b\u7ed3\u6784\u200b\u7684\u200b\u5f3a\u5927\u200b\u3001\u200b\u6602\u8d35\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e5f\u200b\u9010\u6b65\u200b\u767b\u4e0a\u200b\u65f6\u4ee3\u200b\u7684\u200b\u821e\u53f0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Transformer%20%E8%A7%A3%E8%AF%BB/#_12","title":"\u53c2\u8003\u200b\u6750\u6599\u200b\uff1a","text":"<ol> <li>Attention Is All You Need</li> <li>https://lilianweng.github.io/posts/2018-06-24-attention/</li> <li>https://zhuanlan.zhihu.com/p/48508221</li> </ol>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/","title":"ViT\u200b\u89e3\u8bfb","text":"<p> [ViT] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. ICLR 2021. [paper] [code] \u200b\u89e3\u200b\u8bfb\u8005\u200b\uff1a\u200b\u725b\u5fd7\u5eb7\u200b\uff0c\u200b\u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66\u200b\u672c\u79d1\u751f\u200b\uff0cDatawhale\u200b\u6210\u5458\u200b\uff1b\u200b\u5c0f\u996d\u200b\u540c\u5b66\u200b\uff0c\u200b\u9999\u6e2f\u57ce\u5e02\u5927\u5b66\u200b\u7814\u7a76\u751f\u200b  </p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#_1","title":"\u524d\u8a00","text":"<p>Transformer \u200b\u5df2\u7ecf\u200b\u6210\u4e3a\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u7684\u200b\u4e00\u79cd\u200b\u57fa\u7840\u200b\u7f51\u7edc\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u7684\u200b\u5e94\u7528\u200b\u4ecd\u7136\u200b\u6709\u9650\u200b\u3002\u200b\u56e0\u4e3a\u200b Transformer \u200b\u5bf9\u200b\u5e8f\u5217\u200b\u8fdb\u884c\u200b\u5efa\u6a21\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u50cf\u200b\u79cd\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u50cf\u7d20\u200b\u90fd\u200b\u4f5c\u4e3a\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5e8f\u5217\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e0e\u200b\u56fe\u7247\u200b\u7684\u200b\u5927\u5c0f\u200b\u5448\u200b\u5e73\u65b9\u200b\u5173\u7cfb\u200b\uff0c\u200b\u5c06\u200b\u5bfc\u81f4\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u5927\u5927\u589e\u52a0\u200b\u3002\u200b\u73b0\u6709\u200b\u7684\u200b\u5de5\u4f5c\u200b\u8981\u4e48\u200b\u662f\u200b\u5c06\u200b\u6ce8\u610f\u529b\u200b\u4e0e\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\u7ed3\u5408\u200b\u4f7f\u7528\u200b\uff0c\u200b\u8981\u4e48\u200b\u7528\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u66ff\u6362\u200b CNN \u200b\u7684\u200b\u67d0\u4e9b\u200b\u7ec4\u4ef6\u200b\u6216\u8005\u200b\u964d\u4f4e\u200b\u56fe\u7247\u200b\u7684\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u6539\u8fdb\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b convolutional neural network (CNN) \u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6784\u5efa\u200b\u7684\u200b\uff0c\u200b\u4e8e\u662f\u200b\u4eba\u4eec\u200b\u5c31\u200b\u5728\u200b\u5e0c\u671b\u200b\u6709\u200b\u4e00\u79cd\u200b\u5b8c\u5168\u200b\u57fa\u4e8e\u200b Transformer \u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u62e5\u6709\u200b Transformer \u200b\u5168\u5c40\u200b\u5efa\u6a21\u200b\u7684\u200b\u7279\u6027\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4e0d\u8fc7\u200b\u591a\u200b\u4fee\u6539\u200b\u539f\u59cb\u200b Transformer \u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u57fa\u4e8e\u200b\u8fd9\u79cd\u200b motivation\uff0c\u200b\u624d\u200b\u51fa\u73b0\u200b\u4e86\u200b Vision Transformer (ViT) \u200b\u8fd9\u7bc7\u200b\u4f18\u79c0\u200b\u7684\u200b\u5de5\u4f5c\u200b\u3002</p> <p>\u200b\u672c\u6587\u200b\u5c06\u200b\u4ece\u200b\u539f\u7406\u200b\u548c\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\uff0c\u200b\u7ed3\u5408\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u9700\u6c42\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7740\u91cd\u200b\u8bb2\u89e3\u200b\u4ee3\u7801\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u8bba\u6587\u200b\u4e2d\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7ec6\u8282\u200b\u8fd8\u200b\u8bf7\u200b\u5404\u4f4d\u200b\u540c\u5b66\u200b\u8be6\u7ec6\u200b\u9605\u8bfb\u200b\u539f\u200b\u8bba\u6587\u200b\u6216\u200b\u5173\u6ce8\u200b Whalepaper \u200b\u540e\u7eed\u200b\u7684\u200b\u8bba\u6587\u200b\u7cbe\u8bfb\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#vit_1","title":"ViT \u200b\u7684\u200b\u6574\u4f53\u200b\u6d41\u7a0b","text":"<p>\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cViT \u200b\u7684\u200b\u4e3b\u8981\u200b\u601d\u60f3\u200b\u662f\u200b\u5c06\u200b\u56fe\u7247\u200b\u5206\u6210\u200b\u4e00\u4e2a\u200b\u4e00\u4e2a\u200b\u7684\u200b\u5c0f\u200b <code>patch</code>\uff0c\u200b\u5c06\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>patch</code> \u200b\u4f5c\u4e3a\u200b\u5e8f\u5217\u200b\u7684\u200b\u5143\u7d20\u200b\u8f93\u5165\u200b Transformer \u200b\u4e2d\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002 </p> <p>\u200b\u5176\u200b\u5177\u4f53\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\uff1a 1. \u200b\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b\uff1a\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u5c06\u200b\u56fe\u7247\u200b\u5207\u200b\u5206\u6210\u200b\u4e00\u4e2a\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u7684\u200b <code>patch</code>\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u62c9\u5e73\u200b <code>Flatten</code> \u200b\u4e3a\u200b\u4e00\u7ef4\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u6700\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5411\u91cf\u200b\u901a\u8fc7\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b <code>Linear Project</code> \\(\\mathbf{E}\\) \u200b\u5230\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b \\(D\\) \u200b\u7684\u200b\u7a7a\u95f4\u200b\u3002 2. \u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff1a\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\uff1a\u200b\u4e3a\u4e86\u200b\u5b9e\u73b0\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u5f97\u5230\u200b\u7684\u200b\u5411\u91cf\u200b\u4e2d\u200b\u9700\u8981\u200b\u52a0\u5165\u200b\u4e00\u4e2a\u200b <code>classs token</code> \\(\\mathbf{x}_\\text{class}\\) \u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\uff08\u200b\u5982\u4e0a\u56fe\u200b\u4e2d\u200b\u6807\u6ce8\u200b \\(*\\)\u200b\u7684\u200b\u7c89\u8272\u200b\u5411\u91cf\u200b\u6240\u793a\u200b\uff09\u3002\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff1a\u200b\u56fe\u50cf\u200b\u548c\u200b\u6587\u672c\u200b\u4e00\u6837\u200b\u4e5f\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u987a\u5e8f\u200b\u95ee\u9898\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f5c\u8005\u200b\u901a\u8fc7\u200b <code>Position Embedding</code> \\(\\mathbf{E}_{pos}\\) \u200b\u52a0\u5165\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u4fe1\u606f\u200b\uff08\u200b\u5982\u4e0a\u56fe\u200b\u4e2d\u200b\u6807\u6ce8\u200b \\(0-9\\) \u200b\u7684\u200b\u7d2b\u8272\u200b\u5411\u91cf\u200b\u6240\u793a\u200b\uff09\u3002 3. Transformer Encoder\uff1a\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ecf\u8fc7\u200b\u4e0a\u9762\u200b\u64cd\u4f5c\u200b\u7684\u200b <code>token</code> \u200b\u9001\u5165\u200b <code>Transformer Encoder</code>\u3002\u200b\u8fd9\u91cc\u200b\u7684\u200b <code>Transformer Encoder</code> \u200b\u548c\u200b <code>Transformer (Attention is All You Need)</code> \u200b\u6587\u7ae0\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u57fa\u672c\u4e00\u81f4\u200b\uff0c\u200b\u4e3b\u8981\u200b\u662f\u200b\u901a\u8fc7\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\uff0c\u200b\u5bf9\u200b <code>patch</code> \u200b\u4e4b\u95f4\u200b\u8fdb\u884c\u200b\u5168\u5c40\u200b\u7684\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u3002 4. \u200b\u8f93\u51fa\u200b\u4e0e\u200b\u5206\u7c7b\u200b\uff1a\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u83b7\u5f97\u200b <code>class token</code> \u200b\u7ecf\u8fc7\u200b <code>Transformer Encoder</code> \u200b\u5f97\u5230\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u52a0\u200b\u4e00\u4e2a\u200b <code>MLP Head</code> \u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8bba\u6587\u200b\u4ee3\u7801\u200b\u7684\u200b\u8bb2\u89e3\u200b\u4e5f\u200b\u5c06\u200b\u6309\u7167\u200b\u4e0a\u9762\u200b\u7684\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5bf9\u200b\u91cd\u8981\u200b\u6a21\u5757\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u6240\u200b\u5c55\u793a\u200b\u7684\u200bViT\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\u6765\u6e90\u4e8e\u200brwightman/timm\u200b\u5e76\u200b\u8fdb\u884c\u200b\u4e86\u200b\u90e8\u5206\u200b\u7b80\u5316\u200b\uff0c\u200b\u5728\u200b\u6b64\u200b\u611f\u8c22\u200b\u6bcf\u200b\u4e00\u4f4d\u200b\u5f00\u6e90\u200b\u8d21\u732e\u8005\u200b\u6240\u200b\u4f5c\u51fa\u200b\u7684\u200b\u8d21\u732e\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#patch-embedding-linear-projection","title":"\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b Patch Embedding + Linear Projection","text":"<p>\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b \\(\\mathbf{x}\\)\uff0c\u200b\u5176\u200b\u5206\u8fa8\u7387\u200b\u4e3a\u200b \\(H \\times W \\times C\\)\u3002\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u8ba8\u8bba\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d6\u200b ViT \u200b\u7684\u200b\u6807\u51c6\u200b\u8f93\u5165\u200b \\(H \\times W \\times C = 224 \\times 224 \\times 3\\) \u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u5177\u4f53\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u8bb2\u89e3\u200b\u3002\u200b\u901a\u8fc7\u200b\u5207\u5206\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6574\u4e2a\u200b\u56fe\u7247\u200b\u5206\u6210\u200b\u591a\u4e2a\u200b <code>patch</code> \\(\\mathbf{x}_p\\)\uff0c\u200b\u5176\u200b\u5927\u5c0f\u200b\u4e3a\u200b \\(\\(P \\times P \\times C = 16 \\times 16 \\times 3 = 768\u3002\\)\\) \u200b\u8fd9\u6837\u200b\uff0c\u200b\u4e00\u5171\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b <code>Patch</code> \u200b\u7684\u200b\u6570\u91cf\u200b\u4e3a\u200b \\(\\(N={(H \\times W)}/{(P \\times P)} = {(224 \\times 224)}/{(16 \\times 16)} = {(224 / 16)}\\times {(224 / 16)} = 14 \\times 14 = 196\u3002\\)\\) \u200b\u6240\u4ee5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e00\u5f20\u200b \\(224 \\times 224 \\times 3\\) \u200b\u7684\u200b\u6807\u51c6\u200b\u56fe\u7247\u200b\uff0c \u200b\u901a\u8fc7\u200b\u8f6c\u6362\u200b\u5f97\u5230\u200b\u4e86\u200b \\(196\\) \u200b\u4e2a\u200b <code>patch</code>\uff0c\u200b\u6bcf\u4e2a\u200b <code>patch</code> \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u662f\u200b \\(768\\)\u3002</p> <p>\u200b\u5bf9\u200b\u5f97\u5230\u200b\u7684\u200b <code>patch</code> \u200b\u901a\u8fc7\u200b \\(\\mathbf{E} \\in {\\mathbb{R}^{768 \\times D}}\\) \u200b\u8fdb\u884c\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u5230\u200b\u7ef4\u5ea6\u200b \\(D\\)\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6620\u5c04\u200b\u540e\u200b\u7684\u200b <code>patch</code> \u200b\u53eb\u505a\u200b <code>token</code>\uff0c\u200b\u4ee5\u4fbf\u200b\u4e8e\u200b\u548c\u200b\u539f\u672c\u200b Transformer \u200b\u7684\u200b\u672f\u8bed\u200b\u8fdb\u884c\u200b\u7edf\u4e00\u200b\uff08\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u9ed8\u8ba4\u200b\u7684\u200b \\(D\\) \u200b\u4ecd\u7136\u200b\u4e3a\u200b \\(768\\)\u3002\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4e0d\u200b\u635f\u5931\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u91cc\u200b \\(D\\) \u200b\u6ee1\u8db3\u200b\u5927\u4e8e\u200b\u7b49\u4e8e\u200b \\(768\\) \u200b\u5373\u53ef\u200b\uff09\u3002\u200b\u5bf9\u5e94\u200b\u6587\u4e2d\u200b\u516c\u5f0f\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4e3a\u200b\uff1a $$ \\begin{align} [\\mathbf{x}_p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E}; \\cdots; \\mathbf{x}_p^N\\mathbf{E}], \\quad \\mathbf{E}\\in\\mathbb{R}^{(P^2\\cdot C)\\times D}\u3002 \\end{align} $$</p> <p>\u200b\u4ee5\u4e0a\u200b\u662f\u200b\u6309\u7167\u200b\u539f\u200b\u8bba\u6587\u200b\u5bf9\u200b\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b\u7684\u200b\u8bb2\u89e3\u200b\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u4e8c\u7ef4\u200b\u5377\u79ef\u200b <code>nn.Conv2d()</code> \u200b\u4e00\u6b65\u200b\u5b8c\u6210\u200b\u7684\u200b\u3002\u200b\u4e3a\u4e86\u200b\u5b9e\u73b0\u200b\u4e00\u6b65\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f5c\u8005\u200b\u5c06\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u5927\u5c0f\u200b <code>kernal_size</code> \u200b\u76f4\u63a5\u200b\u8bbe\u7f6e\u200b\u4e3a\u4e86\u200b <code>patch_size</code>\uff0c\u200b\u5373\u200b \\(P=16\\)\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u5c06\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u6b65\u957f\u200b <code>stride</code> \u200b\u4e5f\u200b\u8bbe\u7f6e\u200b\u4e3a\u4e86\u200b\u540c\u6837\u200b\u7684\u200b <code>patch_size</code>\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e0d\u200b\u91cd\u590d\u200b\u7684\u200b\u5207\u5272\u200b\u56fe\u7247\u200b\u3002\u200b\u800c\u200b\u5377\u79ef\u200b\u7684\u200b\u7279\u5f81\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5206\u522b\u200b\u8bbe\u200b\u4e3a\u4e86\u200b \\(C=3\\) \u200b\u548c\u200b \\(D=768\\)\uff0c\u200b\u5bf9\u5e94\u200b\u4e0b\u65b9\u200b\u4ee3\u7801\u200b\u7684\u200b <code>in_c</code> \u200b\u548c\u200b <code>embed_dim</code>\u3002 <pre><code>self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n</code></pre> \u200b\u4e00\u5f20\u200b \\(1 \\times 3 \\times 224 \\times 224\\) \u200b\u7684\u200b\u56fe\u50cf\u200b\uff08\u200b\u5176\u4e2d\u200b \\(1\\) \u200b\u662f\u200b <code>batch_size</code> \u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff09\uff0c\u200b\u7ecf\u8fc7\u200b\u4e0a\u8ff0\u200b\u5377\u79ef\u200b\u64cd\u4f5c\u200b\u5f97\u5230\u200b \\(1 \\times 768 \\times 14 \\times 14\\) \u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\uff08\u200b\u4ee3\u7801\u200b\u4e2d\u5c06\u200b \\(14 \\times 14 = 196\\) \u200b\u5f53\u4f5c\u200b <code>grid</code> \u200b\u7684\u200b\u4e2a\u6570\u200b\uff0c\u200b\u5373\u200b <code>grid_size=(14, 14)</code>\uff09\u200b\u7136\u540e\u200b\uff0c\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u62c9\u5e73\u200b <code>flatten(2)</code> \u200b\u5f97\u5230\u200b \\(1 \\times 768 \\times 196\\) \u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\u200b\u56e0\u4e3a\u200b Transformer \u200b\u9700\u8981\u200b\u5c06\u200b\u5e8f\u5217\u200b\u7ef4\u5ea6\u200b\u8c03\u6574\u200b\u5230\u200b\u524d\u9762\u200b\uff0c\u200b\u6211\u4eec\u200b\u518d\u200b\u901a\u8fc7\u200b <code>transpose(1, 2)</code> \u200b\u8c03\u6574\u200b\u7279\u5f81\u200b\u548c\u200b\u5e8f\u5217\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u6700\u7ec8\u200b\u5f97\u5230\u200b\u7684\u200b\u5f20\u91cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b \\(1 \\times 196 \\times 768\\)\u3002\u200b\u5207\u5206\u200b\u3001\u200b\u6620\u5c04\u200b\u3001\u200b\u62c9\u5e73\u200b\u548c\u200b\u7ef4\u5ea6\u200b\u8c03\u6574\u200b\u7edf\u7edf\u200b\u7ecf\u8fc7\u200b\u4e0b\u9762\u200b\u4e00\u6b65\u200b\u64cd\u4f5c\u200b\u5f97\u5230\u200b\uff1a <pre><code>x = self.proj(x).flatten(2).transpose(1, 2)\n</code></pre></p> <p>\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u5168\u90e8\u200b\u88ab\u200b\u5199\u200b\u5728\u200b\u540d\u4e3a\u200b <code>PatchEmbed</code> \u200b\u7684\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c\u200b\u5176\u200b\u5177\u4f53\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p><pre><code>class PatchEmbed(nn.Module):\n    \"\"\"\n    Image --&gt; Patch Embedding --&gt; Linear Proj --&gt; Pos Embedding\n    Image size -&gt; [224,224,3]\n    Patch size -&gt; 16*16\n    Patch num -&gt; (224^2)/(16^2)=196\n    Patch dim -&gt; 16*16*3 =768\n    Patch Embedding: [224,224,3] -&gt; [196,768]\n    Linear Proj: [196,768] -&gt; [196,768]\n    Positional Embedding: [197,768] -&gt; [196,768]\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n        \"\"\"\n        Args:\n            img_size: \u200b\u9ed8\u8ba4\u200b\u53c2\u6570\u200b224\n            patch_size: \u200b\u9ed8\u8ba4\u200b\u53c2\u6570\u200b\u662f\u200b16\n            in_c: \u200b\u8f93\u5165\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\n            embed_dim: 16*16*3 = 768\n            norm_layer: \u200b\u662f\u5426\u200b\u4f7f\u7528\u200bnorm\u200b\u5c42\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u5426\u200b\n        \"\"\"\n        super().__init__()\n        img_size = (img_size, img_size) # -&gt; img_size = (224,224)\n        patch_size = (patch_size, patch_size) # -&gt; patch_size = (16,16)\n        self.img_size = img_size # -&gt; (224,224)\n        self.patch_size = patch_size # -&gt; (16,16)\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # -&gt; grid_size = (14,14)\n        self.num_patches = self.grid_size[0] * self.grid_size[1] # -&gt; num_patches = 196\n        # Patch+linear proj\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b [224,224,3] --&gt; [14,14,768]\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n        # \u200b\u5224\u65ad\u200b\u662f\u5426\u200b\u6709\u200bnorm_layer\u200b\u5c42\u200b\uff0c\u200b\u8981\u662f\u200b\u6ca1\u6709\u200b\u4e0d\u200b\u6539\u53d8\u200b\u8f93\u5165\u200b\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        # \u200b\u8ba1\u7b97\u200b\u5404\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5927\u5c0f\u200b\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n\n        # flatten: [B, C, H, W] -&gt; [B, C, HW], flatten(2)\u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u4ece\u200b2\u200b\u4f4d\u7f6e\u200b\u5f00\u59cb\u200b\u5c55\u5f00\u200b\n        # eg: [1,3,224,224] --&gt; [1,768,14,14] -flatten-&gt;[1,768,196]\n        # transpose: [B, C, HW] -&gt; [B, HW, C]\n        # eg: [1,768,196] -transpose-&gt; [1,196,768]\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x\n</code></pre> **\u200b\u5728\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u662f\u200b\u4e0d\u200b\u8fdb\u884c\u200b <code>layer_norm</code> \u200b\u64cd\u4f5c\u200b\u7684\u200b\uff0c\u200b\u5373\u200b\u5b83\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>nn.Identity()</code>\u3002\u200b\u5bf9\u4e8e\u200b <code>layer_norm</code>\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u4e0b\u9762\u200b\u8fdb\u884c\u200b\u8be6\u7ec6\u200b\u7684\u200b\u8bb2\u89e3\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#class-token-postional-embedding","title":"\u5206\u7c7b\u200b\u8868\u5f81\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b Class Token + Postional Embedding","text":"<p>\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5de6\u4fa7\u200b\u7070\u8272\u200b\u90e8\u5206\u200b\u4e3a\u200b\u52a0\u5165\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u7d2b\u8272\u200b\u90e8\u5206\u200b\u4e3a\u200b\u52a0\u5165\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u3002 </p> <p>\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\uff1aClass Token \u200b\u4e3a\u4e86\u200b\u5b9e\u73b0\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b\u540e\u200b\u7684\u200b\u5411\u91cf\u200b \\([\\mathbf{x}_p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E}; \\cdots; \\mathbf{x}_p^N\\mathbf{E}]\\) \u200b\u4e2d\u200b\u52a0\u5165\u200b\u4e00\u4e2a\u200b <code>class token</code> \\(\\mathbf{x}_\\text{class} \\in \\mathbb{R}^{D}\\) \u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\uff08\u200b\u5982\u4e0a\u56fe\u200b\u4e2d\u200b\u6700\u200b\u5de6\u4fa7\u200b\u6df1\u7070\u8272\u200b\u6846\u200b\u6240\u793a\u200b\uff09\u3002\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u8868\u5f81\u200b\u653e\u7f6e\u200b\u5728\u200b\u5e8f\u5217\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u4f4d\u7f6e\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b \\((196+1) \\times 768\\) \u200b\u7684\u200b\u65b0\u200b\u5f20\u91cf\u200b\uff1a $$ \\begin{align} [\\mathbf{x}_{\\text{class}}; \\mathbf{x}_p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E}; \\cdots; \\mathbf{x}_p^N\\mathbf{E}]  \\end{align} $$ \u200b\u5bf9\u4e8e\u200b\u5177\u4f53\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b <code>nn.Parameter(torch.zeros(1, 1, 768))</code> \u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b <code>cls_token</code>\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8fd9\u4e2a\u200b <code>cls_token</code> \u200b\u6309\u7167\u200b <code>batch_size = x.shape[0]</code> \u200b\u8fdb\u884c\u200b\u590d\u5236\u200b\uff0c\u200b\u6700\u540e\u200b\u5c06\u200b\u5176\u200b\u548c\u200b\u4e4b\u524d\u200b\u7ecf\u8fc7\u200b\u5207\u5206\u200b\u548c\u200b\u6620\u5c04\u200b\u7684\u200b <code>x</code> \u200b\u5e76\u200b\u5728\u200b\u4e00\u8d77\u200b <code>torch.cat((cls_token, x), dim=1)</code>\u3002\u200b\u5176\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a <pre><code>cls_token = nn.Parameter(torch.zeros(1, 1, 768)) # -&gt; cls token\nnn.init.trunc_normal_(self.cls_token, std=0.02) # \u200b\u521d\u59cb\u5316\u200b\ncls_token = cls_token.expand(x.shape[0], -1, -1) # (1,1,768) -&gt; (128,1,768)\nx = torch.cat((cls_token, x), dim=1)  # [128, 197, 768]\n</code></pre></p> <p>**\u200b\u5176\u5b9e\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4e0d\u200b\u52a0\u5165\u200b\u8fd9\u4e2a\u200b <code>cls token</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u8f93\u51fa\u200b <code>token</code> \u200b\u505a\u200b <code>GAP(Global Average Pooling)</code>\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b <code>GAP</code> \u200b\u7684\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u3002</p> <p>\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff1aPostional Embedding \u200b\u56fe\u50cf\u200b\u548c\u200b\u6587\u672c\u200b\u4e00\u6837\u200b\u4e5f\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u987a\u5e8f\u200b\u95ee\u9898\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f5c\u8005\u200b\u901a\u8fc7\u200b <code>Position Embedding</code> \\(\\mathbf{E}_{\\text{pos}}\\in\\mathbb{R}^{(N + 1)\\times D}\\) \u200b\u52a0\u5165\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u4fe1\u606f\u200b\u3002\u200b\u8fd9\u4e2a\u200b <code>Position Embedding</code> \u200b\u548c\u200b\u4e0a\u9762\u200b\u5f97\u5230\u200b\u7684\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\u5f20\u91cf\u200b\uff0c\u200b\u76f4\u63a5\u200b\u76f8\u52a0\u200b\uff1a $$ \\begin{align} \\mathbf{z}0 &amp;= [\\mathbf{x}{\\text{class}}; \\mathbf{x}p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E}; \\cdots; \\mathbf{x}_p^N\\mathbf{E};] + \\mathbf{E}{\\text{pos}}, &amp; \\mathbf{E}&amp;\\in\\mathbb{R}^{(P^2\\cdot C)\\times D}, \\mathbf{E}_{\\text{pos}}\\in\\mathbb{R}^{(N + 1)\\times D} \\end{align} $$</p> <p>\u200b\u4e0e\u200b Transformer \u200b\u4f7f\u7528\u200b\u4f59\u5f26\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u4e0d\u540c\u200b\u7684\u200b\u662f\u200b\uff0cViT \u200b\u901a\u8fc7\u200b<code>nn.Parameter()</code>\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u3002 <pre><code>num_patches = 196\npos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, 768))\nx = x + pos_embed\n</code></pre> **\u200b\u8fd9\u91cc\u200b <code>pos_embed</code> \u200b\u5728\u200b <code>batch_size</code> \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u4e86\u200b boardcast\uff0c\u200b\u6240\u4ee5\u200b\u6240\u6709\u200b\u7684\u200b\u6837\u672c\u200b\u90fd\u200b\u662f\u200b\u540c\u6837\u200b\u7684\u200b <code>pos_embed</code>\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#transformer-encoder","title":"Transformer Encoder","text":"<p>\u200b\u4e0b\u200b\u4e00\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u5e8f\u5217\u200b \\(\\mathbf{z}_0\\) \u200b\u8f93\u5165\u200b Transformer Encoder \u200b\u5373\u53ef\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u6bcf\u4e2a\u200b Transformer Encoder \u200b\u7531\u200b Multi-head Attention\u3001MLP\u3001Norm (Layer Norm,LN) \u200b\u5e76\u200b\u5916\u52a0\u200b shortcut \u200b\u8fde\u63a5\u200b\u5b9e\u73b0\u200b\u3002 $$ \\begin{align} \\mathbf{z}'l &amp;= \\text{MSA}(\\text{LN}(\\mathbf{z}{l-1})) + \\mathbf{z}_{l-1}, &amp; l &amp;=1\\dots L, \\ \\mathbf{z}_l &amp;= \\text{MLP}(\\text{LN}(\\mathbf{z}'_l)) + \\mathbf{z}'_l,  &amp; l &amp;=1\\dots L, \\ \\mathbf{y} &amp;= \\text{LN}(\\mathbf{z}_L^0) \\end{align} $$  <p> \u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u6a21\u5757\u200b\u9010\u4e00\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#multi-head-attention","title":"Multi-head Attention","text":"<p>Multi-head Attention \u200b\u6216\u8005\u200b\u53eb\u505a\u200b Multi-head Self-Attention (MSA) \u200b\u662f\u200b\u7531\u200b\u591a\u4e2a\u200b Self-attention (SA) \u200b\u6a21\u5757\u200b\u7ec4\u6210\u200b\uff0c\u200b\u5b83\u4eec\u200b\u7684\u200b\u6846\u56fe\u200b\u53ef\u200b\u7531\u200b\u4e0b\u9762\u200b\u6240\u793a\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5de6\u4fa7\u200b\u4e3a\u200b SA\uff0c\u200b\u53f3\u4fa7\u200b\u4e3a\u200b MSA\u3002   \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7  <p></p> <p>\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u200b\u7684\u200b SA \u200b\u6a21\u5757\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b \\(\\mathbf{z}\\) \u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u6620\u5c04\u200b \\(\\mathbf{W_{SA}}\\) \u200b\u5f97\u5230\u200b \\(Q, K, V\\) $$ [Q, K, V] = \\mathbf{z} \\mathbf{W}_{\\text{SA}}. $$ \u200b\u5bf9\u4e8e\u200b MSA\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b\u5176\u200b\u8f93\u5165\u200b\u518d\u6b21\u200b\u8fdb\u884c\u200b\u5207\u200b\u5206\u4e3a\u200b \\(k\\) \u200b\u4e2a\u200b\u90e8\u5206\u200b \uff08\\(k=\\)<code>self.num_heads</code>\uff09\uff0c\u200b\u800c\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b\u539f\u672c\u200b\u7ef4\u5ea6\u200b\u7684\u200b \\(k\\) \u200b\u5206\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u5373\u200b <code>C // self.num_heads</code>\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u5c06\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\uff0c\u200b\u5373\u200b <code>q, k, v</code> \u200b\u5230\u200b\u7b2c\u200b 1 \u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c \u200b\u6279\u200b\u5927\u5c0f\u200b <code>batch_size</code> \u200b\u4e3a\u200b\u7b2c\u200b 2 \u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5934\u200b\u7684\u200b\u6570\u91cf\u200b\u6570\u91cf\u200b <code>num_heads</code> \u200b\u4e3a\u200b\u7b2c\u200b 3 \u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5207\u5206\u200b\u5757\u200b\u7684\u200b\u6570\u91cf\u200b <code>num_patches</code> \u200b\u548c\u200b\u6bcf\u4e2a\u200b\u5934\u200b\u7684\u200b\u7279\u5f81\u200b\u7ef4\u5ea6\u200b <code>embed_dim_per_head</code> \u200b\u4e3a\u200b\u6700\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u8fd9\u79cd\u200b\u7ef4\u5ea6\u200b\u8c03\u6574\u200b\uff0c\u200b\u5c06\u200b\u65b9\u4fbf\u200b\u63d0\u53d6\u200b <code>q, k, v</code>\uff0c\u200b\u4ee5\u53ca\u200b\u540e\u9762\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u8ba1\u7b97\u200b\u3002\u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u5bf9\u5e94\u200b\uff1a <pre><code>self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\nqkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\nq, k, v = qkv[0], qkv[1], qkv[2]  # seperate q, k, v\n</code></pre></p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>head</code>\uff0c\u200b\u770b\u4f5c\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5355\u5143\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6bcf\u200b\u4e00\u4e2a\u200b<code>head</code> \u200b\u8fdb\u884c\u200b\u6807\u51c6\u200b\u7684\u200b SA \u200b\u8ba1\u7b97\u200b $$ Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt {D_k}}) \\cdot V $$ \u200b\u7136\u540e\u200b\uff0c\u200b\u8fd9\u4e9b\u200b <code>head</code> \u200b\u4f1a\u200b\u88ab\u200b\u62fc\u63a5\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6700\u7ec8\u200b\u7684\u200b\u8f93\u51fa\u200b\uff1a $$ \\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\     \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i) $$</p> <p>\u200b\u5176\u4e2d\u200b \\(W^O\\) \u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u7ebf\u6027\u53d8\u6362\u200b\u5c42\u200b\uff0c\\(head_i\\) \u200b\u4ee3\u8868\u200b\u7684\u200b\u662f\u200b\u6bcf\u4e2a\u200b <code>head</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5176\u4e2d\u200b \\(W^Q_i\\)\uff0c\\(W^K_i\\), \\(W^V_i\\)\uff0c\u200b\u7b49\u4ef7\u200b\u4e8e\u200b\u6bcf\u4e2a\u200b <code>head</code> \u200b\u7684\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u6743\u91cd\u200b\uff08\u200b\u5982\u200b\u4e0a\u9762\u200b\u8ba1\u7b97\u200b <code>qkv</code>\u200b\u6240\u200b\u8bb2\u200b\uff0c\u200b\u5b9e\u9645\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u5148\u200b\u4e00\u8d77\u200b\u8ba1\u7b97\u200b <code>qkv</code>\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200b <code>head</code> \u200b\u7684\u200b\u5207\u5206\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u6309\u7167\u200b\u9ed8\u8ba4\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4e00\u822c\u200b\u5207\u200b\u5206\u4e3a\u200b \\(k=8\\) \u200b\u4e2a\u5934\u200b\uff0c\u200b\u5176\u4e2d\u200b \\(D_k=D/k = 768/8=96\\)\uff0c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5f52\u4e00\u5316\u200b\u70b9\u200b\u4e58\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u5728\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f5c\u8005\u200b\u5145\u5206\u8003\u8651\u200b\u4e86\u200b\u591a\u5934\u200b\u7684\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u3002\u200b\u901a\u8fc7\u200b\u70b9\u200b\u4e58\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5bf9\u200b\u6240\u6709\u200b\u7684\u200b <code>head</code> \u200b\u4e00\u8d77\u200b\u8ba1\u7b97\u200b\u76f8\u5173\u6027\u200b <code>(q @ k.transpose(-2, -1))</code>\uff0c\u200b\u7136\u540e\u200b\u7ecf\u8fc7\u200b <code>softmax</code> \u200b\u5f97\u5230\u200b\u6743\u91cd\u200b <code>attn</code> \uff08\u200b\u8fd9\u4e9b\u200b\u6743\u91cd\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>[batch_size, num_heads, num_patches + 1, num_patches + 1]</code>\uff09\u3002 \u200b\u4e4b\u540e\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6743\u91cd\u200b <code>attn</code> \u200b\u548c\u200b <code>v</code> \uff08\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>[batch_size, num_heads, num_patches+1, embed_dim_per_head]</code>\uff09 \u200b\u8fdb\u884c\u200b\u70b9\u200b\u4e58\u200b\uff0c\u200b\u5f97\u5230\u200b\u6ce8\u610f\u529b\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u3002\u200b\u8fd9\u91cc\u200b\u5728\u200b\u70b9\u200b\u4e58\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u770b\u200b <code>attn</code> \u200b\u548c\u200b <code>v</code>\u200b\u7684\u200b\u6700\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5206\u522b\u200b\u4e3a\u200b<code>[num_patches + 1, num_patches + 1]</code> \u200b\u548c\u200b <code>[num_patches+1, embed_dim_per_head]</code>\uff0c\u200b\u7ef4\u6301\u200b\u5176\u4ed6\u200b\u7ef4\u5ea6\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ed3\u679c\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>[batch_size, num_heads, num_patches + 1, embed_dim_per_head]</code>\u3002 \u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5c06\u200b\u7279\u5f81\u200b\u7ef4\u5ea6\u200b\u548c\u200b\u591a\u5934\u200b\u7ef4\u5ea6\u200b\u4ea4\u6362\u200b <code>transpose(1, 2)</code> \u200b\u548c\u200b \u200b\u91cd\u7ec4\u200b\u7b2c\u200b2\u200b\u4e2a\u200b\u53ca\u200b\u540e\u9762\u200b\u6240\u6709\u200b\u7684\u200b\u7ef4\u5ea6\u200b <code>reshape(B, N, C)</code>\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>[batch_size, num_patches + 1, total_embed_dim]</code> \u200b\u548c\u200b\u4e0a\u9762\u200b\u516c\u5f0f\u200b\u76f8\u540c\u200b\u7684\u200b\u5e76\u884c\u200b\u591a\u5934\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u3002\u200b\u5176\u200b\u5b8c\u6574\u200b\u5b9e\u73b0\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b <pre><code>class Attention(nn.Module):\n    def __init__(self,\n                 dim,   # \u200b\u8f93\u5165\u200btoken\u200b\u7684\u200bdim\n                 num_heads=8, # attention head\u200b\u7684\u200b\u4e2a\u6570\u200b\n                 qkv_bias=False, # \u200b\u662f\u5426\u200b\u4f7f\u7528\u200bqkv bias\n                 qk_scale=None,\n                 attn_drop_ratio=0.,\n                 proj_drop_ratio=0.):\n        super(Attention, self).__init__()\n        self.num_heads = num_heads\n        # \u200b\u8ba1\u7b97\u200b\u6bcf\u200b\u4e00\u4e2a\u200bhead\u200b\u5904\u7406\u200b\u7684\u200b\u7ef4\u5ea6\u200bhead_dim = dim // num_heads --&gt; 768/8 = 96\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5 # \u200b\u6839\u4e0b\u200bdk\u200b\u64cd\u4f5c\u200b\n        # \u200b\u4f7f\u7528\u200bnn.Linear\u200b\u751f\u6210\u200bw_q,w_k,w_v\uff0c\u200b\u56e0\u4e3a\u200b\u672c\u8d28\u200b\u4e0a\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u53d8\u6362\u200b\u77e9\u9635\u200b\u90fd\u200b\u662f\u200b\u7ebf\u6027\u53d8\u6362\u200b\uff0c\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop_ratio)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop_ratio)\n\n    def forward(self, x):\n        # [batch_size, num_patches + 1, total_embed_dim]\n        # total_embed_dim\u200b\u4e0d\u662f\u200b\u4e00\u200b\u5f00\u59cb\u200b\u5c55\u5f00\u200b\u7684\u200b\u90a3\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u662f\u200b\u7ecf\u8fc7\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u53d8\u6362\u200b\u5c42\u200b\u5f97\u5230\u200b\u7684\u200b\n        B, N, C = x.shape\n\n        # [batch_size, num_patches+1, total_embed_dim] -qkv()-&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]\n        # reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n        # permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # q,k,v = [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        # transpose(-2,-1)\u200b\u5728\u200b\u6700\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8f93\u5165\u200b\u7684\u200b\u5f62\u72b6\u200b[batch_size,num_heads,num_patches+1,embed_dim_per_head]\n        # transpose: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n        # transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n        # reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#mlp","title":"MLP","text":"<p>MLP\u200b\u5c42\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u539f\u59cb\u200bTransformer\u200b\u4e2d\u200b\u7684\u200bFeed Forward Network\u3002</p> <p>In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.</p> <p>\u200b\u4e3a\u4e86\u200b\u7406\u89e3\u200b\u8fd9\u53e5\u200b\u8bdd\u200b\uff0c\u200b\u5373\u200b MLP \u200b\u53ea\u200b\u5bf9\u200b\u5c40\u90e8\u200b\u4fe1\u606f\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5f3a\u8c03\u200b <code>nn.Linear()</code> \u200b\u64cd\u4f5c\u200b\u53ea\u200b\u5bf9\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\u3002\u200b\u90a3\u4e48\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>[batch_size, num_patches + 1, total_embed_dim]</code>\uff0c\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u7ebf\u6027\u200b\u5c42\u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b <code>patch</code> \u200b\u90fd\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\u3002\u200b\u6240\u4ee5\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5c40\u90e8\u200b\u4fe1\u606f\u200b\u7684\u200b\u5efa\u6a21\u200b\u3002\u200b\u5bf9\u4e8e\u200b Attention\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u662f\u200b\u5728\u200b\u4e0d\u540c\u200b\u7684\u200b <code>patch</code> \u200b\u5c42\u9762\u200b\u6216\u8005\u200b\u4e0d\u540c\u200b\u7684\u200b\u5e8f\u5217\u200b\u5c42\u9762\u200b\u8fdb\u884c\u200b\u5efa\u6a21\u200b\uff0c\u200b\u6240\u4ee5\u200b\u662f\u200b\u5168\u5c40\u200b\u4fe1\u606f\u200b\u5efa\u6a21\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4f5c\u8005\u200b\u4f7f\u7528\u200b\u4e86\u200b MLP \u200b\u548c\u200b Attention \u200b\u4e00\u8d77\u200b\u8fdb\u884c\u200b\u5c40\u90e8\u200b\u548c\u200b\u5168\u5c40\u200b\u4fe1\u606f\u200b\u7684\u200b\u63d0\u53d6\u200b\u3002</p> <pre><code>class Mlp(nn.Module):\n    \"\"\"\n    in_features --&gt; hidden_features --&gt; out_features\n    \u200b\u8bba\u6587\u200b\u5b9e\u73b0\u200b\u65f6\u200b\uff1ain_features.shape = out_features.shape\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        # \u200b\u7528\u200bor\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u6216\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5f53\u200bhidden_features/out_features\u200b\u4e3a\u200b\u9ed8\u8ba4\u503c\u200bNone\u200b\u65f6\u200b\n        # \u200b\u6b64\u65f6\u200bout_features/hidden_features=None or in_features = in_features\n        # \u200b\u5f53\u5bf9\u200bout_features\u200b\u6216\u200bhidden_features\u200b\u8fdb\u884c\u200b\u8f93\u5165\u200b\u65f6\u200b\uff0cor\u200b\u64cd\u4f5c\u200b\u5c06\u4f1a\u200b\u9ed8\u8ba4\u200b\u9009\u62e9\u200bor\u200b\u524d\u9762\u200b\u7684\u200b\n        # \u200b\u6b64\u65f6\u200bout_features/hidden_features = out_features/hidden_features\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        # in_features --&gt; hidden_features --&gt; out_features\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#layer-norm","title":"Layer Norm","text":"<p>Normalization \u200b\u6709\u200b\u5f88\u200b\u591a\u79cd\u200b\uff0c\u200b\u4f46\u662f\u200b\u5b83\u4eec\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5171\u540c\u200b\u7684\u200b\u76ee\u7684\u200b\uff0c\u200b\u90a3\u200b\u5c31\u662f\u200b\u628a\u200b\u8f93\u5165\u200b\u8f6c\u5316\u6210\u200b\u5747\u503c\u200b\u4e3a\u200b 0 \u200b\u65b9\u5dee\u200b\u4e3a\u200b 1 \u200b\u7684\u200b\u6570\u636e\u200b\uff08\u200b\u6216\u8005\u200b\u67d0\u4e2a\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u5747\u503c\u200b\u548c\u200b\u65b9\u5dee\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u628a\u200b\u6570\u636e\u200b\u9001\u5165\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u4e4b\u524d\u200b\u8fdb\u884c\u200b Normalization\uff08\u200b\u5f52\u4e00\u5316\u200b\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u5e0c\u671b\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u843d\u200b\u5728\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u9971\u548c\u200b\u533a\u200b\u3002</p> <p>Batch Norm \u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u5728\u200b\u5bf9\u200b\u8fd9\u6279\u200b\u6837\u672c\u200b\u7684\u200b\u540c\u4e00\u200b\u7ef4\u5ea6\u200b\u7279\u5f81\u200b\u505a\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u800c\u200b Layer Norm \u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u5bf9\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\u7279\u5f81\u200b\u505a\u200b\u5f52\u4e00\u5316\u200b\u3002\u200b\u4e3e\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u901a\u8fc7\u200b\u7f16\u7801\u200b\u7684\u200b\u53e5\u5b50\u200b\u201c\u200b\u6211\u200b\u7231\u200b\u5b66\u4e60\u200b\u201d\uff0cBatch Norm \u200b\u662f\u200b\u5bf9\u200b\u8fd9\u200b\u56db\u4e2a\u200b\u5b57\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u800c\u200b Layer Norm \u200b\u662f\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u5b57\u200b\u672c\u8eab\u200b\u7684\u200b\u7279\u5f81\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b Layer Norm\uff0c\u200b\u5176\u200b\u516c\u5f0f\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b \\(\\(L N\\left(x_i\\right)=\\alpha \\times \\frac{x_i-u_L}{\\sqrt{\\sigma_L^2+\\epsilon}}+\\beta\\)\\) \u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>nn.LayerNorm</code> \u200b\u8fdb\u884c\u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#transformer-encoder_1","title":"Transformer Encoder \u200b\u5b8c\u6574\u200b\u4ee3\u7801","text":"<p>\u200b\u6574\u5408\u200b\u4e0a\u9762\u200b Multi-head Attention\u3001MLP\u3001Norm (Layer Norm,LN) \u200b\u5e76\u200b\u5916\u52a0\u200b shortcut \u200b\u8fde\u63a5\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f97\u5230\u200b Transformer Encoder \u200b\u7684\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\u3002 <pre><code>class Block(nn.Module):\n    \"\"\"\n    \u200b\u6bcf\u200b\u4e00\u4e2a\u200bEncoder Block\u200b\u7684\u200b\u6784\u6210\u200b\n    \u200b\u6bcf\u4e2a\u200bEncode Block\u200b\u7684\u200b\u6d41\u7a0b\u200b\uff1anorm1 --&gt; Multi-Head Attention --&gt; norm2 --&gt; MLP\n    \"\"\"\n    def __init__(self,\n                 dim, # \u200b\u8f93\u5165\u200bmlp\u200b\u7684\u200b\u7ef4\u5ea6\u200b\n                 num_heads, # Multi-Head-Attention\u200b\u7684\u200b\u5934\u200b\u4e2a\u6570\u200b\n                 mlp_ratio=4., # hidden_features / in_features = mlp_ratio\n                 qkv_bias=False, # q,k,v\u200b\u7684\u200b\u751f\u6210\u200b\u662f\u5426\u200b\u4f7f\u7528\u200bbias\n                 qk_scale=None,\n                 drop_ratio=0., # dropout\u200b\u7684\u200b\u6bd4\u4f8b\u200b\n                 attn_drop_ratio=0., # \u200b\u6ce8\u610f\u529b\u200bdropout\u200b\u7684\u200b\u6bd4\u4f8b\u200b\n                 drop_path_ratio=0.,\n                 act_layer=nn.GELU, # \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200bGELU\n                 norm_layer=nn.LayerNorm): # Norm\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200bLayerNorm\n        super(Block, self).__init__()\n        # \u200b\u7b2c\u4e00\u5c42\u200bnormalization\n        self.norm1 = norm_layer(dim)\n        # self.attention\u200b\u5c42\u200b\u7684\u200b\u5b9e\u73b0\u200b\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio &gt; 0. else nn.Identity()\n        # \u200b\u7b2c\u4e8c\u5c42\u200bnormalization\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio) # hidden_dim = dim * mlp_ratio\n        # mlp\u200b\u5b9e\u73b0\u200b\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n\n    def forward(self, x):\n        # \u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ViT%E8%A7%A3%E8%AF%BB/#vit_2","title":"ViT \u200b\u5b8c\u6574\u200b\u4ee3\u7801","text":"<p>\u200b\u5bf9\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8fdb\u884c\u200b\u5207\u5206\u200b\u548c\u200b\u5f71\u5c04\u200b\u3001\u200b\u52a0\u5165\u200b\u5206\u7c7b\u200b\u8868\u5f81\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u3001\u200b\u7ecf\u8fc7\u200b Transformer Encoder\u3001\u200b\u7136\u540e\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u5206\u7c7b\u200b\u5934\u200b\u8fdb\u884c\u200b\u8f93\u51fa\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5b8c\u6210\u200b\u4e86\u200b ViT \u200b\u6240\u6709\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u5b8c\u6574\u200b\u7684\u200b ViT \u200b\u4e3b\u8981\u200b\u6a21\u5757\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u89c1\u200b\u4e0b\u65b9\u200b <code>VisionTransformer</code>\u3002</p> <pre><code>class VisionTransformer(nn.Module):\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_c=3,\n                 num_classes=1000,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.0,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 representation_size=None,\n                 distilled=False,\n                 drop_ratio=0.,\n                 attn_drop_ratio=0.,\n                 drop_path_ratio=0.,\n                 embed_layer=PatchEmbed,\n                 norm_layer=None,\n                 act_layer=None):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_c (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n            distilled (bool): model includes a distillation token and head as in DeiT models\n            drop_ratio (float): dropout rate\n            attn_drop_ratio (float): attention dropout rate\n            drop_path_ratio (float): stochastic depth rate\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n        \"\"\"\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        # \u200b\u6bcf\u4e2a\u200bpatch\u200b\u7684\u200b\u56fe\u50cf\u200b\u7ef4\u5ea6\u200b = embed_dim\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        # token\u200b\u7684\u200b\u4e2a\u6570\u200b\u4e3a\u200b1\n        self.num_tokens = 2 if distilled else 1\n        # \u200b\u8bbe\u7f6e\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u548c\u200bnorm\u200b\u51fd\u6570\u200b\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n        # \u200b\u5bf9\u5e94\u200b\u7684\u200b\u5c06\u200b\u56fe\u7247\u200b\u6253\u6210\u200bpatch\u200b\u7684\u200b\u64cd\u4f5c\u200b\n        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        # \u200b\u8bbe\u7f6e\u200b\u5206\u7c7b\u200b\u7684\u200bcls_token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # distilled \u200b\u662f\u200bDeit\u200b\u4e2d\u200b\u7684\u200b \u200b\u8fd9\u91cc\u200b\u4e3a\u200bNone\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n        # pos_embedding \u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_ratio)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n        # \u200b\u4f7f\u7528\u200bnn.Sequential\u200b\u8fdb\u884c\u200b\u6784\u5efa\u200b\uff0cViT\u200b\u4e2d\u200b\u6df1\u5ea6\u200b\u4e3a\u200b12\n        self.blocks = nn.Sequential(*[\n            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n                  norm_layer=norm_layer, act_layer=act_layer)\n            for i in range(depth)\n        ])\n        self.norm = norm_layer(embed_dim)\n\n        # Representation layer\n        if representation_size and not distilled:\n            self.has_logits = True\n            self.num_features = representation_size\n            self.pre_logits = nn.Sequential(OrderedDict([\n                (\"fc\", nn.Linear(embed_dim, representation_size)),\n                (\"act\", nn.Tanh())\n            ]))\n        else:\n            self.has_logits = False\n            self.pre_logits = nn.Identity()\n\n        # Classifier head(s)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes &gt; 0 else nn.Identity()\n\n        # Weight init\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        if self.dist_token is not None:\n            nn.init.trunc_normal_(self.dist_token, std=0.02)\n\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        self.apply(_init_vit_weights)\n\n    def forward_features(self, x):\n        # [B, C, H, W] -&gt; [B, num_patches, embed_dim]\n        x = self.patch_embed(x)  # [B, 196, 768]\n        # [1, 1, 768] -&gt; [B, 1, 768]\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        if self.dist_token is None:\n            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n        else:\n            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n\n        x = self.pos_drop(x + self.pos_embed)\n        x = self.blocks(x)\n        x = self.norm(x)\n        if self.dist_token is None:\n            return self.pre_logits(x[:, 0])\n        else:\n            return x[:, 0], x[:, 1]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        if self.head_dist is not None:\n            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n            if self.training and not torch.jit.is_scripting():\n                # during inference, return the average of both classifier predictions\n                return x, x_dist\n            else:\n                return (x + x_dist) / 2\n        else:\n            x = self.head(x)\n        return x\n\n\ndef _init_vit_weights(m):\n    \"\"\"\n    ViT weight initialization\n    :param m: module\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.trunc_normal_(m.weight, std=.01)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.LayerNorm):\n        nn.init.zeros_(m.bias)\n        nn.init.ones_(m.weight)\n</code></pre> <p>\u200b\u53c2\u8003\u200b\uff1a</p> <p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale https://arxiv.org/pdf/2010.11929.pdf Attention Is All You Need https://arxiv.org/abs/1706.03762</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/","title":"Index","text":"OpenMMLab \u200b\u5b98\u7f51\u200b <sup> HOT </sup> OpenMMLab \u200b\u5f00\u653e\u5e73\u53f0\u200b <sup> TRY IT OUT </sup>    [![PyPI](https://img.shields.io/pypi/v/mmyolo)](https://pypi.org/project/mmyolo) [![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmyolo.readthedocs.io/en/latest/) [![deploy](https://github.com/open-mmlab/mmyolo/workflows/deploy/badge.svg)](https://github.com/open-mmlab/mmyolo/actions) [![codecov](https://codecov.io/gh/open-mmlab/mmyolo/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmyolo) [![license](https://img.shields.io/github/license/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/blob/master/LICENSE) [![open issues](https://isitmaintained.com/badge/open/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/issues) [![issue resolution](https://isitmaintained.com/badge/resolution/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/issues)  [\ud83d\udcd8\u200b\u4f7f\u7528\u200b\u6587\u6863\u200b](https://mmyolo.readthedocs.io/zh_CN/latest/) | [\ud83d\udee0\ufe0f\u200b\u5b89\u88c5\u200b\u6559\u7a0b\u200b](https://mmyolo.readthedocs.io/zh_CN/latest/get_started.html) | [\ud83d\udc40\u200b\u6a21\u578b\u5e93\u200b](https://mmyolo.readthedocs.io/zh_CN/latest/model_zoo.html) | [\ud83c\udd95\u200b\u66f4\u65b0\u200b\u65e5\u5fd7\u200b](https://mmyolo.readthedocs.io/en/latest/notes/changelog.html) | [\ud83e\udd14\u200b\u62a5\u544a\u200b\u95ee\u9898\u200b](https://github.com/open-mmlab/mmyolo/issues/new/choose)      [English](README.md) | \u200b\u7b80\u4f53\u4e2d\u6587"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_1","title":"\u7b80\u4ecb","text":"<p>MMYOLO \u200b\u662f\u200b\u4e00\u4e2a\u200b\u57fa\u4e8e\u200b PyTorch \u200b\u548c\u200b MMDetection \u200b\u7684\u200b YOLO \u200b\u7cfb\u5217\u200b\u7b97\u6cd5\u200b\u5f00\u6e90\u200b\u5de5\u5177\u7bb1\u200b\u3002\u200b\u5b83\u200b\u662f\u200b OpenMMLab \u200b\u9879\u76ee\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>\u200b\u4e3b\u200b\u5206\u652f\u200b\u4ee3\u7801\u200b\u76ee\u524d\u200b\u652f\u6301\u200b PyTorch 1.6 \u200b\u4ee5\u4e0a\u200b\u7684\u200b\u7248\u672c\u200b\u3002 </p> \u200b\u4e3b\u8981\u200b\u7279\u6027\u200b   - **\u200b\u7edf\u4e00\u200b\u4fbf\u6377\u200b\u7684\u200b\u7b97\u6cd5\u200b\u8bc4\u6d4b\u200b**    MMYOLO \u200b\u7edf\u4e00\u200b\u4e86\u200b\u5404\u7c7b\u200b YOLO \u200b\u7b97\u6cd5\u200b\u6a21\u5757\u200b\u7684\u200b\u5b9e\u73b0\u200b, \u200b\u5e76\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u7edf\u4e00\u200b\u7684\u200b\u8bc4\u6d4b\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u516c\u5e73\u200b\u4fbf\u6377\u200b\u5730\u200b\u8fdb\u884c\u200b\u5bf9\u6bd4\u200b\u5206\u6790\u200b\u3002  - **\u200b\u4e30\u5bcc\u200b\u7684\u200b\u5165\u95e8\u200b\u548c\u200b\u8fdb\u9636\u200b\u6587\u6863\u200b**    MMYOLO \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4ece\u200b\u5165\u95e8\u200b\u5230\u200b\u90e8\u7f72\u200b\u5230\u200b\u8fdb\u9636\u200b\u548c\u200b\u7b97\u6cd5\u200b\u89e3\u6790\u200b\u7b49\u200b\u4e00\u7cfb\u5217\u200b\u6587\u6863\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u4e0d\u540c\u200b\u7528\u6237\u200b\u5feb\u901f\u200b\u4e0a\u624b\u200b\u548c\u200b\u6269\u5c55\u200b\u3002  - **\u200b\u6a21\u5757\u5316\u200b\u8bbe\u8ba1\u200b**    MMYOLO \u200b\u5c06\u200b\u6846\u67b6\u200b\u89e3\u200b\u8026\u6210\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u901a\u8fc7\u200b\u7ec4\u5408\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u548c\u200b\u8bad\u7ec3\u200b\u6d4b\u8bd5\u200b\u7b56\u7565\u200b\uff0c\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u4fbf\u6377\u200b\u5730\u200b\u6784\u5efa\u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u3002     \u200b\u56fe\u4e3a\u200b RangeKing@GitHub \u200b\u63d0\u4f9b\u200b\uff0c\u200b\u975e\u5e38\u611f\u8c22\u200b\uff01"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_2","title":"\u6559\u7a0b","text":"<p>MMYOLO \u200b\u57fa\u4e8e\u200b MMDetection \u200b\u5f00\u6e90\u200b\u5e93\u200b\uff0c\u200b\u5e76\u4e14\u200b\u91c7\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u4ee3\u7801\u200b\u7ec4\u7ec7\u200b\u548c\u200b\u8bbe\u8ba1\u200b\u65b9\u5f0f\u200b\u3002\u200b\u4e3a\u4e86\u200b\u66f4\u597d\u200b\u7684\u200b\u4f7f\u7528\u200b\u672c\u200b\u5f00\u6e90\u200b\u5e93\u200b\uff0c\u200b\u8bf7\u200b\u5148\u200b\u9605\u8bfb\u200b MMDetection \u200b\u6982\u8ff0\u200b \u200b\u5bf9\u200b MMDetection \u200b\u8fdb\u884c\u200b\u521d\u6b65\u200b\u5730\u200b\u4e86\u89e3\u200b\u3002</p> <p>MMYOLO \u200b\u7528\u6cd5\u200b\u548c\u200b MMDetection \u200b\u51e0\u4e4e\u200b\u4e00\u81f4\u200b\uff0c\u200b\u6240\u6709\u200b\u6559\u7a0b\u200b\u90fd\u200b\u662f\u200b\u901a\u7528\u200b\u7684\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4e86\u89e3\u200b MMDetection \u200b\u7528\u6237\u200b\u6307\u5357\u200b\u548c\u200b\u8fdb\u9636\u200b\u6307\u5357\u200b \u3002</p> <p>\u200b\u9488\u5bf9\u200b\u548c\u200b MMDetection \u200b\u4e0d\u540c\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u51c6\u5907\u200b\u4e86\u200b\u7528\u6237\u200b\u6307\u5357\u200b\u548c\u200b\u8fdb\u9636\u200b\u6307\u5357\u200b\uff0c\u200b\u8bf7\u200b\u9605\u8bfb\u200b\u6211\u4eec\u200b\u7684\u200b \u200b\u6587\u6863\u200b \u3002</p> <ul> <li> <p>\u200b\u7b97\u6cd5\u200b\u89e3\u8bfb\u200b</p> </li> <li> <p>\u200b\u5fc5\u5907\u200b\u57fa\u7840\u200b</p> <ul> <li>\u200b\u6a21\u578b\u200b\u8bbe\u8ba1\u200b\u76f8\u5173\u200b\u8bf4\u660e\u200b</li> </ul> </li> <li> <p>\u200b\u7b97\u6cd5\u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790\u200b</p> <ul> <li>YOLOv5 \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790\u200b</li> <li>RTMDet \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790\u200b</li> </ul> </li> <li> <p>\u200b\u7528\u6237\u200b\u6307\u5357\u200b</p> </li> <li> <p>\u200b\u8bad\u7ec3\u200b &amp; \u200b\u6d4b\u8bd5\u200b</p> <ul> <li>\u200b\u5b66\u4e60\u200b YOLOv5 \u200b\u914d\u7f6e\u6587\u4ef6\u200b</li> </ul> </li> <li>\u200b\u4ece\u200b\u5165\u95e8\u200b\u5230\u200b\u90e8\u7f72\u200b\u5168\u200b\u6d41\u7a0b\u200b<ul> <li>YOLOv5 \u200b\u4ece\u200b\u5165\u95e8\u200b\u5230\u200b\u90e8\u7f72\u200b\u5168\u200b\u6d41\u7a0b\u200b</li> </ul> </li> <li> <p>\u200b\u5b9e\u7528\u5de5\u5177\u200b</p> <ul> <li>\u200b\u53ef\u89c6\u5316\u200b\u6559\u7a0b\u200b</li> <li>\u200b\u5b9e\u7528\u5de5\u5177\u200b</li> </ul> </li> <li> <p>\u200b\u7b97\u6cd5\u200b\u90e8\u7f72\u200b</p> </li> <li> <p>\u200b\u90e8\u7f72\u200b\u5fc5\u5907\u200b\u6559\u7a0b\u200b</p> <ul> <li>\u200b\u90e8\u7f72\u200b\u5fc5\u5907\u200b\u6559\u7a0b\u200b</li> </ul> </li> <li> <p>\u200b\u90e8\u7f72\u200b\u5168\u200b\u6d41\u7a0b\u200b\u8bf4\u660e\u200b</p> <ul> <li>YOLOv5 \u200b\u90e8\u7f72\u200b\u5168\u200b\u6d41\u7a0b\u200b\u8bf4\u660e\u200b</li> </ul> </li> <li> <p>\u200b\u8fdb\u9636\u200b\u6307\u5357\u200b</p> </li> <li> <p>\u200b\u6570\u636e\u6d41\u200b</p> </li> <li> <p>How to</p> </li> <li> <p>\u200b\u89e3\u8bfb\u200b\u6587\u7ae0\u200b\u548c\u200b\u8d44\u6e90\u200b\u6c47\u603b\u200b</p> </li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_3","title":"\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u548c\u200b\u6a21\u578b\u5e93","text":"<p>\u200b\u6d4b\u8bd5\u200b\u7ed3\u679c\u200b\u548c\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5728\u200b \u200b\u6a21\u578b\u5e93\u200b \u200b\u4e2d\u200b\u627e\u5230\u200b\u3002</p> \u200b\u652f\u6301\u200b\u7684\u200b\u7b97\u6cd5\u200b   - [x] [YOLOv5](configs/yolov5) - [x] [YOLOX](configs/yolox) - [x] [RTMDet](configs/rtmdet) - [x] [YOLOv6](configs/yolov6) - [ ] [PPYOLOE](configs/ppyoloe)(\u200b\u4ec5\u200b\u63a8\u7406\u200b) - [ ] [YOLOv7](configs/yolov7)(\u200b\u4ec5\u200b\u63a8\u7406\u200b)   \u200b\u6a21\u5757\u200b\u7ec4\u4ef6\u200b Backbones Necks Loss Common <ul> <li>YOLOv5CSPDarknet</li> <li>YOLOXCSPDarknet</li> <li>EfficientRep</li> <li>CSPNeXt</li> </ul> <ul> <li>YOLOv5PAFPN</li> <li>YOLOv6RepPAFPN</li> <li>YOLOXPAFPN</li> <li>CSPNeXtPAFPN</li> </ul> <ul> <li>IoULoss</li> </ul> <ul> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_4","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u200b\u8bf7\u200b\u53c2\u8003\u200b FAQ \u200b\u4e86\u89e3\u200b\u5176\u4ed6\u200b\u7528\u6237\u200b\u7684\u200b\u5e38\u89c1\u95ee\u9898\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_5","title":"\u8d21\u732e\u200b\u6307\u5357","text":"<p>\u200b\u6211\u4eec\u200b\u611f\u8c22\u200b\u6240\u6709\u200b\u7684\u200b\u8d21\u732e\u8005\u200b\u4e3a\u200b\u6539\u8fdb\u200b\u548c\u200b\u63d0\u5347\u200b MMYOLO \u200b\u6240\u200b\u4f5c\u51fa\u200b\u7684\u200b\u52aa\u529b\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u4e2d\u200b\u7684\u200b\u9879\u76ee\u200b\u6dfb\u52a0\u200b\u8fdb\u200b\u4e86\u200bGitHub Projects\u200b\u9875\u9762\u200b\uff0c\u200b\u975e\u5e38\u200b\u6b22\u8fce\u200b\u793e\u533a\u200b\u7528\u6237\u200b\u80fd\u200b\u53c2\u4e0e\u200b\u8fdb\u200b\u8fd9\u4e9b\u200b\u9879\u76ee\u200b\u4e2d\u6765\u200b\u3002\u200b\u8bf7\u200b\u53c2\u8003\u200b\u8d21\u732e\u200b\u6307\u5357\u200b\u6765\u200b\u4e86\u89e3\u200b\u53c2\u4e0e\u200b\u9879\u76ee\u200b\u8d21\u732e\u200b\u7684\u200b\u76f8\u5173\u200b\u6307\u5f15\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_6","title":"\u81f4\u8c22","text":"<p>MMYOLO \u200b\u662f\u200b\u4e00\u6b3e\u200b\u7531\u200b\u6765\u81ea\u200b\u4e0d\u540c\u200b\u9ad8\u6821\u200b\u548c\u200b\u4f01\u4e1a\u200b\u7684\u200b\u7814\u53d1\u200b\u4eba\u5458\u200b\u5171\u540c\u200b\u53c2\u4e0e\u200b\u8d21\u732e\u200b\u7684\u200b\u5f00\u6e90\u200b\u9879\u76ee\u200b\u3002\u200b\u6211\u4eec\u200b\u611f\u8c22\u200b\u6240\u6709\u200b\u4e3a\u200b\u9879\u76ee\u200b\u63d0\u4f9b\u200b\u7b97\u6cd5\u200b\u590d\u73b0\u200b\u548c\u200b\u65b0\u200b\u529f\u80fd\u200b\u652f\u6301\u200b\u7684\u200b\u8d21\u732e\u8005\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u63d0\u4f9b\u200b\u5b9d\u8d35\u200b\u53cd\u9988\u200b\u7684\u200b\u7528\u6237\u200b\u3002 \u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u8fd9\u4e2a\u200b\u5de5\u5177\u7bb1\u200b\u548c\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u53ef\u4ee5\u200b\u4e3a\u200b\u793e\u533a\u200b\u63d0\u4f9b\u200b\u7075\u6d3b\u200b\u7684\u200b\u4ee3\u7801\u200b\u5de5\u5177\u200b\uff0c\u200b\u4f9b\u200b\u7528\u6237\u200b\u590d\u73b0\u200b\u5df2\u6709\u200b\u7b97\u6cd5\u200b\u5e76\u200b\u5f00\u53d1\u200b\u81ea\u5df1\u200b\u7684\u200b\u65b0\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ece\u800c\u200b\u4e0d\u65ad\u200b\u4e3a\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u63d0\u4f9b\u200b\u8d21\u732e\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_7","title":"\u5f15\u7528","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u89c9\u5f97\u200b\u672c\u200b\u9879\u76ee\u200b\u5bf9\u200b\u4f60\u200b\u7684\u200b\u7814\u7a76\u200b\u5de5\u4f5c\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b bibtex \u200b\u5f15\u7528\u200b MMYOLO</p> <pre><code>@misc{mmyolo2022,\n    title={{MMYOLO: OpenMMLab YOLO} series toolbox and benchmark},\n    author={MMYOLO Contributors},\n    howpublished = {\\url{https://github.com/open-mmlab/mmyolo}},\n    year={2022}\n}\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#_8","title":"\u5f00\u6e90\u200b\u8bb8\u53ef\u8bc1","text":"<p>\u200b\u8be5\u200b\u9879\u76ee\u200b\u91c7\u7528\u200b GPL 3.0 \u200b\u5f00\u6e90\u200b\u8bb8\u53ef\u8bc1\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#openmmlab","title":"OpenMMLab \u200b\u7684\u200b\u5176\u4ed6\u200b\u9879\u76ee","text":"<ul> <li>MMEngine: OpenMMLab \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u57fa\u7840\u200b\u5e93\u200b</li> <li>MMCV: OpenMMLab \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u57fa\u7840\u200b\u5e93\u200b</li> <li>MIM: MIM \u200b\u662f\u200b OpenMMlab \u200b\u9879\u76ee\u200b\u3001\u200b\u7b97\u6cd5\u200b\u3001\u200b\u6a21\u578b\u200b\u7684\u200b\u7edf\u4e00\u200b\u5165\u53e3\u200b</li> <li>MMClassification: OpenMMLab \u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMDetection: OpenMMLab \u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMDetection3D: OpenMMLab \u200b\u65b0\u4e00\u4ee3\u200b\u901a\u7528\u200b 3D \u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u5e73\u53f0\u200b</li> <li>MMRotate: OpenMMLab \u200b\u65cb\u8f6c\u200b\u6846\u200b\u68c0\u6d4b\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMYOLO: OpenMMLab YOLO \u200b\u7cfb\u5217\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMSegmentation: OpenMMLab \u200b\u8bed\u4e49\u200b\u5206\u5272\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMOCR: OpenMMLab \u200b\u5168\u200b\u6d41\u7a0b\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u8bc6\u522b\u200b\u7406\u89e3\u200b\u5de5\u5177\u5305\u200b</li> <li>MMPose: OpenMMLab \u200b\u59ff\u6001\u200b\u4f30\u8ba1\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMHuman3D: OpenMMLab \u200b\u4eba\u4f53\u200b\u53c2\u6570\u200b\u5316\u200b\u6a21\u578b\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMSelfSup: OpenMMLab \u200b\u81ea\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMRazor: OpenMMLab \u200b\u6a21\u578b\u200b\u538b\u7f29\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMFewShot: OpenMMLab \u200b\u5c11\u200b\u6837\u672c\u200b\u5b66\u4e60\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMAction2: OpenMMLab \u200b\u65b0\u4e00\u4ee3\u200b\u89c6\u9891\u200b\u7406\u89e3\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMTracking: OpenMMLab \u200b\u4e00\u4f53\u5316\u200b\u89c6\u9891\u200b\u76ee\u6807\u200b\u611f\u77e5\u200b\u5e73\u53f0\u200b</li> <li>MMFlow: OpenMMLab \u200b\u5149\u6d41\u200b\u4f30\u8ba1\u200b\u5de5\u5177\u7bb1\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u57fa\u51c6\u200b</li> <li>MMEditing: OpenMMLab \u200b\u56fe\u50cf\u200b\u89c6\u9891\u200b\u7f16\u8f91\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMGeneration: OpenMMLab \u200b\u56fe\u7247\u200b\u89c6\u9891\u200b\u751f\u6210\u200b\u6a21\u578b\u200b\u5de5\u5177\u7bb1\u200b</li> <li>MMDeploy: OpenMMLab \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u6846\u67b6\u200b</li> <li>MMEval: OpenMMLab \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u8bc4\u6d4b\u200b\u5e93\u200b</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/#openmmlab_1","title":"\u6b22\u8fce\u200b\u52a0\u5165\u200b OpenMMLab \u200b\u793e\u533a","text":"<p>\u200b\u626b\u63cf\u200b\u4e0b\u65b9\u200b\u7684\u200b\u4e8c\u7ef4\u7801\u200b\u53ef\u200b\u5173\u6ce8\u200b OpenMMLab \u200b\u56e2\u961f\u200b\u7684\u200b \u200b\u77e5\u4e4e\u200b\u5b98\u65b9\u200b\u8d26\u53f7\u200b\uff0c\u200b\u52a0\u5165\u200b OpenMMLab \u200b\u56e2\u961f\u200b\u7684\u200b \u200b\u5b98\u65b9\u200b\u4ea4\u6d41\u200b QQ \u200b\u7fa4\u200b</p> <p>\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b OpenMMLab \u200b\u793e\u533a\u200b\u4e3a\u200b\u5927\u5bb6\u200b</p> <ul> <li>\ud83d\udce2 \u200b\u5206\u4eab\u200b AI \u200b\u6846\u67b6\u200b\u7684\u200b\u524d\u6cbf\u200b\u6838\u5fc3\u6280\u672f\u200b</li> <li>\ud83d\udcbb \u200b\u89e3\u8bfb\u200b PyTorch \u200b\u5e38\u7528\u200b\u6a21\u5757\u200b\u6e90\u7801\u200b</li> <li>\ud83d\udcf0 \u200b\u53d1\u5e03\u200b OpenMMLab \u200b\u7684\u200b\u76f8\u5173\u200b\u65b0\u95fb\u200b</li> <li>\ud83d\ude80 \u200b\u4ecb\u7ecd\u200b OpenMMLab \u200b\u5f00\u53d1\u200b\u7684\u200b\u524d\u6cbf\u200b\u7b97\u6cd5\u200b</li> <li>\ud83c\udfc3 \u200b\u83b7\u53d6\u200b\u66f4\u200b\u9ad8\u6548\u200b\u7684\u200b\u95ee\u9898\u200b\u7b54\u7591\u200b\u548c\u200b\u610f\u89c1\u53cd\u9988\u200b</li> <li>\ud83d\udd25 \u200b\u63d0\u4f9b\u200b\u4e0e\u200b\u5404\u884c\u5404\u4e1a\u200b\u5f00\u53d1\u8005\u200b\u5145\u5206\u200b\u4ea4\u6d41\u200b\u7684\u200b\u5e73\u53f0\u200b</li> </ul> <p>\u200b\u5e72\u8d27\u200b\u6ee1\u6ee1\u200b \ud83d\udcd8\uff0c\u200b\u7b49\u200b\u4f60\u200b\u6765\u200b\u64a9\u200b \ud83d\udc97\uff0cOpenMMLab \u200b\u793e\u533a\u200b\u671f\u5f85\u200b\u60a8\u200b\u7684\u200b\u52a0\u5165\u200b \ud83d\udc6c</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/","title":"\u6a21\u578b\u200b\u8bbe\u8ba1\u200b\u76f8\u5173\u200b\u8bf4\u660e","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/#yolo","title":"YOLO \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u57fa\u7c7b","text":"<p>\u200b\u4e0b\u56fe\u200b\u4e3a\u200b RangeKing@GitHub \u200b\u63d0\u4f9b\u200b\uff0c\u200b\u975e\u5e38\u611f\u8c22\u200b\uff01</p> <p>YOLO \u200b\u7cfb\u5217\u200b\u7b97\u6cd5\u200b\u5927\u90e8\u5206\u200b\u91c7\u7528\u200b\u4e86\u200b\u7edf\u4e00\u200b\u7684\u200b\u7b97\u6cd5\u200b\u642d\u5efa\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5178\u578b\u200b\u7684\u200b\u5982\u200b Darknet + PAFPN\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u7528\u6237\u200b\u5feb\u901f\u200b\u7406\u89e3\u200b YOLO \u200b\u7cfb\u5217\u200b\u7b97\u6cd5\u200b\u67b6\u6784\u200b\uff0c\u200b\u6211\u4eec\u200b\u7279\u610f\u200b\u8bbe\u8ba1\u200b\u4e86\u200b\u5982\u4e0a\u56fe\u200b\u4e2d\u200b\u7684\u200b BaseBackbone + BaseYOLONeck \u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u62bd\u8c61\u200b BaseBackbone \u200b\u7684\u200b\u597d\u5904\u200b\u5305\u62ec\u200b\uff1a</p> <ol> <li>\u200b\u5b50\u7c7b\u200b\u4e0d\u200b\u9700\u8981\u200b\u5173\u5fc3\u200b forward \u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u53ea\u8981\u200b\u7c7b\u4f3c\u200b\u5efa\u9020\u200b\u8005\u200b\u6a21\u5f0f\u200b\u4e00\u6837\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u5373\u53ef\u200b\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u914d\u7f6e\u200b\u5b9e\u73b0\u200b\u5b9a\u5236\u200b\u63d2\u4ef6\u200b\u529f\u80fd\u200b\uff0c\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u7684\u200b\u63d2\u5165\u200b\u4e00\u4e9b\u200b\u7c7b\u4f3c\u200b\u6ce8\u610f\u529b\u200b\u6a21\u5757\u200b\u3002</li> <li>\u200b\u6240\u6709\u200b\u5b50\u7c7b\u200b\u81ea\u52a8\u200b\u652f\u6301\u200b frozen \u200b\u67d0\u4e9b\u200b stage \u200b\u548c\u200b frozen bn \u200b\u529f\u80fd\u200b\u3002</li> </ol> <p>\u200b\u62bd\u8c61\u200b BaseYOLONeck \u200b\u4e5f\u200b\u6709\u200b\u540c\u6837\u200b\u597d\u5904\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/#basebackbone","title":"BaseBackbone","text":"<p>\u200b\u5982\u4e0a\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5bf9\u4e8e\u200b P5 \u200b\u800c\u8a00\u200b\uff0cBaseBackbone \u200b\u5305\u62ec\u200b 1 \u200b\u4e2a\u200b stem \u200b\u5c42\u200b + 4 \u200b\u4e2a\u200b stage \u200b\u5c42\u200b\u7684\u200b\u7c7b\u4f3c\u200b ResNet \u200b\u7684\u200b\u57fa\u7840\u200b\u7ed3\u6784\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7b97\u6cd5\u200b\u7684\u200b\u4e3b\u5e72\u200b\u7f51\u7edc\u200b\u7ee7\u627f\u200b BaseBackbone\uff0c\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5b9e\u73b0\u200b\u5185\u90e8\u200b\u7684\u200b <code>build_xx</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u57fa\u7840\u200b\u6a21\u5757\u200b\u6765\u200b\u6784\u5efa\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200b\u5185\u90e8\u7ed3\u6784\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/#baseyoloneck","title":"BaseYOLONeck","text":"<p>\u200b\u4e0e\u200b <code>BaseBackbone</code> \u200b\u7684\u200b\u8bbe\u8ba1\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3a\u200b MMYOLO \u200b\u7cfb\u5217\u200b\u7684\u200b Neck \u200b\u5c42\u200b\u8fdb\u884c\u200b\u4e86\u200b\u91cd\u6784\u200b\uff0c\u200b\u4e3b\u8981\u200b\u5206\u4e3a\u200b <code>Reduce \u200b\u5c42\u200b</code>\uff0c <code>UpSample \u200b\u5c42\u200b</code>\uff0c<code>TopDown \u200b\u5c42\u200b</code>\uff0c<code>DownSample \u200b\u5c42\u200b</code>\uff0c<code>BottomUP \u200b\u5c42\u200b</code>\u200b\u4ee5\u53ca\u200b<code>\u200b\u8f93\u51fa\u200b\u5377\u79ef\u200b\u5c42\u200b</code>\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7ed3\u6784\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200b\u91cd\u5199\u200b <code>build_xx</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u5b9e\u73b0\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u5185\u90e8\u7ed3\u6784\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/#basedensehead","title":"BaseDenseHead","text":"<p>MMYOLO \u200b\u7cfb\u5217\u200b\u6cbf\u7528\u200b MMDetection \u200b\u4e2d\u200b\u8bbe\u8ba1\u200b\u7684\u200b <code>BaseDenseHead</code> \u200b\u4f5c\u4e3a\u200b\u5176\u200b Head \u200b\u7ed3\u6784\u200b\u7684\u200b\u57fa\u7c7b\u200b\uff0c\u200b\u4f46\u662f\u200b\u8fdb\u4e00\u6b65\u200b\u62c6\u5206\u200b\u4e86\u200b HeadModule. \u200b\u4ee5\u200b YOLOv5 \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5176\u200b HeadModule \u200b\u4e2d\u200b\u7684\u200b forward \u200b\u5b9e\u73b0\u200b\u4ee3\u66ff\u200b\u4e86\u200b\u539f\u6709\u200b\u7684\u200b forward \u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/model_design/#headmodule","title":"HeadModule","text":"<p>\u200b\u5982\u4e0a\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u865a\u7ebf\u200b\u90e8\u5206\u200b\u4e3a\u200b MMDetection \u200b\u4e2d\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5b9e\u7ebf\u200b\u90e8\u5206\u200b\u4e3a\u200b MMYOLO \u200b\u4e2d\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002MMYOLO\u200b\u7248\u672c\u200b\u4e0e\u200b\u539f\u200b\u5b9e\u73b0\u200b\u76f8\u6bd4\u200b\u5177\u5907\u200b\u5177\u6709\u200b\u4ee5\u4e0b\u200b\u4f18\u52bf\u200b\uff1a</p> <ol> <li>MMDetection \u200b\u4e2d\u5c06\u200b <code>bbox_head</code> \u200b\u62c6\u200b\u5206\u4e3a\u200b <code>assigner</code> + <code>box coder</code> + <code>sampler</code> \u200b\u4e09\u4e2a\u200b\u5927\u200b\u7684\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b 3 \u200b\u4e2a\u200b\u7ec4\u4ef6\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4f20\u9012\u200b\u4e3a\u4e86\u200b\u901a\u7528\u6027\u200b\uff0c\u200b\u9700\u8981\u200b\u5c01\u88c5\u200b\u989d\u5916\u200b\u7684\u200b\u5bf9\u8c61\u200b\u6765\u200b\u5904\u7406\u200b\uff0c\u200b\u7edf\u4e00\u200b\u4e4b\u540e\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u4e0d\u7528\u200b\u8fdb\u884c\u200b\u62c6\u5206\u200b\u3002\u200b\u4e0d\u200b\u523b\u610f\u200b\u5f3a\u6c42\u200b\u5212\u5206\u200b\u4e09\u5927\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u597d\u5904\u200b\u4e3a\u200b\uff1a\u200b\u4e0d\u518d\u200b\u9700\u8981\u200b\u5bf9\u200b\u5185\u90e8\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u5c01\u88c5\u200b\uff0c\u200b\u7b80\u5316\u200b\u4e86\u200b\u4ee3\u7801\u200b\u903b\u8f91\u200b\uff0c\u200b\u51cf\u8f7b\u200b\u4e86\u200b\u793e\u533a\u200b\u4f7f\u7528\u200b\u96be\u5ea6\u200b\u548c\u200b\u7b97\u6cd5\u200b\u590d\u73b0\u200b\u96be\u5ea6\u200b\u3002</li> <li>\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u7528\u6237\u200b\u5728\u200b\u81ea\u5b9a\u4e49\u200b\u5b9e\u73b0\u200b\u7b97\u6cd5\u200b\u65f6\u5019\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e0d\u200b\u4f9d\u8d56\u4e8e\u200b\u539f\u6709\u200b\u6846\u67b6\u200b\uff0c\u200b\u5bf9\u200b\u90e8\u5206\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u6df1\u5ea6\u200b\u4f18\u5316\u200b\u3002</li> </ol> <p>\u200b\u603b\u7684\u6765\u8bf4\u200b\uff0c\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b\u53ea\u200b\u9700\u8981\u200b\u505a\u5230\u200b\u5c06\u200b <code>model</code> + <code>loss_by_feat</code> \u200b\u90e8\u5206\u200b\u89e3\u200b\u8026\u200b\uff0c\u200b\u7528\u6237\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u914d\u7f6e\u200b\u5b9e\u73b0\u200b\u4efb\u610f\u200b\u6a21\u578b\u200b\u914d\u5408\u200b\u4efb\u610f\u200b\u7684\u200b <code>loss_by_feat</code> \u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u4f8b\u5982\u200b\u5c06\u200b YOLOv5 \u200b\u6a21\u578b\u200b\u5e94\u7528\u200b YOLOX \u200b\u7684\u200b <code>loss_by_feat</code> \u200b\u7b49\u200b\u3002</p> <p>\u200b\u4ee5\u200b MMDetection \u200b\u4e2d\u200b YOLOX \u200b\u914d\u7f6e\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5176\u200b Head \u200b\u6a21\u5757\u200b\u914d\u7f6e\u200b\u5199\u6cd5\u200b\u4e3a\u200b\uff1a</p> <pre><code>bbox_head=dict(\n    type='YOLOXHead',\n    num_classes=80,\n    in_channels=128,\n    feat_channels=128,\n    stacked_convs=2,\n    strides=(8, 16, 32),\n    use_depthwise=False,\n    norm_cfg=dict(type='BN', momentum=0.03, eps=0.001),\n    act_cfg=dict(type='Swish'),\n    ...\n    loss_obj=dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=True,\n        reduction='sum',\n        loss_weight=1.0),\n    loss_l1=dict(type='L1Loss', reduction='sum', loss_weight=1.0)),\ntrain_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),\n</code></pre> <p>\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b\u62bd\u53d6\u200b <code>head_module</code> \u200b\u540e\u200b\uff0c\u200b\u65b0\u200b\u7684\u200b\u914d\u7f6e\u200b\u5199\u6cd5\u200b\u4e3a\u200b\uff1a</p> <pre><code>bbox_head=dict(\n    type='YOLOXHead',\n    head_module=dict(\n        type='YOLOXHeadModule',\n        num_classes=80,\n        in_channels=256,\n        feat_channels=256,\n        widen_factor=widen_factor,\n        stacked_convs=2,\n        featmap_strides=(8, 16, 32),\n        use_depthwise=False,\n        norm_cfg=dict(type='BN', momentum=0.03, eps=0.001),\n        act_cfg=dict(type='SiLU', inplace=True),\n    ),\n    ...\n    loss_obj=dict(\n        type='mmdet.CrossEntropyLoss',\n        use_sigmoid=True,\n        reduction='sum',\n        loss_weight=1.0),\n    loss_bbox_aux=dict(type='mmdet.L1Loss', reduction='sum', loss_weight=1.0)),\ntrain_cfg=dict(\n    assigner=dict(\n        type='mmdet.SimOTAAssigner',\n        center_radius=2.5,\n        iou_calculator=dict(type='mmdet.BboxOverlaps2D'))),\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/","title":"RTMDet \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#0","title":"0 \u200b\u7b80\u4ecb","text":"<p>\u200b\u9ad8\u6027\u80fd\u200b\uff0c\u200b\u4f4e\u200b\u5ef6\u65f6\u200b\u7684\u200b\u5355\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u5668\u200b</p> <p>\u200b\u4ee5\u4e0a\u200b\u7ed3\u6784\u56fe\u200b\u7531\u200b RangeKing@github \u200b\u7ed8\u5236\u200b\u3002</p> <p>\u200b\u6700\u8fd1\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\uff0c\u200b\u5f00\u6e90\u200b\u754c\u200b\u6d8c\u73b0\u51fa\u200b\u4e86\u200b\u5927\u91cf\u200b\u7684\u200b\u9ad8\u7cbe\u5ea6\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u9879\u76ee\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6700\u200b\u7a81\u51fa\u200b\u7684\u200b\u5c31\u662f\u200b YOLO \u200b\u7cfb\u5217\u200b\uff0cOpenMMLab \u200b\u4e5f\u200b\u5728\u200b\u4e0e\u200b\u793e\u533a\u200b\u7684\u200b\u5408\u4f5c\u200b\u4e0b\u200b\u63a8\u51fa\u200b\u4e86\u200b MMYOLO\u3002 \u200b\u5728\u200b\u8c03\u7814\u200b\u4e86\u200b\u5f53\u524d\u200b YOLO \u200b\u7cfb\u5217\u200b\u7684\u200b\u8bf8\u591a\u200b\u6539\u8fdb\u200b\u6a21\u578b\u200b\u540e\u200b\uff0cMMDetection \u200b\u6838\u5fc3\u200b\u5f00\u53d1\u8005\u200b\u9488\u5bf9\u200b\u8fd9\u4e9b\u200b\u8bbe\u8ba1\u200b\u4ee5\u53ca\u200b\u8bad\u7ec3\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u7ecf\u9a8c\u6027\u200b\u7684\u200b\u603b\u7ed3\u200b\uff0c\u200b\u5e76\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u63a8\u51fa\u200b\u4e86\u200b\u9ad8\u7cbe\u5ea6\u200b\u3001\u200b\u4f4e\u200b\u5ef6\u65f6\u200b\u7684\u200b\u5355\u200b\u9636\u6bb5\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u5668\u200b RTMDet, Real-time Models for Object Detection (Release to Manufacture)</p> <p>RTMDet \u200b\u7531\u200b tiny/s/m/l/x \u200b\u4e00\u7cfb\u5217\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u7ec4\u6210\u200b\uff0c\u200b\u4e3a\u200b\u4e0d\u540c\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u9009\u62e9\u200b\u3002 \u200b\u5176\u4e2d\u200b\uff0cRTMDet-x \u200b\u5728\u200b 52.6 mAP \u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e0b\u200b\u8fbe\u5230\u200b\u4e86\u200b 300+ FPS \u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</p> <pre><code>\u200b\u6ce8\u200b\uff1a\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u548c\u200b\u7cbe\u5ea6\u200b\u6d4b\u8bd5\u200b\uff08\u200b\u4e0d\u200b\u5305\u542b\u200b NMS\uff09\u200b\u662f\u200b\u5728\u200b 1 \u200b\u5757\u200b NVIDIA 3090 GPU \u200b\u4e0a\u200b\u7684\u200b `TensorRT 8.4.3, cuDNN 8.2.0, FP16, batch size=1` \u200b\u6761\u4ef6\u200b\u91cc\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u3002\n</code></pre> <p>\u200b\u800c\u200b\u6700\u200b\u8f7b\u91cf\u200b\u7684\u200b\u6a21\u578b\u200b RTMDet-tiny\uff0c\u200b\u5728\u200b\u4ec5\u200b\u6709\u200b 4M \u200b\u53c2\u200b\u6570\u91cf\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u4e5f\u200b\u80fd\u591f\u200b\u8fbe\u5230\u200b 40.9 mAP\uff0c\u200b\u4e14\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b \\&lt; 1 ms\u3002</p> <p>\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u662f\u200b\u548c\u200b 300 epoch \u200b\u8bad\u7ec3\u200b\u4e0b\u200b\u7684\u200b\u516c\u5e73\u200b\u5bf9\u6bd4\u200b\uff0c\u200b\u4e3a\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u84b8\u998f\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> mAP Params Flops Inference speed Baseline(YOLOX) 40.2 9M 13.4G 1.2ms + AdamW + Flat Cosine 40.6 (+0.4) 9M 13.4G 1.2ms + CSPNeXt backbone &amp; PAFPN 41.8 (+1.2) 10.07M (+1.07) 14.8G (+1.4) 1.22ms (+0.02) + SepBNHead 41.8 (+0) 8.89M (-1.18) 14.8G 1.22ms + Label Assign &amp; Loss 42.9 (+1.1) 8.89M 14.8G 1.22ms + Cached Mosaic &amp; MixUp 44.2 (+1.3) 8.89M 14.8G 1.22ms + RSB-pretrained backbone 44.5 (+0.3) 8.89M 14.8G 1.22ms <ul> <li>\u200b\u5b98\u65b9\u200b\u5f00\u6e90\u200b\u5730\u5740\u200b\uff1a https://github.com/open-mmlab/mmdetection/blob/3.x/configs/rtmdet/README.md</li> <li>MMYOLO \u200b\u5f00\u6e90\u200b\u5730\u5740\u200b\uff1a https://github.com/open-mmlab/mmyolo/blob/main/configs/rtmdet/README.md</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#1-v10-mmyolo","title":"1 v1.0 \u200b\u7b97\u6cd5\u200b\u539f\u7406\u200b\u548c\u200b MMYOLO \u200b\u5b9e\u73b0\u200b\u89e3\u6790","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#11","title":"1.1 \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6a21\u5757","text":"<p>RTMDet \u200b\u91c7\u7528\u200b\u4e86\u200b\u591a\u79cd\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6765\u200b\u589e\u52a0\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\uff0c\u200b\u4e3b\u8981\u200b\u5305\u62ec\u200b\u5355\u56fe\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b:</p> <ul> <li>RandomResize \u200b\u968f\u673a\u200b\u5c3a\u5ea6\u200b\u53d8\u6362\u200b</li> <li>RandomCrop \u200b\u968f\u673a\u200b\u88c1\u526a\u200b</li> <li>HSVRandomAug \u200b\u989c\u8272\u200b\u7a7a\u95f4\u200b\u589e\u5f3a\u200b</li> <li>RandomFlip \u200b\u968f\u673a\u200b\u6c34\u5e73\u200b\u7ffb\u8f6c\u200b</li> </ul> <p>\u200b\u4ee5\u53ca\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff1a</p> <ul> <li>Mosaic \u200b\u9a6c\u8d5b\u514b\u200b</li> <li>MixUp \u200b\u56fe\u50cf\u200b\u6df7\u5408\u200b</li> </ul> <p>\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u5176\u4e2d\u200b RandomResize \u200b\u8d85\u53c2\u200b\u5728\u200b\u5927\u200b\u6a21\u578b\u200b M,L,X \u200b\u548c\u200b\u5c0f\u200b\u6a21\u578b\u200b S, Tiny \u200b\u4e0a\u200b\u662f\u200b\u4e0d\u200b\u4e00\u6837\u200b\u7684\u200b\uff0c\u200b\u5927\u200b\u6a21\u578b\u200b\u7531\u4e8e\u200b\u53c2\u6570\u200b\u8f83\u200b\u591a\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b large scale jitter \u200b\u7b56\u7565\u200b\u5373\u200b\u53c2\u6570\u200b\u4e3a\u200b (0.1,2.0)\uff0c\u200b\u800c\u200b\u5c0f\u200b\u6a21\u578b\u200b\u91c7\u7528\u200b stand scale jitter \u200b\u7b56\u7565\u200b\u5373\u200b (0.5, 2.0) \u200b\u7b56\u7565\u200b\u3002 MMDetection \u200b\u5f00\u6e90\u200b\u5e93\u4e2d\u200b\u5df2\u7ecf\u200b\u5bf9\u200b\u5355\u56fe\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5c01\u88c5\u200b\uff0c\u200b\u7528\u6237\u200b\u901a\u8fc7\u200b\u7b80\u5355\u200b\u7684\u200b\u4fee\u6539\u200b\u914d\u7f6e\u200b\u5373\u53ef\u200b\u4f7f\u7528\u200b\u5e93\u4e2d\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4efb\u4f55\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u529f\u80fd\u200b\uff0c\u200b\u4e14\u200b\u90fd\u200b\u662f\u200b\u5c5e\u4e8e\u200b\u6bd4\u8f83\u200b\u5e38\u89c4\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u7279\u6b8a\u200b\u4ecb\u7ecd\u200b\u3002\u200b\u4e0b\u9762\u200b\u5c06\u200b\u5177\u4f53\u200b\u4ecb\u7ecd\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u4e0e\u200b YOLOv5 \u200b\u4e0d\u540c\u200b\u7684\u200b\u662f\u200b\uff0cYOLOv5 \u200b\u8ba4\u4e3a\u200b\u5728\u200b S \u200b\u548c\u200b Nano \u200b\u6a21\u578b\u200b\u4e0a\u200b\u4f7f\u7528\u200b MixUp \u200b\u662f\u200b\u8fc7\u5269\u200b\u7684\u200b\uff0c\u200b\u5c0f\u200b\u6a21\u578b\u200b\u4e0d\u200b\u9700\u8981\u200b\u8fd9\u4e48\u200b\u5f3a\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002\u200b\u800c\u200b RTMDet \u200b\u5728\u200b S \u200b\u548c\u200b Tiny \u200b\u4e0a\u200b\u4e5f\u200b\u4f7f\u7528\u200b\u4e86\u200b MixUp\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b RTMDet \u200b\u5728\u200b\u6700\u540e\u200b 20 epoch \u200b\u4f1a\u200b\u5207\u6362\u200b\u4e3a\u200b\u6b63\u5e38\u200b\u7684\u200b aug\uff0c \u200b\u5e76\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u8bc1\u660e\u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b\u662f\u200b\u6709\u6548\u200b\u7684\u200b\u3002 \u200b\u5e76\u4e14\u200b RTMDet \u200b\u4e3a\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5f15\u5165\u200b\u4e86\u200b Cache \u200b\u65b9\u6848\u200b\uff0c\u200b\u6709\u6548\u200b\u5730\u200b\u51cf\u5c11\u200b\u4e86\u200b\u56fe\u50cf\u5904\u7406\u200b\u7684\u200b\u65f6\u95f4\u200b, \u200b\u548c\u200b\u5f15\u5165\u200b\u4e86\u200b\u53ef\u8c03\u200b\u8d85\u53c2\u200b <code>max_cached_images</code> \uff0c\u200b\u5f53\u200b\u4f7f\u7528\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b cache \u200b\u65f6\u200b\uff0c\u200b\u5176\u200b\u6548\u679c\u200b\u7c7b\u4f3c\u200b <code>repeated augmentation</code>\u3002\u200b\u5177\u4f53\u200b\u4ecb\u7ecd\u200b\u5982\u4e0b\u200b\uff1a</p> Use cache ms / 100 imgs Mosaic 87.1 Mosaic \u221a 24.0 MixUp 19.3 MixUp \u221a 12.4 RTMDet-s RTMDet-l Mosaic + MixUp + 20e finetune 43.9 51.3"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#111-cache","title":"1.1.1 \u200b\u4e3a\u200b\u56fe\u50cf\u200b\u6df7\u5408\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5f15\u5165\u200b Cache","text":"<p>Mosaic&amp;MixUp \u200b\u6d89\u53ca\u200b\u5230\u200b\u591a\u200b\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u6df7\u5408\u200b\uff0c\u200b\u5b83\u4eec\u200b\u7684\u200b\u8017\u65f6\u200b\u4f1a\u200b\u662f\u200b\u666e\u901a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b K \u200b\u500d\u200b(K \u200b\u4e3a\u200b\u6df7\u5165\u200b\u56fe\u7247\u200b\u7684\u200b\u6570\u91cf\u200b)\u3002 \u200b\u5982\u200b\u5728\u200b YOLOv5 \u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u505a\u200b Mosaic \u200b\u65f6\u200b\uff0c 4 \u200b\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u4fe1\u606f\u200b\u90fd\u200b\u9700\u8981\u200b\u4ece\u200b\u786c\u76d8\u200b\u4e2d\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u3002 \u200b\u800c\u200b RTMDet \u200b\u53ea\u200b\u9700\u8981\u200b\u91cd\u65b0\u200b\u8f7d\u5165\u200b\u5f53\u524d\u200b\u7684\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u5176\u4f59\u200b\u53c2\u4e0e\u200b\u6df7\u5408\u200b\u589e\u5f3a\u200b\u7684\u200b\u56fe\u7247\u200b\u5219\u200b\u4ece\u200b\u7f13\u5b58\u200b\u961f\u5217\u200b\u4e2d\u200b\u83b7\u53d6\u200b\uff0c\u200b\u901a\u8fc7\u200b\u727a\u7272\u200b\u4e00\u5b9a\u200b\u5185\u5b58\u7a7a\u95f4\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u4e86\u200b\u6548\u7387\u200b\u3002 \u200b\u53e6\u5916\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b cache \u200b\u7684\u200b\u5927\u5c0f\u200b\u4ee5\u53ca\u200b pop \u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u589e\u5f3a\u200b\u7684\u200b\u5f3a\u5ea6\u200b\u3002</p> <p>\u200b\u5982\u56fe\u6240\u793a\u200b\uff0ccache \u200b\u961f\u5217\u200b\u4e2d\u200b\u9884\u5148\u200b\u50a8\u5b58\u200b\u4e86\u200b N \u200b\u5f20\u5df2\u200b\u52a0\u8f7d\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0e\u200b\u6807\u7b7e\u200b\u6570\u636e\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b step \u200b\u4e2d\u200b\u53ea\u200b\u9700\u200b\u52a0\u8f7d\u200b\u4e00\u5f20\u200b\u65b0\u200b\u7684\u200b\u56fe\u7247\u200b\u53ca\u5176\u200b\u6807\u7b7e\u200b\u6570\u636e\u200b\u5e76\u200b\u66f4\u65b0\u200b\u5230\u200b cache \u200b\u961f\u5217\u200b\u4e2d\u200b(cache \u200b\u961f\u5217\u200b\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\u53ef\u200b\u91cd\u590d\u200b\uff0c\u200b\u5982\u56fe\u200b\u4e2d\u200b\u51fa\u73b0\u200b\u4e24\u6b21\u200b img3)\uff0c\u200b\u540c\u65f6\u200b\u5982\u679c\u200b cache \u200b\u961f\u5217\u200b\u957f\u5ea6\u200b\u8d85\u8fc7\u200b\u9884\u8bbe\u200b\u957f\u5ea6\u200b\uff0c\u200b\u5219\u200b\u968f\u673a\u200b pop \u200b\u4e00\u5f20\u200b\u56fe\u200b\uff08\u200b\u4e3a\u4e86\u200b Tiny \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u7a33\u5b9a\u200b\uff0c\u200b\u5728\u200b Tiny \u200b\u6a21\u578b\u200b\u4e2d\u200b\u4e0d\u200b\u91c7\u7528\u200b\u968f\u673a\u200b pop \u200b\u7684\u200b\u65b9\u5f0f\u200b, \u200b\u800c\u662f\u200b\u79fb\u9664\u200b\u6700\u5148\u200b\u52a0\u5165\u200b\u7684\u200b\u56fe\u7247\u200b\uff09\uff0c\u200b\u5f53\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u6df7\u5408\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65f6\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u4ece\u200b cache \u200b\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u9700\u8981\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u62fc\u63a5\u200b\u7b49\u200b\u5904\u7406\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u5168\u90e8\u200b\u4ece\u200b\u786c\u76d8\u200b\u4e2d\u200b\u52a0\u8f7d\u200b\uff0c\u200b\u8282\u7701\u200b\u4e86\u200b\u56fe\u50cf\u200b\u52a0\u8f7d\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</p> <pre><code>cache \u200b\u961f\u5217\u200b\u7684\u200b\u6700\u5927\u200b\u957f\u5ea6\u200b N \u200b\u4e3a\u200b\u53ef\u200b\u8c03\u6574\u200b\u53c2\u6570\u200b\uff0c\u200b\u6839\u636e\u200b\u7ecf\u9a8c\u6027\u200b\u7684\u200b\u539f\u5219\u200b\uff0c\u200b\u5f53\u4e3a\u200b\u6bcf\u200b\u4e00\u5f20\u200b\u9700\u8981\u200b\u6df7\u5408\u200b\u7684\u200b\u56fe\u7247\u200b\u63d0\u4f9b\u200b\u5341\u4e2a\u200b\u7f13\u5b58\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8db3\u591f\u200b\u7684\u200b\u968f\u673a\u6027\u200b\uff0c\u200b\u800c\u200b Mosaic \u200b\u589e\u5f3a\u200b\u662f\u200b\u56db\u5f20\u200b\u56fe\u200b\u6df7\u5408\u200b\uff0c\u200b\u56e0\u6b64\u200b cache \u200b\u6570\u91cf\u200b\u9ed8\u8ba4\u200b N=40\uff0c \u200b\u540c\u7406\u200b MixUp \u200b\u7684\u200b cache \u200b\u6570\u91cf\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b20\uff0c tiny \u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u66f4\u200b\u7a33\u5b9a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6761\u4ef6\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5176\u200b cache \u200b\u6570\u91cf\u200b\u4e5f\u200b\u4e3a\u200b\u5176\u4f59\u200b\u89c4\u683c\u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u534a\u200b\uff08 MixUp \u200b\u4e3a\u200b10\uff0cMosaic \u200b\u4e3a\u200b20\uff09\n</code></pre> <p>\u200b\u5728\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0cMMYOLO \u200b\u8bbe\u8ba1\u200b\u4e86\u200b <code>BaseMiximageTransform</code> \u200b\u7c7b\u6765\u200b\u652f\u6301\u200b\u591a\u200b\u5f20\u200b\u56fe\u50cf\u200b\u6df7\u5408\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff1a</p> <pre><code>if self.use_cached:\n    # Be careful: deep copying can be very time-consuming\n    # if results includes dataset.\n    dataset = results.pop('dataset', None)\n    self.results_cache.append(copy.deepcopy(results))  # \u200b\u5c06\u200b\u5f53\u524d\u200b\u52a0\u8f7d\u200b\u7684\u200b\u56fe\u7247\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u5230\u200b cache \u200b\u4e2d\u200b\n    if len(self.results_cache) &gt; self.max_cached_images:\n        if self.random_pop: # \u200b\u9664\u4e86\u200btiny\u200b\u6a21\u578b\u200b\uff0cself.random_pop=True\n            index = random.randint(0, len(self.results_cache) - 1)\n        else:\n            index = 0\n        self.results_cache.pop(index)\n\n    if len(self.results_cache) &lt;= 4:\n        return results\nelse:\n    assert 'dataset' in results\n    # Be careful: deep copying can be very time-consuming\n    # if results includes dataset.\n    dataset = results.pop('dataset', None)\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#112-mosaic","title":"1.1.2 Mosaic","text":"<p>Mosaic \u200b\u662f\u200b\u5c06\u200b 4 \u200b\u5f20\u56fe\u200b\u62fc\u63a5\u200b\u4e3a\u200b 1 \u200b\u5f20\u5927\u200b\u56fe\u200b\uff0c\u200b\u76f8\u5f53\u4e8e\u200b\u53d8\u76f8\u200b\u7684\u200b\u589e\u52a0\u200b\u4e86\u200b batch size\uff0c\u200b\u5177\u4f53\u6b65\u9aa4\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u6839\u636e\u200b\u7d22\u5f15\u200b\u968f\u673a\u200b\u4ece\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u4e2d\u200b\u518d\u200b\u91c7\u6837\u200b3\u200b\u4e2a\u200b\u56fe\u50cf\u200b\uff0c\u200b\u53ef\u80fd\u200b\u91cd\u590d\u200b</li> </ol> <pre><code>def get_indexes(self, dataset: Union[BaseDataset, list]) -&gt; list:\n    \"\"\"Call function to collect indexes.\n\n    Args:\n        dataset (:obj:`Dataset` or list): The dataset or cached list.\n\n    Returns:\n        list: indexes.\n    \"\"\"\n    indexes = [random.randint(0, len(dataset)) for _ in range(3)]\n    return indexes\n</code></pre> <ol> <li>\u200b\u968f\u673a\u200b\u9009\u51fa\u200b 4 \u200b\u5e45\u200b\u56fe\u50cf\u200b\u76f8\u4ea4\u200b\u7684\u200b\u4e2d\u70b9\u200b\u3002</li> </ol> <pre><code># mosaic center x, y\ncenter_x = int(\n    random.uniform(*self.center_ratio_range) * self.img_scale[1])\ncenter_y = int(\n    random.uniform(*self.center_ratio_range) * self.img_scale[0])\ncenter_position = (center_x, center_y)\n</code></pre> <ol> <li>\u200b\u6839\u636e\u200b\u91c7\u6837\u200b\u7684\u200b index \u200b\u8bfb\u53d6\u200b\u56fe\u7247\u200b\u5e76\u200b\u62fc\u63a5\u200b, \u200b\u62fc\u63a5\u200b\u524d\u4f1a\u200b\u5148\u200b\u8fdb\u884c\u200b <code>keep-ratio</code> \u200b\u7684\u200b resize \u200b\u56fe\u7247\u200b(\u200b\u5373\u200b\u4e3a\u200b\u6700\u5927\u200b\u8fb9\u200b\u4e00\u5b9a\u200b\u662f\u200b 640)\u3002</li> </ol> <pre><code># keep_ratio resize\nscale_ratio_i = min(self.img_scale[0] / h_i,\n                    self.img_scale[1] / w_i)\nimg_i = mmcv.imresize(\n    img_i, (int(w_i * scale_ratio_i), int(h_i * scale_ratio_i)))\n</code></pre> <ol> <li>\u200b\u62fc\u63a5\u200b\u540e\u200b\uff0c\u200b\u628a\u200b bbox \u200b\u548c\u200b label \u200b\u5168\u90e8\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b bbox \u200b\u8fdb\u884c\u200b\u88c1\u526a\u200b\u4f46\u662f\u200b\u4e0d\u200b\u8fc7\u6ee4\u200b(\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u4e00\u4e9b\u200b\u65e0\u6548\u200b\u6846\u200b)</li> </ol> <pre><code>mosaic_bboxes.clip_([2 * self.img_scale[0], 2 * self.img_scale[1]])\n</code></pre> <p>\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5173\u4e8e\u200b Mosaic \u200b\u539f\u7406\u200b\u7684\u200b\u8be6\u60c5\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b YOLOv5 \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790\u200b \u200b\u4e2d\u200b\u7684\u200b Mosaic \u200b\u539f\u7406\u200b\u5206\u6790\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#113-mixup","title":"1.1.3 MixUp","text":"<p>RTMDet \u200b\u7684\u200b MixUp \u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b\u4e0e\u200b YOLOX \u200b\u4e2d\u200b\u4e00\u6837\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u589e\u52a0\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u4e0a\u200b\u6587\u4e2d\u200b\u63d0\u5230\u200b\u7684\u200b cache \u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5173\u4e8e\u200b MixUp \u200b\u539f\u7406\u200b\u7684\u200b\u8be6\u60c5\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b YOLOv5 \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790\u200b \u200b\u4e2d\u200b\u7684\u200b MixUp \u200b\u539f\u7406\u200b\u5206\u6790\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#114","title":"1.1.4 \u200b\u5f3a\u5f31\u200b\u4e24\u200b\u9636\u6bb5\u200b\u8bad\u7ec3","text":"<p>Mosaic+MixUp \u200b\u5931\u771f\u5ea6\u200b\u6bd4\u8f83\u200b\u9ad8\u200b\uff0c\u200b\u6301\u7eed\u200b\u7528\u592a\u5f3a\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5bf9\u6a21\u578b\u200b\u5e76\u4e0d\u4e00\u5b9a\u200b\u6709\u76ca\u200b\u3002YOLOX \u200b\u4e2d\u200b\u7387\u5148\u200b\u4f7f\u7528\u200b\u4e86\u200b\u5f3a\u5f31\u200b\u4e24\u200b\u9636\u6bb5\u200b\u7684\u200b\u8bad\u5ddd\u7ec3\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u5f15\u5165\u200b\u4e86\u200b\u65cb\u8f6c\u200b\uff0c\u200b\u5207\u7247\u200b\u5bfc\u81f4\u200b box \u200b\u6807\u6ce8\u200b\u4ea7\u751f\u200b\u8bef\u5dee\u200b\uff0c\u200b\u9700\u8981\u200b\u5728\u200b\u7b2c\u4e8c\u9636\u6bb5\u200b\u5f15\u5165\u200b\u989d\u5916\u200b\u7684\u200b L1oss \u200b\u6765\u200b\u7ea0\u6b63\u200b\u56de\u5f52\u200b\u5206\u652f\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4f7f\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u65b9\u5f0f\u200b\u66f4\u4e3a\u200b\u901a\u7528\u200b\uff0cRTMDet \u200b\u5728\u200b\u524d\u200b 280 epoch \u200b\u4f7f\u7528\u200b\u4e0d\u5e26\u200b\u65cb\u8f6c\u200b\u7684\u200b Mosaic+MixUp, \u200b\u4e14\u200b\u901a\u8fc7\u200b\u6df7\u5165\u200b 8 \u200b\u5f20\u200b\u56fe\u7247\u200b\u6765\u200b\u63d0\u5347\u200b\u5f3a\u5ea6\u200b\u4ee5\u53ca\u200b\u6b63\u200b\u6837\u672c\u6570\u200b\u3002\u200b\u540e\u200b 20 epoch \u200b\u4f7f\u7528\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u5728\u200b\u6bd4\u8f83\u200b\u5f31\u200b\u7684\u200b\u589e\u5f3a\u200b\u4e0b\u200b\u8fdb\u884c\u200b\u5fae\u8c03\u200b\uff0c\u200b\u540c\u65f6\u200b\u5728\u200b EMA \u200b\u7684\u200b\u4f5c\u7528\u200b\u4e0b\u200b\u5c06\u200b\u53c2\u6570\u200b\u7f13\u6162\u200b\u66f4\u65b0\u200b\u81f3\u200b\u6a21\u578b\u200b\uff0c\u200b\u80fd\u591f\u200b\u5f97\u5230\u200b\u6bd4\u8f83\u200b\u5927\u200b\u7684\u200b\u63d0\u5347\u200b\u3002</p> RTMDet-s RTMDet-l LSJ + rand crop 42.3 46.7 Mosaic+MixUp 41.9 49.8 Mosaic + MixUp + 20e finetune 43.9 51.3"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#12","title":"1.2 \u200b\u6a21\u578b\u200b\u7ed3\u6784","text":"<p>RTMDet \u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u7ed3\u6784\u200b\u548c\u200b YOLOX \u200b\u51e0\u4e4e\u200b\u4e00\u81f4\u200b\uff0c\u200b\u7531\u200b <code>CSPNeXt</code> + <code>CSPNeXtPAFPN</code> + <code>\u200b\u5171\u4eab\u200b\u5377\u79ef\u200b\u6743\u91cd\u200b\u4f46\u200b\u5206\u522b\u200b\u8ba1\u7b97\u200b BN \u200b\u7684\u200b SepBNHead</code> \u200b\u6784\u6210\u200b\u3002\u200b\u5185\u90e8\u200b\u6838\u5fc3\u200b\u6a21\u5757\u200b\u4e5f\u200b\u662f\u200b <code>CSPLayer</code>\uff0c\u200b\u4f46\u200b\u5bf9\u200b\u5176\u4e2d\u200b\u7684\u200b  <code>Basic Block</code> \u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b <code>CSPNeXt Block</code>\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#121-backbone","title":"1.2.1 Backbone","text":"<p><code>CSPNeXt</code> \u200b\u6574\u4f53\u200b\u4ee5\u200b <code>CSPDarknet</code> \u200b\u4e3a\u200b\u57fa\u7840\u200b\uff0c\u200b\u5171\u200b 5 \u200b\u5c42\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5305\u542b\u200b 1 \u200b\u4e2a\u200b <code>Stem Layer</code> \u200b\u548c\u200b 4 \u200b\u4e2a\u200b <code>Stage Layer</code>\uff1a</p> <ul> <li> <p><code>Stem Layer</code> \u200b\u662f\u200b 3 \u200b\u5c42\u200b 3x3 kernel \u200b\u7684\u200b <code>ConvModule</code> \uff0c\u200b\u4e0d\u540c\u4e8e\u200b\u4e4b\u524d\u200b\u7684\u200b <code>Focus</code> \u200b\u6a21\u5757\u200b\u6216\u8005\u200b 1 \u200b\u5c42\u200b 6x6 kernel \u200b\u7684\u200b <code>ConvModule</code> \u3002</p> </li> <li> <p><code>Stage Layer</code> \u200b\u603b\u4f53\u200b\u7ed3\u6784\u200b\u4e0e\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u524d\u200b 3 \u200b\u4e2a\u200b <code>Stage Layer</code> \u200b\u7531\u200b 1 \u200b\u4e2a\u200b <code>ConvModule</code> \u200b\u548c\u200b 1 \u200b\u4e2a\u200b <code>CSPLayer</code>  \u200b\u7ec4\u6210\u200b\u3002\u200b\u7b2c\u200b 4 \u200b\u4e2a\u200b <code>Stage Layer</code> \u200b\u5728\u200b <code>ConvModule</code>  \u200b\u548c\u200b  <code>CSPLayer</code> \u200b\u4e2d\u95f4\u200b\u589e\u52a0\u200b\u4e86\u200b <code>SPPF</code> \u200b\u6a21\u5757\u200b\uff08MMDetection \u200b\u7248\u672c\u200b\u4e3a\u200b <code>SPP</code> \u200b\u6a21\u5757\u200b\uff09\u3002</p> </li> <li> <p>\u200b\u5982\u200b\u6a21\u578b\u200b\u56fe\u200b Details \u200b\u90e8\u5206\u200b\u6240\u793a\u200b\uff0c<code>CSPLayer</code> \u200b\u7531\u200b 3 \u200b\u4e2a\u200b <code>ConvModule</code> + n \u200b\u4e2a\u200b <code>CSPNeXt Block</code>(\u200b\u5e26\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b) + 1 \u200b\u4e2a\u200b  <code>Channel Attention</code> \u200b\u6a21\u5757\u200b\u7ec4\u6210\u200b\u3002<code>ConvModule</code> \u200b\u4e3a\u200b 1 \u200b\u5c42\u200b 3x3 <code>Conv2d</code> + <code>BatchNorm</code> + <code>SiLU</code> \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002<code>Channel Attention</code> \u200b\u6a21\u5757\u200b\u4e3a\u200b 1 \u200b\u5c42\u200b <code>AdaptiveAvgPool2d</code> + 1 \u200b\u5c42\u200b 1x1 <code>Conv2d</code> + <code>Hardsigmoid</code> \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002<code>CSPNeXt Block</code> \u200b\u6a21\u5757\u200b\u5728\u200b\u4e0b\u8282\u200b\u8be6\u7ec6\u200b\u8bb2\u8ff0\u200b\u3002</p> </li> <li> <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u9605\u8bfb\u200b Backbone - <code>CSPNeXt</code> \u200b\u7684\u200b\u6e90\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b \u200b\u70b9\u6b64\u200b \u200b\u8df3\u8f6c\u200b\u3002</p> </li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#122-cspnext-block","title":"1.2.2 CSPNeXt Block","text":"<p>Darknet \uff08\u200b\u56fe\u200b a\uff09\u200b\u4f7f\u7528\u200b 1x1 \u200b\u4e0e\u200b 3x3 \u200b\u5377\u79ef\u200b\u7684\u200b <code>Basic Block</code>\u3002YOLOv6 \u3001YOLOv7 \u3001PPYOLO-E \uff08\u200b\u56fe\u200b b &amp; c\uff09\u200b\u4f7f\u7528\u200b\u4e86\u200b\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b Block\u3002\u200b\u4f46\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4ee3\u4ef7\u200b\u9ad8\u200b\uff0c\u200b\u4e14\u200b\u4e0d\u6613\u200b\u91cf\u5316\u200b\uff0c\u200b\u9700\u8981\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u6765\u200b\u5f25\u8865\u200b\u91cf\u5316\u200b\u8bef\u5dee\u200b\u3002 RTMDet \u200b\u5219\u200b\u501f\u9274\u200b\u4e86\u200b\u6700\u8fd1\u200b\u6bd4\u8f83\u200b\u70ed\u95e8\u200b\u7684\u200b ConvNeXt \u3001RepLKNet \u200b\u7684\u200b\u505a\u6cd5\u200b\uff0c\u200b\u4e3a\u200b <code>Basic Block</code> \u200b\u52a0\u5165\u200b\u4e86\u200b\u5927\u200b kernel \u200b\u7684\u200b <code>depth-wise</code> \u200b\u5377\u79ef\u200b\uff08\u200b\u56fe\u200b d\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u547d\u540d\u200b\u4e3a\u200b <code>CSPNeXt Block</code>\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u4e0d\u540c\u200b kernel \u200b\u5927\u5c0f\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\u3002</p> Kernel  size params flops latency-bs1-TRT-FP16 / ms mAP 3x3 50.8 79.61G 2.1 50.0 5x5 50.92M 79.7G 2.11 50.9 7x7 51.1 80.34G 2.73 51.1 <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u9605\u8bfb\u200b <code>Basic Block</code> \u200b\u548c\u200b <code>CSPNeXt Block</code> \u200b\u6e90\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u70b9\u6b64\u200b\u8df3\u8f6c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#123-stage-block","title":"1.2.3 \u200b\u8c03\u6574\u200b\u68c0\u6d4b\u5668\u200b\u4e0d\u540c\u200b stage \u200b\u95f4\u200b\u7684\u200b block \u200b\u6570","text":"<p>\u200b\u7531\u4e8e\u200b <code>CSPNeXt Block</code> \u200b\u5185\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>depth-wise</code> \u200b\u5377\u79ef\u200b\uff0c\u200b\u5355\u4e2a\u200b block \u200b\u5185\u200b\u7684\u200b\u5c42\u6570\u200b\u589e\u591a\u200b\u3002\u200b\u5982\u679c\u200b\u4fdd\u6301\u200b\u539f\u6709\u200b\u7684\u200b stage \u200b\u5185\u200b\u7684\u200b block \u200b\u6570\u200b\uff0c\u200b\u5219\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u5927\u5e45\u200b\u964d\u4f4e\u200b\u3002</p> <p>RTMDet \u200b\u91cd\u65b0\u200b\u8c03\u6574\u200b\u4e86\u200b\u4e0d\u540c\u200b stage \u200b\u95f4\u200b\u7684\u200b block \u200b\u6570\u200b\uff0c\u200b\u5e76\u200b\u8c03\u6574\u200b\u4e86\u200b\u901a\u9053\u200b\u7684\u200b\u8d85\u53c2\u200b\uff0c\u200b\u5728\u200b\u4fdd\u8bc1\u200b\u4e86\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u63d0\u5347\u200b\u4e86\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u4e0d\u540c\u200b block \u200b\u6570\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\u3002</p> Num  blocks params flops latency-bs1-TRT-FP16 / ms mAP L+3-9-9-3 53.4 86.28 2.6 51.4 L+3-6-6-3 50.92M 79.7G 2.11 50.9 L+3-6-6-3  + channel attention 52.3M 79.9G 2.4 51.3 <p>\u200b\u6700\u540e\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u6a21\u578b\u200b\u7684\u200b block \u200b\u6570\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u89c1\u200b\u6e90\u7801\u200b \u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#124-neck","title":"1.2.4 Neck","text":"<p>Neck \u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u548c\u200b YOLOX \u200b\u51e0\u4e4e\u200b\u4e00\u6837\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u5185\u90e8\u200b\u7684\u200b block \u200b\u8fdb\u884c\u200b\u4e86\u200b\u66ff\u6362\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#125-backbone-neck","title":"1.2.5 Backbone \u200b\u4e0e\u200b Neck \u200b\u4e4b\u95f4\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u548c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u7684\u200b\u5747\u8861","text":"<p>EfficientDet \u3001NASFPN \u200b\u7b49\u200b\u5de5\u4f5c\u200b\u5728\u200b\u6539\u8fdb\u200b Neck \u200b\u65f6\u200b\u5f80\u5f80\u200b\u805a\u7126\u200b\u4e8e\u200b\u5982\u4f55\u200b\u4fee\u6539\u200b\u7279\u5f81\u200b\u878d\u5408\u200b\u7684\u200b\u65b9\u5f0f\u200b\u3002 \u200b\u4f46\u200b\u5f15\u5165\u200b\u8fc7\u591a\u200b\u7684\u200b\u8fde\u63a5\u200b\u4f1a\u200b\u589e\u52a0\u200b\u68c0\u6d4b\u5668\u200b\u7684\u200b\u5ef6\u65f6\u200b\uff0c\u200b\u5e76\u200b\u589e\u52a0\u200b\u5185\u5b58\u200b\u5f00\u9500\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b RTMDet \u200b\u9009\u62e9\u200b\u4e0d\u200b\u5f15\u5165\u200b\u989d\u5916\u200b\u7684\u200b\u8fde\u63a5\u200b\uff0c\u200b\u800c\u662f\u200b\u6539\u53d8\u200b Backbone \u200b\u4e0e\u200b Neck \u200b\u95f4\u200b\u53c2\u200b\u6570\u91cf\u200b\u7684\u200b\u914d\u6bd4\u200b\u3002\u200b\u8be5\u200b\u914d\u6bd4\u200b\u662f\u200b\u901a\u8fc7\u200b\u624b\u52a8\u200b\u8c03\u6574\u200b Backbone \u200b\u548c\u200b Neck \u200b\u7684\u200b <code>expand_ratio</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff0c\u200b\u5176\u200b\u6570\u503c\u200b\u5728\u200b Backbone \u200b\u548c\u200b Neck \u200b\u4e2d\u200b\u90fd\u200b\u4e3a\u200b 0.5\u3002<code>expand_ratio</code>  \u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u6539\u53d8\u200b  <code>CSPLayer</code>  \u200b\u4e2d\u200b\u5404\u5c42\u200b\u901a\u9053\u200b\u6570\u200b\u7684\u200b\u53c2\u6570\u200b\uff08\u200b\u5177\u4f53\u200b\u53ef\u89c1\u200b\u6a21\u578b\u200b\u56fe\u200b <code>CSPLayer</code> \u200b\u90e8\u5206\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u60f3\u200b\u8fdb\u884c\u200b\u4e0d\u540c\u200b\u914d\u6bd4\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b backbone {expand_ratio} \u200b\u548c\u200b neck {expand_ratio} \u200b\u53c2\u6570\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u5b9e\u9a8c\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5f53\u200b Neck \u200b\u5728\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u5360\u200b\u6bd4\u200b\u66f4\u200b\u9ad8\u65f6\u200b\uff0c\u200b\u5ef6\u65f6\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u4e14\u200b\u5bf9\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u5f71\u54cd\u200b\u5f88\u5c0f\u200b\u3002\u200b\u4f5c\u8005\u200b\u5728\u200b\u76f4\u64ad\u200b\u7b54\u7591\u200b\u65f6\u200b\u56de\u590d\u200b\uff0cRTMDet \u200b\u5728\u200b Neck \u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u53c2\u8003\u200b\u4e86\u200b GiraffeDet \u200b\u7684\u200b\u505a\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u50cf\u200b GiraffeDet \u200b\u4e00\u6837\u200b\u5f15\u5165\u200b\u989d\u5916\u200b\u8fde\u63a5\u200b\uff08\u200b\u8be6\u7ec6\u200b\u53ef\u200b\u53c2\u89c1\u200b RTMDet \u200b\u53d1\u5e03\u200b\u89c6\u9891\u200b 31\u200b\u5206\u200b40\u200b\u79d2\u5de6\u53f3\u200b\u7684\u200b\u5185\u5bb9\u200b\uff09\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u4e0d\u540c\u200b\u53c2\u200b\u6570\u91cf\u200b\u914d\u6bd4\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\u3002</p> Model  size Backbone Neck params flops latency  / ms mAP S 47% 45% 8.54M 15.76G 1.21 43.9 S 63% 29% 9.01M 15.85G 1.37 43.7 L 47% 45% 50.92M 79.7G 2.11 50.9 L 63% 29% 57.43M 93.73 2.57 51.0 <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u9605\u8bfb\u200b Neck - <code>CSPNeXtPAFPN</code> \u200b\u7684\u200b\u6e90\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u70b9\u6b64\u200b \u200b\u8df3\u8f6c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#126-head","title":"1.2.6 Head","text":"<p>\u200b\u4f20\u7edf\u200b\u7684\u200b YOLO \u200b\u7cfb\u5217\u200b\u90fd\u200b\u4f7f\u7528\u200b\u540c\u4e00\u200b Head \u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u548c\u200b\u56de\u5f52\u200b\u3002YOLOX \u200b\u5219\u200b\u5c06\u200b\u5206\u7c7b\u200b\u548c\u200b\u56de\u5f52\u200b\u5206\u652f\u200b\u89e3\u200b\u8026\u200b\uff0cPPYOLO-E \u200b\u548c\u200b YOLOv6 \u200b\u5219\u200b\u5f15\u5165\u200b\u4e86\u200b TOOD \u200b\u4e2d\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002\u200b\u5b83\u4eec\u200b\u5728\u200b\u4e0d\u540c\u200b\u7279\u5f81\u200b\u5c42\u7ea7\u200b\u4e4b\u95f4\u200b\u90fd\u200b\u4f7f\u7528\u200b\u72ec\u7acb\u200b\u7684\u200b Head\uff0c\u200b\u56e0\u6b64\u200b Head \u200b\u5728\u200b\u6a21\u578b\u200b\u4e2d\u200b\u4e5f\u200b\u5360\u6709\u200b\u8f83\u200b\u591a\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u3002</p> <p>RTMDet \u200b\u53c2\u8003\u200b\u4e86\u200b NAS-FPN \u200b\u4e2d\u200b\u7684\u200b\u505a\u6cd5\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>SepBNHead</code>\uff0c\u200b\u5728\u200b\u4e0d\u540c\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u5171\u4eab\u200b\u5377\u79ef\u200b\u6743\u91cd\u200b\uff0c\u200b\u4f46\u662f\u200b\u72ec\u7acb\u200b\u8ba1\u7b97\u200b BN\uff08BatchNorm\uff09 \u200b\u7684\u200b\u7edf\u8ba1\u200b\u91cf\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u4e0d\u540c\u200b\u7ed3\u6784\u200b Head \u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\u3002</p> Head  type params flops latency  / ms mAP Fully-shared  head 52.32 80.23 2.44 48.0 Separated  head 57.03 80.23 2.44 51.2 SepBN head 52.32 80.23 2.44 51.3 <p>\u200b\u540c\u65f6\u200b\uff0cRTMDet \u200b\u4e5f\u200b\u5ef6\u7eed\u200b\u4e86\u200b\u4f5c\u8005\u200b\u4e4b\u524d\u200b\u5728\u200b NanoDet \u200b\u4e2d\u200b\u7684\u200b\u601d\u60f3\u200b\uff0c\u200b\u4f7f\u7528\u200b Quality Focal Loss \uff0c\u200b\u5e76\u200b\u53bb\u6389\u200b Objectness \u200b\u5206\u652f\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200b Head \u200b\u8f7b\u91cf\u5316\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u9605\u8bfb\u200b Head \u200b\u4e2d\u200b <code>RTMDetSepBNHeadModule</code> \u200b\u7684\u200b\u6e90\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u70b9\u6b64\u200b \u200b\u8df3\u8f6c\u200b\u3002</p> <pre><code>\u200b\u6ce8\u200b\uff1aMMYOLO \u200b\u548c\u200b MMDetection \u200b\u4e2d\u200b Neck \u200b\u548c\u200b Head \u200b\u7684\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u7a0d\u200b\u6709\u200b\u4e0d\u540c\u200b\u3002\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#13","title":"1.3 \u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5339\u914d\u200b\u7b56\u7565","text":"<p>\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u6216\u8005\u200b\u79f0\u4e3a\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u7b56\u7565\u200b <code>Label Assignment</code> \u200b\u662f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u6700\u200b\u6838\u5fc3\u200b\u7684\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b, \u200b\u66f4\u597d\u200b\u7684\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u5f80\u5f80\u200b\u80fd\u591f\u200b\u4f7f\u5f97\u200b\u7f51\u7edc\u200b\u66f4\u597d\u200b\u5b66\u4e60\u200b\u5230\u200b\u7269\u4f53\u200b\u7684\u200b\u7279\u5f81\u200b\u4ee5\u200b\u63d0\u9ad8\u200b\u68c0\u6d4b\u200b\u80fd\u529b\u200b\u3002</p> <p>\u200b\u65e9\u671f\u200b\u7684\u200b\u6837\u672c\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u4e00\u822c\u200b\u90fd\u200b\u662f\u200b\u57fa\u4e8e\u200b <code>\u200b\u7a7a\u95f4\u200b\u4ee5\u53ca\u200b\u5c3a\u5ea6\u200b\u4fe1\u606f\u200b\u7684\u200b\u5148\u9a8c\u200b</code> \u200b\u6765\u200b\u51b3\u5b9a\u200b\u6837\u672c\u200b\u7684\u200b\u9009\u53d6\u200b\u3002 \u200b\u5178\u578b\u200b\u6848\u4f8b\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>FCOS</code> \u200b\u4e2d\u5148\u200b\u9650\u5b9a\u200b\u7f51\u683c\u200b\u4e2d\u5fc3\u70b9\u200b\u5728\u200b <code>GT</code> \u200b\u5185\u200b\u7b5b\u9009\u200b\u540e\u200b\u7136\u540e\u200b\u518d\u200b\u901a\u8fc7\u200b\u4e0d\u540c\u200b\u7279\u5f81\u200b\u5c42\u200b\u9650\u5236\u200b\u5c3a\u5bf8\u200b\u6765\u200b\u51b3\u5b9a\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b</li> <li><code>RetinaNet</code> \u200b\u5219\u200b\u662f\u200b\u901a\u8fc7\u200b <code>Anchor</code> \u200b\u4e0e\u200b <code>GT</code> \u200b\u7684\u200b\u6700\u5927\u200b <code>IOU</code> \u200b\u5339\u914d\u200b\u6765\u200b\u5212\u5206\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b</li> <li><code>YOLOV5</code> \u200b\u7684\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5219\u200b\u662f\u200b\u901a\u8fc7\u200b\u6837\u672c\u200b\u7684\u200b\u5bbd\u9ad8\u6bd4\u200b\u5148\u200b\u7b5b\u9009\u200b\u4e00\u90e8\u5206\u200b, \u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u9009\u53d6\u200b <code>GT</code> \u200b\u4e2d\u5fc3\u200b\u843d\u200b\u5728\u200b\u7684\u200b <code>Grid</code> \u200b\u4ee5\u53ca\u200b\u4e34\u8fd1\u200b\u7684\u200b\u4e24\u4e2a\u200b\u4f5c\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b</li> </ul> <p>\u200b\u4f46\u662f\u200b\u4e0a\u8ff0\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u662f\u200b\u5c5e\u4e8e\u200b\u57fa\u4e8e\u200b <code>\u200b\u5148\u9a8c\u200b</code> \u200b\u7684\u200b\u9759\u6001\u200b\u5339\u914d\u200b\u7b56\u7565\u200b, \u200b\u5c31\u662f\u200b\u6837\u672c\u200b\u7684\u200b\u9009\u53d6\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6839\u636e\u200b\u4eba\u200b\u7684\u200b\u7ecf\u9a8c\u200b\u89c4\u5b9a\u200b\u7684\u200b\u3002 \u200b\u4e0d\u4f1a\u200b\u968f\u7740\u200b\u7f51\u7edc\u200b\u7684\u200b\u4f18\u5316\u200b\u800c\u200b\u8fdb\u884c\u200b\u81ea\u52a8\u200b\u4f18\u5316\u200b\u9009\u53d6\u200b\u5230\u200b\u66f4\u597d\u200b\u7684\u200b\u6837\u672c\u200b, \u200b\u8fd1\u4e9b\u5e74\u200b\u6d8c\u73b0\u200b\u4e86\u200b\u8bb8\u591a\u200b\u4f18\u79c0\u200b\u7684\u200b\u52a8\u6001\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\uff1a</p> <ul> <li><code>OTA</code> \u200b\u63d0\u51fa\u200b\u4f7f\u7528\u200b <code>Sinkhorn</code> \u200b\u8fed\u4ee3\u200b\u6c42\u89e3\u200b\u5339\u914d\u200b\u4e2d\u200b\u7684\u200b\u6700\u4f18\u200b\u4f20\u8f93\u200b\u95ee\u9898\u200b</li> <li><code>YOLOX</code> \u200b\u4e2d\u200b\u4f7f\u7528\u200b <code>OTA</code> \u200b\u7684\u200b\u8fd1\u4f3c\u7b97\u6cd5\u200b <code>SimOTA</code> , <code>TOOD</code> \u200b\u5c06\u200b\u5206\u7c7b\u200b\u5206\u6570\u200b\u4ee5\u53ca\u200b <code>IOU</code> \u200b\u76f8\u4e58\u200b\u8ba1\u7b97\u200b <code>Cost</code> \u200b\u77e9\u9635\u200b\u8fdb\u884c\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u7b49\u7b49\u200b</li> </ul> <p>\u200b\u8fd9\u4e9b\u200b\u7b97\u6cd5\u200b\u5c06\u200b <code>\u200b\u9884\u6d4b\u200b\u7684\u200b Bboxes \u200b\u4e0e\u200b GT \u200b\u7684\u200b IOU</code> \u200b\u548c\u200b <code>\u200b\u5206\u7c7b\u200b\u5206\u6570\u200b</code> \u200b\u6216\u8005\u200b\u662f\u200b\u5bf9\u5e94\u200b <code>\u200b\u5206\u7c7b\u200b Loss</code> \u200b\u548c\u200b <code>\u200b\u56de\u5f52\u200b Loss</code> \u200b\u62ff\u6765\u200b\u8ba1\u7b97\u200b <code>Matching Cost</code> \u200b\u77e9\u9635\u200b\u518d\u200b\u901a\u8fc7\u200b <code>top-k</code> \u200b\u7684\u200b\u65b9\u5f0f\u200b\u52a8\u6001\u200b\u51b3\u5b9a\u200b\u6837\u672c\u200b\u9009\u53d6\u200b\u4ee5\u53ca\u200b\u6837\u672c\u200b\u4e2a\u6570\u200b\u3002\u200b\u901a\u8fc7\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b, \u200b\u5728\u200b\u7f51\u7edc\u200b\u4f18\u5316\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u9009\u53d6\u200b\u5bf9\u200b\u5206\u7c7b\u200b\u6216\u8005\u200b\u56de\u5f52\u200b\u66f4\u52a0\u200b\u654f\u611f\u200b\u6709\u6548\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u6837\u672c\u200b, \u200b\u5b83\u200b\u4e0d\u518d\u200b\u53ea\u200b\u4f9d\u8d56\u200b\u5148\u9a8c\u200b\u7684\u200b\u9759\u6001\u200b\u7684\u200b\u4fe1\u606f\u200b, \u200b\u800c\u662f\u200b\u4f7f\u7528\u200b\u5f53\u524d\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u53bb\u200b\u52a8\u6001\u200b\u5bfb\u627e\u200b\u6700\u4f18\u200b\u7684\u200b\u5339\u914d\u200b, \u200b\u53ea\u8981\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u8d8a\u200b\u51c6\u786e\u200b, \u200b\u5339\u914d\u200b\u7b97\u6cd5\u200b\u6c42\u5f97\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e5f\u200b\u4f1a\u200b\u66f4\u200b\u4f18\u79c0\u200b\u3002\u200b\u4f46\u662f\u200b\u5728\u200b\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u7684\u200b\u521d\u671f\u200b, \u200b\u7f51\u7edc\u200b\u7684\u200b\u5206\u7c7b\u200b\u4ee5\u53ca\u200b\u56de\u5f52\u200b\u662f\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b, \u200b\u8fd9\u4e2a\u200b\u65f6\u5019\u200b\u8fd8\u662f\u200b\u9700\u8981\u200b <code>\u200b\u5148\u9a8c\u200b</code> \u200b\u6765\u200b\u7ea6\u675f\u200b, \u200b\u4ee5\u200b\u8fbe\u5230\u200b <code>\u200b\u51b7\u542f\u52a8\u200b</code> \u200b\u7684\u200b\u6548\u679c\u200b\u3002</p> <p><code>RTMDet</code> \u200b\u4f5c\u8005\u200b\u4e5f\u200b\u662f\u200b\u91c7\u7528\u200b\u4e86\u200b\u52a8\u6001\u200b\u7684\u200b <code>SimOTA</code> \u200b\u505a\u6cd5\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u5176\u200b\u5bf9\u200b\u52a8\u6001\u200b\u7684\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5206\u914d\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\u3002 \u200b\u4e4b\u524d\u200b\u7684\u200b\u52a8\u6001\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\uff08 <code>HungarianAssigner</code> \u3001<code>OTA</code> \uff09\u200b\u5f80\u5f80\u200b\u4f7f\u7528\u200b\u4e0e\u200b <code>Loss</code> \u200b\u5b8c\u5168\u4e00\u81f4\u200b\u7684\u200b\u4ee3\u4ef7\u200b\u51fd\u6570\u200b\u4f5c\u4e3a\u200b\u5339\u914d\u200b\u7684\u200b\u4f9d\u636e\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7ecf\u8fc7\u200b\u5b9e\u9a8c\u200b\u53d1\u73b0\u200b\u8fd9\u200b\u5e76\u4e0d\u4e00\u5b9a\u200b\u65f6\u200b\u6700\u4f18\u200b\u7684\u200b\u3002 \u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b <code>Soften</code> \u200b\u7684\u200b <code>Cost</code> \u200b\u4ee5\u53ca\u200b\u5148\u9a8c\u200b\uff0c\u200b\u80fd\u591f\u200b\u63d0\u5347\u200b\u6027\u80fd\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#131-bbox","title":"1.3.1 Bbox \u200b\u7f16\u89e3\u7801\u200b\u8fc7\u7a0b","text":"<p>RTMDet \u200b\u7684\u200b BBox Coder \u200b\u91c7\u7528\u200b\u7684\u200b\u662f\u200b <code>mmdet.DistancePointBBoxCoder</code>\u3002</p> <p>\u200b\u8be5\u7c7b\u200b\u7684\u200b docstring \u200b\u4e3a\u200b <code>This coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original.</code></p> <p>\u200b\u7f16\u7801\u5668\u200b\u5c06\u200b gt bboxes (x1, y1, x2, y2) \u200b\u7f16\u7801\u200b\u4e3a\u200b (top, bottom, left, right)\uff0c\u200b\u5e76\u4e14\u200b\u89e3\u7801\u200b\u81f3\u539f\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u3002</p> <p>MMDet \u200b\u7f16\u7801\u200b\u7684\u200b\u6838\u5fc3\u200b\u6e90\u7801\u200b\uff1a</p> <pre><code>def bbox2distance(points: Tensor, bbox: Tensor, ...) -&gt; Tensor:\n    \"\"\"\n        points (Tensor): \u200b\u76f8\u5f53\u4e8e\u200b scale \u200b\u503c\u200b stride \uff0c\u200b\u4e14\u200b\u6bcf\u4e2a\u200b\u9884\u6d4b\u200b\u70b9\u4ec5\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u6b63\u65b9\u5f62\u200b anchor \u200b\u7684\u200b anchor point [x, y]\uff0cShape (n, 2) or (b, n, 2).\n        bbox (Tensor): Bbox \u200b\u4e3a\u200b\u4e58\u200b\u4e0a\u200b stride \u200b\u7684\u200b\u7f51\u7edc\u200b\u9884\u6d4b\u503c\u200b\uff0c\u200b\u683c\u5f0f\u200b\u4e3a\u200b xyxy\uff0cShape (n, 4) or (b, n, 4).\n    \"\"\"\n    # \u200b\u8ba1\u7b97\u200b\u70b9\u200b\u8ddd\u79bb\u200b\u56db\u8fb9\u200b\u7684\u200b\u8ddd\u79bb\u200b\n    left = points[..., 0] - bbox[..., 0]\n    top = points[..., 1] - bbox[..., 1]\n    right = bbox[..., 2] - points[..., 0]\n    bottom = bbox[..., 3] - points[..., 1]\n\n    ...\n\n    return torch.stack([left, top, right, bottom], -1)\n</code></pre> <p>MMDetection \u200b\u89e3\u7801\u200b\u7684\u200b\u6838\u5fc3\u200b\u6e90\u7801\u200b\uff1a</p> <pre><code>def distance2bbox(points: Tensor, distance: Tensor, ...) -&gt; Tensor:\n    \"\"\"\n        \u200b\u901a\u8fc7\u200b\u8ddd\u79bb\u200b\u53cd\u7b97\u200b bbox \u200b\u7684\u200b xyxy\n        points (Tensor): \u200b\u6b63\u65b9\u5f62\u200b\u7684\u200b\u9884\u6d4b\u200b anchor \u200b\u7684\u200b anchor point [x, y]\uff0cShape (B, N, 2) or (N, 2).\n        distance (Tensor): \u200b\u8ddd\u79bb\u200b\u56db\u8fb9\u200b\u7684\u200b\u8ddd\u79bb\u200b\u3002(left, top, right, bottom). Shape (B, N, 4) or (N, 4)\n    \"\"\"\n\n    # \u200b\u53cd\u7b97\u200b bbox xyxy\n    x1 = points[..., 0] - distance[..., 0]\n    y1 = points[..., 1] - distance[..., 1]\n    x2 = points[..., 0] + distance[..., 2]\n    y2 = points[..., 1] + distance[..., 3]\n\n    bboxes = torch.stack([x1, y1, x2, y2], -1)\n\n    ...\n\n    return bboxes\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#132","title":"1.3.2 \u200b\u5339\u914d\u200b\u7b56\u7565","text":"<p><code>RTMDet</code> \u200b\u63d0\u51fa\u200b\u4e86\u200b <code>Dynamic Soft Label Assigner</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u6807\u7b7e\u200b\u7684\u200b\u52a8\u6001\u200b\u5339\u914d\u200b\u7b56\u7565\u200b, \u200b\u8be5\u200b\u65b9\u6cd5\u200b\u4e3b\u8981\u200b\u5305\u62ec\u200b\u4f7f\u7528\u200b \u200b\u4f4d\u7f6e\u200b\u5148\u9a8c\u200b\u4fe1\u606f\u200b\u635f\u5931\u200b , \u200b\u6837\u672c\u200b\u56de\u5f52\u200b\u635f\u5931\u200b , \u200b\u6837\u672c\u200b\u5206\u7c7b\u200b\u635f\u5931\u200b , \u200b\u540c\u65f6\u200b\u5bf9\u200b\u4e09\u4e2a\u200b\u635f\u5931\u200b\u8fdb\u884c\u200b\u4e86\u200b <code>Soft</code> \u200b\u5904\u7406\u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u8c03\u4f18\u200b, \u200b\u4ee5\u200b\u8fbe\u5230\u6700\u4f73\u200b\u7684\u200b\u52a8\u6001\u200b\u5339\u914d\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u8be5\u200b\u65b9\u6cd5\u200b Matching Cost \u200b\u77e9\u9635\u200b\u7531\u200b\u5982\u4e0b\u200b\u635f\u5931\u200b\u6784\u6210\u200b\uff1a</p> <pre><code>cost_matrix = soft_cls_cost + iou_cost + soft_center_prior\n</code></pre> <ol> <li>Soft_Center_Prior</li> </ol> <pre><code>C\\_{center} = \\\\alpha^{|x\\_{pred}-x\\_{gt}|-\\\\beta}\n</code></pre> <pre><code># valid_prior Tensor[N,4] \u200b\u8868\u793a\u200banchor point\n# 4\u200b\u5206\u522b\u200b\u8868\u793a\u200b x, y, \u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7279\u5f81\u200b\u5c42\u200b\u7684\u200b stride, stride\ngt_center = (gt_bboxes[:, :2] + gt_bboxes[:, 2:]) / 2.0\nvalid_prior = priors[valid_mask]\nstrides = valid_prior[:, 2]\n# \u200b\u8ba1\u7b97\u200bgt\u200b\u4e0e\u200banchor point\u200b\u7684\u200b\u4e2d\u5fc3\u200b\u8ddd\u79bb\u200b\u5e76\u200b\u8f6c\u6362\u200b\u5230\u200b\u7279\u5f81\u200b\u56fe\u200b\u5c3a\u5ea6\u200b\ndistance = (valid_prior[:, None, :2] - gt_center[None, :, :]\n                    ).pow(2).sum(-1).sqrt() / strides[:, None]\n# \u200b\u4ee5\u200b10\u200b\u4e3a\u5e95\u200b\u8ba1\u7b97\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u8f6f\u5316\u200b\u635f\u5931\u200b,\u200b\u9650\u5b9a\u200b\u5728\u200bgt\u200b\u7684\u200b6\u200b\u4e2a\u200b\u5355\u5143\u683c\u200b\u4ee5\u5185\u200b\nsoft_center_prior = torch.pow(10, distance - 3)\n</code></pre> <ol> <li>IOU_Cost</li> </ol> <pre><code>C\\_{reg} = -log(IOU)\n</code></pre> <pre><code># \u200b\u8ba1\u7b97\u200b\u56de\u5f52\u200b bboxes \u200b\u548c\u200b gts \u200b\u7684\u200b iou\npairwise_ious = self.iou_calculator(valid_decoded_bbox, gt_bboxes)\n# \u200b\u5c06\u200b iou \u200b\u4f7f\u7528\u200b log \u200b\u8fdb\u884c\u200b soft , iou \u200b\u8d8a\u5c0f\u200b cost \u200b\u66f4\u200b\u5c0f\u200b\niou_cost = -torch.log(pairwise_ious + EPS) * 3\n</code></pre> <ol> <li>Soft_Cls_Cost</li> </ol> <pre><code>C\\_{cls} = CE(P,Y\\_{soft}) \\*(Y\\_{soft}-P)^2\n</code></pre> <pre><code># \u200b\u751f\u6210\u200b\u5206\u7c7b\u200b\u6807\u7b7e\u200b\n gt_onehot_label = (\n    F.one_hot(gt_labels.to(torch.int64),\n              pred_scores.shape[-1]).float().unsqueeze(0).repeat(\n                  num_valid, 1, 1))\nvalid_pred_scores = valid_pred_scores.unsqueeze(1).repeat(1, num_gt, 1)\n# \u200b\u4e0d\u200b\u5355\u5355\u200b\u5c06\u200b\u5206\u7c7b\u200b\u6807\u7b7e\u200b\u4e3a\u200b01,\u200b\u800c\u662f\u200b\u6362\u6210\u200b\u4e0e\u200b gt \u200b\u7684\u200b iou\nsoft_label = gt_onehot_label * pairwise_ious[..., None]\n# \u200b\u4f7f\u7528\u200b quality focal loss \u200b\u8ba1\u7b97\u200b\u5206\u7c7b\u200b\u635f\u5931\u200b cost ,\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u7684\u200b\u5206\u7c7b\u200b\u635f\u5931\u200b\u8ba1\u7b97\u200b\u4fdd\u6301\u4e00\u81f4\u200b\nscale_factor = soft_label - valid_pred_scores.sigmoid()\nsoft_cls_cost = F.binary_cross_entropy_with_logits(\n    valid_pred_scores, soft_label,\n    reduction='none') * scale_factor.abs().pow(2.0)\nsoft_cls_cost = soft_cls_cost.sum(dim=-1)\n</code></pre> <p>\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u200b\u4e0a\u8ff0\u200b\u4e09\u4e2a\u200b\u635f\u5931\u200b\u7684\u200b\u548c\u200b\u5f97\u5230\u200b\u6700\u7ec8\u200b\u7684\u200b <code>cost_matrix</code> \u200b\u540e\u200b, \u200b\u518d\u200b\u4f7f\u7528\u200b <code>SimOTA</code> \u200b\u51b3\u5b9a\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>GT</code> \u200b\u5339\u914d\u200b\u7684\u200b\u6837\u672c\u200b\u7684\u200b\u4e2a\u6570\u200b\u5e76\u200b\u51b3\u5b9a\u200b\u6700\u7ec8\u200b\u7684\u200b\u6837\u672c\u200b\u3002\u200b\u5177\u4f53\u64cd\u4f5c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <ol> <li>\u200b\u9996\u5148\u200b\u901a\u8fc7\u200b\u81ea\u200b\u9002\u5e94\u200b\u8ba1\u7b97\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>gt</code> \u200b\u8981\u200b\u9009\u53d6\u200b\u7684\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\uff1a \u200b\u53d6\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>gt</code> \u200b\u4e0e\u200b\u6240\u6709\u200b <code>bboxes</code> \u200b\u524d\u200b <code>13</code> \u200b\u5927\u200b\u7684\u200b <code>iou</code>, \u200b\u5f97\u5230\u200b\u5b83\u4eec\u200b\u7684\u200b\u548c\u200b\u53d6\u6574\u200b\u540e\u200b\u4f5c\u4e3a\u200b\u8fd9\u4e2a\u200b <code>gt</code> \u200b\u7684\u200b <code>\u200b\u6837\u672c\u200b\u6570\u76ee\u200b</code> , \u200b\u6700\u5c11\u200b\u4e3a\u200b <code>1</code> \u200b\u4e2a\u200b, \u200b\u8bb0\u200b\u4e3a\u200b <code>dynamic_ks</code>\u3002</li> <li>\u200b\u5bf9\u4e8e\u200b\u6bcf\u200b\u4e00\u4e2a\u200b <code>gt</code> , \u200b\u5c06\u200b\u5176\u200b <code>cost_matrix</code> \u200b\u77e9\u9635\u200b\u524d\u200b <code>dynamic_ks</code> \u200b\u5c0f\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u4f5c\u4e3a\u200b\u8be5\u200b <code>gt</code> \u200b\u7684\u200b\u6b63\u200b\u6837\u672c\u200b\u3002</li> <li>\u200b\u5bf9\u4e8e\u200b\u67d0\u200b\u4e00\u4e2a\u200b <code>bbox</code>, \u200b\u5982\u679c\u200b\u88ab\u200b\u5339\u914d\u200b\u5230\u200b\u591a\u4e2a\u200b <code>gt</code> \u200b\u5c31\u200b\u5c06\u200b\u4e0e\u200b\u8fd9\u4e9b\u200b <code>gts</code> \u200b\u7684\u200b <code>cost_marix</code> \u200b\u4e2d\u200b\u6700\u5c0f\u200b\u7684\u200b\u90a3\u4e2a\u200b\u4f5c\u4e3a\u200b\u5176\u200b <code>label</code>\u3002</li> </ol> <p>\u200b\u5728\u200b\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u521d\u671f\u200b\uff0c\u200b\u56e0\u200b\u53c2\u6570\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u56de\u5f52\u200b\u548c\u200b\u5206\u7c7b\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b <code>Cost</code> \u200b\u5f80\u5f80\u200b\u8f83\u5927\u200b, \u200b\u8fd9\u65f6\u5019\u200b <code>IOU</code> \u200b\u6bd4\u8f83\u200b\u5c0f\u200b\uff0c \u200b\u9009\u53d6\u200b\u7684\u200b\u6837\u672c\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u4e3b\u8981\u200b\u8d77\u200b\u4f5c\u7528\u200b\u7684\u200b\u662f\u200b <code>Soft_center_prior</code> \u200b\u4e5f\u200b\u5c31\u662f\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f18\u5148\u200b\u9009\u53d6\u200b\u4f4d\u7f6e\u200b\u8ddd\u79bb\u200b <code>GT</code> \u200b\u6bd4\u8f83\u200b\u8fd1\u200b\u7684\u200b\u6837\u672c\u200b\u4f5c\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u7b26\u5408\u200b\u4eba\u4eec\u200b\u7684\u200b\u7406\u89e3\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u200b\u524d\u671f\u200b\u7ed9\u200b\u5c11\u91cf\u200b\u5e76\u4e14\u200b\u6709\u200b\u8db3\u591f\u200b\u8d28\u91cf\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u4ee5\u200b\u8fbe\u5230\u200b\u51b7\u542f\u52a8\u200b\u3002 \u200b\u5f53\u200b\u7f51\u7edc\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\u8fc7\u540e\u200b\uff0c\u200b\u5206\u7c7b\u200b\u5206\u652f\u200b\u548c\u200b\u56de\u5f52\u200b\u5206\u652f\u200b\u90fd\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u7684\u200b\u4f18\u5316\u200b\u540e\u200b\uff0c\u200b\u8fd9\u65f6\u200b <code>IOU</code> \u200b\u53d8\u200b\u5927\u200b\uff0c \u200b\u9009\u53d6\u200b\u7684\u200b\u6837\u672c\u200b\u4e5f\u200b\u9010\u6e10\u200b\u589e\u591a\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u7f51\u7edc\u200b\u4e5f\u200b\u6709\u200b\u80fd\u529b\u200b\u5b66\u4e60\u200b\u5230\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u540c\u65f6\u200b\u56e0\u4e3a\u200b <code>IOU_Cost</code> \u200b\u4ee5\u53ca\u200b <code>Soft_Cls_Cost</code> \u200b\u53d8\u5c0f\u200b\uff0c\u200b\u7f51\u7edc\u200b\u4e5f\u200b\u4f1a\u200b\u52a8\u6001\u200b\u7684\u200b\u627e\u5230\u200b\u66f4\u200b\u6709\u5229\u200b\u4f18\u5316\u200b\u5206\u7c7b\u200b\u4ee5\u53ca\u200b\u56de\u5f52\u200b\u7684\u200b\u6837\u672c\u200b\u70b9\u200b\u3002</p> <p>\u200b\u5728\u200b <code>Resnet50-1x</code> \u200b\u7684\u200b\u4e09\u79cd\u200b\u635f\u5931\u200b\u7684\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\uff1a</p> Soft_cls_cost Soft_center_prior Log_IoU_cost mAP \u00d7 \u00d7 \u00d7 39.9 \u221a \u00d7 \u00d7 40.3 \u221a \u221a \u00d7 40.8 \u221a \u221a \u221a 41.3 <p>\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u4e3b\u6d41\u200b <code>Assign</code> \u200b\u65b9\u6cd5\u200b\u5728\u200b <code>Resnet50-1x</code> \u200b\u7684\u200b\u5bf9\u6bd4\u200b\u5b9e\u9a8c\u200b\uff1a</p> method mAP ATSS 39.2 PAA 40.4 OTA 40.7 TOOD(w/o TAH) 40.7 Ours 41.3 <p>\u200b\u65e0\u8bba\u662f\u200b <code>Resnet50-1x</code> \u200b\u8fd8\u662f\u200b\u6807\u51c6\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u4e0b\u200b\uff0c\u200b\u8fd8\u662f\u200b\u5728\u200b<code>300epoch</code> + <code>havy augmentation</code>,  \u200b\u76f8\u6bd4\u200b\u4e8e\u200b <code>SimOTA</code> \u3001 <code>OTA</code> \u200b\u4ee5\u53ca\u200b <code>TOOD</code> \u200b\u4e2d\u200b\u7684\u200b <code>TAL</code> \u200b\u5747\u200b\u6709\u200b\u63d0\u5347\u200b\u3002</p> 300e + Mosaic &amp; MixUP mAP RTMDet-s + SimOTA 43.2 RTMDet-s + DSLA 44.5"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#14-loss","title":"1.4 Loss \u200b\u8bbe\u8ba1","text":"<p>\u200b\u53c2\u4e0e\u200b Loss \u200b\u8ba1\u7b97\u200b\u7684\u200b\u5171\u6709\u200b\u4e24\u4e2a\u200b\u503c\u200b\uff1a<code>loss_cls</code> \u200b\u548c\u200b <code>loss_bbox</code>\uff0c\u200b\u5176\u200b\u5404\u81ea\u200b\u4f7f\u7528\u200b\u7684\u200b Loss \u200b\u65b9\u6cd5\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>loss_cls</code>\uff1a<code>mmdet.QualityFocalLoss</code></li> <li><code>loss_bbox</code>\uff1a<code>mmdet.GIoULoss</code></li> </ul> <p>\u200b\u6743\u91cd\u200b\u6bd4\u4f8b\u200b\u662f\u200b\uff1a<code>loss_cls</code> : <code>loss_bbox</code> = <code>1 : 2</code></p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#qualityfocalloss","title":"QualityFocalLoss","text":"<p>Quality Focal Loss (QFL) \u200b\u662f\u200b Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection \u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>\u200b\u666e\u901a\u200b\u7684\u200b Focal Loss \u200b\u516c\u5f0f\u200b\uff1a</p> <pre><code>{FL}(p) = -(1-p_t)^\\gamma\\log(p_t),p_t = \\begin{cases}\np, &amp; {when} \\ y = 1 \\\\\n1 - p, &amp; {when} \\ y = 0\n\\end{cases}\n</code></pre> <p>\u200b\u5176\u4e2d\u200b {math}<code>y\\in{1,0}</code> \u200b\u6307\u5b9a\u200b\u771f\u5b9e\u200b\u7c7b\u200b\uff0c{math}<code>p\\in[0,1]</code> \u200b\u8868\u793a\u200b\u6807\u7b7e\u200b {math}<code>y = 1</code> \u200b\u7684\u200b\u7c7b\u200b\u4f30\u8ba1\u200b\u6982\u7387\u200b\u3002{math}<code>\\gamma</code> \u200b\u662f\u200b\u53ef\u8c03\u200b\u805a\u7126\u200b\u53c2\u6570\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0cFL \u200b\u7531\u200b\u6807\u51c6\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u90e8\u5206\u200b {math}<code>-\\log(p_t)</code> \u200b\u548c\u200b\u52a8\u6001\u200b\u6bd4\u4f8b\u200b\u56e0\u5b50\u200b\u90e8\u5206\u200b {math}<code>-(1-p_t)^\\gamma</code> \u200b\u7ec4\u6210\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6bd4\u4f8b\u200b\u56e0\u5b50\u200b {math}<code>-(1-p_t)^\\gamma</code> \u200b\u5728\u200b\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u81ea\u52a8\u200b\u964d\u4f4e\u200b\u7b80\u5355\u200b\u7c7b\u200b\u5bf9\u4e8e\u200b loss \u200b\u7684\u200b\u6bd4\u91cd\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fc5\u901f\u200b\u5c06\u200b\u6a21\u578b\u200b\u96c6\u4e2d\u200b\u5728\u200b\u56f0\u96be\u200b\u7c7b\u4e0a\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b {math}<code>y = 0</code> \u200b\u8868\u793a\u200b\u8d28\u91cf\u200b\u5f97\u200b\u5206\u4e3a\u200b 0 \u200b\u7684\u200b\u8d1f\u200b\u6837\u672c\u200b\uff0c{math}<code>0 &lt; y \\leq 1</code> \u200b\u8868\u793a\u200b\u76ee\u6807\u200b IoU \u200b\u5f97\u200b\u5206\u4e3a\u200b y \u200b\u7684\u200b\u6b63\u200b\u6837\u672c\u200b\u3002\u200b\u4e3a\u4e86\u200b\u9488\u5bf9\u200b\u8fde\u7eed\u200b\u7684\u200b\u6807\u7b7e\u200b\uff0c\u200b\u6269\u5c55\u200b FL \u200b\u7684\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li>\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u90e8\u5206\u200b {math}<code>-\\log(p_t)</code> \u200b\u6269\u5c55\u200b\u4e3a\u200b\u5b8c\u6574\u200b\u7248\u672c\u200b {math}<code>-((1-y)\\log(1-\\sigma)+y\\log(\\sigma))</code></li> <li>\u200b\u6bd4\u4f8b\u200b\u56e0\u5b50\u200b\u90e8\u5206\u200b {math}<code>-(1-p_t)^\\gamma</code> \u200b\u88ab\u200b\u6cdb\u5316\u200b\u4e3a\u200b\u4f30\u8ba1\u200b {math}<code>\\gamma</code> \u200b\u4e0e\u5176\u200b\u8fde\u7eed\u200b\u6807\u7b7e\u200b {math}<code>y</code> \u200b\u7684\u200b\u7edd\u5bf9\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u5373\u200b {math}<code>|y-\\sigma|^\\beta (\\beta \\geq 0)</code> \u3002</li> </ol> <p>\u200b\u7ed3\u5408\u200b\u4e0a\u9762\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\u4e4b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f97\u51fa\u200b QFL \u200b\u7684\u200b\u516c\u5f0f\u200b\uff1a</p> <pre><code>{QFL}(\\sigma) = -|y-\\sigma|^\\beta((1-y)\\log(1-\\sigma)+y\\log(\\sigma))\n</code></pre> <p>\u200b\u5177\u4f53\u200b\u4f5c\u7528\u200b\u662f\u200b\uff1a\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u79bb\u6563\u200b\u6807\u7b7e\u200b\u7684\u200b <code>focal loss</code> \u200b\u6cdb\u5316\u200b\u5230\u200b\u8fde\u7eed\u200b\u6807\u7b7e\u200b\u4e0a\u200b\uff0c\u200b\u5c06\u200b bboxes \u200b\u4e0e\u200b gt \u200b\u7684\u200b IoU \u200b\u7684\u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u5206\u6570\u200b\u7684\u200b\u6807\u7b7e\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u5206\u7c7b\u200b\u5206\u6570\u200b\u4e3a\u200b\u8868\u5f81\u200b\u56de\u5f52\u200b\u8d28\u91cf\u200b\u7684\u200b\u5206\u6570\u200b\u3002</p> <p>MMDetection \u200b\u5b9e\u73b0\u200b\u6e90\u7801\u200b\u7684\u200b\u6838\u5fc3\u200b\u90e8\u5206\u200b\uff1a</p> <pre><code>@weighted_loss\ndef quality_focal_loss(pred, target, beta=2.0):\n    \"\"\"\n        pred (torch.Tensor): \u200b\u7528\u200b\u5f62\u72b6\u200b\uff08N\uff0cC\uff09\u200b\u8054\u5408\u200b\u8868\u793a\u200b\u9884\u6d4b\u200b\u5206\u7c7b\u200b\u548c\u200b\u8d28\u91cf\u200b\uff08IoU\uff09\uff0cC\u200b\u662f\u200b\u7c7b\u200b\u7684\u200b\u6570\u91cf\u200b\u3002\n        target (tuple([torch.Tensor])): \u200b\u76ee\u6807\u200b\u7c7b\u522b\u200b\u6807\u7b7e\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b\uff08N\uff0c\uff09\uff0c\u200b\u76ee\u6807\u200b\u8d28\u91cf\u200b\u6807\u7b7e\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\uff08N\uff0c\uff0c\uff09\u3002\n        beta (float): \u200b\u8ba1\u7b97\u200b\u6bd4\u4f8b\u200b\u56e0\u5b50\u200b\u7684\u200b \u03b2 \u200b\u53c2\u6570\u200b.\n    \"\"\"\n    ...\n\n    # label\u200b\u8868\u793a\u200b\u7c7b\u522b\u200bid\uff0cscore\u200b\u8868\u793a\u200b\u8d28\u91cf\u200b\u5206\u6570\u200b\n    label, score = target\n\n    # \u200b\u8d1f\u200b\u6837\u672c\u200b\u8d28\u91cf\u200b\u5206\u6570\u200b0\u200b\u6765\u200b\u8fdb\u884c\u200b\u76d1\u7763\u200b\n    pred_sigmoid = pred.sigmoid()\n    scale_factor = pred_sigmoid\n    zerolabel = scale_factor.new_zeros(pred.shape)\n\n    # \u200b\u8ba1\u7b97\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u90e8\u5206\u200b\u7684\u200b\u503c\u200b\n    loss = F.binary_cross_entropy_with_logits(\n        pred, zerolabel, reduction='none') * scale_factor.pow(beta)\n\n    # \u200b\u5f97\u51fa\u200b IoU \u200b\u5728\u200b\u533a\u95f4\u200b (0,1] \u200b\u7684\u200b bbox\n    # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n    bg_class_ind = pred.size(1)\n    pos = ((label &gt;= 0) &amp; (label &lt; bg_class_ind)).nonzero().squeeze(1)\n    pos_label = label[pos].long()\n\n    # \u200b\u6b63\u200b\u6837\u672c\u200b\u7531\u200b IoU \u200b\u8303\u56f4\u200b\u5728\u200b (0,1] \u200b\u7684\u200b bbox \u200b\u6765\u200b\u76d1\u7763\u200b\n    # \u200b\u8ba1\u7b97\u200b\u52a8\u6001\u200b\u6bd4\u4f8b\u200b\u56e0\u5b50\u200b\n    scale_factor = score[pos] - pred_sigmoid[pos, pos_label]\n\n    # \u200b\u8ba1\u7b97\u200b\u4e24\u200b\u90e8\u5206\u200b\u7684\u200b loss\n    loss[pos, pos_label] = F.binary_cross_entropy_with_logits(\n        pred[pos, pos_label], score[pos],\n        reduction='none') * scale_factor.abs().pow(beta)\n\n    # \u200b\u5f97\u51fa\u200b\u6700\u7ec8\u200b loss\n    loss = loss.sum(dim=1, keepdim=False)\n    return loss\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#giouloss","title":"GIoULoss","text":"<p>\u200b\u8bba\u6587\u200b\uff1aGeneralized Intersection over Union: A Metric and A Loss for Bounding Box Regression</p> <p>GIoU Loss \u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u4e24\u4e2a\u200b\u6846\u200b\u91cd\u53e0\u200b\u533a\u57df\u200b\u7684\u200b\u5173\u7cfb\u200b\uff0c\u200b\u91cd\u53e0\u200b\u533a\u57df\u200b\u8d8a\u5927\u200b\uff0c\u200b\u635f\u5931\u200b\u8d8a\u5c0f\u200b\uff0c\u200b\u53cd\u4e4b\u200b\u8d8a\u5927\u200b\u3002\u200b\u800c\u4e14\u200b GIoU \u200b\u662f\u200b\u5728\u200b [0,2] \u200b\u4e4b\u95f4\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5176\u503c\u200b\u88ab\u200b\u9650\u5236\u200b\u5728\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u8303\u56f4\u200b\u5185\u200b\uff0c\u200b\u6240\u4ee5\u200b\u7f51\u7edc\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u5267\u70c8\u200b\u7684\u200b\u6ce2\u52a8\u200b\uff0c\u200b\u8bc1\u660e\u200b\u4e86\u200b\u5176\u200b\u5177\u6709\u200b\u6bd4\u8f83\u200b\u597d\u200b\u7684\u200b\u7a33\u5b9a\u6027\u200b\u3002</p> <p>\u200b\u4e0b\u56fe\u200b\u662f\u200b\u57fa\u672c\u200b\u7684\u200b\u5b9e\u73b0\u200b\u6d41\u7a0b\u56fe\u200b\uff1a</p> <p>MMDetection \u200b\u5b9e\u73b0\u200b\u6e90\u7801\u200b\u7684\u200b\u6838\u5fc3\u200b\u90e8\u5206\u200b\uff1a</p> <pre><code>def bbox_overlaps(bboxes1, bboxes2, mode='iou', is_aligned=False, eps=1e-6):\n    ...\n\n    # \u200b\u6c42\u200b\u4e24\u4e2a\u200b\u533a\u57df\u200b\u7684\u200b\u9762\u79ef\u200b\n    area1 = (bboxes1[..., 2] - bboxes1[..., 0]) * (\n        bboxes1[..., 3] - bboxes1[..., 1])\n    area2 = (bboxes2[..., 2] - bboxes2[..., 0]) * (\n        bboxes2[..., 3] - bboxes2[..., 1])\n\n    if is_aligned:\n        # \u200b\u5f97\u51fa\u200b\u4e24\u4e2a\u200b bbox \u200b\u91cd\u5408\u200b\u7684\u200b\u5de6\u4e0a\u89d2\u200b lt \u200b\u548c\u200b\u53f3\u4e0b\u89d2\u200b rb\n        lt = torch.max(bboxes1[..., :2], bboxes2[..., :2])  # [B, rows, 2]\n        rb = torch.min(bboxes1[..., 2:], bboxes2[..., 2:])  # [B, rows, 2]\n\n        # \u200b\u6c42\u200b\u91cd\u5408\u200b\u9762\u79ef\u200b\n        wh = fp16_clamp(rb - lt, min=0)\n        overlap = wh[..., 0] * wh[..., 1]\n\n        if mode in ['iou', 'giou']:\n            ...\n        else:\n            union = area1\n        if mode == 'giou':\n            # \u200b\u5f97\u51fa\u200b\u4e24\u4e2a\u200b bbox \u200b\u6700\u5c0f\u200b\u51f8\u200b\u95ed\u5408\u200b\u6846\u200b\u7684\u200b\u5de6\u4e0a\u89d2\u200b lt \u200b\u548c\u200b\u53f3\u4e0b\u89d2\u200b rb\n            enclosed_lt = torch.min(bboxes1[..., :2], bboxes2[..., :2])\n            enclosed_rb = torch.max(bboxes1[..., 2:], bboxes2[..., 2:])\n    else:\n        ...\n\n    # \u200b\u6c42\u200b\u91cd\u5408\u200b\u9762\u79ef\u200b / gt bbox \u200b\u9762\u79ef\u200b \u200b\u7684\u200b\u6bd4\u7387\u200b\uff0c\u200b\u5373\u200b IoU\n    eps = union.new_tensor([eps])\n    union = torch.max(union, eps)\n    ious = overlap / union\n\n    ...\n\n    # \u200b\u6c42\u200b\u6700\u5c0f\u200b\u51f8\u200b\u95ed\u5408\u200b\u6846\u200b\u9762\u79ef\u200b\n    enclose_wh = fp16_clamp(enclosed_rb - enclosed_lt, min=0)\n    enclose_area = enclose_wh[..., 0] * enclose_wh[..., 1]\n    enclose_area = torch.max(enclose_area, eps)\n\n    # \u200b\u8ba1\u7b97\u200b giou\n    gious = ious - (enclose_area - union) / enclose_area\n    return gious\n\n@weighted_loss\ndef giou_loss(pred, target, eps=1e-7):\n    gious = bbox_overlaps(pred, target, mode='giou', is_aligned=True, eps=eps)\n    loss = 1 - gious\n    return loss\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#15","title":"1.5 \u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u548c\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#16","title":"1.6 \u200b\u63a8\u7406\u200b\u548c\u200b\u540e\u200b\u5904\u7406\u8fc7\u7a0b","text":"<p>(1) \u200b\u7279\u5f81\u200b\u56fe\u200b\u8f93\u5165\u200b</p> <p>\u200b\u9884\u6d4b\u200b\u7684\u200b\u56fe\u7247\u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\u4e3a\u200b 640 x 640, \u200b\u901a\u9053\u200b\u6570\u4e3a\u200b 3 ,\u200b\u7ecf\u8fc7\u200b CSPNeXt, CSPNeXtPAFPN \u200b\u5c42\u200b\u7684\u200b 8 \u200b\u500d\u200b\u300116 \u200b\u500d\u200b\u300132 \u200b\u500d\u200b\u4e0b\u200b\u91c7\u6837\u200b\u5f97\u5230\u200b 80 x 80, 40 x 40, 20 x 20 \u200b\u4e09\u4e2a\u200b\u5c3a\u5bf8\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u3002\u200b\u4ee5\u200b rtmdet-l \u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u6b64\u65f6\u200b\u4e09\u5c42\u200b\u901a\u9053\u200b\u6570\u90fd\u200b\u4e3a\u200b 256\uff0c\u200b\u7ecf\u8fc7\u200b <code>bbox_head</code> \u200b\u5c42\u200b\u5f97\u5230\u200b\u4e24\u4e2a\u200b\u5206\u652f\u200b\uff0c\u200b\u5206\u522b\u200b\u4e3a\u200b <code>rtm_cls</code> \u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\uff0c\u200b\u5c06\u200b\u901a\u9053\u200b\u6570\u4ece\u200b 256 \u200b\u53d8\u4e3a\u200b 80\uff0c80 \u200b\u5bf9\u5e94\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b; <code>rtm_reg</code> \u200b\u8fb9\u6846\u200b\u56de\u5f52\u200b\u5206\u652f\u200b\u5c06\u200b\u901a\u9053\u200b\u6570\u4ece\u200b 256 \u200b\u53d8\u4e3a\u200b 4\uff0c4 \u200b\u4ee3\u8868\u200b\u6846\u200b\u7684\u200b\u5750\u6807\u200b\u3002</p> <p>(2) \u200b\u521d\u59cb\u5316\u200b\u7f51\u683c\u200b</p> <p>\u200b\u6839\u636e\u200b\u7279\u5f81\u200b\u56fe\u200b\u5c3a\u5bf8\u200b\u521d\u59cb\u5316\u200b\u4e09\u4e2a\u200b\u7f51\u683c\u200b\uff0c\u200b\u5927\u5c0f\u200b\u5206\u522b\u200b\u4e3a\u200b 6400 (80 x 80)\u30011600 (40 x 40)\u3001400 (20 x 20)\uff0c\u200b\u5982\u200b\u7b2c\u4e00\u4e2a\u200b\u5c42\u200b shape \u200b\u4e3a\u200b torch.Size([ 6400, 2 ])\uff0c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u662f\u200b 2\uff0c\u200b\u4e3a\u200b\u7f51\u683c\u200b\u70b9\u200b\u7684\u200b\u6a2a\u200b\u7eb5\u5750\u6807\u200b\uff0c\u200b\u800c\u200b 6400 \u200b\u8868\u793a\u200b\u5f53\u524d\u200b\u7279\u5f81\u200b\u5c42\u200b\u7684\u200b\u7f51\u683c\u200b\u70b9\u200b\u6570\u91cf\u200b\u3002</p> <p>(3) \u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b</p> <p>\u200b\u7ecf\u8fc7\u200b <code>_predict_by_feat_single</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u5c06\u200b\u4ece\u200b head \u200b\u63d0\u53d6\u200b\u7684\u200b\u5355\u4e00\u200b\u56fe\u50cf\u200b\u7684\u200b\u7279\u5f81\u200b\u8f6c\u6362\u200b\u4e3a\u200b bbox \u200b\u7ed3\u679c\u200b\u8f93\u5165\u200b\uff0c\u200b\u5f97\u5230\u200b\u4e09\u4e2a\u200b\u5217\u8868\u200b <code>cls_score_list</code>\uff0c<code>bbox_pred_list</code>\uff0c<code>mlvl_priors</code>\uff0c\u200b\u8be6\u7ec6\u200b\u5927\u5c0f\u200b\u5982\u56fe\u6240\u793a\u200b\u3002\u200b\u4e4b\u540e\u200b\u5206\u522b\u200b\u904d\u5386\u200b\u4e09\u4e2a\u200b\u7279\u5f81\u200b\u5c42\u200b\uff0c\u200b\u5206\u522b\u200b\u5bf9\u200b class \u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3001bbox \u200b\u56de\u5f52\u200b\u5206\u652f\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002\u200b\u4ee5\u200b\u7b2c\u4e00\u5c42\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5bf9\u200b bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b [ 4\uff0c80\uff0c80 ] \u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b\u4e3a\u200b [ 6400\uff0c4 ]\uff0c\u200b\u5bf9\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b [ 80\uff0c80\uff0c80 ] \u200b\u53d8\u5316\u200b\u4e3a\u200b [ 6400\uff0c80 ]\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u505a\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u786e\u4fdd\u200b\u7c7b\u522b\u200b\u7f6e\u4fe1\u5ea6\u200b\u5728\u200b 0 - 1 \u200b\u4e4b\u95f4\u200b\u3002</p> <p>(4) \u200b\u9608\u503c\u200b\u8fc7\u6ee4\u200b</p> <p>\u200b\u5148\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b <code>nms_pre</code> \u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5148\u200b\u8fc7\u6ee4\u200b\u5927\u90e8\u5206\u200b\u7f6e\u4fe1\u5ea6\u200b\u6bd4\u8f83\u200b\u4f4e\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u6bd4\u5982\u200b <code>score_thr</code> \u200b\u9608\u503c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b 0.05\uff0c\u200b\u5219\u200b\u53bb\u9664\u200b\u5f53\u524d\u200b\u9884\u6d4b\u200b\u7f6e\u4fe1\u5ea6\u200b\u4f4e\u4e8e\u200b 0.05 \u200b\u7684\u200b\u7ed3\u679c\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u5f97\u5230\u200b bbox \u200b\u5750\u6807\u200b\u3001\u200b\u6240\u5728\u200b\u7f51\u683c\u200b\u7684\u200b\u5750\u6807\u200b\u3001\u200b\u7f6e\u4fe1\u5ea6\u200b\u3001\u200b\u6807\u7b7e\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u7ecf\u8fc7\u200b\u4e09\u4e2a\u200b\u7279\u5f81\u200b\u5c42\u200b\u904d\u5386\u200b\u4e4b\u540e\u200b\uff0c\u200b\u5206\u522b\u200b\u6574\u5408\u200b\u8fd9\u200b\u4e09\u4e2a\u200b\u5c42\u200b\u5f97\u5230\u200b\u7684\u200b\u7684\u200b\u56db\u4e2a\u200b\u4fe1\u606f\u200b\u653e\u5165\u200b results \u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</p> <p>(5) \u200b\u8fd8\u539f\u200b\u5230\u200b\u539f\u56fe\u200b\u5c3a\u5ea6\u200b</p> <p>\u200b\u6700\u540e\u200b\u5c06\u200b\u7f51\u7edc\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u6620\u5c04\u200b\u5230\u200b\u6574\u56fe\u200b\u5f53\u4e2d\u200b\uff0c\u200b\u5f97\u5230\u200b bbox \u200b\u5728\u200b\u6574\u56fe\u200b\u4e2d\u200b\u7684\u200b\u5750\u6807\u503c\u200b</p> <p>(6) NMS</p> <p>\u200b\u8fdb\u884c\u200b nms \u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6700\u7ec8\u200b\u9884\u6d4b\u200b\u5f97\u5230\u200b\u7684\u200b\u8fd4\u56de\u503c\u200b\u4e3a\u200b\u7ecf\u8fc7\u200b\u540e\u5904\u7406\u200b\u7684\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u68c0\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5305\u542b\u200b\u5206\u7c7b\u200b\u7f6e\u4fe1\u5ea6\u200b\uff0c\u200b\u6846\u200b\u7684\u200b labels\uff0c\u200b\u6846\u200b\u7684\u200b\u56db\u4e2a\u200b\u5750\u6807\u200b</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/rtmdet_description/#2","title":"2 \u200b\u603b\u7ed3","text":"<p>\u200b\u672c\u6587\u200b\u5bf9\u200b RTMDet \u200b\u539f\u7406\u200b\u548c\u200b\u5728\u200b MMYOLO \u200b\u5b9e\u73b0\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8be6\u7ec6\u200b\u89e3\u6790\u200b\uff0c\u200b\u5e0c\u671b\u200b\u80fd\u200b\u5e2e\u52a9\u200b\u7528\u6237\u200b\u7406\u89e3\u200b\u7b97\u6cd5\u200b\u5b9e\u73b0\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u540c\u65f6\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff1a\u200b\u7531\u4e8e\u200b RTMDet \u200b\u672c\u8eab\u200b\u4e5f\u200b\u5728\u200b\u4e0d\u65ad\u66f4\u65b0\u200b\uff0c \u200b\u672c\u200b\u5f00\u6e90\u200b\u5e93\u200b\u4e5f\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u8fed\u4ee3\u200b\uff0c\u200b\u8bf7\u200b\u53ca\u65f6\u200b\u9605\u8bfb\u200b\u548c\u200b\u540c\u6b65\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/","title":"YOLOv5 \u200b\u539f\u7406\u200b\u548c\u200b\u5b9e\u73b0\u200b\u5168\u200b\u89e3\u6790","text":""},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#0","title":"0 \u200b\u7b80\u4ecb","text":"<p>\u200b\u4ee5\u4e0a\u200b\u7ed3\u6784\u56fe\u200b\u7531\u200b RangeKing@github \u200b\u7ed8\u5236\u200b\u3002</p> <p>YOLOv5 \u200b\u662f\u200b\u4e00\u4e2a\u200b\u9762\u5411\u200b\u5b9e\u65f6\u200b\u5de5\u4e1a\u200b\u5e94\u7528\u200b\u800c\u200b\u5f00\u6e90\u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u53d7\u5230\u200b\u4e86\u200b\u5e7f\u6cdb\u200b\u5173\u6ce8\u200b\u3002\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u8ba9\u200b YOLOv5 \u200b\u7206\u706b\u200b\u7684\u200b\u539f\u56e0\u200b\u4e0d\u200b\u5355\u7eaf\u200b\u5728\u4e8e\u200b YOLOv5 \u200b\u7b97\u6cd5\u200b\u672c\u8eab\u200b\u7684\u200b\u4f18\u5f02\u6027\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5728\u4e8e\u200b\u5f00\u6e90\u200b\u5e93\u200b\u7684\u200b\u5b9e\u7528\u200b\u548c\u200b\u9c81\u68d2\u6027\u200b\u3002\u200b\u7b80\u5355\u200b\u6765\u8bf4\u200b YOLOv5 \u200b\u5f00\u6e90\u200b\u5e93\u200b\u7684\u200b\u4e3b\u8981\u200b\u7279\u70b9\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u53cb\u597d\u200b\u548c\u200b\u5b8c\u5584\u200b\u7684\u200b\u90e8\u7f72\u200b\u652f\u6301\u200b</li> <li>\u200b\u7b97\u6cd5\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u6781\u5feb\u200b\uff0c\u200b\u5728\u200b 300 epoch \u200b\u60c5\u51b5\u200b\u4e0b\u200b\u8bad\u7ec3\u200b\u65f6\u957f\u200b\u548c\u200b\u5927\u90e8\u5206\u200b one-stage \u200b\u7b97\u6cd5\u200b\u5982\u200b RetinaNet\u3001ATSS \u200b\u548c\u200b two-stage \u200b\u7b97\u6cd5\u200b\u5982\u200b Faster R-CNN \u200b\u5728\u200b 12 epoch \u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u63a5\u8fd1\u200b</li> <li>\u200b\u6846\u67b6\u200b\u8fdb\u884c\u200b\u4e86\u200b\u975e\u5e38\u200b\u591a\u200b\u7684\u200b corner case \u200b\u4f18\u5316\u200b\uff0c\u200b\u529f\u80fd\u200b\u548c\u200b\u6587\u6863\u200b\u4e5f\u200b\u6bd4\u8f83\u200b\u4e30\u5bcc\u200b</li> </ol> <p>\u200b\u672c\u6587\u200b\u5c06\u200b\u4ece\u200b YOLOv5 \u200b\u7b97\u6cd5\u200b\u672c\u8eab\u200b\u539f\u7406\u200b\u8bb2\u200b\u8d77\u200b\uff0c\u200b\u7136\u540e\u200b\u91cd\u70b9\u200b\u5206\u6790\u200b MMYOLO \u200b\u4e2d\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002\u200b\u5173\u4e8e\u200b YOLOv5 \u200b\u7684\u200b\u4f7f\u7528\u6307\u5357\u200b\u548c\u200b\u901f\u5ea6\u200b\u7b49\u200b\u5bf9\u6bd4\u200b\u8bf7\u200b\u9605\u8bfb\u200b\u672c\u6587\u200b\u7684\u200b\u540e\u7eed\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u5e0c\u671b\u200b\u672c\u6587\u200b\u80fd\u591f\u200b\u6210\u4e3a\u200b\u4f60\u200b\u5165\u95e8\u200b\u548c\u200b\u638c\u63e1\u200b YOLOv5 \u200b\u7684\u200b\u6838\u5fc3\u200b\u6587\u6863\u200b\u3002\u200b\u7531\u4e8e\u200b YOLOv5 \u200b\u672c\u8eab\u200b\u4e5f\u200b\u5728\u200b\u4e0d\u65ad\u200b\u8fed\u4ee3\u200b\u66f4\u65b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u7684\u200b\u66f4\u65b0\u200b\u672c\u200b\u6587\u6863\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u9605\u8bfb\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p> <p>MMYOLO \u200b\u5b9e\u73b0\u200b\u914d\u7f6e\u200b\uff1ahttps://github.com/open-mmlab/mmyolo/blob/main/configs/yolov5/</p> <p>YOLOv5 \u200b\u5b98\u65b9\u200b\u5f00\u6e90\u200b\u5e93\u200b\u5730\u5740\u200b\uff1ahttps://github.com/ultralytics/yolov5</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#1-v61-mmyolo","title":"1 v6.1 \u200b\u7b97\u6cd5\u200b\u539f\u7406\u200b\u548c\u200b MMYOLO \u200b\u5b9e\u73b0\u200b\u89e3\u6790","text":"<p>YOLOv5 \u200b\u5b98\u65b9\u200b release \u200b\u5730\u5740\u200b\uff1ahttps://github.com/ultralytics/yolov5/releases/tag/v6.1</p> <p>\u200b\u6027\u80fd\u200b\u5982\u200b\u4e0a\u8868\u200b\u6240\u793a\u200b\u3002YOLOv5 \u200b\u6709\u200b P5 \u200b\u548c\u200b P6 \u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u8bad\u7ec3\u200b\u8f93\u5165\u200b\u5c3a\u5ea6\u200b\u7684\u200b\u6a21\u578b\u200b\uff0cP6 \u200b\u5373\u200b\u4e3a\u200b 1280x1280 \u200b\u8f93\u5165\u200b\u7684\u200b\u5927\u200b\u6a21\u578b\u200b\uff0c\u200b\u901a\u5e38\u200b\u7528\u200b\u7684\u200b\u662f\u200b P5 \u200b\u5e38\u89c4\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f93\u5165\u200b\u5c3a\u5bf8\u200b\u662f\u200b 640x640 \u3002\u200b\u672c\u6587\u200b\u89e3\u8bfb\u200b\u7684\u200b\u4e5f\u200b\u662f\u200b P5 \u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u901a\u5e38\u200b\u6765\u8bf4\u200b\uff0c\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5206\u6210\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3001\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3001loss \u200b\u8ba1\u7b97\u200b\u7b49\u200b\u7ec4\u4ef6\u200b\uff0cYOLOv5 \u200b\u4e5f\u200b\u4e00\u6837\u200b\uff0c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p>\u200b\u4e0b\u9762\u200b\u5c06\u200b\u4ece\u200b\u539f\u7406\u200b\u548c\u200b\u7ed3\u5408\u200b MMYOLO \u200b\u7684\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u7b80\u8981\u200b\u5206\u6790\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#11","title":"1.1 \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6a21\u5757","text":"<p>YOLOv5 \u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6bd4\u8f83\u200b\u591a\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>Mosaic \u200b\u9a6c\u8d5b\u514b\u200b</li> <li>RandomAffine \u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b</li> <li>MixUp</li> <li>\u200b\u56fe\u50cf\u200b\u6a21\u7cca\u200b\u7b49\u200b\u91c7\u7528\u200b Albu \u200b\u5e93\u200b\u5b9e\u73b0\u200b\u7684\u200b\u53d8\u6362\u200b</li> <li>HSV \u200b\u989c\u8272\u200b\u7a7a\u95f4\u200b\u589e\u5f3a\u200b</li> <li>\u200b\u968f\u673a\u200b\u6c34\u5e73\u200b\u7ffb\u8f6c\u200b</li> </ul> <p>\u200b\u5176\u4e2d\u200b Mosaic \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6982\u7387\u200b\u4e3a\u200b 1\uff0c\u200b\u8868\u793a\u200b\u4e00\u5b9a\u200b\u4f1a\u200b\u89e6\u53d1\u200b\uff0c\u200b\u800c\u200b\u5bf9\u4e8e\u200b small \u200b\u548c\u200b nano \u200b\u4e24\u4e2a\u200b\u7248\u672c\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0d\u200b\u4f7f\u7528\u200b MixUp\uff0c\u200b\u5176\u4ed6\u200b\u7684\u200b l/m/x \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u5219\u200b\u91c7\u7528\u200b\u4e86\u200b 0.1 \u200b\u7684\u200b\u6982\u7387\u200b\u89e6\u53d1\u200b MixUp\u3002\u200b\u5c0f\u200b\u6a21\u578b\u200b\u80fd\u529b\u200b\u6709\u9650\u200b\uff0c\u200b\u4e00\u822c\u200b\u4e0d\u4f1a\u200b\u91c7\u7528\u200b MixUp \u200b\u7b49\u200b\u5f3a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7b56\u7565\u200b\u3002</p> <p>\u200b\u5176\u200b\u6838\u5fc3\u200b\u7684\u200b Mosaic + RandomAffine + MixUp \u200b\u8fc7\u7a0b\u200b\u7b80\u8981\u200b\u7ed8\u5236\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u4e0b\u9762\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u7b80\u8981\u200b\u5206\u6790\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#111-mosaic","title":"1.1.1 Mosaic \u200b\u9a6c\u8d5b\u514b","text":"<p>Mosaic \u200b\u5c5e\u4e8e\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5728\u200b\u8fd0\u884c\u200b\u65f6\u5019\u200b\u9700\u8981\u200b 4 \u200b\u5f20\u200b\u56fe\u7247\u200b\u62fc\u63a5\u200b\uff0c\u200b\u53d8\u76f8\u200b\u7684\u200b\u76f8\u5f53\u4e8e\u200b\u589e\u52a0\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u7684\u200b batch size\u3002\u200b\u5176\u200b\u8fd0\u884c\u200b\u8fc7\u7a0b\u200b\u7b80\u8981\u200b\u6982\u51b5\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u968f\u673a\u200b\u751f\u6210\u200b\u62fc\u63a5\u200b\u540e\u200b 4 \u200b\u5f20\u56fe\u200b\u7684\u200b\u4ea4\u63a5\u200b\u4e2d\u5fc3\u70b9\u200b\u5750\u6807\u200b\uff0c\u200b\u6b64\u65f6\u200b\u5c31\u200b\u76f8\u5f53\u4e8e\u200b\u786e\u5b9a\u200b\u4e86\u200b 4 \u200b\u5f20\u200b\u62fc\u63a5\u200b\u56fe\u7247\u200b\u7684\u200b\u4ea4\u63a5\u70b9\u200b</li> <li>\u200b\u968f\u673a\u200b\u9009\u51fa\u200b\u53e6\u5916\u200b 3 \u200b\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u7d22\u5f15\u200b\u4ee5\u53ca\u200b\u8bfb\u53d6\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6807\u6ce8\u200b</li> <li>\u200b\u5bf9\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u91c7\u7528\u200b\u4fdd\u6301\u200b\u5bbd\u9ad8\u6bd4\u200b\u7684\u200b resize \u200b\u64cd\u4f5c\u200b\u5c06\u200b\u5176\u7f29\u200b\u653e\u5230\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b</li> <li>\u200b\u6309\u7167\u200b\u4e0a\u4e0b\u5de6\u53f3\u200b\u89c4\u5219\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u5728\u200b\u5f85\u200b\u8f93\u51fa\u200b\u56fe\u7247\u200b\u4e2d\u200b\u5e94\u8be5\u200b\u653e\u7f6e\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u56fe\u7247\u200b\u53ef\u80fd\u200b\u51fa\u754c\u200b\u6545\u200b\u8fd8\u200b\u9700\u8981\u200b\u8ba1\u7b97\u200b\u88c1\u526a\u200b\u5750\u6807\u200b</li> <li>\u200b\u5229\u7528\u200b\u88c1\u526a\u200b\u5750\u6807\u200b\u5c06\u200b\u7f29\u653e\u200b\u540e\u200b\u7684\u200b\u56fe\u7247\u200b\u88c1\u526a\u200b\uff0c\u200b\u7136\u540e\u200b\u8d34\u200b\u5230\u200b\u524d\u9762\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u5176\u4f59\u200b\u4f4d\u7f6e\u200b\u5168\u90e8\u200b\u8865\u200b 114 \u200b\u50cf\u7d20\u200b\u503c\u200b</li> <li>\u200b\u5bf9\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u6807\u6ce8\u200b\u4e5f\u200b\u8fdb\u884c\u200b\u76f8\u5e94\u200b\u5904\u7406\u200b</li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u7531\u4e8e\u200b\u62fc\u63a5\u200b\u4e86\u200b 4 \u200b\u5f20\u56fe\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8f93\u51fa\u200b\u56fe\u7247\u200b\u9762\u79ef\u200b\u4f1a\u200b\u6269\u5927\u200b 4 \u200b\u500d\u200b\uff0c\u200b\u4ece\u200b 640x640 \u200b\u53d8\u6210\u200b 1280x1280\uff0c\u200b\u56e0\u6b64\u200b\u8981\u200b\u60f3\u200b\u6062\u590d\u200b\u4e3a\u200b 640x640\uff0c \u200b\u5fc5\u987b\u200b\u8981\u200b\u518d\u200b\u63a5\u200b\u4e00\u4e2a\u200b RandomAffine \u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b\uff0c\u200b\u5426\u5219\u200b\u56fe\u7247\u200b\u9762\u79ef\u200b\u5c31\u200b\u4e00\u76f4\u200b\u662f\u200b\u6269\u5927\u200b 4 \u200b\u500d\u200b\u7684\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#112-randomaffine","title":"1.1.2 RandomAffine \u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362","text":"<p>\u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b\u6709\u200b\u4e24\u4e2a\u200b\u76ee\u7684\u200b\uff1a</p> <ol> <li>\u200b\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u51e0\u4f55\u200b\u4eff\u5c04\u53d8\u6362\u200b</li> <li>\u200b\u5c06\u200b Mosaic \u200b\u8f93\u51fa\u200b\u7684\u200b\u6269\u5927\u200b 4 \u200b\u500d\u200b\u7684\u200b\u56fe\u7247\u200b\u8fd8\u539f\u200b\u4e3a\u200b 640x640 \u200b\u5c3a\u5bf8\u200b</li> </ol> <p>\u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b\u5305\u62ec\u200b\u5e73\u79fb\u200b\u3001\u200b\u65cb\u8f6c\u200b\u3001\u200b\u7f29\u653e\u200b\u3001\u200b\u9519\u5207\u200b\u7b49\u200b\u51e0\u4f55\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u540c\u65f6\u200b\u7531\u4e8e\u200b Mosaic \u200b\u548c\u200b RandomAffine \u200b\u5c5e\u4e8e\u200b\u6bd4\u8f83\u200b\u5f3a\u200b\u7684\u200b\u589e\u5f3a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f1a\u200b\u5f15\u5165\u200b\u8f83\u5927\u200b\u566a\u58f0\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u5bf9\u200b\u589e\u5f3a\u200b\u540e\u200b\u7684\u200b\u6807\u6ce8\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0c\u200b\u8fc7\u6ee4\u200b\u89c4\u5219\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u589e\u5f3a\u200b\u540e\u200b\u7684\u200b gt bbox \u200b\u5bbd\u9ad8\u8981\u200b\u5927\u4e8e\u200b wh_thr</li> <li>\u200b\u589e\u5f3a\u200b\u540e\u200b\u7684\u200b gt bbox \u200b\u9762\u79ef\u200b\u548c\u200b\u589e\u5f3a\u200b\u524d\u200b\u7684\u200b gt bbox \u200b\u9762\u79ef\u200b\u6bd4\u8981\u200b\u5927\u4e8e\u200b ar_thr\uff0c\u200b\u9632\u6b62\u200b\u589e\u5f3a\u200b\u592a\u200b\u4e25\u91cd\u200b</li> <li>\u200b\u6700\u5927\u200b\u5bbd\u9ad8\u6bd4\u200b\u8981\u200b\u5c0f\u4e8e\u200b area_thr\uff0c\u200b\u9632\u6b62\u200b\u5bbd\u9ad8\u6bd4\u200b\u6539\u53d8\u200b\u592a\u200b\u591a\u200b</li> </ol> <p>\u200b\u7531\u4e8e\u200b\u65cb\u8f6c\u200b\u540e\u200b\u6807\u6ce8\u200b\u6846\u4f1a\u200b\u53d8\u5927\u200b\u5bfc\u81f4\u200b\u4e0d\u200b\u51c6\u786e\u200b\uff0c\u200b\u56e0\u6b64\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u91cc\u9762\u200b\u5f88\u5c11\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u65cb\u8f6c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#113-mixup","title":"1.1.3 MixUp","text":"<p>MixUp \u200b\u548c\u200b Mosaic \u200b\u7c7b\u4f3c\u200b\u4e5f\u200b\u5c5e\u4e8e\u200b\u6df7\u5408\u200b\u56fe\u7247\u200b\u7c7b\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\u3002\u200b\u968f\u673a\u200b\u9009\u51fa\u200b\u53e6\u5916\u200b\u4e00\u5f20\u200b\u56fe\u540e\u200b\u5c06\u200b\u4e24\u56fe\u200b\u518d\u200b\u968f\u673a\u200b\u6df7\u5408\u200b\u3002\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\u65b9\u6cd5\u200b\u6709\u200b\u591a\u79cd\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u505a\u6cd5\u200b\u662f\u200b\u8981\u4e48\u200b\u5c06\u200b label \u200b\u76f4\u63a5\u200b\u62fc\u63a5\u200b\u8d77\u6765\u200b\uff0c\u200b\u8981\u4e48\u200b\u5c06\u200b label \u200b\u4e5f\u200b\u91c7\u7528\u200b alpha \u200b\u65b9\u6cd5\u200b\u6df7\u5408\u200b\u3002\u200b\u539f\u4f5c\u8005\u200b\u7684\u200b\u505a\u6cd5\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u5bf9\u200b label \u200b\u5373\u200b\u76f4\u63a5\u200b\u62fc\u63a5\u200b\uff0c\u200b\u800c\u200b\u56fe\u7247\u200b\u901a\u8fc7\u200b\u5206\u5e03\u200b\u91c7\u6837\u200b\u6df7\u5408\u200b\u3002</p> <p>\u200b\u9700\u8981\u200b\u7279\u522b\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff1a YOLOv5 \u200b\u5b9e\u73b0\u200b\u7684\u200b MixUp \u200b\u4e2d\u200b\uff0c\u200b\u968f\u673a\u200b\u51fa\u6765\u200b\u7684\u200b\u53e6\u200b\u4e00\u5f20\u200b\u56fe\u200b\u4e5f\u200b\u9700\u8981\u200b\u7ecf\u8fc7\u200b Mosaic \u200b\u9a6c\u8d5b\u514b\u200b + RandomAffine \u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b \u200b\u7684\u200b\u589e\u5f3a\u200b\u540e\u200b\u624d\u80fd\u200b\u6df7\u5408\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u548c\u200b\u5176\u4ed6\u200b\u5f00\u6e90\u200b\u5e93\u200b\u5b9e\u73b0\u200b\u53ef\u80fd\u200b\u4e0d\u592a\u200b\u4e00\u6837\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#114","title":"1.1.4 \u200b\u56fe\u50cf\u200b\u6a21\u7cca\u200b\u548c\u200b\u5176\u4ed6\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7b56\u7565","text":"<p>\u200b\u5269\u4e0b\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5305\u62ec\u200b</p> <ul> <li>\u200b\u56fe\u50cf\u200b\u6a21\u7cca\u200b\u7b49\u200b\u91c7\u7528\u200b Albu \u200b\u5e93\u200b\u5b9e\u73b0\u200b\u7684\u200b\u53d8\u6362\u200b</li> <li>HSV \u200b\u989c\u8272\u200b\u7a7a\u95f4\u200b\u589e\u5f3a\u200b</li> <li>\u200b\u968f\u673a\u200b\u6c34\u5e73\u200b\u7ffb\u8f6c\u200b</li> </ul> <p>MMDetection \u200b\u5f00\u6e90\u200b\u5e93\u4e2d\u200b\u5df2\u7ecf\u200b\u5bf9\u200b Albu \u200b\u7b2c\u4e09\u65b9\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5e93\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5c01\u88c5\u200b\uff0c\u200b\u4f7f\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u7b80\u5355\u200b\u7684\u200b\u901a\u8fc7\u200b\u914d\u7f6e\u200b\u5373\u53ef\u200b\u4f7f\u7528\u200b Albu \u200b\u5e93\u4e2d\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4efb\u4f55\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u529f\u80fd\u200b\u3002\u200b\u800c\u200b HSV \u200b\u989c\u8272\u200b\u7a7a\u95f4\u200b\u589e\u5f3a\u200b\u548c\u200b\u968f\u673a\u200b\u6c34\u5e73\u200b\u7ffb\u8f6c\u200b\u90fd\u200b\u662f\u200b\u5c5e\u4e8e\u200b\u6bd4\u8f83\u200b\u5e38\u89c4\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u7279\u6b8a\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#115-mmyolo","title":"1.1.5 MMYOLO \u200b\u5b9e\u73b0\u200b\u89e3\u6790","text":"<p>\u200b\u5e38\u89c4\u200b\u7684\u200b\u5355\u56fe\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u4f8b\u5982\u200b\u968f\u673a\u200b\u7ffb\u8f6c\u200b\u7b49\u200b\u6bd4\u8f83\u200b\u5bb9\u6613\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u800c\u200b Mosaic \u200b\u7c7b\u200b\u7684\u200b\u6df7\u5408\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5219\u200b\u4e0d\u200b\u592a\u200b\u5bb9\u6613\u200b\u3002\u200b\u5728\u200b MMDetection \u200b\u590d\u73b0\u200b\u7684\u200b YOLOX \u200b\u7b97\u6cd5\u200b\u4e2d\u200b\u63d0\u51fa\u200b\u4e86\u200b MultiImageMixDataset \u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u88c5\u200b\u5668\u200b\u7684\u200b\u6982\u5ff5\u200b\uff0c\u200b\u5176\u200b\u5b9e\u73b0\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u5bf9\u4e8e\u200b Mosaic \u200b\u7b49\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7b56\u7565\u200b\uff0c\u200b\u4f1a\u200b\u9700\u8981\u200b\u989d\u5916\u200b\u5b9e\u73b0\u200b\u4e00\u4e2a\u200b <code>get_indexes</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u83b7\u53d6\u200b\u5176\u4ed6\u200b\u56fe\u7247\u200b\u7d22\u5f15\u200b\uff0c\u200b\u7136\u540e\u200b\u7528\u200b\u5f97\u5230\u200b\u7684\u200b 4 \u200b\u5f20\u200b\u56fe\u7247\u200b\u4fe1\u606f\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b Mosaic \u200b\u589e\u5f3a\u200b\u4e86\u200b\u3002 \u200b\u4ee5\u200b MMDetection \u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u7684\u200b YOLOX \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5176\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5199\u6cd5\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>train_pipeline = [\n    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),\n    dict(\n        type='RandomAffine',\n        scaling_ratio_range=(0.1, 2),\n        border=(-img_scale[0] // 2, -img_scale[1] // 2)),\n    dict(\n        type='MixUp',\n        img_scale=img_scale,\n        ratio_range=(0.8, 1.6),\n        pad_val=114.0),\n    ...\n]\n\ntrain_dataset = dict(\n    # use MultiImageMixDataset wrapper to support mosaic and mixup\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type='CocoDataset',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ]),\n    pipeline=train_pipeline)\n</code></pre> <p>MultiImageMixDataset \u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u88c5\u200b\u5668\u200b\u4f20\u5165\u200b\u4e00\u4e2a\u200b\u5305\u62ec\u200b Mosaic \u200b\u548c\u200b RandAffine \u200b\u7b49\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u800c\u200b CocoDataset \u200b\u4e2d\u200b\u4e5f\u200b\u9700\u8981\u200b\u4f20\u5165\u200b\u4e00\u4e2a\u200b\u5305\u62ec\u200b\u56fe\u7247\u200b\u548c\u200b\u6807\u6ce8\u200b\u52a0\u8f7d\u200b\u7684\u200b pipeline\u3002\u200b\u901a\u8fc7\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u7684\u200b\u5b9e\u73b0\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\u4e0a\u8ff0\u200b\u5b9e\u73b0\u200b\u6709\u200b\u4e00\u4e2a\u200b\u7f3a\u70b9\u200b\uff1a \u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u719f\u6089\u200b MMDetection \u200b\u7684\u200b\u7528\u6237\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5176\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u5fd8\u8bb0\u200b Mosaic \u200b\u5fc5\u987b\u200b\u8981\u200b\u548c\u200b MultiImageMixDataset \u200b\u914d\u5408\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u62a5\u9519\u200b\uff0c\u200b\u800c\u4e14\u200b\u8fd9\u6837\u200b\u4f1a\u200b\u52a0\u5927\u200b\u590d\u6742\u5ea6\u200b\u548c\u200b\u7406\u89e3\u200b\u96be\u5ea6\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b\u6211\u4eec\u200b\u8fdb\u4e00\u6b65\u200b\u8fdb\u884c\u200b\u4e86\u200b\u7b80\u5316\u200b\u3002\u200b\u76f4\u63a5\u200b\u8ba9\u200b pipeline \u200b\u80fd\u591f\u200b\u83b7\u53d6\u200b\u5230\u200b dataset \u200b\u5bf9\u8c61\u200b\uff0c\u200b\u6b64\u65f6\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5c06\u200b Mosaic \u200b\u7b49\u200b\u6df7\u5408\u200b\u7c7b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u5b9e\u73b0\u200b\u548c\u200b\u4f7f\u7528\u200b\u53d8\u6210\u200b\u548c\u200b\u968f\u673a\u200b\u7ffb\u8f6c\u200b\u4e00\u6837\u200b\u3002 \u200b\u6b64\u65f6\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b YOLOX \u200b\u7684\u200b\u914d\u7f6e\u200b\u5199\u6cd5\u200b\u53d8\u6210\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>pre_transform = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True)\n]\n\ntrain_pipeline = [\n    *pre_transform,\n    dict(\n        type='Mosaic',\n        img_scale=img_scale,\n        pad_val=114.0,\n        pre_transform=pre_transform),\n    dict(\n        type='mmdet.RandomAffine',\n        scaling_ratio_range=(0.1, 2),\n        border=(-img_scale[0] // 2, -img_scale[1] // 2)),\n    dict(\n        type='YOLOXMixUp',\n        img_scale=img_scale,\n        ratio_range=(0.8, 1.6),\n        pad_val=114.0,\n        pre_transform=pre_transform),\n    ...\n]\n</code></pre> <p>\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4e0d\u518d\u200b\u9700\u8981\u200b MultiImageMixDataset \u200b\u4e86\u200b\uff0c\u200b\u4f7f\u7528\u200b\u548c\u200b\u7406\u89e3\u200b\u4e0a\u200b\u4f1a\u200b\u66f4\u52a0\u200b\u7b80\u5355\u200b\u3002</p> <p>\u200b\u56de\u5230\u200b YOLOv5 \u200b\u914d\u7f6e\u200b\u4e0a\u200b\uff0c\u200b\u56e0\u4e3a\u200b YOLOv5 \u200b\u5b9e\u73b0\u200b\u7684\u200b MixUp \u200b\u4e2d\u200b\uff0c\u200b\u968f\u673a\u200b\u9009\u51fa\u200b\u6765\u200b\u7684\u200b\u53e6\u200b\u4e00\u5f20\u200b\u56fe\u200b\u4e5f\u200b\u9700\u8981\u200b\u7ecf\u8fc7\u200b Mosaic \u200b\u9a6c\u8d5b\u514b\u200b+RandomAffine \u200b\u968f\u673a\u200b\u4eff\u5c04\u53d8\u6362\u200b \u200b\u589e\u5f3a\u200b\u540e\u200b\u624d\u80fd\u200b\u6df7\u5408\u200b\uff0c\u200b\u6545\u200bYOLOv5-m \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u914d\u7f6e\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>pre_transform = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True)\n]\n\nmosaic_transform= [\n    dict(\n        type='Mosaic',\n        img_scale=img_scale,\n        pad_val=114.0,\n        pre_transform=pre_transform),\n    dict(\n        type='YOLOv5RandomAffine',\n        max_rotate_degree=0.0,\n        max_shear_degree=0.0,\n        scaling_ratio_range=(0.1, 1.9),  # scale = 0.9\n        border=(-img_scale[0] // 2, -img_scale[1] // 2),\n        border_val=(114, 114, 114))\n]\n\ntrain_pipeline = [\n    *pre_transform,\n    *mosaic_transform,\n    dict(\n        type='YOLOv5MixUp',\n        prob=0.1,\n        pre_transform=[\n            *pre_transform,\n            *mosaic_transform\n        ]),\n    ...\n]\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#12","title":"1.2 \u200b\u7f51\u7edc\u7ed3\u6784","text":"<p>\u200b\u672c\u200b\u5c0f\u7ed3\u200b\u7531\u200b RangeKing@github \u200b\u64b0\u5199\u200b\uff0c\u200b\u975e\u5e38\u611f\u8c22\u200b\uff01\uff01\uff01</p> <p>YOLOv5 \u200b\u7f51\u7edc\u7ed3\u6784\u200b\u662f\u200b\u6807\u51c6\u200b\u7684\u200b <code>CSPDarknet</code> + <code>PAFPN</code> + <code>\u200b\u975e\u89e3\u200b\u8026\u200b Head</code>\u3002</p> <p>YOLOv5 \u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5927\u5c0f\u200b\u7531\u200b <code>deepen_factor</code> \u200b\u548c\u200b <code>widen_factor</code> \u200b\u4e24\u4e2a\u200b\u53c2\u6570\u200b\u51b3\u5b9a\u200b\u3002\u200b\u5176\u4e2d\u200b <code>deepen_factor</code> \u200b\u63a7\u5236\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u6df1\u5ea6\u200b\uff0c\u200b\u5373\u200b <code>CSPLayer</code> \u200b\u4e2d\u200b <code>DarknetBottleneck</code> \u200b\u6a21\u5757\u200b\u5806\u53e0\u200b\u7684\u200b\u6570\u91cf\u200b\uff1b<code>widen_factor</code> \u200b\u63a7\u5236\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5bbd\u5ea6\u200b\uff0c\u200b\u5373\u200b\u6a21\u5757\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u3002\u200b\u4ee5\u200b YOLOv5-l \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5176\u200b <code>deepen_factor = widen_factor = 1.0</code> \uff0c\u200b\u6574\u4f53\u200b\u7ed3\u6784\u56fe\u200b\u5982\u200b\u4e0a\u200b\u6240\u793a\u200b\u3002</p> <p>\u200b\u56fe\u200b\u7684\u200b\u4e0a\u534a\u90e8\u200b\u5206\u4e3a\u200b\u6a21\u578b\u200b\u603b\u89c8\u200b\uff1b\u200b\u4e0b\u200b\u534a\u90e8\u200b\u5206\u4e3a\u200b\u5177\u4f53\u200b\u7f51\u7edc\u7ed3\u6784\u200b\uff0c\u200b\u5176\u4e2d\u200b\u7684\u200b\u6a21\u5757\u200b\u5747\u200b\u6807\u6709\u200b\u5e8f\u53f7\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u4e0e\u200b YOLOv5 \u200b\u5b98\u65b9\u200b\u4ed3\u5e93\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5bf9\u5e94\u200b\uff1b\u200b\u4e2d\u95f4\u200b\u90e8\u5206\u200b\u4e3a\u200b\u5404\u5b50\u200b\u6a21\u5757\u200b\u7684\u200b\u5177\u4f53\u200b\u6784\u6210\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60f3\u200b\u4f7f\u7528\u200b netron \u200b\u53ef\u89c6\u5316\u200b\u7f51\u7edc\u200b\u7ed3\u6784\u56fe\u200b\u7ec6\u8282\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5728\u200b netron \u200b\u4e2d\u5c06\u200b MMDeploy \u200b\u5bfc\u51fa\u200b\u7684\u200b ONNX \u200b\u6587\u4ef6\u200b\u683c\u5f0f\u6587\u4ef6\u200b\u6253\u5f00\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#121-backbone","title":"1.2.1 Backbone","text":"<p>\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b <code>CSPDarknet</code> \u200b\u7ee7\u627f\u200b\u81ea\u200b <code>BaseBackbone</code>\uff0c\u200b\u6574\u4f53\u200b\u7ed3\u6784\u200b\u548c\u200b <code>ResNet</code> \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5171\u200b 5 \u200b\u5c42\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5305\u542b\u200b 1 \u200b\u4e2a\u200b <code>Stem Layer</code> \u200b\u548c\u200b 4 \u200b\u4e2a\u200b <code>Stage Layer</code>\uff1a</p> <ul> <li><code>Stem Layer</code> \u200b\u662f\u200b 1 \u200b\u4e2a\u200b 6x6 kernel \u200b\u7684\u200b <code>ConvModule</code>\uff0c\u200b\u76f8\u8f83\u200b\u4e8e\u200b v6.1 \u200b\u7248\u672c\u200b\u4e4b\u524d\u200b\u7684\u200b <code>Focus</code> \u200b\u6a21\u5757\u200b\u66f4\u52a0\u200b\u9ad8\u6548\u200b\u3002</li> <li>\u200b\u524d\u200b 3 \u200b\u4e2a\u200b <code>Stage Layer</code> \u200b\u5747\u200b\u7531\u200b 1 \u200b\u4e2a\u200b <code>ConvModule</code> \u200b\u548c\u200b 1 \u200b\u4e2a\u200b <code>CSPLayer</code> \u200b\u7ec4\u6210\u200b\u3002\u200b\u5982\u4e0a\u56fe\u200b Details \u200b\u90e8\u5206\u200b\u6240\u793a\u200b\u3002   \u200b\u5176\u4e2d\u200b <code>ConvModule</code> \u200b\u4e3a\u200b 3x3\u200b\u7684\u200b <code>Conv2d</code> + <code>BatchNorm</code> + <code>SiLU \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b</code>\u3002<code>CSPLayer</code> \u200b\u5373\u200b YOLOv5 \u200b\u5b98\u65b9\u200b\u4ed3\u5e93\u200b\u4e2d\u200b\u7684\u200b C3 \u200b\u6a21\u5757\u200b\uff0c\u200b\u7531\u200b 3 \u200b\u4e2a\u200b <code>ConvModule</code> + n \u200b\u4e2a\u200b <code>DarknetBottleneck</code>(\u200b\u5e26\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b) \u200b\u7ec4\u6210\u200b\u3002</li> <li>\u200b\u7b2c\u200b 4 \u200b\u4e2a\u200b <code>Stage Layer</code> \u200b\u5728\u200b\u6700\u540e\u200b\u589e\u52a0\u200b\u4e86\u200b <code>SPPF</code> \u200b\u6a21\u5757\u200b\u3002<code>SPPF</code> \u200b\u6a21\u5757\u200b\u662f\u200b\u5c06\u200b\u8f93\u5165\u200b\u4e32\u884c\u200b\u901a\u8fc7\u200b\u591a\u4e2a\u200b 5x5 \u200b\u5927\u5c0f\u200b\u7684\u200b <code>MaxPool2d</code> \u200b\u5c42\u200b\uff0c\u200b\u4e0e\u200b <code>SPP</code>  \u200b\u6a21\u5757\u200b\u6548\u679c\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\u3002</li> <li>P5 \u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u4f1a\u200b\u5728\u200b <code>Stage Layer</code> 2-4 \u200b\u4e4b\u540e\u200b\u5206\u522b\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200b\u7279\u5f81\u200b\u56fe\u200b\u8fdb\u5165\u200b <code>Neck</code> \u200b\u7ed3\u6784\u200b\u3002\u200b\u4ee5\u200b 640x640 \u200b\u8f93\u5165\u200b\u56fe\u7247\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5176\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u4e3a\u200b (B,256,80,80)\u3001 (B,512,40,40) \u200b\u548c\u200b (B,1024,20,20)\uff0c\u200b\u5bf9\u5e94\u200b\u7684\u200b stride \u200b\u5206\u522b\u200b\u4e3a\u200b 8/16/32\u3002</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#122-neck","title":"1.2.2 Neck","text":"<p>YOLOv5 \u200b\u5b98\u65b9\u200b\u4ed3\u5e93\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u5e76\u200b\u6ca1\u6709\u200b Neck \u200b\u90e8\u5206\u200b\uff0c\u200b\u4e3a\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u76f8\u5bf9\u200b\u5e94\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b98\u65b9\u200b\u4ed3\u5e93\u200b\u7684\u200b <code>Head</code> \u200b\u62c6\u200b\u5206\u6210\u200b <code>PAFPN</code> \u200b\u548c\u200b <code>Head</code> \u200b\u4e24\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u57fa\u4e8e\u200b <code>BaseYOLONeck</code> \u200b\u7ed3\u6784\u200b\uff0cYOLOv5 <code>Neck</code> \u200b\u4e5f\u200b\u662f\u200b\u9075\u5faa\u200b\u540c\u200b\u4e00\u5957\u200b\u6784\u5efa\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e0d\u200b\u5b58\u5728\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u6211\u4eec\u200b\u91c7\u7528\u200b <code>nn.Identity</code> \u200b\u4ee3\u66ff\u200b\u3002</p> <p>Neck \u200b\u6a21\u5757\u200b\u8f93\u51fa\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u548c\u200b Backbone \u200b\u5b8c\u5168\u4e00\u81f4\u200b\u5373\u200b\u4e3a\u200b (B,256,80,80)\u3001 (B,512,40,40) \u200b\u548c\u200b  (B,1024,20,20)\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#123-head","title":"1.2.3 Head","text":"<p>YOLOv5 Head \u200b\u7ed3\u6784\u200b\u548c\u200b YOLOv3 \u200b\u5b8c\u5168\u200b\u4e00\u6837\u200b\uff0c\u200b\u4e3a\u200b <code>\u200b\u975e\u89e3\u200b\u8026\u200b Head</code>\u3002Head \u200b\u6a21\u5757\u200b\u53ea\u200b\u5305\u62ec\u200b 3 \u200b\u4e2a\u200b\u4e0d\u200b\u5171\u4eab\u200b\u6743\u91cd\u200b\u7684\u200b\u5377\u79ef\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5c06\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u56fe\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u800c\u5df2\u200b\u3002</p> <p>\u200b\u524d\u9762\u200b\u7684\u200b PAFPN \u200b\u4f9d\u7136\u200b\u662f\u200b\u8f93\u51fa\u200b 3 \u200b\u4e2a\u200b\u4e0d\u540c\u200b\u5c3a\u5ea6\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0cshape \u200b\u4e3a\u200b (B,256,80,80)\u3001 (B,512,40,40) \u200b\u548c\u200b (B,1024,20,20)\u3002 \u200b\u7531\u4e8e\u200b YOLOv5 \u200b\u662f\u975e\u200b\u89e3\u200b\u8026\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5373\u200b\u5206\u7c7b\u200b\u548c\u200b bbox \u200b\u68c0\u6d4b\u200b\u7b49\u200b\u90fd\u200b\u662f\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u7684\u200b\u4e0d\u540c\u200b\u901a\u9053\u200b\u4e2d\u200b\u5b8c\u6210\u200b\u3002\u200b\u4ee5\u200b COCO 80 \u200b\u7c7b\u4e3a\u4f8b\u200b\uff0c\u200b\u5728\u200b\u8f93\u5165\u200b\u4e3a\u200b 640x640 \u200b\u5206\u8fa8\u7387\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5176\u200b Head \u200b\u6a21\u5757\u200b\u8f93\u51fa\u200b\u7684\u200b shape \u200b\u5206\u522b\u200b\u4e3a\u200b (B, 3x(4+1+80),80,80), (B, 3x(4+1+80),40,40) \u200b\u548c\u200b (B, 3x(4+1+80),20,20)\u3002\u200b\u5176\u4e2d\u200b 3 \u200b\u8868\u793a\u200b 3 \u200b\u4e2a\u200b anchor\uff0c4 \u200b\u8868\u793a\u200b bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\uff0c1 \u200b\u8868\u793a\u200b obj \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\uff0c80 \u200b\u8868\u793a\u200b COCO \u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#13","title":"1.3 \u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5339\u914d\u200b\u7b56\u7565","text":"<p>\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u7684\u200b\u6838\u5fc3\u200b\u662f\u200b\u786e\u5b9a\u200b\u9884\u6d4b\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u6240\u6709\u200b\u4f4d\u7f6e\u200b\u4e2d\u200b\u54ea\u4e9b\u200b\u4f4d\u7f6e\u200b\u5e94\u8be5\u200b\u662f\u200b\u6b63\u200b\u6837\u672c\u200b\uff0c\u200b\u54ea\u4e9b\u200b\u662f\u200b\u8d1f\u200b\u6837\u672c\u200b\uff0c\u200b\u751a\u81f3\u200b\u6709\u4e9b\u200b\u662f\u200b\u5ffd\u7565\u200b\u6837\u672c\u200b\u3002 \u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u662f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\u7684\u200b\u6838\u5fc3\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u53ef\u4ee5\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\u7b97\u6cd5\u200b\u6027\u80fd\u200b\u3002</p> <p>YOLOV5 \u200b\u7684\u200b\u5339\u914d\u200b\u7b56\u7565\u200b\u7b80\u5355\u200b\u603b\u7ed3\u200b\u4e3a\u200b\uff1a\u200b\u91c7\u7528\u200b\u4e86\u200b anchor \u200b\u548c\u200b gt_bbox \u200b\u7684\u200b shape \u200b\u5339\u914d\u200b\u5ea6\u200b\u4f5c\u4e3a\u200b\u5212\u5206\u200b\u89c4\u5219\u200b\uff0c\u200b\u540c\u65f6\u200b\u5f15\u5165\u200b\u8de8\u200b\u90bb\u57df\u200b\u7f51\u683c\u200b\u7b56\u7565\u200b\u6765\u200b\u589e\u52a0\u200b\u6b63\u200b\u6837\u672c\u200b\u3002 \u200b\u5176\u200b\u4e3b\u8981\u200b\u5305\u62ec\u200b\u5982\u4e0b\u200b\u4e24\u4e2a\u200b\u6838\u5fc3\u200b\u6b65\u9aa4\u200b\uff1a</p> <ol> <li>\u200b\u5bf9\u4e8e\u200b\u4efb\u4f55\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff0c\u200b\u629b\u5f03\u200b\u4e86\u200b\u5e38\u7528\u200b\u7684\u200b\u57fa\u4e8e\u200b Max IoU \u200b\u5339\u914d\u200b\u7684\u200b\u89c4\u5219\u200b\uff0c\u200b\u800c\u662f\u200b\u76f4\u63a5\u200b\u91c7\u7528\u200b shape \u200b\u89c4\u5219\u200b\u5339\u914d\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u8be5\u200b GT Bbox \u200b\u548c\u200b\u5f53\u524d\u200b\u5c42\u200b\u7684\u200b Anchor \u200b\u8ba1\u7b97\u200b\u5bbd\u9ad8\u6bd4\u200b\uff0c\u200b\u5982\u679c\u200b\u5bbd\u9ad8\u200b\u6bd4\u4f8b\u200b\u5927\u4e8e\u200b\u8bbe\u5b9a\u200b\u9608\u503c\u200b\uff0c\u200b\u5219\u200b\u8bf4\u660e\u200b\u8be5\u200b GT Bbox \u200b\u548c\u200b Anchor \u200b\u5339\u914d\u200b\u5ea6\u200b\u4e0d\u591f\u200b\uff0c\u200b\u5c06\u200b\u8be5\u200b GT Bbox \u200b\u6682\u65f6\u200b\u4e22\u6389\u200b\uff0c\u200b\u5728\u200b\u8be5\u5c42\u200b\u9884\u6d4b\u200b\u4e2d\u8be5\u200b GT Bbox \u200b\u5bf9\u5e94\u200b\u7684\u200b\u7f51\u683c\u200b\u5185\u200b\u7684\u200b\u9884\u6d4b\u200b\u4f4d\u7f6e\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u8d1f\u200b\u6837\u672c\u200b</li> <li>\u200b\u5bf9\u4e8e\u200b\u5269\u4e0b\u200b\u7684\u200b GT Bbox(\u200b\u4e5f\u200b\u5c31\u662f\u200b\u5339\u914d\u200b\u4e0a\u200b\u7684\u200b GT Bbox)\uff0c\u200b\u8ba1\u7b97\u200b\u5176\u843d\u200b\u5728\u200b\u54ea\u4e2a\u200b\u7f51\u683c\u200b\u5185\u200b\uff0c\u200b\u540c\u65f6\u200b\u5229\u7528\u200b\u56db\u820d\u4e94\u5165\u200b\u89c4\u5219\u200b\uff0c\u200b\u627e\u51fa\u200b\u6700\u8fd1\u200b\u7684\u200b\u4e24\u4e2a\u200b\u7f51\u683c\u200b\uff0c\u200b\u5c06\u200b\u8fd9\u200b\u4e09\u4e2a\u200b\u7f51\u683c\u200b\u90fd\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u8d1f\u8d23\u200b\u9884\u6d4b\u200b\u8be5\u200b GT Bbox \u200b\u7684\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7c97\u7565\u200b\u4f30\u8ba1\u200b\u6b63\u200b\u6837\u672c\u6570\u200b\u76f8\u6bd4\u200b\u4e4b\u524d\u200b\u7684\u200b YOLO \u200b\u7cfb\u5217\u200b\uff0c\u200b\u81f3\u5c11\u200b\u589e\u52a0\u200b\u4e86\u200b\u4e09\u500d\u200b</li> </ol> <p>\u200b\u4e0b\u9762\u200b\u4f1a\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u8fdb\u884c\u200b\u8be6\u7ec6\u200b\u8bf4\u660e\u200b\uff0c\u200b\u90e8\u5206\u200b\u63cf\u8ff0\u200b\u548c\u200b\u56fe\u793a\u200b\u76f4\u63a5\u200b\u6216\u200b\u95f4\u63a5\u200b\u53c2\u8003\u200b\u81ea\u200b\u5b98\u65b9\u200b Repo\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#131-anchor","title":"1.3.1 Anchor \u200b\u8bbe\u7f6e","text":"<p>YOLOv5 \u200b\u662f\u200b Anchor-based \u200b\u7684\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u5176\u200b Anchor size \u200b\u7684\u200b\u83b7\u53d6\u200b\u65b9\u5f0f\u200b\u4e0e\u200b YOLOv3 \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u4f7f\u7528\u200b\u805a\u7c7b\u200b\u83b7\u5f97\u200b\uff0c\u200b\u5176\u200b\u4e0d\u540c\u4e4b\u5904\u200b\u5728\u4e8e\u200b\u805a\u7c7b\u200b\u4f7f\u7528\u200b\u7684\u200b\u6807\u51c6\u200b\u4e0d\u518d\u200b\u662f\u200b\u57fa\u4e8e\u200b IoU \u200b\u7684\u200b\uff0c\u200b\u800c\u662f\u200b\u4f7f\u7528\u200b\u5f62\u72b6\u200b\u4e0a\u200b\u7684\u200b\u5bbd\u9ad8\u6bd4\u200b\u4f5c\u4e3a\u200b\u805a\u7c7b\u200b\u51c6\u5219\u200b(\u200b\u5373\u200b shape-match )\u3002</p> <p>\u200b\u5728\u200b\u7528\u6237\u200b\u66f4\u6362\u200b\u4e86\u200b\u6570\u636e\u200b\u96c6\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b MMYOLO \u200b\u91cc\u200b\u5e26\u6709\u200b\u7684\u200b Anchor \u200b\u5206\u6790\u200b\u5de5\u5177\u200b\uff0c\u200b\u5bf9\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u5206\u6790\u200b\uff0c\u200b\u786e\u5b9a\u200b\u5408\u9002\u200b\u7684\u200b Anchor size\u3002</p> <pre><code>python tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm v5-k-means\n --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5728\u200b config \u200b\u6587\u4ef6\u200b \u200b\u91cc\u200b\u4fee\u6539\u200b\u9ed8\u8ba4\u200b Anchor size:</p> <pre><code>anchors = [[(10, 13), (16, 30), (33, 23)], [(30, 61), (62, 45), (59, 119)],\n           [(116, 90), (156, 198), (373, 326)]]\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#132-bbox","title":"1.3.2 Bbox \u200b\u7f16\u89e3\u7801\u200b\u8fc7\u7a0b","text":"<p>\u200b\u5728\u200b Anchor-based \u200b\u7b97\u6cd5\u200b\u4e2d\u200b\uff0c\u200b\u9884\u6d4b\u200b\u6846\u200b\u901a\u5e38\u200b\u4f1a\u200b\u57fa\u4e8e\u200b Anchor \u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\uff0c\u200b\u7136\u540e\u200b\u9884\u6d4b\u200b\u53d8\u6362\u200b\u91cf\u200b\uff0c\u200b\u8fd9\u200b\u5bf9\u5e94\u200b GT Bbox \u200b\u7f16\u7801\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u9884\u6d4b\u200b\u540e\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b Pred Bbox \u200b\u89e3\u7801\u200b\uff0c\u200b\u8fd8\u539f\u200b\u4e3a\u200b\u771f\u5b9e\u200b\u5c3a\u5ea6\u200b\u7684\u200b Bbox\uff0c\u200b\u8fd9\u200b\u5bf9\u5e94\u200b Pred Bbox \u200b\u89e3\u7801\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u5728\u200b YOLOv3 \u200b\u4e2d\u200b\uff0c\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\u4e3a\u200b\uff1a</p> <pre><code>b_x=\\sigma(t_x)+c_x  \\\\\nb_y=\\sigma(t_y)+c_y  \\\\\nb_w=a_w\\cdot e^{t_w} \\\\\nb_h=a_h\\cdot e^{t_h} \\\\\n</code></pre> <p>\u200b\u516c\u5f0f\u200b\u4e2d\u200b\uff0c</p> <pre><code>a_w \u200b\u4ee3\u8868\u200b Anchor \u200b\u7684\u200b\u5bbd\u5ea6\u200b \\\\\nc_x \u200b\u4ee3\u8868\u200b Grid \u200b\u6240\u5904\u200b\u7684\u200b\u5750\u6807\u200b \\\\\n\\sigma \u200b\u4ee3\u8868\u200b Sigmoid \u200b\u516c\u5f0f\u200b\u3002\n</code></pre> <p>\u200b\u800c\u200b\u5728\u200b YOLOv5 \u200b\u4e2d\u200b\uff0c\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\u4e3a\u200b\uff1a</p> <pre><code>b_x=(2\\cdot\\sigma(t_x)-0.5)+c_x   \\\\\nb_y=(2\\cdot\\sigma(t_y)-0.5)+c_y   \\\\\nb_w=a_w\\cdot(2\\cdot\\sigma(t_w))^2   \\\\\nb_h=a_h\\cdot(2\\cdot\\sigma(t_h))^2\n</code></pre> <p>\u200b\u6539\u8fdb\u200b\u4e4b\u200b\u5904\u200b\u4e3b\u8981\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u4e24\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u4e2d\u5fc3\u70b9\u200b\u5750\u6807\u200b\u8303\u56f4\u200b\u4ece\u200b (0, 1) \u200b\u8c03\u6574\u200b\u81f3\u200b (-0.5, 1.5)</li> <li>\u200b\u5bbd\u9ad8\u200b\u8303\u56f4\u200b\u4ece\u200b</li> </ul> <pre><code>(0\uff0c+\\infty)\n</code></pre> <p>\u200b\u8c03\u6574\u200b\u81f3\u200b</p> <pre><code>(0\uff0c4a_{wh})\n</code></pre> <p>\u200b\u8fd9\u4e2a\u200b\u6539\u8fdb\u200b\u5177\u6709\u200b\u4ee5\u4e0b\u200b\u597d\u5904\u200b\uff1a</p> <ul> <li>\u200b\u65b0\u200b\u7684\u200b\u4e2d\u5fc3\u70b9\u200b\u8bbe\u7f6e\u200b\u80fd\u200b\u66f4\u597d\u200b\u7684\u200b\u9884\u6d4b\u200b\u5230\u200b 0 \u200b\u548c\u200b 1\u3002\u200b\u8fd9\u200b\u6709\u52a9\u4e8e\u200b\u66f4\u200b\u7cbe\u51c6\u200b\u56de\u5f52\u200b\u51fa\u200b box \u200b\u5750\u6807\u200b\u3002</li> </ul> <ul> <li>\u200b\u5bbd\u9ad8\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\u4e2d\u200b exp(x) \u200b\u662f\u200b\u65e0\u754c\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4f1a\u200b\u5bfc\u81f4\u200b\u68af\u5ea6\u200b\u5931\u53bb\u200b\u63a7\u5236\u200b\uff0c\u200b\u9020\u6210\u200b\u8bad\u7ec3\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\u3002YOLOv5 \u200b\u4e2d\u200b\u6539\u8fdb\u200b\u540e\u200b\u7684\u200b\u5bbd\u200b\u9ad8\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\u4f18\u5316\u200b\u4e86\u200b\u6b64\u200b\u95ee\u9898\u200b\u3002</li> </ul>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#133","title":"1.3.3 \u200b\u5339\u914d\u200b\u7b56\u7565","text":"<p>\u200b\u5728\u200b MMYOLO \u200b\u8bbe\u8ba1\u200b\u4e2d\u200b\uff0c\u200b\u65e0\u8bba\u200b\u7f51\u7edc\u200b\u662f\u200b Anchor-based \u200b\u8fd8\u662f\u200b Anchor-free\uff0c\u200b\u6211\u4eec\u200b\u7edf\u4e00\u200b\u4f7f\u7528\u200b prior \u200b\u79f0\u547c\u200b Anchor\u3002</p> <p>\u200b\u6b63\u200b\u6837\u672c\u200b\u5339\u914d\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u4e24\u6b65\u200b\uff1a</p> <p>(1) \u201c\u200b\u6bd4\u4f8b\u200b\u201d\u200b\u6bd4\u8f83\u200b</p> <p>\u200b\u5c06\u200b GT Bbox \u200b\u7684\u200b WH \u200b\u4e0e\u200b Prior \u200b\u7684\u200b WH \u200b\u8fdb\u884c\u200b\u201c\u200b\u6bd4\u4f8b\u200b\u201d\u200b\u6bd4\u8f83\u200b\u3002</p> <p>\u200b\u6bd4\u8f83\u200b\u6d41\u7a0b\u200b\uff1a</p> <pre><code>r_w = w\\_{gt} / w\\_{pt}    \\\\\nr_h = h\\_{gt} / h\\_{pt}    \\\\\nr_w^{max}=max(r_w, 1/r_w)  \\\\\nr_h^{max}=max(r_h, 1/r_h)  \\\\\nr^{max}=max(r_w^{max}, r_h^{max})   \\\\\nif\\ \\ r_{max} &lt; prior\\_match\\_thr:   match!\n</code></pre> <p>\u200b\u6b64\u5904\u200b\u6211\u4eec\u200b\u7528\u200b\u4e00\u4e2a\u200b GT Bbox \u200b\u4e0e\u200b P3 \u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b Prior \u200b\u8fdb\u884c\u200b\u5339\u914d\u200b\u7684\u200b\u6848\u4f8b\u200b\u8fdb\u884c\u200b\u8bb2\u89e3\u200b\u548c\u200b\u56fe\u793a\u200b\uff1a</p> <p>prior1 \u200b\u5339\u914d\u200b\u5931\u8d25\u200b\u7684\u200b\u539f\u56e0\u200b\u662f\u200b</p> <pre><code>h\\_{gt}\\ /\\ h\\_{prior}\\ =\\ 4.8\\ &gt;\\ prior\\_match\\_thr\n</code></pre> <p>(2) \u200b\u4e3a\u200b\u6b65\u9aa4\u200b 1 \u200b\u4e2d\u200b match \u200b\u7684\u200b GT \u200b\u5206\u914d\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6b63\u200b\u6837\u672c\u200b</p> <p>\u200b\u4f9d\u7136\u200b\u6cbf\u7528\u200b\u4e0a\u9762\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff1a</p> <p>GT Bbox (cx, cy, w, h) \u200b\u503c\u4e3a\u200b (26, 37, 36, 24)\uff0c</p> <p>Prior WH \u200b\u503c\u4e3a\u200b [(15, 5), (24, 16), (16, 24)]\uff0c\u200b\u5728\u200b P3 \u200b\u7279\u5f81\u200b\u56fe\u4e0a\u200b\uff0cstride \u200b\u4e3a\u200b 8\u3002\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u200b\uff0cprior2 \u200b\u548c\u200b prior3 \u200b\u80fd\u591f\u200b match\u3002</p> <p>\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\uff1a</p> <p>(2.1) \u200b\u5c06\u200b GT Bbox \u200b\u7684\u200b\u4e2d\u5fc3\u70b9\u200b\u5750\u6807\u200b\u5bf9\u5e94\u200b\u5230\u200b P3 \u200b\u7684\u200b grid \u200b\u4e0a\u200b</p> <pre><code>GT_x^{center_grid}=26/8=3.25  \\\\\nGT_y^{center_grid}=37/8=4.625\n</code></pre> <p>(2.2) \u200b\u5c06\u200b GT Bbox \u200b\u4e2d\u5fc3\u70b9\u200b\u6240\u5728\u200b\u7684\u200b grid \u200b\u5206\u6210\u200b\u56db\u4e2a\u200b\u8c61\u9650\u200b\uff0c\u200b\u7531\u4e8e\u200b\u4e2d\u5fc3\u70b9\u200b\u843d\u200b\u5728\u200b\u4e86\u200b\u5de6\u4e0b\u89d2\u200b\u7684\u200b\u8c61\u9650\u200b\u5f53\u4e2d\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4f1a\u200b\u5c06\u200b\u7269\u4f53\u200b\u7684\u200b\u5de6\u200b\u3001\u200b\u4e0b\u200b\u4e24\u4e2a\u200b grid \u200b\u4e5f\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u6b63\u200b\u6837\u672c\u200b</p> <p>\u200b\u4e0b\u56fe\u200b\u5c55\u793a\u4e2d\u5fc3\u200b\u70b9\u200b\u843d\u5230\u200b\u4e0d\u540c\u200b\u4f4d\u7f6e\u200b\u65f6\u200b\u7684\u200b\u6b63\u200b\u6837\u672c\u200b\u5206\u914d\u60c5\u51b5\u200b\uff1a</p> <p>\u200b\u90a3\u4e48\u200b YOLOv5 \u200b\u7684\u200b Assign \u200b\u65b9\u5f0f\u200b\u5177\u4f53\u200b\u5e26\u6765\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u6539\u8fdb\u200b\uff1f</p> <ul> <li> <p>\u200b\u4e00\u4e2a\u200b GT Bbox \u200b\u80fd\u591f\u200b\u5339\u914d\u200b\u591a\u4e2a\u200b Prior</p> </li> <li> <p>\u200b\u4e00\u4e2a\u200b GT Bbox \u200b\u548c\u200b\u4e00\u4e2a\u200bPrior \u200b\u5339\u914d\u200b\u65f6\u200b\uff0c\u200b\u80fd\u200b\u5206\u914d\u200b 1-3 \u200b\u4e2a\u200b\u6b63\u200b\u6837\u672c\u200b</p> </li> <li> <p>\u200b\u4ee5\u4e0a\u200b\u7b56\u7565\u200b\u80fd\u200b\u9002\u5ea6\u200b\u7f13\u89e3\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u6b63\u8d1f\u200b\u6837\u672c\u200b\u4e0d\u200b\u5747\u8861\u200b\u95ee\u9898\u200b\u3002</p> </li> </ul> <p>\u200b\u800c\u200b YOLOv5 \u200b\u4e2d\u200b\u7684\u200b\u56de\u5f52\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u548c\u200b Assign \u200b\u65b9\u5f0f\u200b\u662f\u200b\u76f8\u4e92\u200b\u547c\u5e94\u200b\u7684\u200b\uff1a</p> <ol> <li>\u200b\u4e2d\u5fc3\u70b9\u200b\u56de\u5f52\u200b\u65b9\u5f0f\u200b\uff1a</li> </ol> <ol> <li>WH \u200b\u56de\u5f52\u200b\u65b9\u5f0f\u200b\uff1a</li> </ol>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#14-loss","title":"1.4 Loss \u200b\u8bbe\u8ba1","text":"<p>YOLOv5 \u200b\u4e2d\u200b\u603b\u5171\u200b\u5305\u542b\u200b 3 \u200b\u4e2a\u200b Loss\uff0c\u200b\u5206\u522b\u200b\u4e3a\u200b\uff1a</p> <ul> <li>Classes loss\uff1a\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b BCE loss</li> <li>Objectness loss\uff1a\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b BCE loss</li> <li>Location loss\uff1a\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b CIoU loss</li> </ul> <p>\u200b\u4e09\u4e2a\u200b loss \u200b\u6309\u7167\u200b\u4e00\u5b9a\u200b\u6bd4\u4f8b\u200b\u6c47\u603b\u200b\uff1a</p> <pre><code>Loss=\\lambda_1L_{cls}+\\lambda_2L_{obj}+\\lambda_3L_{loc}\n</code></pre> <p>P3\u3001P4\u3001P5 \u200b\u5c42\u200b\u5bf9\u5e94\u200b\u7684\u200b Objectness loss \u200b\u6309\u7167\u200b\u4e0d\u540c\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u76f8\u52a0\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u662f\u200b</p> <pre><code>obj_level_weights=[4., 1., 0.4]\n</code></pre> <pre><code>L_{obj}=4.0\\cdot L_{obj}^{small}+1.0\\cdot L_{obj}^{medium}+0.4\\cdot L_{obj}^{large}\n</code></pre> <p>\u200b\u5728\u200b\u590d\u73b0\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b YOLOv5 \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b CIoU \u200b\u4e0e\u200b\u76ee\u524d\u200b\u6700\u65b0\u200b\u5b98\u65b9\u200b CIoU \u200b\u5b58\u5728\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5dee\u8ddd\u200b\uff0c\u200b\u5dee\u8ddd\u200b\u4f53\u73b0\u200b\u5728\u200b alpha \u200b\u53c2\u6570\u200b\u7684\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u5b98\u65b9\u200b\u7248\u672c\u200b\uff1a</p> <p>\u200b\u53c2\u8003\u8d44\u6599\u200b\uff1ahttps://github.com/Zzh-tju/CIoU/blob/master/layers/modules/multibox_loss.py#L53-L55</p> <pre><code>alpha = (ious &gt; 0.5).float() * v / (1 - ious + v)\n</code></pre> <p>YOLOv5 \u200b\u7248\u672c\u200b\uff1a</p> <pre><code>alpha = v / (v - ious + (1 + eps))\n</code></pre> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u6709\u8da3\u200b\u7684\u200b\u7ec6\u8282\u200b\uff0c\u200b\u540e\u7eed\u200b\u9700\u8981\u200b\u6d4b\u8bd5\u200b\u4e0d\u540c\u200b alpha \u200b\u8ba1\u7b97\u200b\u65b9\u5f0f\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5e26\u6765\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u5dee\u8ddd\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#15","title":"1.5 \u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u548c\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b","text":"<p>YOLOv5 \u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u200b\u8fdb\u884c\u200b\u975e\u5e38\u200b\u7cbe\u7ec6\u200b\u7684\u200b\u63a7\u5236\u200b\uff0c\u200b\u7b80\u5355\u200b\u6765\u8bf4\u200b\u5305\u62ec\u200b\u5982\u4e0b\u200b\u90e8\u5206\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#151","title":"1.5.1 \u200b\u4f18\u5316\u200b\u5668\u200b\u5206\u7ec4","text":"<p>\u200b\u5c06\u200b\u4f18\u5316\u200b\u53c2\u6570\u200b\u5206\u6210\u200b Conv/Bias/BN \u200b\u4e09\u7ec4\u200b\uff0c\u200b\u5728\u200b WarmUp \u200b\u9636\u6bb5\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7ec4\u200b\u91c7\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b lr \u200b\u4ee5\u53ca\u200b momentum \u200b\u66f4\u65b0\u200b\u66f2\u7ebf\u200b\u3002 \u200b\u540c\u65f6\u200b\u5728\u200b WarmUp \u200b\u9636\u6bb5\u200b\u91c7\u7528\u200b\u7684\u200b\u662f\u200b iter-based \u200b\u66f4\u65b0\u200b\u7b56\u7565\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u975e\u200b WarmUp \u200b\u9636\u6bb5\u200b\u5219\u200b\u53d8\u6210\u200b epoch-based \u200b\u66f4\u65b0\u200b\u7b56\u7565\u200b\uff0c\u200b\u53ef\u8c13\u200b\u662f\u200b trick \u200b\u5341\u8db3\u200b\u3002</p> <p>MMYOLO \u200b\u4e2d\u662f\u200b\u91c7\u7528\u200b YOLOv5OptimizerConstructor \u200b\u4f18\u5316\u200b\u5668\u200b\u6784\u9020\u200b\u5668\u200b\u5b9e\u73b0\u200b\u4f18\u5316\u200b\u5668\u200b\u53c2\u6570\u200b\u5206\u7ec4\u200b\u3002\u200b\u4f18\u5316\u200b\u5668\u200b\u6784\u9020\u200b\u5668\u200b\u7684\u200b\u4f5c\u7528\u200b\u5c31\u662f\u200b\u5bf9\u200b\u4e00\u4e9b\u200b\u7279\u6b8a\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u200b\u521d\u59cb\u5316\u200b\u8fc7\u7a0b\u200b\u8fdb\u884c\u200b\u7cbe\u7ec6\u5316\u200b\u63a7\u5236\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u6ee1\u8db3\u200b\u9700\u6c42\u200b\u3002</p> <p>\u200b\u800c\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u200b\u91c7\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u8c03\u5ea6\u200b\u66f2\u7ebf\u200b\u529f\u80fd\u200b\u5219\u200b\u662f\u200b\u901a\u8fc7\u200b YOLOv5ParamSchedulerHook \u200b\u5b9e\u73b0\u200b\u3002\u200b\u800c\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u200b\u91c7\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u8c03\u5ea6\u200b\u66f2\u7ebf\u200b\u529f\u80fd\u200b\u5219\u200b\u662f\u200b\u901a\u8fc7\u200b YOLOv5ParamSchedulerHook \u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#152-weight-decay","title":"1.5.2 weight decay \u200b\u53c2\u6570\u200b\u81ea\u200b\u9002\u5e94","text":"<p>\u200b\u4f5c\u8005\u200b\u9488\u5bf9\u200b\u4e0d\u540c\u200b\u7684\u200b batch size \u200b\u91c7\u7528\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b weight decay \u200b\u7b56\u7565\u200b\uff0c\u200b\u5177\u4f53\u6765\u8bf4\u200b\u4e3a\u200b\uff1a</p> <ol> <li>\u200b\u5f53\u200b\u8bad\u7ec3\u200b batch size \\&lt;= 64 \u200b\u65f6\u200b\uff0cweight decay \u200b\u4e0d\u53d8\u200b</li> <li>\u200b\u5f53\u200b\u8bad\u7ec3\u200b batch size &gt; 64 \u200b\u65f6\u200b\uff0cweight decay \u200b\u4f1a\u200b\u6839\u636e\u200b\u603b\u200b batch size \u200b\u8fdb\u884c\u200b\u7ebf\u6027\u200b\u7f29\u653e\u200b</li> </ol> <p>MMYOLO \u200b\u4e5f\u200b\u662f\u200b\u901a\u8fc7\u200b YOLOv5OptimizerConstructor \u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#153","title":"1.5.3 \u200b\u68af\u5ea6\u200b\u7d2f\u52a0","text":"<p>\u200b\u4e3a\u4e86\u200b\u6700\u5927\u5316\u200b\u4e0d\u540c\u200b batch size \u200b\u60c5\u51b5\u200b\u4e0b\u200b\u7684\u200b\u6027\u80fd\u200b\uff0c\u200b\u4f5c\u8005\u200b\u8bbe\u7f6e\u200b\u603b\u200b batch size \u200b\u5c0f\u4e8e\u200b 64 \u200b\u65f6\u5019\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u5f00\u542f\u200b\u68af\u5ea6\u200b\u7d2f\u52a0\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u548c\u200b\u5927\u90e8\u5206\u200b YOLO \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5305\u62ec\u200b\u5982\u4e0b\u200b\u7b56\u7565\u200b\uff1a</p> <ol> <li>\u200b\u6ca1\u6709\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b</li> <li>\u200b\u6ca1\u6709\u200b\u91c7\u7528\u200b\u591a\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u540c\u65f6\u200b\u53ef\u4ee5\u200b\u5f00\u542f\u200b cudnn.benchmark \u200b\u8fdb\u4e00\u6b65\u200b\u52a0\u901f\u200b\u8bad\u7ec3\u200b</li> <li>\u200b\u4f7f\u7528\u200b\u4e86\u200b EMA \u200b\u7b56\u7565\u200b\u5e73\u6ed1\u200b\u6a21\u578b\u200b</li> <li>\u200b\u9ed8\u8ba4\u200b\u91c7\u7528\u200b AMP \u200b\u81ea\u52a8\u200b\u6df7\u5408\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b</li> </ol> <p>\u200b\u9700\u8981\u200b\u7279\u610f\u200b\u8bf4\u660e\u200b\u7684\u200b\u662f\u200b\uff1aYOLOv5 \u200b\u5b98\u65b9\u200b\u5bf9\u4e8e\u200b small \u200b\u6a21\u578b\u200b\u662f\u200b\u91c7\u7528\u200b\u5355\u5361\u200b v100 \u200b\u8bad\u7ec3\u200b\uff0cbs \u200b\u4e3a\u200b 128\uff0c\u200b\u800c\u200b m/l/x \u200b\u7b49\u200b\u662f\u200b\u91c7\u7528\u200b\u4e0d\u540c\u200b\u6570\u76ee\u200b\u7684\u200b\u591a\u5361\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff0c \u200b\u8fd9\u79cd\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u4e0d\u592a\u200b\u89c4\u8303\u200b\uff0c\u200b\u4e3a\u6b64\u200b\u5728\u200b MMYOLO \u200b\u4e2d\u200b\u5168\u90e8\u200b\u91c7\u7528\u200b\u4e86\u200b 8 \u200b\u5361\u200b\uff0c\u200b\u6bcf\u5361\u200b 16 bs \u200b\u7684\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b\u6027\u80fd\u200b\u5dee\u5f02\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u65f6\u5019\u200b\u5f00\u542f\u200b\u4e86\u200b SyncBN\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#16","title":"1.6 \u200b\u63a8\u7406\u200b\u548c\u200b\u540e\u200b\u5904\u7406\u8fc7\u7a0b","text":"<p>YOLOv5 \u200b\u540e\u200b\u5904\u7406\u8fc7\u7a0b\u200b\u548c\u200b YOLOv3 \u200b\u975e\u5e38\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b YOLO \u200b\u7cfb\u5217\u200b\u7684\u200b\u540e\u5904\u7406\u200b\u903b\u8f91\u200b\u90fd\u200b\u662f\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#161","title":"1.6.1 \u200b\u6838\u5fc3\u200b\u63a7\u5236\u53c2\u6570","text":"<ol> <li>multi_label</li> </ol> <p>\u200b\u5bf9\u4e8e\u200b\u591a\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u6765\u8bf4\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u662f\u5426\u662f\u200b\u591a\u200b\u6807\u7b7e\u200b\u4efb\u52a1\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u540c\u4e00\u4e2a\u200b\u9884\u6d4b\u200b\u4f4d\u7f6e\u200b\u4f1a\u200b\u9884\u6d4b\u200b\u7684\u200b\u591a\u4e2a\u200b\u7c7b\u522b\u200b\u6982\u7387\u200b\uff0c\u200b\u548c\u200b\u662f\u5426\u200b\u5f53\u4f5c\u200b\u5355\u7c7b\u200b\u5904\u7406\u200b\u3002\u200b\u56e0\u4e3a\u200b YOLOv5 \u200b\u91c7\u7528\u200b sigmoid \u200b\u9884\u6d4b\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5728\u200b\u8003\u8651\u200b\u591a\u200b\u6807\u7b7e\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4e00\u4e2a\u200b\u7269\u4f53\u200b\u68c0\u6d4b\u200b\u51fa\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u6846\u200b\uff0c\u200b\u8fd9\u200b\u6709\u52a9\u4e8e\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b mAP\uff0c\u200b\u4f46\u662f\u200b\u4e0d\u5229\u4e8e\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u3002 \u200b\u56e0\u6b64\u200b\u5728\u200b\u9700\u8981\u200b\u7b97\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u65f6\u5019\u200b multi_label \u200b\u662f\u200b True\uff0c\u200b\u800c\u200b\u63a8\u7406\u200b\u6216\u8005\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u65f6\u5019\u200b\u662f\u200b False</p> <ol> <li>score_thr \u200b\u548c\u200b nms_thr</li> </ol> <p>score_thr \u200b\u9608\u503c\u200b\u7528\u4e8e\u200b\u8fc7\u6ee4\u200b\u7c7b\u522b\u200b\u5206\u503c\u200b\uff0c\u200b\u4f4e\u4e8e\u200b\u5206\u503c\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\u5f53\u505a\u200b\u80cc\u666f\u200b\u5904\u7406\u200b\uff0cnms_thr \u200b\u662f\u200b nms \u200b\u65f6\u200b\u9608\u503c\u200b\u3002\u200b\u540c\u6837\u200b\u7684\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b mAP \u200b\u9636\u6bb5\u200b\u53ef\u4ee5\u200b\u5c06\u200b score_thr \u200b\u8bbe\u7f6e\u200b\u7684\u200b\u975e\u5e38\u4f4e\u200b\uff0c\u200b\u8fd9\u200b\u901a\u5e38\u200b\u80fd\u591f\u200b\u63d0\u9ad8\u200b\u53ec\u56de\u200b\u7387\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b mAP\uff0c\u200b\u4f46\u662f\u200b\u5bf9\u4e8e\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u6765\u8bf4\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\uff0c\u200b\u4e14\u4f1a\u200b\u5bfc\u81f4\u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\u6781\u6162\u200b\u3002\u200b\u4e3a\u6b64\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u548c\u200b\u63a8\u7406\u200b\u9636\u6bb5\u200b\u4f1a\u200b\u8bbe\u7f6e\u200b\u4e0d\u540c\u200b\u7684\u200b\u9608\u503c\u200b</p> <ol> <li>nms_pre \u200b\u548c\u200b max_per_img</li> </ol> <p>nms_pre \u200b\u8868\u793a\u200b nms \u200b\u524d\u200b\u7684\u200b\u6700\u5927\u200b\u4fdd\u7559\u200b\u68c0\u6d4b\u200b\u6846\u200b\u6570\u76ee\u200b\uff0c\u200b\u8fd9\u200b\u901a\u5e38\u200b\u662f\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b nms \u200b\u8fd0\u884c\u200b\u65f6\u5019\u200b\u8f93\u5165\u6846\u200b\u8fc7\u591a\u200b\u5bfc\u81f4\u200b\u901f\u5ea6\u200b\u8fc7\u200b\u6162\u200b\u95ee\u9898\u200b\uff0c\u200b\u9ed8\u8ba4\u503c\u200b\u662f\u200b 30000\u3002 max_per_img \u200b\u8868\u793a\u200b\u6700\u7ec8\u200b\u4fdd\u7559\u200b\u7684\u200b\u6700\u5927\u200b\u68c0\u6d4b\u200b\u6846\u200b\u6570\u76ee\u200b\uff0c\u200b\u901a\u5e38\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b 300\u3002</p> <p>\u200b\u4ee5\u200b COCO 80 \u200b\u7c7b\u4e3a\u4f8b\u200b\uff0c\u200b\u5047\u8bbe\u200b\u8f93\u5165\u200b\u56fe\u7247\u5927\u5c0f\u200b\u4e3a\u200b 640x640</p> <p>\u200b\u5176\u200b\u63a8\u7406\u200b\u548c\u200b\u540e\u200b\u5904\u7406\u8fc7\u7a0b\u200b\u4e3a\u200b\uff1a</p> <p>(1) \u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b</p> <p>YOLOv5 \u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u56fe\u200b\u5c3a\u5ea6\u200b\u4e3a\u200b 80x80\u300140x40 \u200b\u548c\u200b 20x20 \u200b\u7684\u200b\u4e09\u4e2a\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u4f4d\u7f6e\u200b\u5171\u200b 3 \u200b\u4e2a\u200b anchor\uff0c\u200b\u56e0\u6b64\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u56fe\u200b\u901a\u9053\u200b\u4e3a\u200b 3x(5+80)=255\u3002 YOLOv5 \u200b\u662f\u975e\u200b\u89e3\u200b\u8026\u200b\u8f93\u51fa\u200b\u5934\u200b\uff0c\u200b\u800c\u200b\u5176\u4ed6\u200b\u5927\u90e8\u5206\u200b\u7b97\u6cd5\u200b\u90fd\u200b\u662f\u200b\u89e3\u200b\u8026\u200b\u8f93\u51fa\u200b\u5934\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u7edf\u4e00\u200b\u540e\u5904\u7406\u200b\u903b\u8f91\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u524d\u200b\u5c06\u200b\u5176\u200b\u8fdb\u884c\u200b\u89e3\u200b\u8026\u200b\uff0c\u200b\u5206\u6210\u200b\u4e86\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3001bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u548c\u200b obj \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3002</p> <p>\u200b\u5c06\u200b\u4e09\u4e2a\u200b\u4e0d\u540c\u200b\u5c3a\u5ea6\u200b\u7684\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3001bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u548c\u200b obj \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u8fdb\u884c\u200b\u62fc\u63a5\u200b\uff0c\u200b\u5e76\u200b\u8fdb\u884c\u200b\u7ef4\u5ea6\u200b\u53d8\u6362\u200b\u3002\u200b\u4e3a\u4e86\u200b\u540e\u7eed\u200b\u65b9\u4fbf\u200b\u5904\u7406\u200b\uff0c\u200b\u4f1a\u200b\u5c06\u200b\u539f\u5148\u200b\u7684\u200b\u901a\u9053\u200b\u7ef4\u5ea6\u200b\u7f6e\u6362\u200b\u5230\u200b\u6700\u540e\u200b\uff0c\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u3001bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u548c\u200b obj \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u7684\u200b shape \u200b\u5206\u522b\u200b\u4e3a\u200b (b, 3x80x80+3x40x40+3x20x20, 80)=(b,25200,80)\uff0c(b,25200,4)\uff0c(b,25200,1)\u3002</p> <p>(2) \u200b\u89e3\u7801\u200b\u8fd8\u539f\u200b\u5230\u200b\u539f\u56fe\u200b\u5c3a\u5ea6\u200b</p> <p>\u200b\u5206\u7c7b\u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u548c\u200b obj \u200b\u5206\u652f\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b sigmoid \u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b bbox \u200b\u9884\u6d4b\u200b\u5206\u652f\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u89e3\u7801\u200b\uff0c\u200b\u8fd8\u539f\u200b\u4e3a\u200b\u771f\u5b9e\u200b\u7684\u200b\u539f\u56fe\u200b\u89e3\u7801\u200b\u540e\u200b xyxy \u200b\u683c\u5f0f\u200b</p> <p>(3) \u200b\u7b2c\u4e00\u6b21\u200b\u9608\u503c\u200b\u8fc7\u6ee4\u200b</p> <p>\u200b\u904d\u5386\u200b batch \u200b\u4e2d\u200b\u7684\u200b\u6bcf\u5f20\u200b\u56fe\u200b\uff0c\u200b\u7136\u540e\u200b\u7528\u200b score_thr \u200b\u5bf9\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u503c\u200b\u8fdb\u884c\u200b\u9608\u503c\u200b\u8fc7\u6ee4\u200b\uff0c\u200b\u53bb\u6389\u200b\u4f4e\u4e8e\u200b score_thr \u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b</p> <p>(4) \u200b\u7b2c\u4e8c\u6b21\u200b\u9608\u503c\u200b\u8fc7\u6ee4\u200b</p> <p>\u200b\u5c06\u200b obj \u200b\u9884\u6d4b\u200b\u5206\u503c\u200b\u548c\u200b\u8fc7\u6ee4\u200b\u540e\u200b\u7684\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u5206\u503c\u200b\u76f8\u4e58\u200b\uff0c\u200b\u7136\u540e\u200b\u4f9d\u7136\u200b\u91c7\u7528\u200b score_thr \u200b\u8fdb\u884c\u200b\u9608\u503c\u200b\u8fc7\u6ee4\u200b\u3002 \u200b\u5728\u200b\u8fd9\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8fd8\u200b\u9700\u8981\u200b\u8003\u8651\u200b multi_label \u200b\u548c\u200b nms_pre\uff0c\u200b\u786e\u4fdd\u200b\u8fc7\u6ee4\u200b\u540e\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\u6570\u76ee\u200b\u4e0d\u4f1a\u200b\u591a\u4e8e\u200b nms_pre\u3002</p> <p>(5) \u200b\u8fd8\u539f\u200b\u5230\u200b\u539f\u56fe\u200b\u5c3a\u5ea6\u200b\u548c\u200b nms</p> <p>\u200b\u57fa\u4e8e\u200b\u524d\u200b\u5904\u7406\u8fc7\u7a0b\u200b\uff0c\u200b\u5c06\u200b\u5269\u4e0b\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\u8fd8\u539f\u200b\u5230\u200b\u7f51\u7edc\u200b\u8f93\u51fa\u200b\u524d\u200b\u7684\u200b\u539f\u56fe\u200b\u5c3a\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b nms \u200b\u5373\u53ef\u200b\u3002\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6846\u200b\u4e0d\u80fd\u200b\u591a\u4e8e\u200b max_per_img\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#162-batch-shape","title":"1.6.2 batch shape \u200b\u7b56\u7565","text":"<p>\u200b\u4e3a\u4e86\u200b\u52a0\u901f\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u4f5c\u8005\u200b\u63d0\u51fa\u200b\u4e86\u200b batch shape \u200b\u7b56\u7565\u200b\uff0c\u200b\u5176\u200b\u6838\u5fc3\u200b\u539f\u5219\u200b\u662f\u200b\uff1a\u200b\u786e\u4fdd\u200b\u5728\u200b batch \u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u540c\u4e00\u4e2a\u200b batch \u200b\u5185\u200b\u7684\u200b\u56fe\u7247\u200b pad \u200b\u50cf\u7d20\u200b\u6700\u5c11\u200b\uff0c\u200b\u4e0d\u200b\u8981\u6c42\u200b\u6574\u4e2a\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6240\u6709\u200b batch \u200b\u7684\u200b\u56fe\u7247\u200b\u5c3a\u5ea6\u200b\u4e00\u6837\u200b\u3002</p> <p>\u200b\u5176\u200b\u5927\u6982\u200b\u6d41\u7a0b\u200b\u662f\u200b\uff1a\u200b\u5c06\u200b\u6574\u4e2a\u200b\u6d4b\u8bd5\u200b\u6216\u8005\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u7684\u200b\u5bbd\u9ad8\u6bd4\u200b\u8fdb\u884c\u200b\u6392\u5e8f\u200b\uff0c\u200b\u7136\u540e\u200b\u4f9d\u636e\u200b batch \u200b\u8bbe\u7f6e\u200b\u5c06\u200b\u6392\u5e8f\u200b\u540e\u200b\u7684\u200b\u56fe\u7247\u200b\u7ec4\u6210\u200b\u4e00\u4e2a\u200b batch\uff0c \u200b\u540c\u65f6\u200b\u8ba1\u7b97\u200b\u8fd9\u4e2a\u200b batch \u200b\u5185\u200b\u6700\u4f73\u200b\u7684\u200b batch shape\uff0c\u200b\u9632\u6b62\u200b pad \u200b\u50cf\u7d20\u200b\u8fc7\u591a\u200b\u3002\u200b\u6700\u4f73\u200b batch shape \u200b\u8ba1\u7b97\u200b\u539f\u5219\u200b\u4e3a\u200b\u5728\u200b\u4fdd\u6301\u200b\u5bbd\u9ad8\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u8fdb\u884c\u200b pad\uff0c\u200b\u4e0d\u200b\u8ffd\u6c42\u200b\u6b63\u65b9\u5f62\u200b\u56fe\u7247\u200b\u8f93\u51fa\u200b\u3002</p> <pre><code>        image_shapes = []\n        for data_info in data_list:\n            image_shapes.append((data_info['width'], data_info['height']))\n\n        image_shapes = np.array(image_shapes, dtype=np.float64)\n\n        n = len(image_shapes)  # number of images\n        batch_index = np.floor(np.arange(n) / self.batch_size).astype(\n            np.int)  # batch index\n        number_of_batches = batch_index[-1] + 1  # number of batches\n\n        aspect_ratio = image_shapes[:, 1] / image_shapes[:, 0]  # aspect ratio\n        irect = aspect_ratio.argsort()\n\n        data_list = [data_list[i] for i in irect]\n\n        aspect_ratio = aspect_ratio[irect]\n        # Set training image shapes\n        shapes = [[1, 1]] * number_of_batches\n        for i in range(number_of_batches):\n            aspect_ratio_index = aspect_ratio[batch_index == i]\n            min_index, max_index = aspect_ratio_index.min(\n            ), aspect_ratio_index.max()\n            if max_index &lt; 1:\n                shapes[i] = [max_index, 1]\n            elif min_index &gt; 1:\n                shapes[i] = [1, 1 / min_index]\n\n        batch_shapes = np.ceil(\n            np.array(shapes) * self.img_size / self.size_divisor +\n            self.pad).astype(np.int) * self.size_divisor\n\n        for i, data_info in enumerate(data_list):\n            data_info['batch_shape'] = batch_shapes[batch_index[i]]\n</code></pre>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/MMYOLO%E5%AE%9E%E7%8E%B0/yolov5_description/#2","title":"2 \u200b\u603b\u7ed3","text":"<p>\u200b\u672c\u6587\u200b\u5bf9\u200b YOLOv5 \u200b\u539f\u7406\u200b\u548c\u200b\u5728\u200b MMYOLO \u200b\u5b9e\u73b0\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8be6\u7ec6\u200b\u89e3\u6790\u200b\uff0c\u200b\u5e0c\u671b\u200b\u80fd\u200b\u5e2e\u52a9\u200b\u7528\u6237\u200b\u7406\u89e3\u200b\u7b97\u6cd5\u200b\u5b9e\u73b0\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u540c\u65f6\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff1a\u200b\u7531\u4e8e\u200b YOLOv5 \u200b\u672c\u8eab\u200b\u4e5f\u200b\u5728\u200b\u4e0d\u65ad\u66f4\u65b0\u200b\uff0c\u200b\u672c\u200b\u5f00\u6e90\u200b\u5e93\u200b\u4e5f\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u8fed\u4ee3\u200b\uff0c\u200b\u8bf7\u200b\u53ca\u65f6\u200b\u9605\u8bfb\u200b\u548c\u200b\u540c\u6b65\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"10-%E7%AC%AC%E5%8D%81%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/YOLO%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/torch%E5%AE%9E%E7%8E%B0/","title":"Index","text":"<p>\u200b\u8fd9\u662f\u200btorch\u200b\u7248\u672c\u200b\u7684\u200byolo\u200b\u89e3\u8bfb\u200b</p>"},{"location":"notebook/","title":"Index","text":"<p>\u200b\u8be5\u200b\u6587\u4ef6\u5939\u200b\u8bb0\u5f55\u200b\u4e86\u200b\u6f14\u793a\u200b\u4f7f\u7528\u200b\u7684\u200bnotebook\uff0c\u200b\u5305\u62ec\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a - \u200b\u7b2c\u96f6\u200b\u7ae0\u200b \u200b\u524d\u7f6e\u200b\u77e5\u8bc6\u200b\u8865\u5145\u200b - \u200b\u7b2c\u4e8c\u7ae0\u200b PyTorch\u200b\u57fa\u7840\u77e5\u8bc6\u200b - \u200b\u7b2c\u56db\u7ae0\u200b PyTorch\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b   - \u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b     - fashion-mnist\u200b\u5206\u7c7b\u200b\u5b9e\u6218\u200b     - \u200b\u679c\u852c\u200b\u5206\u7c7b\u200b\u5b9e\u6218\u200b - \u200b\u7b2c\u516d\u7ae0\u200b PyTorch\u200b\u8fdb\u9636\u200b\u8bad\u7ec3\u200b\u6280\u5de7\u200b - \u200b\u7b2c\u516b\u7ae0\u200b PyTorch\u200b\u751f\u6001\u200b\u7b80\u4ecb\u200b</p> <p>\u200b\u540e\u7eed\u200b\u6f14\u793a\u200b\u4ee3\u7801\u200b\u5747\u4f1a\u200b\u6dfb\u52a0\u200b\u8fdb\u6765\u200b</p>"},{"location":"notebook/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA%EF%BC%9APyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"\u4ee3\u7801\u200b\u6f14\u793a\u200b\uff1aPyTorch\u200b\u57fa\u7840\u77e5\u8bc6","text":"<p>\u200b\u4ee3\u7801\u200b\u6f14\u793a\u200b\u90e8\u5206\u200b\uff1a\u200b\u914d\u5408\u200b\u672c\u7ae0\u200b\u5b66\u4e60\u6750\u6599\u200b\u4f7f\u7528\u200b \u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\uff1a\u200b\u5f20\u91cf\u200b\u8fd0\u7b97\u200b\u793a\u4f8b\u200b \u200b\u8fd9\u91cc\u200b\u5c06\u200b\u6f14\u793a\u200bTensor\u200b\u7684\u200b\u4e00\u4e9b\u200b\u57fa\u672c\u64cd\u4f5c\u200b</p> In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch In\u00a0[5]: Copied! <pre>?torch.tensor\n</pre> ?torch.tensor In\u00a0[3]: Copied! <pre># \u200b\u521b\u5efa\u200btensor\uff0c\u200b\u7528\u200bdtype\u200b\u6307\u5b9a\u200b\u7c7b\u578b\u200b\u3002\u200b\u6ce8\u610f\u200b\u7c7b\u578b\u200b\u8981\u200b\u5339\u914d\u200b\na = torch.tensor(1.0, dtype=torch.float)\nb = torch.tensor(1, dtype=torch.long)\nc = torch.tensor(1.0, dtype=torch.int8)\nprint(a, b, c)\n</pre> # \u200b\u521b\u5efa\u200btensor\uff0c\u200b\u7528\u200bdtype\u200b\u6307\u5b9a\u200b\u7c7b\u578b\u200b\u3002\u200b\u6ce8\u610f\u200b\u7c7b\u578b\u200b\u8981\u200b\u5339\u914d\u200b a = torch.tensor(1.0, dtype=torch.float) b = torch.tensor(1, dtype=torch.long) c = torch.tensor(1.0, dtype=torch.int8) print(a, b, c) <pre>tensor(1.) tensor(1) tensor(1, dtype=torch.int8)\n</pre> <pre>/tmp/ipykernel_11770/1264937814.py:4: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n  c = torch.tensor(1.0, dtype=torch.int8)\n</pre> In\u00a0[4]: Copied! <pre># \u200b\u4f7f\u7528\u200b\u6307\u5b9a\u200b\u7c7b\u578b\u200b\u51fd\u6570\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b\u7684\u200btensor\nd = torch.FloatTensor(2,3)\ne = torch.IntTensor(2)\nf = torch.IntTensor([1,2,3,4])  #\u200b\u5bf9\u4e8e\u200bpython\u200b\u5df2\u7ecf\u200b\u5b9a\u4e49\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8f6c\u6362\u200b\nprint(d, '\\n', e, '\\n', f)\n</pre> # \u200b\u4f7f\u7528\u200b\u6307\u5b9a\u200b\u7c7b\u578b\u200b\u51fd\u6570\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b\u7684\u200btensor d = torch.FloatTensor(2,3) e = torch.IntTensor(2) f = torch.IntTensor([1,2,3,4])  #\u200b\u5bf9\u4e8e\u200bpython\u200b\u5df2\u7ecf\u200b\u5b9a\u4e49\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8f6c\u6362\u200b print(d, '\\n', e, '\\n', f) <pre>tensor([[ 7.2398e-07,  4.5710e-41, -2.0912e+23],\n        [ 3.0812e-41,  6.7262e-43,  0.0000e+00]]) \n tensor([64,  0], dtype=torch.int32) \n tensor([1, 2, 3, 4], dtype=torch.int32)\n</pre> In\u00a0[6]: Copied! <pre># tensor\u200b\u548c\u200bnumpy array\u200b\u4e4b\u95f4\u200b\u7684\u200b\u76f8\u4e92\u200b\u8f6c\u6362\u200b\nimport numpy as np\n\ng = np.array([[1,2,3],[4,5,6]])\nh = torch.tensor(g)\nprint(h)\ni = torch.from_numpy(g)\nprint(i)\nj = h.numpy()\nprint(j)\n</pre> # tensor\u200b\u548c\u200bnumpy array\u200b\u4e4b\u95f4\u200b\u7684\u200b\u76f8\u4e92\u200b\u8f6c\u6362\u200b import numpy as np  g = np.array([[1,2,3],[4,5,6]]) h = torch.tensor(g) print(h) i = torch.from_numpy(g) print(i) j = h.numpy() print(j) <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n[[1 2 3]\n [4 5 6]]\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1atorch.tensor\u200b\u521b\u5efa\u200b\u5f97\u5230\u200b\u7684\u200b\u5f20\u91cf\u200b\u548c\u200b\u539f\u200b\u6570\u636e\u200b\u662f\u200b\u4e0d\u200b\u5171\u4eab\u5185\u5b58\u200b\u7684\u200b\uff0c\u200b\u5f20\u91cf\u200b\u5bf9\u5e94\u200b\u7684\u200b\u53d8\u91cf\u200b\u662f\u200b\u72ec\u7acb\u200b\u53d8\u91cf\u200b\u3002 \u200b\u800c\u200btorch.from_numpy()\u200b\u548c\u200btorch.as_tensor()\u200b\u4ece\u200bnumpy array\u200b\u521b\u5efa\u200b\u5f97\u5230\u200b\u7684\u200b\u5f20\u91cf\u200b\u548c\u200b\u539f\u200b\u6570\u636e\u200b\u662f\u200b\u5171\u4eab\u5185\u5b58\u200b\u7684\u200b\uff0c\u200b\u5f20\u91cf\u200b\u5bf9\u5e94\u200b\u7684\u200b\u53d8\u91cf\u200b\u4e0d\u662f\u200b\u72ec\u7acb\u200b\u53d8\u91cf\u200b\uff0c\u200b\u4fee\u6539\u200bnumpy array\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u5bf9\u5e94\u200btensor\u200b\u7684\u200b\u6539\u53d8\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>g[0,0] = 100\nprint(i)\n</pre> g[0,0] = 100 print(i) In\u00a0[10]: Copied! <pre># \u200b\u5e38\u89c1\u200b\u7684\u200b\u6784\u9020\u200bTensor\u200b\u7684\u200b\u51fd\u6570\u200b\nk = torch.rand(2, 3) \nl = torch.ones(2, 3)\nm = torch.zeros(2, 3)\nn = torch.arange(0, 10, 2)\nprint(k, '\\n', l, '\\n', m, '\\n', n)\n</pre> # \u200b\u5e38\u89c1\u200b\u7684\u200b\u6784\u9020\u200bTensor\u200b\u7684\u200b\u51fd\u6570\u200b k = torch.rand(2, 3)  l = torch.ones(2, 3) m = torch.zeros(2, 3) n = torch.arange(0, 10, 2) print(k, '\\n', l, '\\n', m, '\\n', n) <pre>tensor([[0.2652, 0.0650, 0.5593],\n        [0.7864, 0.0015, 0.4458]]) \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n tensor([[0., 0., 0.],\n        [0., 0., 0.]]) \n tensor([0, 2, 4, 6, 8])\n</pre> In\u00a0[11]: Copied! <pre># \u200b\u67e5\u770b\u200btensor\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4fe1\u606f\u200b\uff08\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff09\nprint(k.shape)\nprint(k.size())\n</pre> # \u200b\u67e5\u770b\u200btensor\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4fe1\u606f\u200b\uff08\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff09 print(k.shape) print(k.size()) <pre>torch.Size([2, 3])\ntorch.Size([2, 3])\n</pre> In\u00a0[12]: Copied! <pre># tensor\u200b\u7684\u200b\u8fd0\u7b97\u200b\no = torch.add(k,l)\nprint(o)\n</pre> # tensor\u200b\u7684\u200b\u8fd0\u7b97\u200b o = torch.add(k,l) print(o) <pre>tensor([[1.2652, 1.0650, 1.5593],\n        [1.7864, 1.0015, 1.4458]])\n</pre> In\u00a0[13]: Copied! <pre># tensor\u200b\u7684\u200b\u7d22\u5f15\u200b\u65b9\u5f0f\u200b\u4e0e\u200bnumpy\u200b\u7c7b\u4f3c\u200b\nprint(o[:,1])\nprint(o[0,:])\n</pre> # tensor\u200b\u7684\u200b\u7d22\u5f15\u200b\u65b9\u5f0f\u200b\u4e0e\u200bnumpy\u200b\u7c7b\u4f3c\u200b print(o[:,1]) print(o[0,:]) <pre>tensor([1.0650, 1.0015])\ntensor([1.2652, 1.0650, 1.5593])\n</pre> In\u00a0[16]: Copied! <pre># \u200b\u6539\u53d8\u200btensor\u200b\u5f62\u72b6\u200b\u7684\u200b\u795e\u5668\u200b\uff1aview\nprint(o.view((3,2)))\nprint(o.view(-1,2))\n</pre> # \u200b\u6539\u53d8\u200btensor\u200b\u5f62\u72b6\u200b\u7684\u200b\u795e\u5668\u200b\uff1aview print(o.view((3,2))) print(o.view(-1,2)) <pre>tensor([[1.2652, 1.0650],\n        [1.5593, 1.7864],\n        [1.0015, 1.4458]])\ntensor([[1.2652, 1.0650],\n        [1.5593, 1.7864],\n        [1.0015, 1.4458]])\n</pre> In\u00a0[17]: Copied! <pre># tensor\u200b\u7684\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b\uff08\u200b\u4f7f\u7528\u200b\u65f6\u8981\u200b\u6ce8\u610f\u200b\u8fd9\u4e2a\u200b\u7279\u6027\u200b\uff09\np = torch.arange(1, 3).view(1, 2)\nprint(p)\nq = torch.arange(1, 4).view(3, 1)\nprint(q)\nprint(p + q)\n</pre> # tensor\u200b\u7684\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b\uff08\u200b\u4f7f\u7528\u200b\u65f6\u8981\u200b\u6ce8\u610f\u200b\u8fd9\u4e2a\u200b\u7279\u6027\u200b\uff09 p = torch.arange(1, 3).view(1, 2) print(p) q = torch.arange(1, 4).view(3, 1) print(q) print(p + q) <pre>tensor([[1, 2]])\ntensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\n</pre> In\u00a0[18]: Copied! <pre># \u200b\u6269\u5c55\u200b&amp;\u200b\u538b\u7f29\u200btensor\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff1asqueeze\nprint(o)\nr = o.unsqueeze(1)\nprint(r)\nprint(r.shape)\n</pre> # \u200b\u6269\u5c55\u200b&amp;\u200b\u538b\u7f29\u200btensor\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff1asqueeze print(o) r = o.unsqueeze(1) print(r) print(r.shape) <pre>tensor([[1.2652, 1.0650, 1.5593],\n        [1.7864, 1.0015, 1.4458]])\ntensor([[[1.2652, 1.0650, 1.5593]],\n\n        [[1.7864, 1.0015, 1.4458]]])\ntorch.Size([2, 1, 3])\n</pre> In\u00a0[19]: Copied! <pre>s = r.squeeze(0)\nprint(s)\nprint(s.shape)\n</pre> s = r.squeeze(0) print(s) print(s.shape) <pre>tensor([[[1.2652, 1.0650, 1.5593]],\n\n        [[1.7864, 1.0015, 1.4458]]])\ntorch.Size([2, 1, 3])\n</pre> In\u00a0[20]: Copied! <pre>t = r.squeeze(1)\nprint(t)\nprint(t.shape)\n</pre> t = r.squeeze(1) print(t) print(t.shape) <pre>tensor([[1.2652, 1.0650, 1.5593],\n        [1.7864, 1.0015, 1.4458]])\ntorch.Size([2, 3])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>\u200b\u7b2c\u4e8c\u200b\u90e8\u5206\u200b\uff1a\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u793a\u4f8b\u200b \u200b\u8fd9\u91cc\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u51fd\u6570\u200b  $y=x_1+2*x_2$  \u200b\u6765\u200b\u8bf4\u660e\u200bPyTorch\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\u7684\u200b\u8fc7\u7a0b\u200b</p> In\u00a0[22]: Copied! <pre>import torch\n\nx1 = torch.tensor(1.0, requires_grad=True)\nx2 = torch.tensor(2.0, requires_grad=True)\ny = x1 + 2*x2\nprint(y)\n</pre> import torch  x1 = torch.tensor(1.0, requires_grad=True) x2 = torch.tensor(2.0, requires_grad=True) y = x1 + 2*x2 print(y) <pre>tensor(5., grad_fn=&lt;AddBackward0&gt;)\n</pre> In\u00a0[23]: Copied! <pre># \u200b\u9996\u5148\u200b\u67e5\u770b\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u6c42\u5bfc\u200b\nprint(x1.requires_grad)\nprint(x2.requires_grad)\nprint(y.requires_grad)\n</pre> # \u200b\u9996\u5148\u200b\u67e5\u770b\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u6c42\u5bfc\u200b print(x1.requires_grad) print(x2.requires_grad) print(y.requires_grad) <pre>True\nTrue\nTrue\n</pre> In\u00a0[24]: Copied! <pre># \u200b\u67e5\u770b\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u5bfc\u6570\u200b\u5927\u5c0f\u200b\u3002\u200b\u6b64\u65f6\u200b\u56e0\u4e3a\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5bfc\u6570\u200b\u90fd\u200b\u4e0d\u200b\u5b58\u5728\u200b\nprint(x1.grad.data)\nprint(x2.grad.data)\nprint(y.grad.data)\n</pre> # \u200b\u67e5\u770b\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u5bfc\u6570\u200b\u5927\u5c0f\u200b\u3002\u200b\u6b64\u65f6\u200b\u56e0\u4e3a\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5bfc\u6570\u200b\u90fd\u200b\u4e0d\u200b\u5b58\u5728\u200b print(x1.grad.data) print(x2.grad.data) print(y.grad.data) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_11770/1707027577.py in &lt;module&gt;\n      1 # \u200b\u67e5\u770b\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u5bfc\u6570\u200b\u5927\u5c0f\u200b\u3002\u200b\u6b64\u65f6\u200b\u56e0\u4e3a\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5bfc\u6570\u200b\u90fd\u200b\u4e0d\u200b\u5b58\u5728\u200b\n----&gt; 2 print(x1.grad.data)\n      3 print(x2.grad.data)\n      4 print(y.grad.data)\n\nAttributeError: 'NoneType' object has no attribute 'data'</pre> In\u00a0[25]: Copied! <pre>x1\n</pre> x1 Out[25]: <pre>tensor(1., requires_grad=True)</pre> In\u00a0[26]: Copied! <pre>## \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u540e\u200b\u770b\u200b\u5bfc\u6570\u200b\u5927\u5c0f\u200b\ny = x1 + 2*x2\ny.backward()\nprint(x1.grad.data)\nprint(x2.grad.data)\n</pre> ## \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u540e\u200b\u770b\u200b\u5bfc\u6570\u200b\u5927\u5c0f\u200b y = x1 + 2*x2 y.backward() print(x1.grad.data) print(x2.grad.data) <pre>tensor(1.)\ntensor(2.)\n</pre> In\u00a0[30]: Copied! <pre># \u200b\u5bfc\u6570\u200b\u662f\u200b\u4f1a\u200b\u7d2f\u79ef\u200b\u7684\u200b\uff0c\u200b\u91cd\u590d\u200b\u8fd0\u884c\u200b\u76f8\u540c\u200b\u547d\u4ee4\u200b\uff0cgrad\u200b\u4f1a\u200b\u589e\u52a0\u200b\ny = x1 + 2*x2\ny.backward()\nprint(x1.grad.data)\nprint(x2.grad.data)\n</pre> # \u200b\u5bfc\u6570\u200b\u662f\u200b\u4f1a\u200b\u7d2f\u79ef\u200b\u7684\u200b\uff0c\u200b\u91cd\u590d\u200b\u8fd0\u884c\u200b\u76f8\u540c\u200b\u547d\u4ee4\u200b\uff0cgrad\u200b\u4f1a\u200b\u589e\u52a0\u200b y = x1 + 2*x2 y.backward() print(x1.grad.data) print(x2.grad.data) <pre>tensor(5.)\ntensor(10.)\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u200b\u6240\u4ee5\u200b\u6bcf\u6b21\u200b\u8ba1\u7b97\u200b\u524d\u200b\u9700\u8981\u200b\u6e05\u9664\u200b\u5f53\u200b\u524d\u5bfc\u200b\u6570\u503c\u200b\u907f\u514d\u200b\u7d2f\u79ef\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u529f\u80fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bpytorch\u200b\u7684\u200boptimizer\u200b\u5b9e\u73b0\u200b\u3002\u200b\u540e\u7eed\u200b\u4f1a\u200b\u8bb2\u200b\u5230\u200b\n</pre> # \u200b\u6240\u4ee5\u200b\u6bcf\u6b21\u200b\u8ba1\u7b97\u200b\u524d\u200b\u9700\u8981\u200b\u6e05\u9664\u200b\u5f53\u200b\u524d\u5bfc\u200b\u6570\u503c\u200b\u907f\u514d\u200b\u7d2f\u79ef\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u529f\u80fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bpytorch\u200b\u7684\u200boptimizer\u200b\u5b9e\u73b0\u200b\u3002\u200b\u540e\u7eed\u200b\u4f1a\u200b\u8bb2\u200b\u5230\u200b In\u00a0[31]: Copied! <pre># \u200b\u5c1d\u8bd5\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u6c42\u5bfc\u200b\uff0c\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4ec0\u4e48\u200b\u60c5\u51b5\u200b\uff1f\nx1 = torch.tensor(1.0, requires_grad=False)\nx2 = torch.tensor(2.0, requires_grad=False)\ny = x1 + 2*x2\ny.backward()\n</pre> # \u200b\u5c1d\u8bd5\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u6c42\u5bfc\u200b\uff0c\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u4ec0\u4e48\u200b\u60c5\u51b5\u200b\uff1f x1 = torch.tensor(1.0, requires_grad=False) x2 = torch.tensor(2.0, requires_grad=False) y = x1 + 2*x2 y.backward() <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_11770/4087792071.py in &lt;module&gt;\n      3 x2 = torch.tensor(2.0, requires_grad=False)\n      4 y = x1 + 2*x2\n----&gt; 5 y.backward()\n\n/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    253                 create_graph=create_graph,\n    254                 inputs=inputs)\n--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    256 \n    257     def register_hook(self, hook):\n\n/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    145         retain_graph = create_graph\n    146 \n--&gt; 147     Variable._execution_engine.run_backward(\n    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n    149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/","title":"Transforms","text":"In\u00a0[105]: Copied! <pre>from PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# \u200b\u52a0\u8f7d\u200b\u539f\u59cb\u200b\u56fe\u7247\u200b\nimg = Image.open(\"./figures/lenna.jpg\") \nprint(img.size)\nplt.imshow(img)\n</pre> from PIL import Image from torchvision import transforms import matplotlib.pyplot as plt %matplotlib inline # \u200b\u52a0\u8f7d\u200b\u539f\u59cb\u200b\u56fe\u7247\u200b img = Image.open(\"./figures/lenna.jpg\")  print(img.size) plt.imshow(img) <pre>(316, 316)\n</pre> Out[105]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbb7b4f0&gt;</pre> In\u00a0[106]: Copied! <pre># \u200b\u5bf9\u200b\u7ed9\u5b9a\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u5207\u5272\u200b\n# \u200b\u5bf9\u200b\u56fe\u7247\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u653e\u5927\u200b\u5207\u5272\u200b\uff0c\u200b\u8d85\u51fa\u200b\u56fe\u7247\u5927\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\u586b\u200b0\nimg_centercrop1 = transforms.CenterCrop((500,500))(img)\nprint(img_centercrop1.size)\n# \u200b\u5bf9\u200b\u56fe\u7247\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u7f29\u5c0f\u200b\u5207\u5272\u200b\uff0c\u200b\u8d85\u51fa\u200b\u671f\u671b\u200b\u5927\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\u5254\u9664\u200b\nimg_centercrop2 = transforms.CenterCrop((224,224))(img)\nprint(img_centercrop2.size)\nplt.subplot(1,3,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(1,3,2),plt.imshow(img_centercrop1),plt.title(\"500 * 500\")\nplt.subplot(1,3,3),plt.imshow(img_centercrop2),plt.title(\"224 * 224\")\nplt.show()\n</pre> # \u200b\u5bf9\u200b\u7ed9\u5b9a\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u5207\u5272\u200b # \u200b\u5bf9\u200b\u56fe\u7247\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u653e\u5927\u200b\u5207\u5272\u200b\uff0c\u200b\u8d85\u51fa\u200b\u56fe\u7247\u5927\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\u586b\u200b0 img_centercrop1 = transforms.CenterCrop((500,500))(img) print(img_centercrop1.size) # \u200b\u5bf9\u200b\u56fe\u7247\u200b\u6cbf\u200b\u4e2d\u5fc3\u200b\u7f29\u5c0f\u200b\u5207\u5272\u200b\uff0c\u200b\u8d85\u51fa\u200b\u671f\u671b\u200b\u5927\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\u5254\u9664\u200b img_centercrop2 = transforms.CenterCrop((224,224))(img) print(img_centercrop2.size) plt.subplot(1,3,1),plt.imshow(img),plt.title(\"Original\") plt.subplot(1,3,2),plt.imshow(img_centercrop1),plt.title(\"500 * 500\") plt.subplot(1,3,3),plt.imshow(img_centercrop2),plt.title(\"224 * 224\") plt.show() <pre>(500, 500)\n(224, 224)\n</pre> In\u00a0[107]: Copied! <pre># \u200b\u5bf9\u200b\u56fe\u7247\u200b\u7684\u200b\u4eae\u5ea6\u200b\uff0c\u200b\u5bf9\u6bd4\u5ea6\u200b\uff0c\u200b\u9971\u548c\u5ea6\u200b\uff0c\u200b\u8272\u8c03\u200b\u8fdb\u884c\u200b\u6539\u53d8\u200b\nimg_CJ = transforms.ColorJitter(brightness=1,contrast=0.5,saturation=0.5,hue=0.5)(img)\nprint(img_CJ.size)\nplt.imshow(img_CJ)\n</pre> # \u200b\u5bf9\u200b\u56fe\u7247\u200b\u7684\u200b\u4eae\u5ea6\u200b\uff0c\u200b\u5bf9\u6bd4\u5ea6\u200b\uff0c\u200b\u9971\u548c\u5ea6\u200b\uff0c\u200b\u8272\u8c03\u200b\u8fdb\u884c\u200b\u6539\u53d8\u200b img_CJ = transforms.ColorJitter(brightness=1,contrast=0.5,saturation=0.5,hue=0.5)(img) print(img_CJ.size) plt.imshow(img_CJ) <pre>(316, 316)\n</pre> Out[107]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbcb6400&gt;</pre> In\u00a0[108]: Copied! <pre>img_grey_c3 = transforms.Grayscale(num_output_channels=3)(img)\nimg_grey_c1 = transforms.Grayscale(num_output_channels=1)(img)\nplt.subplot(1,2,1),plt.imshow(img_grey_c3),plt.title(\"channels=3\")\nplt.subplot(1,2,2),plt.imshow(img_grey_c1),plt.title(\"channels=1\")\nplt.show()\n</pre> img_grey_c3 = transforms.Grayscale(num_output_channels=3)(img) img_grey_c1 = transforms.Grayscale(num_output_channels=1)(img) plt.subplot(1,2,1),plt.imshow(img_grey_c3),plt.title(\"channels=3\") plt.subplot(1,2,2),plt.imshow(img_grey_c1),plt.title(\"channels=1\") plt.show() In\u00a0[109]: Copied! <pre># \u200b\u7b49\u200b\u6bd4\u200b\u7f29\u653e\u200b\nimg_resize = transforms.Resize(224)(img)\nprint(img_resize.size)\nplt.imshow(img_resize)\n</pre> # \u200b\u7b49\u200b\u6bd4\u200b\u7f29\u653e\u200b img_resize = transforms.Resize(224)(img) print(img_resize.size) plt.imshow(img_resize) <pre>(224, 224)\n</pre> Out[109]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbdb0580&gt;</pre> In\u00a0[110]: Copied! <pre># \u200b\u7b49\u200b\u6bd4\u200b\u7f29\u653e\u200b \u200b\u4e0d\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u6b64\u200b\u8f6c\u6362\u200b\u4ee5\u200b\u652f\u6301\u200b\u8c03\u6574\u200b\u5927\u5c0f\u200b\nimg_scale = transforms.Scale(224)(img)\nprint(img_scale.size)\nplt.imshow(img_scale)\n</pre> # \u200b\u7b49\u200b\u6bd4\u200b\u7f29\u653e\u200b \u200b\u4e0d\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u6b64\u200b\u8f6c\u6362\u200b\u4ee5\u200b\u652f\u6301\u200b\u8c03\u6574\u200b\u5927\u5c0f\u200b img_scale = transforms.Scale(224)(img) print(img_scale.size) plt.imshow(img_scale) <pre>(224, 224)\n</pre> Out[110]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbe06df0&gt;</pre> In\u00a0[111]: Copied! <pre># \u200b\u968f\u673a\u200b\u88c1\u526a\u200b\u6210\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b\n# \u200b\u8bbe\u7acb\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\nimport torch\ntorch.manual_seed(31)\n# \u200b\u968f\u673a\u200b\u88c1\u526a\u200b\nimg_randowm_crop1 = transforms.RandomCrop(224)(img)\nimg_randowm_crop2 = transforms.RandomCrop(224)(img)\nprint(img_randowm_crop1.size)\nplt.subplot(1,2,1),plt.imshow(img_randowm_crop1)\nplt.subplot(1,2,2),plt.imshow(img_randowm_crop2)\nplt.show()\n</pre> # \u200b\u968f\u673a\u200b\u88c1\u526a\u200b\u6210\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b # \u200b\u8bbe\u7acb\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b import torch torch.manual_seed(31) # \u200b\u968f\u673a\u200b\u88c1\u526a\u200b img_randowm_crop1 = transforms.RandomCrop(224)(img) img_randowm_crop2 = transforms.RandomCrop(224)(img) print(img_randowm_crop1.size) plt.subplot(1,2,1),plt.imshow(img_randowm_crop1) plt.subplot(1,2,2),plt.imshow(img_randowm_crop2) plt.show() <pre>(224, 224)\n</pre> In\u00a0[112]: Copied! <pre># \u200b\u968f\u673a\u200b\u5de6\u53f3\u200b\u65cb\u8f6c\u200b\n# \u200b\u8bbe\u7acb\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4e0d\u200b\u65cb\u8f6c\u200b\nimport torch\ntorch.manual_seed(31)\n\nimg_random_H = transforms.RandomHorizontalFlip()(img)\nprint(img_random_H.size)\nplt.imshow(img_random_H)\n</pre> # \u200b\u968f\u673a\u200b\u5de6\u53f3\u200b\u65cb\u8f6c\u200b # \u200b\u8bbe\u7acb\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4e0d\u200b\u65cb\u8f6c\u200b import torch torch.manual_seed(31)  img_random_H = transforms.RandomHorizontalFlip()(img) print(img_random_H.size) plt.imshow(img_random_H) <pre>(316, 316)\n</pre> Out[112]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbf138b0&gt;</pre> In\u00a0[113]: Copied! <pre># \u200b\u968f\u673a\u200b\u5782\u76f4\u200b\u65b9\u5411\u200b\u65cb\u8f6c\u200b\nimg_random_V = transforms.RandomVerticalFlip()(img)\nprint(img_random_V.size)\nplt.imshow(img_random_V)\n</pre> # \u200b\u968f\u673a\u200b\u5782\u76f4\u200b\u65b9\u5411\u200b\u65cb\u8f6c\u200b img_random_V = transforms.RandomVerticalFlip()(img) print(img_random_V.size) plt.imshow(img_random_V) <pre>(316, 316)\n</pre> Out[113]: <pre>&lt;matplotlib.image.AxesImage at 0x157bbf67af0&gt;</pre> In\u00a0[114]: Copied! <pre># \u200b\u968f\u673a\u200b\u88c1\u526a\u200b\u6210\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b\nimg_random_resizecrop = transforms.RandomResizedCrop(224,scale=(0.5,0.5))(img)\nprint(img_random_resizecrop.size)\nplt.imshow(img_random_resizecrop)\n</pre> # \u200b\u968f\u673a\u200b\u88c1\u526a\u200b\u6210\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b img_random_resizecrop = transforms.RandomResizedCrop(224,scale=(0.5,0.5))(img) print(img_random_resizecrop.size) plt.imshow(img_random_resizecrop) <pre>(224, 224)\n</pre> Out[114]: <pre>&lt;matplotlib.image.AxesImage at 0x157bcf976a0&gt;</pre> In\u00a0[115]: Copied! <pre># \u200b\u5bf9\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u64cd\u4f5c\u200b\u53ef\u80fd\u200b\u662f\u200b\u591a\u79cd\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200btransforms.Compose()\u200b\u5c06\u200b\u4ed6\u4eec\u200b\u7ec4\u88c5\u200b\u8d77\u6765\u200b\ntransformer = transforms.Compose([\n    transforms.Resize(256),\n    transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),\n    transforms.RandomVerticalFlip(),\n])\nimg_transform = transformer(img)\nplt.imshow(img_transform)\n</pre> # \u200b\u5bf9\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u64cd\u4f5c\u200b\u53ef\u80fd\u200b\u662f\u200b\u591a\u79cd\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200btransforms.Compose()\u200b\u5c06\u200b\u4ed6\u4eec\u200b\u7ec4\u88c5\u200b\u8d77\u6765\u200b transformer = transforms.Compose([     transforms.Resize(256),     transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),     transforms.RandomVerticalFlip(), ]) img_transform = transformer(img) plt.imshow(img_transform) Out[115]: <pre>&lt;matplotlib.image.AxesImage at 0x157bcff0fa0&gt;</pre>"},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transforms","title":"transforms\u200b\u5b9e\u6218\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformscentercropsize","title":"transforms.CenterCrop(size)\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformscolorjitterbrightness0-contrast0-saturation0-hue0","title":"transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsgrayscalenum_output_channels","title":"transforms.Grayscale(num_output_channels)\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsresize","title":"transforms.Resize\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsscale","title":"transforms.Scale\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsrandomcrop","title":"transforms.RandomCrop\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsrandomhorizontalflip","title":"transforms.RandomHorizontalFlip\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsrandomverticalflip","title":"transforms.RandomVerticalFlip\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#transformsrandomresizedcrop","title":"transforms.RandomResizedCrop\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B/transforms/#tranformscompose","title":"\u5bf9\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u7ec4\u5408\u200b\u53d8\u5316\u200b tranforms.Compose()\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E4%B8%8E%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/","title":"\u8bf4\u660e","text":"In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport collections\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n</pre> import os import numpy as np import collections import torch import torch.nn as nn import torch.nn.functional as F import torchvision <p>Point 1\uff1a\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b PyTorch\u200b\u4e2d\u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u4e3b\u8981\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u4e09\u79cd\u200b\u65b9\u5f0f\u200b\uff1a</p> <ul> <li>Sequential</li> <li>ModuleList</li> <li>ModuleDict</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## \u200b\u8bb2\u89e3\u200b\u70b9\u200b\uff1a\u200b\u4f7f\u7528\u200bordered dict\u200b\u66f4\u200b\u6709\u52a9\u4e8e\u200b\u4e00\u76ee\u4e86\u7136\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e4b\u540e\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b\u4e5f\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\n</pre> ## \u200b\u8bb2\u89e3\u200b\u70b9\u200b\uff1a\u200b\u4f7f\u7528\u200bordered dict\u200b\u66f4\u200b\u6709\u52a9\u4e8e\u200b\u4e00\u76ee\u4e86\u7136\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e4b\u540e\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b\u4e5f\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b In\u00a0[2]: Copied! <pre>## Sequential: Direct list\nimport torch.nn as nn\nnet1 = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 10), \n        )\nprint(net1)\n</pre> ## Sequential: Direct list import torch.nn as nn net1 = nn.Sequential(         nn.Linear(784, 256),         nn.ReLU(),         nn.Linear(256, 10),          ) print(net1) <pre>Sequential(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n</pre> In\u00a0[3]: Copied! <pre>## Sequential: Ordered Dict\nimport collections\nimport torch.nn as nn\nnet2 = nn.Sequential(collections.OrderedDict([\n          ('fc1', nn.Linear(784, 256)),\n          ('relu1', nn.ReLU()),\n          ('fc2', nn.Linear(256, 10))\n          ]))\nprint(net2)\n</pre> ## Sequential: Ordered Dict import collections import torch.nn as nn net2 = nn.Sequential(collections.OrderedDict([           ('fc1', nn.Linear(784, 256)),           ('relu1', nn.ReLU()),           ('fc2', nn.Linear(256, 10))           ])) print(net2) <pre>Sequential(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (relu1): ReLU()\n  (fc2): Linear(in_features=256, out_features=10, bias=True)\n)\n</pre> In\u00a0[4]: Copied! <pre># \u200b\u8bd5\u4e00\u4e0b\u200b\na = torch.rand(4,784)\nout1 = net1(a)\nout2 = net2(a)\nprint(out1.shape==out2.shape, out1.shape)\n</pre> # \u200b\u8bd5\u4e00\u4e0b\u200b a = torch.rand(4,784) out1 = net1(a) out2 = net2(a) print(out1.shape==out2.shape, out1.shape) <pre>True torch.Size([4, 10])\n</pre> In\u00a0[5]: Copied! <pre>## ModuleList\nnet3 = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\nnet3.append(nn.Linear(256, 10)) # # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200bappend\u200b\u64cd\u4f5c\u200b\nprint(net3[-1])  # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200b\u7d22\u5f15\u200b\u8bbf\u95ee\u200b\nprint(net3)\n</pre> ## ModuleList net3 = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()]) net3.append(nn.Linear(256, 10)) # # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200bappend\u200b\u64cd\u4f5c\u200b print(net3[-1])  # \u200b\u7c7b\u4f3c\u200bList\u200b\u7684\u200b\u7d22\u5f15\u200b\u8bbf\u95ee\u200b print(net3) <pre>Linear(in_features=256, out_features=10, bias=True)\nModuleList(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n</pre> In\u00a0[6]: Copied! <pre># \u200b\u6ce8\u610f\u200bModuleList \u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b\nout3 = net3(a)\n</pre> # \u200b\u6ce8\u610f\u200bModuleList \u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b out3 = net3(a) <pre>\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n&lt;ipython-input-6-0abc9bfebdbc&gt; in &lt;module&gt;\n      1 # \u200b\u6ce8\u610f\u200bModuleList \u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b\n----&gt; 2 out3 = net3(a)\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1102             return forward_call(*input, **kwargs)\n   1103         # Do not call functions when jit is used\n   1104         full_backward_hooks, non_full_backward_hooks = [], []\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _forward_unimplemented(self, *input)\n    199         registered hooks while the latter silently ignores them.\n    200     \"\"\"\n--&gt; 201     raise NotImplementedError\n    202 \n    203 \n\nNotImplementedError: </pre> In\u00a0[7]: Copied! <pre>class Net3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.modulelist = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n        self.modulelist.append(nn.Linear(256, 10))\n    \n    def forward(self, x):\n        for layer in self.modulelist:\n            x = layer(x)\n        return x\nnet3_ = Net3()\nout3_ = net3_(a)\nprint(out3_.shape)\n</pre> class Net3(nn.Module):     def __init__(self):         super().__init__()         self.modulelist = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])         self.modulelist.append(nn.Linear(256, 10))          def forward(self, x):         for layer in self.modulelist:             x = layer(x)         return x net3_ = Net3() out3_ = net3_(a) print(out3_.shape) <pre>torch.Size([4, 10])\n</pre> In\u00a0[8]: Copied! <pre>## ModuleDict\nnet4 = nn.ModuleDict({\n    'linear': nn.Linear(784, 256),\n    'act': nn.ReLU(),\n})\nnet4['output'] = nn.Linear(256, 10) # \u200b\u6dfb\u52a0\u200b\nprint(net4['linear']) # \u200b\u8bbf\u95ee\u200b\nprint(net4.output)\n</pre> ## ModuleDict net4 = nn.ModuleDict({     'linear': nn.Linear(784, 256),     'act': nn.ReLU(), }) net4['output'] = nn.Linear(256, 10) # \u200b\u6dfb\u52a0\u200b print(net4['linear']) # \u200b\u8bbf\u95ee\u200b print(net4.output) <pre>Linear(in_features=784, out_features=256, bias=True)\nLinear(in_features=256, out_features=10, bias=True)\n</pre> In\u00a0[11]: Copied! <pre># \u200b\u540c\u6837\u200b\u5730\u200b\uff0cModuleDict\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b\u3002\n# \u200b\u6b63\u786e\u200b\u4f7f\u7528\u200b\u65b9\u5f0f\u200b\u540c\u200b\u4e0a\u200b\nout4 = net4(a)\n</pre> # \u200b\u540c\u6837\u200b\u5730\u200b\uff0cModuleDict\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b\u3002 # \u200b\u6b63\u786e\u200b\u4f7f\u7528\u200b\u65b9\u5f0f\u200b\u540c\u200b\u4e0a\u200b out4 = net4(a) <pre>\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n&lt;ipython-input-11-965aa5709079&gt; in &lt;module&gt;\n      1 # \u200b\u540c\u6837\u200b\u5730\u200b\uff0cModuleDict\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u200b\u53ea\u662f\u200b\u5c06\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u5757\u200b\u50a8\u5b58\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6b64\u5904\u200b\u5e94\u200b\u62a5\u9519\u200b\u3002\n      2 # \u200b\u6b63\u786e\u200b\u4f7f\u7528\u200b\u65b9\u5f0f\u200b\u540c\u200b\u4e0a\u200b\n----&gt; 3 out4 = net4(a)\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1101                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1102             return forward_call(*input, **kwargs)\n   1103         # Do not call functions when jit is used\n   1104         full_backward_hooks, non_full_backward_hooks = [], []\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _forward_unimplemented(self, *input)\n    199         registered hooks while the latter silently ignores them.\n    200     \"\"\"\n--&gt; 201     raise NotImplementedError\n    202 \n    203 \n\nNotImplementedError: </pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Point 2\uff1a\u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\u5feb\u901f\u200b\u642d\u5efa\u200b\u590d\u6742\u200b\u7f51\u7edc\u200b \u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u63a2\u7d22\u200b\u5982\u4f55\u200b\u5229\u7528\u200b\u6a21\u578b\u200b\u5757\u200b\uff0c\u200b\u5feb\u901f\u200b\u6784\u5efa\u200bU-Net\u200b\u7f51\u7edc\u200b  \u200b\u7ec4\u6210\u200bU-Net\u200b\u7684\u200b\u6a21\u578b\u200b\u5757\u200b\u4e3b\u8981\u200b\u6709\u200b\u5982\u4e0b\u200b\u51e0\u4e2a\u200b\u90e8\u5206\u200b\uff1a 1\uff09\u200b\u6bcf\u4e2a\u200b\u5b50\u5757\u200b\u5185\u90e8\u200b\u7684\u200b\u4e24\u6b21\u200b\u5377\u79ef\u200b\uff08Double Convolution\uff09 2\uff09\u200b\u5de6\u4fa7\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e0b\u200b\u91c7\u6837\u200b\u8fde\u63a5\u200b\uff0c\u200b\u5373\u200b\u6700\u5927\u200b\u6c60\u5316\u200b\uff08Max pooling\uff09 3\uff09\u200b\u53f3\u4fa7\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e0a\u200b\u91c7\u6837\u200b\u8fde\u63a5\u200b\uff08Up sampling\uff09 4\uff09\u200b\u8f93\u51fa\u200b\u5c42\u200b\u7684\u200b\u5904\u7406\u200b</p> <p>\u200b\u9664\u200b\u6a21\u578b\u200b\u5757\u200b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u6a21\u578b\u200b\u5757\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6a2a\u5411\u200b\u8fde\u63a5\u200b\uff0c\u200b\u8f93\u5165\u200b\u548c\u200bU-Net\u200b\u5e95\u90e8\u200b\u7684\u200b\u8fde\u63a5\u200b\u7b49\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5355\u72ec\u200b\u7684\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bforward\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002 \uff08\u200b\u53c2\u8003\u200b\uff1ahttps://github.com/milesial/Pytorch-UNet \uff09</p> In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport collections\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n</pre> import os import numpy as np import collections import torch import torch.nn as nn import torch.nn.functional as F import torchvision In\u00a0[2]: Copied! <pre>class DoubleConv(nn.Module):\n    \"\"\"(convolution =&gt; [BN] =&gt; ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n</pre> class DoubleConv(nn.Module):     \"\"\"(convolution =&gt; [BN] =&gt; ReLU) * 2\"\"\"      def __init__(self, in_channels, out_channels, mid_channels=None):         super().__init__()         if not mid_channels:             mid_channels = out_channels         self.double_conv = nn.Sequential(             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),             nn.BatchNorm2d(mid_channels),             nn.ReLU(inplace=True),             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),             nn.BatchNorm2d(out_channels),             nn.ReLU(inplace=True)         )      def forward(self, x):         return self.double_conv(x)  In\u00a0[3]: Copied! <pre>class Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n</pre> class Down(nn.Module):     \"\"\"Downscaling with maxpool then double conv\"\"\"      def __init__(self, in_channels, out_channels):         super().__init__()         self.maxpool_conv = nn.Sequential(             nn.MaxPool2d(2),             DoubleConv(in_channels, out_channels)         )      def forward(self, x):         return self.maxpool_conv(x)  In\u00a0[4]: Copied! <pre>class Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n</pre> class Up(nn.Module):     \"\"\"Upscaling then double conv\"\"\"      def __init__(self, in_channels, out_channels, bilinear=True):         super().__init__()          # if bilinear, use the normal convolutions to reduce the number of channels         if bilinear:             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)             self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)         else:             self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)             self.conv = DoubleConv(in_channels, out_channels)      def forward(self, x1, x2):         x1 = self.up(x1)         # input is CHW         diffY = x2.size()[2] - x1.size()[2]         diffX = x2.size()[3] - x1.size()[3]          x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,                         diffY // 2, diffY - diffY // 2])         # if you have padding issues, see         # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a         # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd         x = torch.cat([x2, x1], dim=1)         return self.conv(x) In\u00a0[5]: Copied! <pre>class OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n</pre> class OutConv(nn.Module):     def __init__(self, in_channels, out_channels):         super(OutConv, self).__init__()         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)      def forward(self, x):         return self.conv(x) In\u00a0[6]: Copied! <pre>## \u200b\u7ec4\u88c5\u200b\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n</pre> ## \u200b\u7ec4\u88c5\u200b class UNet(nn.Module):     def __init__(self, n_channels, n_classes, bilinear=True):         super(UNet, self).__init__()         self.n_channels = n_channels         self.n_classes = n_classes         self.bilinear = bilinear          self.inc = DoubleConv(n_channels, 64)         self.down1 = Down(64, 128)         self.down2 = Down(128, 256)         self.down3 = Down(256, 512)         factor = 2 if bilinear else 1         self.down4 = Down(512, 1024 // factor)         self.up1 = Up(1024, 512 // factor, bilinear)         self.up2 = Up(512, 256 // factor, bilinear)         self.up3 = Up(256, 128 // factor, bilinear)         self.up4 = Up(128, 64, bilinear)         self.outc = OutConv(64, n_classes)      def forward(self, x):         x1 = self.inc(x)         x2 = self.down1(x1)         x3 = self.down2(x2)         x4 = self.down3(x3)         x5 = self.down4(x4)         x = self.up1(x5, x4)         x = self.up2(x, x3)         x = self.up3(x, x2)         x = self.up4(x, x1)         logits = self.outc(x)         return logits In\u00a0[7]: Copied! <pre>unet = UNet(3,1)\nunet\n</pre> unet = UNet(3,1) unet Out[7]: <pre>UNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (down1): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc): OutConv(\n    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Point 3\uff1a\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b \u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u6700\u540e\u200b\u7684\u200b\u5206\u5272\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u7684\u200b\uff08\u200b\u5373\u200bmask\u200b\u4e0d\u6b62\u200b0\u200b\u548c\u200b1\uff0c\u200b\u8fd8\u6709\u200b2\uff0c3\uff0c4\u200b\u7b49\u503c\u200b\u4ee3\u8868\u200b\u5176\u4ed6\u200b\u76ee\u6807\u200b\uff09\uff0c\u200b\u9700\u8981\u200b\u5bf9\u6a21\u578b\u200b\u7279\u5b9a\u200b\u5c42\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\u3002 \u200b\u6b64\u5916\u200b\u8fd8\u6709\u200b\u4e24\u79cd\u200b\u60c5\u51b5\u200b\u7684\u200b\u6a21\u578b\u200b\u4fee\u6539\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4e5f\u200b\u505a\u200b\u6f14\u793a\u200b\uff1a</p> <ul> <li>\u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u5165\u200b</li> <li>\u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u51fa\u200b</li> </ul> In\u00a0[19]: Copied! <pre>## \u200b\u4fee\u6539\u200b\u7279\u5b9a\u200b\u5c42\u200b\nimport copy\nunet1 = copy.deepcopy(unet)\nunet1.outc\n</pre> ## \u200b\u4fee\u6539\u200b\u7279\u5b9a\u200b\u5c42\u200b import copy unet1 = copy.deepcopy(unet) unet1.outc Out[19]: <pre>OutConv(\n  (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)</pre> In\u00a0[21]: Copied! <pre>b = torch.rand(1,3,224,224)\nout_unet1 = unet1(b)\nprint(out_unet1.shape)\n</pre> b = torch.rand(1,3,224,224) out_unet1 = unet1(b) print(out_unet1.shape) <pre>torch.Size([1, 1, 224, 224])\n</pre> In\u00a0[23]: Copied! <pre>unet1.outc = OutConv(64, 5)\nunet1.outc\n</pre> unet1.outc = OutConv(64, 5) unet1.outc Out[23]: <pre>OutConv(\n  (conv): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n)</pre> In\u00a0[24]: Copied! <pre>out_unet1 = unet1(b)\nprint(out_unet1.shape)\n</pre> out_unet1 = unet1(b) print(out_unet1.shape) <pre>torch.Size([1, 5, 224, 224])\n</pre> In\u00a0[25]: Copied! <pre>## \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u5165\u200b\nclass UNet2(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet2, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x, add_variable):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = x + add_variable   #\u200b\u4fee\u6539\u200b\u70b9\u200b\n        logits = self.outc(x)\n        return logits\nunet2 = UNet2(3,1)\n\nc = torch.rand(1,1,224,224)\nout_unet2 = unet2(b, c)\nprint(out_unet2.shape)\n</pre> ## \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u5165\u200b class UNet2(nn.Module):     def __init__(self, n_channels, n_classes, bilinear=True):         super(UNet2, self).__init__()         self.n_channels = n_channels         self.n_classes = n_classes         self.bilinear = bilinear          self.inc = DoubleConv(n_channels, 64)         self.down1 = Down(64, 128)         self.down2 = Down(128, 256)         self.down3 = Down(256, 512)         factor = 2 if bilinear else 1         self.down4 = Down(512, 1024 // factor)         self.up1 = Up(1024, 512 // factor, bilinear)         self.up2 = Up(512, 256 // factor, bilinear)         self.up3 = Up(256, 128 // factor, bilinear)         self.up4 = Up(128, 64, bilinear)         self.outc = OutConv(64, n_classes)      def forward(self, x, add_variable):         x1 = self.inc(x)         x2 = self.down1(x1)         x3 = self.down2(x2)         x4 = self.down3(x3)         x5 = self.down4(x4)         x = self.up1(x5, x4)         x = self.up2(x, x3)         x = self.up3(x, x2)         x = self.up4(x, x1)         x = x + add_variable   #\u200b\u4fee\u6539\u200b\u70b9\u200b         logits = self.outc(x)         return logits unet2 = UNet2(3,1)  c = torch.rand(1,1,224,224) out_unet2 = unet2(b, c) print(out_unet2.shape) <pre>torch.Size([1, 1, 224, 224])\n</pre> In\u00a0[26]: Copied! <pre>## \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u51fa\u200b\nclass UNet3(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet3, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits, x5  # \u200b\u4fee\u6539\u200b\u70b9\u200b\nunet3 = UNet3(3,1)\n\nc = torch.rand(1,1,224,224)\nout_unet3, mid_out = unet3(b)\nprint(out_unet3.shape, mid_out.shape)\n</pre> ## \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u8f93\u51fa\u200b class UNet3(nn.Module):     def __init__(self, n_channels, n_classes, bilinear=True):         super(UNet3, self).__init__()         self.n_channels = n_channels         self.n_classes = n_classes         self.bilinear = bilinear          self.inc = DoubleConv(n_channels, 64)         self.down1 = Down(64, 128)         self.down2 = Down(128, 256)         self.down3 = Down(256, 512)         factor = 2 if bilinear else 1         self.down4 = Down(512, 1024 // factor)         self.up1 = Up(1024, 512 // factor, bilinear)         self.up2 = Up(512, 256 // factor, bilinear)         self.up3 = Up(256, 128 // factor, bilinear)         self.up4 = Up(128, 64, bilinear)         self.outc = OutConv(64, n_classes)      def forward(self, x):         x1 = self.inc(x)         x2 = self.down1(x1)         x3 = self.down2(x2)         x4 = self.down3(x3)         x5 = self.down4(x4)         x = self.up1(x5, x4)         x = self.up2(x, x3)         x = self.up3(x, x2)         x = self.up4(x, x1)         logits = self.outc(x)         return logits, x5  # \u200b\u4fee\u6539\u200b\u70b9\u200b unet3 = UNet3(3,1)  c = torch.rand(1,1,224,224) out_unet3, mid_out = unet3(b) print(out_unet3.shape, mid_out.shape) <pre>torch.Size([1, 1, 224, 224]) torch.Size([1, 512, 14, 14])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Point 4\uff1a\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u4e0e\u200b\u8bfb\u53d6\u200b \u200b\u8fd9\u91cc\u200b\u76f8\u5e94\u200b\u8003\u8651\u200b\u5355\u5361\u200b\u548c\u200b\u591a\u5361\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u7684\u200b\u6a21\u578b\u200b\u5b58\u53d6\u200b\u60c5\u51b5\u200b</p> In\u00a0[\u00a0]: Copied! <pre>## \u200b\u8bb2\u89e3\u200b\u70b9\u200b\uff1a\u200b\u56de\u5230\u200bjupyter\u200b\u7684\u200b\u6587\u4ef6\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u770b\u200b\u4fdd\u5b58\u200b\u7684\u200b\u7ed3\u679c\u200b\n</pre> ## \u200b\u8bb2\u89e3\u200b\u70b9\u200b\uff1a\u200b\u56de\u5230\u200bjupyter\u200b\u7684\u200b\u6587\u4ef6\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u770b\u200b\u4fdd\u5b58\u200b\u7684\u200b\u7ed3\u679c\u200b In\u00a0[27]: Copied! <pre>unet\n</pre> unet Out[27]: <pre>UNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (down1): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc): OutConv(\n    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n)</pre> In\u00a0[28]: Copied! <pre>unet.state_dict()\n</pre> unet.state_dict() Out[28]: <pre>OrderedDict([('inc.double_conv.0.weight',\n              tensor([[[[-0.1569, -0.0516,  0.1381],\n                        [-0.0167,  0.1114, -0.1482],\n                        [-0.1659, -0.0492, -0.1526]],\n              \n                       [[ 0.0871,  0.1102, -0.1270],\n                        [ 0.1058,  0.0541, -0.0767],\n                        [ 0.1247,  0.1813,  0.1895]],\n              \n                       [[ 0.0929, -0.1305,  0.0531],\n                        [-0.0972, -0.1668, -0.0183],\n                        [-0.1754, -0.0862,  0.0373]]],\n              \n              \n                      [[[-0.0014,  0.1440, -0.0519],\n                        [ 0.1643,  0.1829,  0.1713],\n                        [-0.0702, -0.0426,  0.0083]],\n              \n                       [[ 0.1057,  0.0303,  0.0280],\n                        [-0.0306, -0.0898,  0.1635],\n                        [-0.1388, -0.0430,  0.0839]],\n              \n                       [[ 0.0840,  0.1753,  0.0916],\n                        [ 0.0819,  0.1624,  0.1901],\n                        [ 0.1914,  0.0483, -0.0875]]],\n              \n              \n                      [[[ 0.1197, -0.1618, -0.1778],\n                        [ 0.0866, -0.0638, -0.1615],\n                        [ 0.1437, -0.1523, -0.1007]],\n              \n                       [[-0.1395, -0.0602, -0.0457],\n                        [ 0.0582, -0.1701,  0.0586],\n                        [-0.1828,  0.0463,  0.1460]],\n              \n                       [[ 0.0735,  0.0299, -0.0629],\n                        [-0.0345, -0.0038,  0.0794],\n                        [-0.0958, -0.1519, -0.0411]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1095,  0.0703, -0.0860],\n                        [-0.1243, -0.0596, -0.1636],\n                        [ 0.0819,  0.0457,  0.1248]],\n              \n                       [[-0.1077, -0.1394,  0.0295],\n                        [ 0.1442, -0.1271,  0.1462],\n                        [-0.1011,  0.1301, -0.1294]],\n              \n                       [[-0.1653, -0.1431, -0.1031],\n                        [ 0.0511,  0.1370,  0.0210],\n                        [-0.1709,  0.0438, -0.0352]]],\n              \n              \n                      [[[-0.0893,  0.1826, -0.0856],\n                        [-0.1679,  0.0620,  0.1056],\n                        [-0.0206, -0.1745, -0.0500]],\n              \n                       [[ 0.0784,  0.0502,  0.1084],\n                        [-0.0746, -0.1213,  0.0849],\n                        [-0.1682, -0.1131, -0.1769]],\n              \n                       [[ 0.1111, -0.0814,  0.1804],\n                        [-0.0183,  0.0950, -0.0082],\n                        [-0.0761, -0.0757, -0.1657]]],\n              \n              \n                      [[[ 0.0543, -0.0157, -0.1387],\n                        [ 0.1503,  0.1388,  0.0653],\n                        [ 0.1474, -0.0991, -0.1478]],\n              \n                       [[ 0.0953, -0.1215,  0.1848],\n                        [-0.0360,  0.0052, -0.1841],\n                        [-0.1859, -0.0946,  0.1727]],\n              \n                       [[-0.0668, -0.0142,  0.1517],\n                        [-0.1101,  0.0217, -0.1021],\n                        [-0.1509,  0.0912,  0.1346]]]])),\n             ('inc.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.num_batches_tracked', tensor(0)),\n             ('inc.double_conv.3.weight',\n              tensor([[[[-4.1079e-02,  2.4625e-02, -5.8618e-03],\n                        [-3.6583e-02, -1.7239e-02,  2.4723e-02],\n                        [-2.0914e-03,  3.0168e-02, -2.0448e-02]],\n              \n                       [[ 4.1381e-03, -2.0328e-02, -2.9454e-02],\n                        [ 1.0681e-02, -3.6947e-02, -1.4246e-02],\n                        [-3.8679e-03,  2.3515e-02,  7.0796e-03]],\n              \n                       [[-3.3515e-02,  2.3345e-02, -5.7584e-04],\n                        [ 3.0752e-02, -3.5342e-02, -3.0192e-02],\n                        [ 3.0137e-02,  4.9735e-03,  3.0268e-02]],\n              \n                       ...,\n              \n                       [[ 2.6247e-02,  3.5036e-02, -2.7703e-02],\n                        [ 1.2037e-02, -1.1631e-02, -3.5691e-02],\n                        [ 1.8343e-02,  2.3172e-02, -2.3284e-02]],\n              \n                       [[ 3.9720e-02, -2.9578e-02, -3.8113e-02],\n                        [ 6.7576e-04, -4.0048e-02, -6.3216e-05],\n                        [ 1.9008e-02,  3.8545e-02,  3.0812e-02]],\n              \n                       [[-6.7981e-03, -1.5902e-03,  3.7965e-02],\n                        [ 8.6753e-03, -1.4569e-03, -1.9033e-02],\n                        [-2.0683e-02, -2.7206e-02,  2.5007e-02]]],\n              \n              \n                      [[[-1.3453e-02,  4.8410e-03,  6.3604e-03],\n                        [ 1.4860e-02, -1.9902e-04, -3.7245e-02],\n                        [ 1.2965e-02,  9.0473e-03,  2.3664e-02]],\n              \n                       [[-3.6142e-02, -2.9932e-02, -2.7691e-02],\n                        [ 2.6747e-02,  2.1051e-02, -6.9610e-03],\n                        [ 1.6672e-02,  2.4121e-02,  3.9934e-02]],\n              \n                       [[ 1.8793e-02,  3.8492e-02, -1.8463e-02],\n                        [ 2.4193e-02,  1.2931e-02, -2.9170e-02],\n                        [-2.2503e-02,  7.4183e-03, -9.9386e-03]],\n              \n                       ...,\n              \n                       [[-3.5583e-02,  1.0415e-02,  2.6884e-03],\n                        [-2.4120e-02, -1.6516e-02, -3.5117e-02],\n                        [-1.1389e-02, -3.2349e-02, -5.4190e-03]],\n              \n                       [[ 1.0794e-02, -1.4699e-02, -3.9218e-02],\n                        [ 7.2620e-03,  2.3942e-02, -9.0866e-03],\n                        [-3.9156e-02, -2.2665e-02,  3.0706e-02]],\n              \n                       [[ 2.5315e-02,  3.8635e-02, -1.4174e-03],\n                        [ 4.2061e-03, -3.3006e-02, -2.6736e-02],\n                        [-1.2201e-02,  2.4348e-02, -2.8096e-02]]],\n              \n              \n                      [[[-2.9801e-02,  1.3935e-02, -2.9342e-02],\n                        [-4.2913e-03,  9.5715e-03,  3.7494e-02],\n                        [ 2.2639e-02,  1.3474e-02,  2.3872e-02]],\n              \n                       [[ 1.6016e-03,  2.9424e-02,  2.3341e-02],\n                        [-1.2055e-02, -3.9560e-02, -1.5007e-02],\n                        [ 2.5384e-02, -4.1246e-02,  2.9730e-02]],\n              \n                       [[ 2.2965e-02, -2.7511e-02, -1.2306e-02],\n                        [-1.4792e-02,  2.7210e-03, -3.1689e-02],\n                        [ 3.1452e-02, -2.1154e-02,  3.2495e-02]],\n              \n                       ...,\n              \n                       [[ 6.1211e-03, -1.7085e-03,  1.0614e-02],\n                        [-1.3250e-03,  2.0869e-02,  7.6367e-03],\n                        [-3.3447e-02, -3.5193e-02, -3.4296e-02]],\n              \n                       [[ 2.6182e-02, -9.0026e-03,  4.3130e-03],\n                        [-1.9488e-02,  3.6438e-02, -2.9620e-02],\n                        [-4.0476e-02,  8.5702e-03,  2.2612e-02]],\n              \n                       [[ 1.9338e-03, -1.3990e-02,  8.3609e-03],\n                        [-1.3580e-02, -3.6543e-02,  2.8900e-02],\n                        [ 2.8948e-02, -2.2145e-03, -2.4276e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.0462e-03,  3.9649e-02,  1.0557e-02],\n                        [ 3.1926e-02,  3.8248e-02,  9.8494e-03],\n                        [ 1.2289e-03, -1.9980e-02, -3.3557e-02]],\n              \n                       [[-4.0275e-02,  1.1621e-02,  1.1366e-02],\n                        [-1.9881e-02,  6.3696e-03,  4.0948e-02],\n                        [-1.5219e-02, -1.6628e-02,  2.8343e-03]],\n              \n                       [[ 2.7490e-02,  3.5501e-02,  3.2039e-02],\n                        [ 3.5091e-03,  1.1285e-02,  1.5338e-02],\n                        [ 1.9410e-02, -5.1183e-03, -2.9545e-02]],\n              \n                       ...,\n              \n                       [[-2.0173e-02,  3.1788e-02,  8.5245e-03],\n                        [ 1.2969e-02,  1.4843e-02,  1.5726e-02],\n                        [ 3.1018e-02, -2.0554e-02,  1.6326e-02]],\n              \n                       [[-3.5004e-02,  3.6636e-02,  5.2004e-03],\n                        [ 2.9926e-02,  3.7449e-02,  6.1300e-04],\n                        [-5.1867e-04, -4.0083e-02, -3.0298e-02]],\n              \n                       [[-1.5009e-02,  4.1003e-02,  7.9811e-03],\n                        [ 6.5824e-03, -2.2011e-02,  8.9981e-03],\n                        [ 1.5385e-02, -3.9503e-02,  4.1086e-02]]],\n              \n              \n                      [[[-2.8993e-02, -3.7376e-02,  1.1231e-02],\n                        [ 1.7329e-02, -5.8507e-03,  1.9821e-02],\n                        [ 2.0648e-02, -3.9886e-02,  1.6316e-02]],\n              \n                       [[ 3.2519e-02,  1.6676e-02,  1.2690e-03],\n                        [ 1.6236e-03,  4.4074e-03, -2.0494e-02],\n                        [-3.6117e-02,  1.2012e-02, -2.8950e-02]],\n              \n                       [[-3.4818e-02, -1.8692e-02, -6.5148e-03],\n                        [-3.8199e-02, -2.1533e-03, -2.6669e-02],\n                        [ 2.0359e-03, -1.0877e-02,  3.2552e-02]],\n              \n                       ...,\n              \n                       [[ 2.6173e-03, -3.7495e-02,  8.6743e-03],\n                        [ 4.8354e-04,  4.1075e-02, -6.5880e-03],\n                        [ 3.3915e-02,  3.9410e-03, -1.2893e-02]],\n              \n                       [[ 2.6528e-02, -4.0759e-02,  1.9229e-02],\n                        [ 2.2432e-02, -3.9180e-03,  2.6232e-02],\n                        [ 1.2603e-02, -3.1149e-03, -1.4234e-02]],\n              \n                       [[-2.9655e-03,  1.3039e-03, -2.7197e-02],\n                        [ 3.9957e-02, -1.5892e-02,  2.0109e-02],\n                        [ 1.4106e-03,  6.4586e-04,  8.9162e-03]]],\n              \n              \n                      [[[ 3.1019e-02,  3.9165e-02, -2.7102e-02],\n                        [-3.8747e-02, -2.9976e-02, -8.2251e-04],\n                        [ 3.1431e-02, -9.7356e-03,  1.1533e-02]],\n              \n                       [[-8.6869e-03,  3.6680e-02,  1.8349e-02],\n                        [-3.1113e-02, -2.5772e-02, -1.2013e-02],\n                        [ 2.4810e-02,  2.1669e-02, -3.3620e-02]],\n              \n                       [[-3.0419e-02,  7.3520e-03, -1.9823e-02],\n                        [ 3.8660e-02,  2.6089e-02,  3.0254e-02],\n                        [ 1.4994e-02,  1.0452e-02,  3.4261e-02]],\n              \n                       ...,\n              \n                       [[-3.2601e-02, -3.6214e-02,  3.6512e-02],\n                        [-3.7527e-02, -2.9699e-02,  1.5305e-02],\n                        [-2.4764e-02,  2.2672e-02,  2.2486e-02]],\n              \n                       [[ 1.1033e-02,  3.0824e-02,  2.4714e-02],\n                        [-2.1154e-02,  2.5543e-02,  1.0087e-02],\n                        [ 2.3082e-02, -3.0461e-02,  3.4150e-02]],\n              \n                       [[-1.8519e-02, -7.6047e-03,  2.7975e-02],\n                        [-6.4077e-03, -2.6562e-02,  9.9592e-03],\n                        [-2.9076e-02, -2.5703e-02, -2.9623e-02]]]])),\n             ('inc.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.num_batches_tracked', tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[ 0.0357, -0.0264,  0.0201],\n                        [ 0.0235, -0.0205,  0.0169],\n                        [ 0.0325, -0.0087, -0.0301]],\n              \n                       [[-0.0252,  0.0130,  0.0105],\n                        [ 0.0278,  0.0094, -0.0272],\n                        [ 0.0324,  0.0047,  0.0045]],\n              \n                       [[-0.0352, -0.0399, -0.0170],\n                        [ 0.0144,  0.0158, -0.0144],\n                        [-0.0233,  0.0018, -0.0334]],\n              \n                       ...,\n              \n                       [[ 0.0116, -0.0235, -0.0296],\n                        [-0.0242,  0.0119,  0.0299],\n                        [ 0.0114,  0.0182,  0.0288]],\n              \n                       [[-0.0316, -0.0088, -0.0152],\n                        [-0.0325, -0.0183, -0.0030],\n                        [-0.0355, -0.0339,  0.0363]],\n              \n                       [[-0.0135,  0.0221,  0.0305],\n                        [-0.0268,  0.0040, -0.0396],\n                        [-0.0201,  0.0218, -0.0349]]],\n              \n              \n                      [[[ 0.0126,  0.0043, -0.0306],\n                        [-0.0146,  0.0352,  0.0244],\n                        [ 0.0250,  0.0273,  0.0250]],\n              \n                       [[-0.0412,  0.0087,  0.0332],\n                        [ 0.0187, -0.0076, -0.0089],\n                        [-0.0151, -0.0058, -0.0293]],\n              \n                       [[-0.0167, -0.0200,  0.0142],\n                        [-0.0356,  0.0294,  0.0118],\n                        [-0.0244, -0.0215,  0.0074]],\n              \n                       ...,\n              \n                       [[-0.0035,  0.0137, -0.0314],\n                        [ 0.0138, -0.0057,  0.0048],\n                        [ 0.0214, -0.0232, -0.0108]],\n              \n                       [[-0.0412, -0.0090, -0.0090],\n                        [-0.0287,  0.0126,  0.0135],\n                        [ 0.0138,  0.0354, -0.0151]],\n              \n                       [[ 0.0006, -0.0026,  0.0229],\n                        [ 0.0340,  0.0215,  0.0193],\n                        [-0.0062,  0.0044,  0.0232]]],\n              \n              \n                      [[[ 0.0393,  0.0131, -0.0272],\n                        [-0.0268, -0.0212,  0.0276],\n                        [-0.0300,  0.0367, -0.0406]],\n              \n                       [[ 0.0010, -0.0226, -0.0340],\n                        [ 0.0188,  0.0097, -0.0116],\n                        [ 0.0346, -0.0155,  0.0074]],\n              \n                       [[ 0.0277, -0.0405,  0.0331],\n                        [ 0.0064,  0.0333,  0.0368],\n                        [ 0.0375,  0.0212, -0.0242]],\n              \n                       ...,\n              \n                       [[-0.0069,  0.0186, -0.0329],\n                        [ 0.0099, -0.0293,  0.0133],\n                        [ 0.0385,  0.0099,  0.0152]],\n              \n                       [[ 0.0165,  0.0133,  0.0077],\n                        [-0.0347, -0.0064,  0.0321],\n                        [-0.0038, -0.0347,  0.0405]],\n              \n                       [[ 0.0055, -0.0044, -0.0135],\n                        [ 0.0195,  0.0027,  0.0329],\n                        [-0.0107,  0.0344, -0.0313]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0298, -0.0407, -0.0166],\n                        [-0.0002, -0.0221,  0.0067],\n                        [ 0.0178,  0.0013, -0.0193]],\n              \n                       [[-0.0238,  0.0293,  0.0269],\n                        [ 0.0277,  0.0384,  0.0140],\n                        [-0.0363, -0.0101,  0.0253]],\n              \n                       [[ 0.0334, -0.0225, -0.0067],\n                        [-0.0341,  0.0260, -0.0054],\n                        [ 0.0118,  0.0148,  0.0336]],\n              \n                       ...,\n              \n                       [[-0.0390,  0.0067, -0.0146],\n                        [-0.0058, -0.0076,  0.0248],\n                        [-0.0309, -0.0162, -0.0044]],\n              \n                       [[ 0.0156,  0.0133, -0.0077],\n                        [-0.0084, -0.0258,  0.0351],\n                        [ 0.0133, -0.0063,  0.0344]],\n              \n                       [[ 0.0333,  0.0093, -0.0372],\n                        [-0.0002,  0.0405, -0.0157],\n                        [-0.0018, -0.0008,  0.0080]]],\n              \n              \n                      [[[ 0.0330, -0.0097, -0.0083],\n                        [-0.0216,  0.0057, -0.0085],\n                        [ 0.0082,  0.0023,  0.0381]],\n              \n                       [[-0.0320,  0.0131, -0.0137],\n                        [-0.0037,  0.0201, -0.0339],\n                        [ 0.0327,  0.0375, -0.0072]],\n              \n                       [[-0.0085, -0.0173,  0.0102],\n                        [ 0.0381,  0.0038,  0.0299],\n                        [ 0.0261,  0.0366,  0.0206]],\n              \n                       ...,\n              \n                       [[-0.0330, -0.0098, -0.0026],\n                        [ 0.0038,  0.0086,  0.0258],\n                        [-0.0036,  0.0356, -0.0383]],\n              \n                       [[ 0.0014,  0.0289, -0.0069],\n                        [-0.0358, -0.0261, -0.0318],\n                        [-0.0223, -0.0333,  0.0221]],\n              \n                       [[ 0.0099, -0.0044,  0.0356],\n                        [-0.0416,  0.0245,  0.0219],\n                        [-0.0125, -0.0308, -0.0395]]],\n              \n              \n                      [[[-0.0059, -0.0348, -0.0104],\n                        [-0.0281, -0.0408,  0.0101],\n                        [-0.0012,  0.0124, -0.0115]],\n              \n                       [[-0.0382, -0.0336,  0.0156],\n                        [-0.0337,  0.0008,  0.0405],\n                        [-0.0058, -0.0384, -0.0303]],\n              \n                       [[-0.0357,  0.0154,  0.0037],\n                        [ 0.0079,  0.0382, -0.0023],\n                        [-0.0099,  0.0091, -0.0170]],\n              \n                       ...,\n              \n                       [[-0.0194,  0.0131, -0.0097],\n                        [-0.0112, -0.0016, -0.0009],\n                        [-0.0198, -0.0326, -0.0109]],\n              \n                       [[ 0.0248, -0.0348, -0.0202],\n                        [-0.0041, -0.0386, -0.0109],\n                        [-0.0228, -0.0399,  0.0372]],\n              \n                       [[-0.0010, -0.0073,  0.0204],\n                        [-0.0288,  0.0141,  0.0010],\n                        [-0.0160, -0.0138,  0.0360]]]])),\n             ('down1.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.1305e-02, -1.2684e-03,  2.4892e-02],\n                        [-2.6919e-02, -1.1080e-02,  6.1028e-04],\n                        [-6.9626e-03,  2.4179e-02,  7.0370e-03]],\n              \n                       [[-8.0535e-03, -1.8495e-04, -2.7226e-02],\n                        [-1.6500e-02,  3.6307e-03,  2.3883e-02],\n                        [-7.6892e-03,  2.6147e-02,  1.8880e-02]],\n              \n                       [[-6.3356e-04, -7.4601e-03, -7.9877e-03],\n                        [ 1.3430e-02, -1.9490e-02,  3.8737e-03],\n                        [-1.6122e-02, -1.8464e-02,  2.0742e-02]],\n              \n                       ...,\n              \n                       [[ 1.8362e-03, -1.1564e-02, -2.8767e-02],\n                        [ 5.5608e-03,  6.5534e-03,  1.5489e-02],\n                        [-1.3676e-02, -2.4228e-02,  1.2859e-02]],\n              \n                       [[ 1.7046e-02,  3.1059e-03, -1.3043e-02],\n                        [-1.1144e-02,  8.5697e-03, -9.9781e-03],\n                        [ 6.2510e-03, -2.7031e-02, -8.6106e-03]],\n              \n                       [[ 2.8901e-02,  1.9356e-02, -2.5723e-02],\n                        [-2.0941e-02,  1.2509e-02,  2.8496e-02],\n                        [-1.6640e-02, -3.5848e-03, -1.0853e-02]]],\n              \n              \n                      [[[ 1.2726e-02, -1.6195e-02,  1.4709e-02],\n                        [-2.0562e-02, -2.8356e-02,  1.0373e-02],\n                        [ 1.6941e-02, -1.7723e-02,  2.5551e-02]],\n              \n                       [[-1.9462e-02,  2.7471e-02, -1.6930e-02],\n                        [-2.7676e-03, -1.4025e-03,  1.7487e-02],\n                        [ 1.6080e-02,  2.9447e-02, -1.8378e-02]],\n              \n                       [[ 2.8415e-03, -1.0617e-02, -1.0754e-03],\n                        [ 2.2315e-02, -1.2144e-02, -1.7454e-02],\n                        [-2.4725e-02, -1.4872e-02,  1.2383e-02]],\n              \n                       ...,\n              \n                       [[ 2.1383e-02, -2.6270e-02, -1.2159e-02],\n                        [-2.1438e-02, -2.4603e-02, -1.3974e-02],\n                        [-2.2166e-02,  2.9069e-02,  1.0996e-02]],\n              \n                       [[ 2.6262e-02, -3.3151e-03,  2.6866e-02],\n                        [-1.1902e-02,  2.3779e-03,  2.6081e-02],\n                        [ 5.4771e-03,  7.5126e-04, -8.3137e-03]],\n              \n                       [[ 2.5385e-02,  7.2457e-03, -1.6735e-02],\n                        [-4.7629e-03, -1.2607e-02, -4.5772e-03],\n                        [ 1.6854e-02,  1.9901e-02,  2.8703e-02]]],\n              \n              \n                      [[[-2.8001e-02, -4.4546e-04, -2.0191e-02],\n                        [ 2.4830e-02, -2.2498e-02, -2.0728e-02],\n                        [-1.0464e-02,  2.7569e-02,  2.9056e-02]],\n              \n                       [[-2.7124e-02, -7.6276e-03,  2.4910e-02],\n                        [-5.0865e-03, -1.3039e-02, -1.9636e-02],\n                        [-2.0727e-02, -2.3310e-02, -1.5865e-02]],\n              \n                       [[ 7.5711e-03,  7.3599e-03, -2.2980e-02],\n                        [-2.5551e-02,  2.2718e-02,  1.5489e-02],\n                        [-3.0655e-04,  1.2903e-02, -2.2033e-02]],\n              \n                       ...,\n              \n                       [[-1.5014e-02, -7.5347e-04,  1.6599e-03],\n                        [-5.4850e-03,  1.3427e-02,  2.9824e-03],\n                        [ 2.4041e-02,  1.7558e-03,  1.0491e-02]],\n              \n                       [[-1.7517e-02,  2.2218e-02,  2.1117e-02],\n                        [-8.5116e-05,  2.7633e-02,  1.1950e-03],\n                        [ 2.3484e-02, -2.0629e-02, -7.9562e-03]],\n              \n                       [[ 6.6841e-03, -2.7769e-02, -2.2987e-02],\n                        [-2.4637e-02,  2.2629e-02, -1.2457e-02],\n                        [-1.0986e-02, -1.6586e-02, -4.0791e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.6628e-03,  2.6667e-02,  6.7481e-03],\n                        [-1.4348e-02, -1.9016e-02,  2.1977e-02],\n                        [ 1.1526e-02,  2.0264e-03, -1.9429e-02]],\n              \n                       [[-1.5399e-02,  2.4140e-02,  1.7281e-02],\n                        [-5.1553e-05,  2.7146e-03, -2.2730e-02],\n                        [-2.2137e-02,  1.5756e-02,  9.6129e-03]],\n              \n                       [[-5.2356e-03,  1.8795e-02,  1.4753e-02],\n                        [-2.9235e-02, -2.4725e-02, -9.9595e-03],\n                        [-2.5816e-02, -1.2593e-02, -1.4906e-02]],\n              \n                       ...,\n              \n                       [[-5.1329e-04,  2.4464e-02,  1.0491e-02],\n                        [ 1.6588e-03, -1.9864e-02, -2.4729e-02],\n                        [-5.7917e-03,  1.2495e-02,  7.5220e-03]],\n              \n                       [[ 1.5368e-02, -2.5456e-02, -1.4819e-02],\n                        [-2.5614e-02, -2.3670e-03,  2.6447e-02],\n                        [-5.4125e-03, -4.6167e-03, -7.2054e-04]],\n              \n                       [[-1.7071e-02, -2.6587e-03,  2.1725e-02],\n                        [-2.8988e-02,  3.1809e-03,  1.3815e-03],\n                        [ 6.4158e-03, -2.6444e-04,  1.8910e-02]]],\n              \n              \n                      [[[ 2.5009e-02,  4.4661e-03, -2.5017e-02],\n                        [ 6.8237e-03,  1.3778e-02,  6.8838e-03],\n                        [-1.5440e-02, -1.2293e-03,  2.2054e-02]],\n              \n                       [[-1.6465e-02,  1.3906e-02,  2.9242e-02],\n                        [ 2.2392e-02, -6.8427e-03, -2.1006e-02],\n                        [ 2.3828e-02, -1.8528e-02,  4.6238e-03]],\n              \n                       [[ 2.6324e-02, -3.9792e-03, -2.8550e-02],\n                        [ 9.2739e-03,  8.2617e-03, -2.5574e-02],\n                        [ 1.6078e-02,  1.6129e-02,  6.8392e-03]],\n              \n                       ...,\n              \n                       [[ 2.7127e-02, -1.3369e-02,  8.5266e-03],\n                        [-1.0530e-02, -2.0817e-02, -8.6817e-03],\n                        [-2.9038e-02, -2.4825e-03,  1.3813e-02]],\n              \n                       [[ 1.2809e-02, -2.7485e-02, -2.8767e-02],\n                        [-5.6553e-03,  1.9724e-02,  1.1964e-02],\n                        [ 5.6818e-03,  1.9974e-02, -1.8658e-02]],\n              \n                       [[ 2.8031e-02, -2.4776e-02, -3.0622e-03],\n                        [ 1.4898e-02,  2.7475e-03, -2.2119e-02],\n                        [ 5.8204e-03,  6.9012e-03, -2.6735e-02]]],\n              \n              \n                      [[[ 9.7910e-03,  1.7056e-02, -4.8750e-03],\n                        [ 3.8653e-03,  9.2350e-03, -2.7748e-02],\n                        [ 2.4542e-02, -9.4870e-03,  2.7431e-02]],\n              \n                       [[ 1.5725e-03,  5.4433e-03,  6.2727e-03],\n                        [ 2.9122e-02,  1.9450e-02, -1.4450e-02],\n                        [ 7.3775e-03,  2.3615e-02, -1.2452e-02]],\n              \n                       [[-7.7901e-04,  5.2408e-03,  1.3440e-02],\n                        [ 1.1745e-02, -2.4794e-02,  5.6418e-03],\n                        [ 1.4150e-02, -1.9262e-02, -6.3717e-04]],\n              \n                       ...,\n              \n                       [[ 4.6180e-03,  2.1094e-03, -2.5070e-02],\n                        [-1.9577e-02,  2.3995e-02, -1.5351e-02],\n                        [-2.1875e-02, -2.0034e-03,  3.7910e-03]],\n              \n                       [[ 2.1114e-03,  2.1738e-02,  1.3168e-03],\n                        [-9.2969e-03,  1.9882e-02,  5.0677e-03],\n                        [ 6.9171e-03,  2.1555e-02, -1.1559e-02]],\n              \n                       [[-2.8176e-02, -2.6783e-02,  2.4445e-02],\n                        [ 1.4733e-02,  4.4278e-03,  7.2822e-03],\n                        [-2.4972e-02, -1.4935e-02,  2.7857e-02]]]])),\n             ('down1.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-2.0874e-03,  2.8328e-02,  3.8197e-03],\n                        [ 2.0103e-02, -2.4530e-02,  3.5383e-03],\n                        [ 1.2657e-02,  2.5045e-02,  5.3281e-03]],\n              \n                       [[ 9.3871e-03,  2.5844e-02, -1.4631e-02],\n                        [ 2.7466e-02, -1.0389e-02,  1.5178e-02],\n                        [ 2.8453e-02,  1.3451e-02, -1.1607e-03]],\n              \n                       [[ 2.0450e-02,  1.3948e-02, -1.8822e-02],\n                        [-1.6178e-03,  2.4138e-02,  1.6494e-02],\n                        [-2.7684e-02, -1.6600e-02,  2.5942e-03]],\n              \n                       ...,\n              \n                       [[-2.5010e-03,  2.1981e-02,  1.0307e-02],\n                        [ 1.0725e-02,  2.8690e-02, -1.7391e-02],\n                        [ 3.5500e-03,  2.0341e-03,  5.9864e-03]],\n              \n                       [[-8.7539e-03,  1.3636e-02,  2.7444e-02],\n                        [-5.3241e-03,  1.4782e-02, -1.6061e-02],\n                        [ 2.8436e-02, -2.6700e-02, -5.3704e-03]],\n              \n                       [[-2.3932e-02,  6.0354e-03,  2.0279e-02],\n                        [-2.7523e-02, -2.8895e-02,  2.0104e-02],\n                        [-6.3520e-03,  8.0765e-03,  2.4935e-03]]],\n              \n              \n                      [[[-1.0771e-02, -3.8036e-03, -2.3648e-02],\n                        [-1.3159e-02,  2.4382e-02,  2.5068e-02],\n                        [-1.8793e-02, -2.5927e-02,  1.6405e-02]],\n              \n                       [[ 4.6219e-03,  2.3189e-02, -1.0743e-02],\n                        [ 2.8896e-02, -2.2556e-02,  5.3712e-03],\n                        [-8.8788e-03, -8.3982e-03, -9.5629e-03]],\n              \n                       [[-2.3292e-02,  1.9044e-02,  1.8797e-03],\n                        [-1.7992e-02, -2.8691e-02,  1.8576e-03],\n                        [-2.4593e-02,  8.3165e-03, -5.6803e-03]],\n              \n                       ...,\n              \n                       [[-2.7325e-02, -1.6579e-02, -2.7656e-02],\n                        [-1.4223e-02,  6.2641e-03, -2.7416e-02],\n                        [-1.8046e-02,  1.1367e-02, -1.2150e-02]],\n              \n                       [[-3.4729e-03,  5.4115e-04, -1.9539e-02],\n                        [ 1.6914e-02, -1.1351e-02,  2.0686e-02],\n                        [-1.0540e-02, -2.7865e-02,  3.4599e-03]],\n              \n                       [[-1.5403e-02, -5.0929e-03, -2.0951e-02],\n                        [ 1.8758e-02, -1.5846e-02, -2.6030e-02],\n                        [ 2.3687e-02, -2.6410e-02,  5.7963e-03]]],\n              \n              \n                      [[[-2.6278e-02, -1.2930e-02, -1.6344e-02],\n                        [ 8.9017e-03, -1.8674e-02, -1.6698e-02],\n                        [-1.0313e-02,  9.8180e-03,  1.0110e-02]],\n              \n                       [[-2.1049e-02,  1.4577e-02, -1.8113e-02],\n                        [-2.0648e-02, -1.4387e-02, -2.4280e-04],\n                        [-2.0775e-02, -4.0661e-03,  2.7782e-02]],\n              \n                       [[-2.7178e-02,  4.2496e-03, -2.3201e-02],\n                        [ 1.0937e-02, -6.5350e-03, -2.3540e-02],\n                        [-2.9455e-02,  2.3027e-02, -2.7718e-02]],\n              \n                       ...,\n              \n                       [[-2.1814e-02,  1.5335e-02, -2.3714e-02],\n                        [-2.8257e-02,  2.3738e-02, -1.3762e-02],\n                        [-3.1294e-03,  9.6518e-03,  6.7151e-03]],\n              \n                       [[-2.5689e-02,  4.9199e-03,  1.6813e-02],\n                        [ 2.7413e-02, -2.5757e-02, -2.6320e-02],\n                        [ 2.8428e-02, -1.9982e-02, -6.2184e-03]],\n              \n                       [[-4.9595e-03, -2.2561e-02,  2.1508e-02],\n                        [ 6.1043e-03, -1.9141e-02, -1.6917e-02],\n                        [-2.2802e-02, -7.2276e-03,  1.1010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8587e-04,  2.5234e-02,  1.2862e-02],\n                        [ 6.4087e-03,  2.9456e-03, -6.2891e-03],\n                        [ 1.3295e-02,  1.1122e-02, -3.8489e-03]],\n              \n                       [[ 2.4627e-02, -8.6374e-03,  9.6317e-03],\n                        [-4.4341e-03, -2.0696e-03,  5.3607e-05],\n                        [ 2.7382e-02, -1.1736e-03, -2.8442e-03]],\n              \n                       [[ 7.9895e-03, -6.4228e-03,  9.2783e-03],\n                        [ 1.0661e-03, -2.7210e-02,  2.9449e-02],\n                        [ 2.8375e-03, -2.2452e-02, -3.4423e-03]],\n              \n                       ...,\n              \n                       [[ 7.1594e-03, -2.7026e-02, -6.7921e-03],\n                        [-1.5202e-02, -7.0004e-04, -6.5862e-03],\n                        [ 2.7967e-02,  2.5300e-02,  5.7218e-03]],\n              \n                       [[ 1.9714e-02,  2.5212e-02,  2.6632e-02],\n                        [ 3.6115e-03, -2.2397e-02, -1.0878e-02],\n                        [-1.3762e-02,  4.6104e-04,  1.6057e-02]],\n              \n                       [[ 2.5034e-02, -2.9420e-02, -1.7739e-02],\n                        [ 1.0064e-02, -2.8722e-02, -1.6836e-02],\n                        [ 1.7448e-02,  2.8111e-02,  1.4150e-03]]],\n              \n              \n                      [[[-1.5742e-02, -1.3421e-02,  2.7663e-02],\n                        [-1.5744e-02,  2.0141e-03,  1.1419e-03],\n                        [ 2.5981e-02,  1.0222e-02, -1.5587e-02]],\n              \n                       [[ 1.3669e-02,  5.2103e-03, -7.6013e-03],\n                        [-1.6173e-02,  5.6269e-04,  2.4350e-03],\n                        [ 2.4261e-03,  2.5788e-02, -2.8097e-02]],\n              \n                       [[-1.4888e-02, -1.7731e-02, -6.4337e-03],\n                        [ 2.2471e-02,  2.3679e-04, -1.1437e-02],\n                        [-5.8912e-03,  1.0241e-02,  1.8909e-02]],\n              \n                       ...,\n              \n                       [[-1.4776e-02,  2.1398e-02,  8.8336e-04],\n                        [-3.3876e-03,  9.3768e-03, -5.3336e-03],\n                        [-4.4843e-03, -5.7139e-03, -6.8183e-03]],\n              \n                       [[-2.0888e-02, -2.4299e-02, -1.6261e-02],\n                        [-2.0847e-02,  1.3012e-02,  2.1894e-02],\n                        [-4.3075e-03,  2.1090e-02,  2.2750e-02]],\n              \n                       [[-1.7861e-02, -2.5487e-02, -9.7013e-03],\n                        [-2.8849e-03, -2.6374e-02, -2.2423e-02],\n                        [ 3.2294e-03,  1.0469e-02, -2.7943e-02]]],\n              \n              \n                      [[[ 4.1885e-03, -2.7628e-02, -2.5770e-02],\n                        [ 1.4383e-02, -3.2527e-03, -2.1710e-02],\n                        [-1.4146e-02,  7.5708e-03, -1.2968e-02]],\n              \n                       [[ 6.4110e-03,  1.5356e-02, -1.1846e-02],\n                        [ 2.1303e-02,  6.4434e-03, -2.6370e-02],\n                        [ 1.7484e-02,  1.9423e-02,  2.9357e-02]],\n              \n                       [[ 3.5598e-03,  2.6142e-02, -2.6987e-02],\n                        [ 9.4496e-03,  1.8193e-02,  1.0256e-02],\n                        [ 3.0655e-03,  2.6695e-03, -9.7217e-04]],\n              \n                       ...,\n              \n                       [[ 1.2180e-02,  2.1096e-02, -2.4789e-02],\n                        [ 6.3251e-03,  3.0475e-03, -6.8353e-03],\n                        [ 1.8787e-02, -9.2431e-03,  1.7185e-02]],\n              \n                       [[-1.1940e-02,  1.8412e-02,  1.7622e-02],\n                        [ 2.1504e-02,  2.3440e-02,  1.1492e-02],\n                        [-1.6089e-02, -1.5441e-02,  2.1249e-02]],\n              \n                       [[-2.3543e-02, -2.0001e-02, -2.0346e-02],\n                        [ 2.0520e-02,  2.9473e-03, -1.2873e-02],\n                        [ 1.3080e-02, -1.3335e-02,  2.4488e-02]]]])),\n             ('down2.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-0.0199, -0.0207, -0.0025],\n                        [-0.0202,  0.0202, -0.0180],\n                        [-0.0126,  0.0164, -0.0123]],\n              \n                       [[ 0.0062, -0.0141,  0.0168],\n                        [ 0.0078,  0.0006, -0.0096],\n                        [ 0.0036, -0.0188,  0.0195]],\n              \n                       [[-0.0073, -0.0065, -0.0040],\n                        [ 0.0086,  0.0105,  0.0089],\n                        [-0.0055,  0.0144, -0.0161]],\n              \n                       ...,\n              \n                       [[ 0.0131, -0.0028, -0.0143],\n                        [-0.0057, -0.0096, -0.0171],\n                        [-0.0130, -0.0047, -0.0005]],\n              \n                       [[-0.0046, -0.0177,  0.0125],\n                        [-0.0102,  0.0154,  0.0072],\n                        [ 0.0206,  0.0169, -0.0156]],\n              \n                       [[ 0.0036,  0.0074,  0.0056],\n                        [ 0.0112, -0.0127, -0.0147],\n                        [ 0.0001,  0.0135,  0.0017]]],\n              \n              \n                      [[[-0.0075, -0.0151,  0.0206],\n                        [ 0.0001, -0.0105, -0.0072],\n                        [ 0.0066,  0.0189,  0.0178]],\n              \n                       [[ 0.0086, -0.0003,  0.0005],\n                        [ 0.0185, -0.0089, -0.0045],\n                        [ 0.0166, -0.0010,  0.0182]],\n              \n                       [[-0.0107, -0.0202,  0.0050],\n                        [-0.0029, -0.0139,  0.0134],\n                        [ 0.0037,  0.0136, -0.0140]],\n              \n                       ...,\n              \n                       [[ 0.0171,  0.0028,  0.0002],\n                        [ 0.0165,  0.0112,  0.0014],\n                        [-0.0089, -0.0016,  0.0104]],\n              \n                       [[-0.0161, -0.0097, -0.0042],\n                        [ 0.0174,  0.0107,  0.0100],\n                        [-0.0053, -0.0070,  0.0113]],\n              \n                       [[-0.0016, -0.0070,  0.0061],\n                        [ 0.0017,  0.0160,  0.0013],\n                        [ 0.0057,  0.0200, -0.0160]]],\n              \n              \n                      [[[-0.0060, -0.0105, -0.0198],\n                        [-0.0150, -0.0083,  0.0156],\n                        [-0.0090,  0.0120, -0.0199]],\n              \n                       [[ 0.0127,  0.0145, -0.0122],\n                        [ 0.0110, -0.0001, -0.0018],\n                        [ 0.0039,  0.0206, -0.0076]],\n              \n                       [[ 0.0101,  0.0061, -0.0136],\n                        [ 0.0194, -0.0136,  0.0016],\n                        [-0.0007,  0.0173,  0.0011]],\n              \n                       ...,\n              \n                       [[-0.0134, -0.0127, -0.0165],\n                        [ 0.0041, -0.0118,  0.0110],\n                        [ 0.0044,  0.0060,  0.0036]],\n              \n                       [[ 0.0056, -0.0185,  0.0055],\n                        [ 0.0114, -0.0050, -0.0185],\n                        [ 0.0116, -0.0140, -0.0148]],\n              \n                       [[ 0.0145,  0.0188, -0.0130],\n                        [ 0.0065, -0.0171,  0.0036],\n                        [-0.0037, -0.0078,  0.0077]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0090,  0.0069, -0.0124],\n                        [-0.0150, -0.0065,  0.0094],\n                        [-0.0195, -0.0163, -0.0144]],\n              \n                       [[-0.0142,  0.0055, -0.0013],\n                        [-0.0149, -0.0092,  0.0063],\n                        [ 0.0007,  0.0089,  0.0060]],\n              \n                       [[-0.0055, -0.0047, -0.0065],\n                        [-0.0140,  0.0113, -0.0194],\n                        [-0.0049,  0.0079,  0.0079]],\n              \n                       ...,\n              \n                       [[-0.0111, -0.0127,  0.0139],\n                        [ 0.0075, -0.0173, -0.0109],\n                        [ 0.0204, -0.0063, -0.0174]],\n              \n                       [[ 0.0198,  0.0142,  0.0200],\n                        [ 0.0188,  0.0201, -0.0102],\n                        [ 0.0027, -0.0103, -0.0160]],\n              \n                       [[ 0.0090,  0.0116,  0.0114],\n                        [-0.0037, -0.0078,  0.0121],\n                        [-0.0192, -0.0149, -0.0202]]],\n              \n              \n                      [[[ 0.0045, -0.0102,  0.0195],\n                        [-0.0163, -0.0012,  0.0005],\n                        [ 0.0079, -0.0045,  0.0198]],\n              \n                       [[ 0.0181,  0.0146, -0.0039],\n                        [ 0.0095,  0.0106, -0.0055],\n                        [ 0.0028,  0.0103,  0.0006]],\n              \n                       [[ 0.0039, -0.0051, -0.0071],\n                        [-0.0123, -0.0141,  0.0050],\n                        [-0.0146,  0.0068,  0.0163]],\n              \n                       ...,\n              \n                       [[-0.0144,  0.0072, -0.0097],\n                        [-0.0070,  0.0141,  0.0089],\n                        [-0.0034,  0.0030,  0.0124]],\n              \n                       [[ 0.0143, -0.0146, -0.0182],\n                        [-0.0080,  0.0061, -0.0181],\n                        [ 0.0166,  0.0175, -0.0116]],\n              \n                       [[-0.0095, -0.0014, -0.0191],\n                        [ 0.0184, -0.0074, -0.0144],\n                        [ 0.0201, -0.0136, -0.0001]]],\n              \n              \n                      [[[-0.0022, -0.0024,  0.0035],\n                        [-0.0075, -0.0206,  0.0173],\n                        [-0.0160,  0.0207,  0.0060]],\n              \n                       [[-0.0073,  0.0075, -0.0149],\n                        [-0.0112,  0.0081, -0.0034],\n                        [-0.0176, -0.0169,  0.0041]],\n              \n                       [[-0.0040,  0.0199, -0.0174],\n                        [ 0.0103,  0.0153, -0.0109],\n                        [-0.0044, -0.0160, -0.0072]],\n              \n                       ...,\n              \n                       [[ 0.0142, -0.0045,  0.0044],\n                        [-0.0134, -0.0153, -0.0110],\n                        [-0.0178,  0.0051, -0.0051]],\n              \n                       [[ 0.0090,  0.0175,  0.0111],\n                        [ 0.0201, -0.0061,  0.0081],\n                        [-0.0037,  0.0166,  0.0074]],\n              \n                       [[-0.0069,  0.0019, -0.0200],\n                        [-0.0047, -0.0145,  0.0192],\n                        [-0.0100,  0.0121, -0.0193]]]])),\n             ('down2.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-4.6348e-03,  9.8509e-03,  1.6142e-02],\n                        [ 2.6825e-05, -8.4992e-03,  3.6535e-04],\n                        [-2.0749e-02, -2.7181e-03,  1.4475e-02]],\n              \n                       [[ 1.0194e-02,  6.9748e-03,  1.3849e-02],\n                        [ 1.4200e-03,  2.5024e-03,  1.5259e-02],\n                        [ 1.1671e-02,  4.0497e-03,  8.7697e-03]],\n              \n                       [[-4.4309e-03, -1.1845e-02, -1.6037e-02],\n                        [-7.8910e-03, -9.7038e-03,  5.6008e-03],\n                        [-1.6987e-02,  7.1697e-03,  1.7236e-02]],\n              \n                       ...,\n              \n                       [[-1.1635e-02,  1.8610e-02,  1.4086e-02],\n                        [-1.1576e-02, -1.9610e-03, -1.8455e-02],\n                        [-8.6874e-03, -1.1485e-02, -5.8817e-03]],\n              \n                       [[-1.3743e-02,  1.2879e-02,  2.2404e-03],\n                        [-6.8730e-03,  1.0492e-02,  8.4602e-03],\n                        [ 1.9366e-03, -1.0892e-02,  9.0133e-03]],\n              \n                       [[-6.9619e-03, -1.7941e-02, -1.1306e-02],\n                        [-6.8960e-03, -6.8894e-03, -6.9923e-04],\n                        [ 1.0807e-02,  1.8476e-02,  1.9441e-02]]],\n              \n              \n                      [[[ 6.4426e-03,  7.5100e-03,  6.7503e-03],\n                        [-1.8439e-02,  1.4277e-02, -1.0381e-02],\n                        [-1.7296e-02, -1.2204e-02,  5.2923e-03]],\n              \n                       [[-6.8046e-03,  6.3742e-03, -1.1632e-02],\n                        [ 4.2213e-03,  2.0774e-02, -3.7589e-03],\n                        [ 1.6312e-02,  7.4283e-04,  1.2614e-02]],\n              \n                       [[-6.7564e-03, -1.0808e-02, -1.6746e-02],\n                        [-6.2140e-03,  9.3120e-03, -9.2284e-03],\n                        [ 2.8789e-03,  1.2397e-03,  1.5193e-02]],\n              \n                       ...,\n              \n                       [[-1.4065e-02, -4.0645e-03, -1.4819e-02],\n                        [ 7.9262e-03, -1.4440e-02, -1.3676e-02],\n                        [ 8.2918e-04,  1.0951e-02,  6.6675e-03]],\n              \n                       [[ 1.8929e-02, -1.6932e-02,  7.8811e-03],\n                        [ 1.6661e-02, -1.4852e-02, -6.1440e-03],\n                        [-4.3739e-03,  1.0890e-02,  1.2552e-03]],\n              \n                       [[ 1.6674e-02,  8.4053e-03, -5.2151e-03],\n                        [-1.8711e-02, -6.0464e-04,  4.8782e-03],\n                        [-1.0599e-02, -8.5500e-03, -4.4493e-04]]],\n              \n              \n                      [[[ 7.4150e-03, -1.7817e-02, -9.8810e-03],\n                        [ 1.5139e-02, -5.4702e-03,  3.1069e-03],\n                        [ 1.6121e-02, -2.4298e-03, -3.4243e-03]],\n              \n                       [[ 5.2642e-03, -1.7880e-02, -1.8678e-02],\n                        [ 2.9048e-03,  1.0568e-02, -2.8701e-04],\n                        [-4.0345e-05, -2.8312e-03,  6.9242e-03]],\n              \n                       [[ 1.2557e-02,  1.3475e-02, -1.1946e-02],\n                        [ 1.0504e-02, -1.1848e-02,  1.4417e-02],\n                        [-1.8312e-02,  1.1722e-02, -6.9120e-03]],\n              \n                       ...,\n              \n                       [[ 1.9895e-02,  1.5509e-02,  1.9991e-02],\n                        [-1.5190e-02, -1.9972e-02, -1.3091e-02],\n                        [-1.1537e-02, -6.8988e-03,  1.1122e-02]],\n              \n                       [[ 1.0277e-02, -9.5677e-03,  1.4165e-02],\n                        [ 5.0890e-03,  1.1992e-02,  2.0542e-02],\n                        [-9.9942e-04,  1.1082e-02, -5.1328e-03]],\n              \n                       [[ 1.0213e-02, -4.6551e-03, -5.2989e-03],\n                        [ 1.5165e-02, -1.7655e-02,  5.5892e-03],\n                        [ 1.1311e-02, -1.2807e-02, -1.2253e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4459e-02,  4.5380e-04, -2.9677e-03],\n                        [ 1.8889e-02, -1.6052e-02, -1.5562e-02],\n                        [ 1.3935e-03, -1.6170e-02,  2.0204e-02]],\n              \n                       [[ 1.0080e-02, -3.7539e-03, -1.5059e-02],\n                        [ 6.8971e-03, -8.5807e-03,  1.5525e-02],\n                        [ 1.4992e-03, -7.8594e-03,  7.5005e-03]],\n              \n                       [[ 3.7703e-03,  9.6159e-03,  1.6808e-02],\n                        [-1.1511e-02, -1.9614e-02, -1.7621e-02],\n                        [ 6.5007e-03, -1.5883e-02, -1.3063e-02]],\n              \n                       ...,\n              \n                       [[ 1.1717e-02,  1.3965e-03, -5.3536e-03],\n                        [ 1.4582e-02, -1.8533e-03, -1.5276e-02],\n                        [-2.0322e-02, -1.0361e-02, -6.1722e-03]],\n              \n                       [[ 5.0393e-04,  3.0661e-03, -9.3391e-03],\n                        [-5.0653e-03,  1.3716e-02,  9.7900e-03],\n                        [-2.0547e-02,  1.3067e-02,  1.6991e-03]],\n              \n                       [[-8.7317e-03,  1.5140e-02, -9.8445e-03],\n                        [-2.9895e-03,  1.0854e-02, -7.8243e-03],\n                        [ 1.5019e-03,  1.9270e-02,  9.2994e-03]]],\n              \n              \n                      [[[-3.2868e-03, -1.6655e-03,  1.3082e-02],\n                        [ 7.1859e-03, -1.9157e-03, -3.5394e-03],\n                        [-1.9397e-02,  5.5216e-03, -1.8486e-02]],\n              \n                       [[ 9.8068e-03,  2.6197e-03,  4.8447e-04],\n                        [ 1.5565e-02,  1.1252e-02,  1.8660e-02],\n                        [ 3.1310e-03,  6.5078e-03, -1.4506e-02]],\n              \n                       [[-1.5900e-02, -3.8698e-03,  4.6403e-03],\n                        [ 1.0163e-02,  1.0891e-02,  1.9025e-02],\n                        [-7.0364e-03,  1.0454e-02,  7.3635e-03]],\n              \n                       ...,\n              \n                       [[ 1.5563e-02, -1.9394e-02,  1.5875e-03],\n                        [-4.1397e-03, -7.3719e-04, -8.6707e-03],\n                        [-1.5182e-02,  1.4803e-02, -1.7555e-02]],\n              \n                       [[-7.9233e-04,  1.1101e-03,  1.7634e-03],\n                        [ 1.5103e-02, -1.4403e-02,  1.4855e-02],\n                        [-7.4607e-03,  7.4488e-03, -1.7282e-02]],\n              \n                       [[ 1.4080e-02,  1.6888e-02,  1.6374e-02],\n                        [ 7.7976e-03, -6.2802e-03, -3.1626e-03],\n                        [ 2.0682e-02, -1.9079e-02,  1.3276e-02]]],\n              \n              \n                      [[[ 1.8058e-02, -9.1462e-03, -7.2015e-03],\n                        [-6.4691e-03, -2.9027e-03,  9.6589e-03],\n                        [-1.3747e-02,  1.9787e-02,  1.9956e-02]],\n              \n                       [[-1.1408e-02, -2.4681e-05,  7.7289e-03],\n                        [ 1.9633e-02, -8.2515e-03,  1.3016e-02],\n                        [-1.8417e-02,  1.8677e-02, -1.1818e-02]],\n              \n                       [[ 1.9430e-02,  1.0222e-02, -5.9156e-03],\n                        [ 1.5036e-02,  9.4860e-03,  2.0289e-03],\n                        [-6.1385e-03, -6.8786e-03, -1.0498e-02]],\n              \n                       ...,\n              \n                       [[ 1.8626e-02, -4.7810e-03,  1.8702e-02],\n                        [-7.9554e-03, -1.7242e-02, -1.2626e-03],\n                        [ 1.9328e-02, -5.6285e-03, -1.1736e-02]],\n              \n                       [[-4.1653e-04, -1.8020e-02, -1.2647e-02],\n                        [-4.7124e-03,  3.7225e-03,  3.3474e-03],\n                        [-2.6790e-03,  6.2666e-03,  3.8707e-03]],\n              \n                       [[ 1.9958e-03, -6.2181e-03, -1.5993e-02],\n                        [ 4.3567e-03,  2.8269e-03,  2.0313e-02],\n                        [-1.6953e-02, -1.2477e-02, -6.3685e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.3495e-02,  1.1336e-02,  3.2999e-03],\n                        [ 1.0248e-02,  4.9058e-03,  1.6721e-03],\n                        [ 1.4577e-02,  1.2254e-02, -1.0996e-02]],\n              \n                       [[ 2.8387e-03, -1.2857e-02, -6.3248e-04],\n                        [ 1.0179e-02, -7.9369e-03,  9.4359e-03],\n                        [ 2.8751e-03, -1.1316e-02, -2.7018e-03]],\n              \n                       [[ 1.3239e-02,  1.3039e-03, -1.3213e-02],\n                        [-8.4236e-03,  2.3438e-03, -1.4353e-02],\n                        [ 9.7540e-03,  7.3673e-03,  9.9629e-04]],\n              \n                       ...,\n              \n                       [[-1.2715e-02, -5.7416e-03,  8.1590e-04],\n                        [ 1.2467e-02,  5.0082e-03, -9.3793e-03],\n                        [-1.0866e-02,  6.1197e-03,  2.4678e-03]],\n              \n                       [[-1.3211e-02, -6.7648e-03,  1.4521e-02],\n                        [-5.5102e-03, -5.2198e-03,  1.0626e-02],\n                        [-1.1742e-02, -6.2968e-03, -3.1413e-03]],\n              \n                       [[ 5.9503e-04, -9.2838e-03,  2.2524e-03],\n                        [ 4.4587e-03, -6.3728e-04, -1.4285e-02],\n                        [-5.1423e-03, -5.7166e-03,  1.2934e-02]]],\n              \n              \n                      [[[ 1.8463e-03, -5.4794e-04, -1.8946e-03],\n                        [ 9.7586e-04,  3.5177e-03, -4.0504e-03],\n                        [-6.2299e-03,  5.2996e-03,  1.3720e-02]],\n              \n                       [[-5.9090e-03,  1.6445e-03,  2.7570e-03],\n                        [-9.9673e-04, -1.0245e-02,  5.6605e-03],\n                        [ 1.1391e-02, -1.1658e-02, -1.1734e-02]],\n              \n                       [[-1.1735e-02,  2.4595e-03,  5.7827e-03],\n                        [ 7.1670e-03, -1.6270e-03,  1.0687e-02],\n                        [ 6.0396e-03, -7.3033e-04, -8.5946e-03]],\n              \n                       ...,\n              \n                       [[ 1.1671e-02,  1.3118e-02, -1.3291e-02],\n                        [ 6.1538e-03, -6.0592e-04,  6.6185e-03],\n                        [ 1.2829e-03, -1.3731e-02,  1.4932e-03]],\n              \n                       [[-7.4605e-03,  6.8828e-04, -1.2302e-04],\n                        [-8.1735e-03,  1.2001e-02,  7.8193e-03],\n                        [ 2.0528e-03, -6.3210e-03,  1.3449e-02]],\n              \n                       [[ 2.9136e-03,  6.6908e-03, -3.7520e-03],\n                        [ 9.3340e-03, -4.1290e-03, -1.4161e-02],\n                        [-5.5939e-03,  5.1468e-03,  7.5768e-05]]],\n              \n              \n                      [[[ 7.9902e-03,  8.0955e-03,  1.0381e-02],\n                        [ 6.6680e-03,  2.9378e-03,  6.6944e-03],\n                        [-2.3877e-03, -4.8883e-03,  8.5533e-03]],\n              \n                       [[-1.2371e-02, -1.2348e-02,  4.0223e-03],\n                        [-6.9362e-03, -1.0553e-02,  5.3495e-03],\n                        [ 4.4429e-04,  5.7790e-03, -2.5581e-03]],\n              \n                       [[ 2.1132e-03, -1.0715e-02,  3.1263e-03],\n                        [ 1.4578e-02, -4.7421e-03, -4.1220e-03],\n                        [ 7.7216e-03, -7.0857e-03, -4.0999e-03]],\n              \n                       ...,\n              \n                       [[-1.2722e-02,  4.8952e-03,  3.1216e-03],\n                        [-3.6589e-03,  3.9157e-03,  7.6172e-05],\n                        [ 6.6556e-03,  1.3619e-02, -1.0715e-02]],\n              \n                       [[-8.3624e-03,  2.8966e-03,  7.7819e-03],\n                        [ 9.6693e-03, -1.3035e-02, -1.2682e-02],\n                        [-1.2393e-02,  1.4095e-02, -9.9444e-03]],\n              \n                       [[-2.6372e-03, -9.4880e-03, -4.2093e-03],\n                        [ 2.4768e-03,  5.2376e-03, -1.6081e-03],\n                        [ 1.4001e-03,  8.7849e-03, -6.4915e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.1331e-03, -1.0245e-02,  5.5679e-03],\n                        [-1.3925e-02, -5.4960e-03, -6.4326e-03],\n                        [ 1.0665e-03,  9.3625e-03, -1.0900e-02]],\n              \n                       [[-1.2820e-02, -1.4185e-02,  7.6603e-03],\n                        [ 5.5901e-03, -7.7663e-03, -1.3632e-02],\n                        [-7.8664e-03,  3.8328e-03, -6.1660e-03]],\n              \n                       [[ 2.2009e-03,  1.2656e-02, -5.1460e-03],\n                        [-7.3644e-03, -1.2076e-03,  1.9836e-03],\n                        [-1.4580e-03, -8.4020e-04,  1.0106e-02]],\n              \n                       ...,\n              \n                       [[ 7.8239e-03,  8.2156e-03,  5.3135e-03],\n                        [ 7.6519e-03,  2.5644e-03,  9.5596e-03],\n                        [ 1.2521e-02,  7.5805e-03, -1.3987e-02]],\n              \n                       [[ 1.0951e-02,  7.9635e-04, -6.1090e-03],\n                        [ 7.5488e-03,  1.2158e-02, -1.4382e-02],\n                        [-3.4198e-03, -3.9887e-03, -3.8113e-03]],\n              \n                       [[-1.1689e-02,  9.5688e-03, -5.1517e-03],\n                        [-1.1460e-02, -4.0730e-03, -5.6413e-03],\n                        [ 7.0657e-03,  2.6805e-03, -5.1478e-03]]],\n              \n              \n                      [[[-9.6095e-03, -1.3585e-03, -7.0119e-03],\n                        [ 9.6654e-03,  1.0712e-02,  1.0401e-02],\n                        [-3.5123e-03,  1.3850e-02,  1.0464e-02]],\n              \n                       [[-1.1702e-02, -7.7455e-03, -5.3939e-03],\n                        [-1.2093e-02, -8.4871e-03, -3.2977e-03],\n                        [-1.0420e-02,  8.9802e-03, -4.9594e-03]],\n              \n                       [[-1.2320e-02,  2.4707e-03, -2.3200e-03],\n                        [-3.9590e-03,  1.1381e-02, -3.2109e-03],\n                        [-1.9178e-03, -1.3853e-02, -4.3691e-03]],\n              \n                       ...,\n              \n                       [[ 1.0142e-02,  1.3061e-02,  1.1623e-02],\n                        [-5.8694e-03, -6.4008e-04,  1.3774e-02],\n                        [ 6.2873e-03,  3.2907e-03, -8.4393e-03]],\n              \n                       [[ 3.5045e-03,  4.6928e-03,  1.1195e-02],\n                        [ 5.2034e-03, -9.1595e-03,  1.1639e-02],\n                        [-7.8218e-03,  7.5058e-03, -1.4309e-02]],\n              \n                       [[-2.4525e-03, -3.6981e-03,  1.1964e-02],\n                        [-1.2757e-02, -5.8314e-03, -1.1045e-02],\n                        [ 6.1323e-03,  1.4707e-02, -9.2333e-03]]],\n              \n              \n                      [[[ 5.0627e-03,  1.4049e-02,  7.1501e-03],\n                        [-1.3210e-02,  1.1269e-02,  2.2428e-03],\n                        [-9.7158e-03,  5.5631e-03, -1.2279e-02]],\n              \n                       [[-9.5874e-03, -5.4147e-04,  1.4689e-02],\n                        [ 4.4917e-03, -1.3910e-02, -3.7383e-04],\n                        [-7.5597e-03,  9.3203e-03, -7.5512e-03]],\n              \n                       [[-1.4322e-02, -1.1102e-02,  1.1979e-02],\n                        [ 6.4091e-03, -1.3175e-02,  2.6744e-04],\n                        [ 1.1095e-03,  6.2741e-03,  5.1492e-04]],\n              \n                       ...,\n              \n                       [[ 1.3908e-02,  9.8417e-03,  9.4988e-03],\n                        [ 1.1376e-02,  1.9947e-04, -8.0265e-03],\n                        [-1.1771e-02, -1.0298e-02, -2.5397e-03]],\n              \n                       [[-2.3932e-03,  1.3351e-02,  1.0970e-02],\n                        [ 1.2986e-02,  3.9482e-03, -8.2351e-03],\n                        [-1.0508e-02, -3.3115e-03, -8.0658e-03]],\n              \n                       [[-2.9153e-03,  1.4376e-02, -3.0430e-03],\n                        [ 1.3600e-02, -2.1507e-03, -4.3007e-03],\n                        [-3.6526e-03,  8.3328e-03,  8.7380e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-1.3104e-02,  9.6535e-03,  7.0547e-03],\n                        [ 6.8489e-03,  5.6884e-03, -3.3797e-03],\n                        [-1.3077e-02,  1.1413e-02, -8.2186e-03]],\n              \n                       [[-6.4877e-03,  1.2398e-02,  1.4672e-02],\n                        [-2.8377e-03,  2.9911e-03,  8.6744e-03],\n                        [ 4.6708e-03, -1.9309e-03, -1.3963e-02]],\n              \n                       [[-8.8996e-04, -1.3098e-02, -1.2099e-02],\n                        [ 1.1789e-02, -6.3457e-03,  8.4533e-03],\n                        [ 6.9120e-04,  3.7103e-03, -3.9384e-03]],\n              \n                       ...,\n              \n                       [[-1.4631e-02,  7.6187e-03,  1.3055e-02],\n                        [ 8.7348e-03,  2.2455e-03,  1.4252e-02],\n                        [-7.8609e-03,  6.6497e-03,  1.2674e-02]],\n              \n                       [[ 1.0928e-02,  8.1940e-03,  1.4620e-03],\n                        [ 1.1112e-03, -7.0720e-03, -1.2397e-02],\n                        [ 1.3073e-02,  2.2528e-03,  6.1473e-03]],\n              \n                       [[-1.1589e-02, -9.5213e-03, -5.2496e-03],\n                        [-1.1412e-02, -1.3629e-02,  7.4268e-03],\n                        [-6.4922e-03,  1.1146e-02, -9.5554e-03]]],\n              \n              \n                      [[[ 2.3625e-05, -1.3995e-02, -7.6334e-03],\n                        [-9.4009e-03, -9.2042e-03,  5.7072e-03],\n                        [ 9.9287e-03, -5.7740e-03,  8.9586e-03]],\n              \n                       [[ 1.4008e-02, -1.0200e-02,  1.3237e-02],\n                        [ 1.4621e-02, -1.2051e-02,  6.9597e-03],\n                        [ 1.2422e-02, -8.4337e-03, -7.5494e-03]],\n              \n                       [[ 5.7422e-04, -8.9031e-03,  1.4246e-02],\n                        [-3.9909e-03, -1.2648e-05,  7.5228e-03],\n                        [ 4.5517e-03, -8.1091e-03, -2.5926e-03]],\n              \n                       ...,\n              \n                       [[ 1.7802e-03,  1.2118e-02, -8.6626e-04],\n                        [-6.0965e-04, -5.6477e-03, -4.7239e-03],\n                        [-1.4231e-03, -1.1298e-02,  4.0613e-03]],\n              \n                       [[ 2.4961e-05,  4.4265e-03,  1.4223e-02],\n                        [ 2.2458e-03,  1.3728e-02, -1.1796e-02],\n                        [-7.2479e-03,  1.2696e-02,  4.3921e-03]],\n              \n                       [[ 1.4457e-02, -1.0118e-02,  1.3083e-02],\n                        [-7.3051e-03,  1.3544e-02, -1.2357e-02],\n                        [ 3.5746e-03, -1.3268e-02, -9.3003e-03]]],\n              \n              \n                      [[[-3.1621e-03,  1.4471e-02,  1.0941e-02],\n                        [ 1.2192e-02,  5.9600e-03,  7.0732e-03],\n                        [ 1.6198e-03, -1.1914e-02, -1.1316e-02]],\n              \n                       [[-8.1733e-03, -4.6493e-03,  1.3078e-02],\n                        [-5.0052e-03, -1.0437e-02,  9.8975e-03],\n                        [-1.3412e-02, -8.9157e-03,  1.3293e-02]],\n              \n                       [[-5.0194e-03,  6.6695e-03,  3.4234e-04],\n                        [-1.3336e-02,  1.4430e-03,  7.5926e-03],\n                        [-1.0269e-03,  1.0630e-02, -8.4293e-03]],\n              \n                       ...,\n              \n                       [[ 1.0040e-02, -9.6519e-03,  1.1701e-02],\n                        [ 6.5308e-05,  3.5704e-03, -1.2048e-02],\n                        [-9.5033e-03, -1.2604e-02, -1.2307e-02]],\n              \n                       [[-6.6415e-03, -1.0024e-02,  1.3435e-02],\n                        [-6.3868e-03, -1.4265e-02, -2.8581e-03],\n                        [-1.3789e-02,  1.1855e-02,  7.1601e-03]],\n              \n                       [[-9.1238e-03,  4.7032e-05, -2.2387e-03],\n                        [ 4.9879e-04,  7.7738e-03,  5.1973e-03],\n                        [ 3.4793e-03,  9.1406e-03, -9.1121e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.2879e-03,  1.1191e-03, -6.0251e-03],\n                        [-3.2071e-03,  5.4502e-03,  1.2839e-04],\n                        [ 5.8309e-03, -1.3948e-02,  3.9841e-03]],\n              \n                       [[ 1.0795e-02,  5.7343e-03,  3.2873e-03],\n                        [ 5.4282e-03, -1.0134e-02,  3.3486e-03],\n                        [ 5.0658e-03, -1.4290e-02,  3.9768e-03]],\n              \n                       [[-1.4718e-02, -4.8749e-03,  8.8550e-03],\n                        [-1.2116e-02,  3.9706e-03, -1.5341e-04],\n                        [-5.6044e-03,  9.2914e-03,  2.6309e-03]],\n              \n                       ...,\n              \n                       [[ 1.1578e-02,  4.7662e-03,  1.0865e-02],\n                        [-9.9621e-03,  7.2204e-03,  6.7652e-03],\n                        [ 6.1930e-03,  5.5036e-03, -4.8385e-03]],\n              \n                       [[-1.1982e-02,  9.0713e-03, -6.7553e-03],\n                        [ 1.0392e-02, -6.3635e-03, -1.1598e-03],\n                        [ 1.0464e-02,  4.0243e-03,  1.4345e-03]],\n              \n                       [[ 3.2504e-03,  1.4237e-02, -7.7320e-03],\n                        [-1.0245e-02, -8.5657e-03, -1.2735e-02],\n                        [-3.5816e-03,  1.3560e-02, -1.2678e-02]]],\n              \n              \n                      [[[-1.4336e-02, -4.6926e-03,  1.3425e-02],\n                        [ 1.3409e-02, -6.8928e-03, -9.7946e-03],\n                        [-1.4182e-02, -8.6928e-03, -1.4202e-02]],\n              \n                       [[-5.0576e-03, -9.8077e-03,  5.6572e-03],\n                        [-1.4611e-02,  4.4676e-03, -1.3235e-02],\n                        [ 3.6478e-03,  4.1773e-04,  1.4504e-02]],\n              \n                       [[-8.5665e-03, -6.6888e-03, -5.9852e-03],\n                        [ 1.8548e-03,  1.2795e-02, -6.3900e-03],\n                        [-1.3038e-02,  7.2169e-03,  9.2560e-03]],\n              \n                       ...,\n              \n                       [[-5.8375e-03,  8.9250e-03,  1.2109e-02],\n                        [-1.3653e-02,  1.3453e-02, -6.7649e-03],\n                        [-1.2166e-02, -1.3578e-02, -1.2037e-03]],\n              \n                       [[-5.5372e-03, -3.9234e-03, -2.1640e-03],\n                        [-8.1456e-03, -8.1486e-03,  4.8608e-05],\n                        [-7.9746e-03,  3.5861e-03, -5.4110e-03]],\n              \n                       [[ 9.0684e-03, -4.6523e-03,  8.6029e-03],\n                        [-3.5470e-03, -2.6329e-03,  4.1187e-03],\n                        [-1.7698e-03,  3.1339e-03, -1.3087e-02]]],\n              \n              \n                      [[[ 1.3993e-02,  1.0210e-02, -9.8379e-03],\n                        [-3.6017e-03,  1.5505e-03, -7.5702e-03],\n                        [-1.3827e-03, -1.4429e-02, -1.3696e-02]],\n              \n                       [[ 1.2335e-02,  8.3124e-03, -4.6792e-03],\n                        [ 4.8468e-03,  1.3626e-04,  9.8758e-03],\n                        [-2.6817e-03,  3.2997e-03, -9.7415e-04]],\n              \n                       [[ 3.1673e-03, -7.1938e-03, -1.4500e-03],\n                        [-9.1013e-03,  8.4705e-03, -9.5864e-03],\n                        [ 1.6714e-03, -1.4101e-02,  1.1644e-02]],\n              \n                       ...,\n              \n                       [[ 1.4320e-02,  4.4366e-03, -5.8747e-03],\n                        [-8.1688e-03, -6.9629e-03,  3.0317e-04],\n                        [-1.2110e-02, -1.3646e-02, -6.0113e-03]],\n              \n                       [[-3.7647e-04,  7.6979e-03,  3.3129e-03],\n                        [ 7.6917e-03, -1.9005e-03,  6.3914e-03],\n                        [-2.9271e-03,  1.0327e-02, -9.8557e-03]],\n              \n                       [[ 1.1749e-02,  3.9048e-03, -7.2822e-03],\n                        [ 1.4049e-02,  1.3569e-02,  2.5594e-03],\n                        [ 1.2890e-02,  5.6545e-03,  6.2168e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-1.0162e-02, -7.9513e-03, -1.4126e-02],\n                        [-6.2557e-03, -9.7779e-03,  1.0858e-02],\n                        [ 9.1498e-03,  3.0958e-04,  9.0409e-03]],\n              \n                       [[-7.6646e-03, -9.0559e-03, -8.4516e-04],\n                        [-1.2277e-02,  2.7770e-03,  2.4928e-03],\n                        [ 2.1196e-03, -2.7451e-03, -1.3663e-02]],\n              \n                       [[-8.4018e-03,  3.2803e-03, -6.1505e-03],\n                        [ 1.3116e-02,  8.8065e-03,  4.6064e-03],\n                        [ 9.4382e-03, -7.7282e-03,  1.0306e-02]],\n              \n                       ...,\n              \n                       [[ 6.6357e-03, -2.2279e-03, -8.7835e-03],\n                        [-5.1093e-03,  3.9618e-03,  8.8206e-03],\n                        [ 1.4141e-02,  1.3784e-02,  1.1771e-02]],\n              \n                       [[-5.9949e-03, -1.3745e-04,  7.4454e-03],\n                        [-9.2404e-03,  1.3126e-02,  9.9188e-03],\n                        [-6.8859e-03, -1.4138e-02, -9.2198e-03]],\n              \n                       [[-1.4438e-02,  1.1573e-02,  1.1146e-02],\n                        [-8.7031e-03, -4.6383e-03,  7.3338e-03],\n                        [ 1.1381e-02, -9.0583e-03, -2.5293e-03]]],\n              \n              \n                      [[[-1.3852e-02, -6.8651e-03,  2.3293e-03],\n                        [ 1.2269e-02,  6.5710e-03,  3.9793e-03],\n                        [-7.3067e-03, -5.9318e-03, -6.7658e-03]],\n              \n                       [[ 9.5927e-03, -7.6682e-03, -1.3819e-02],\n                        [-9.0626e-03,  3.5546e-03, -8.5062e-03],\n                        [ 1.7261e-03, -2.6030e-03, -1.4632e-02]],\n              \n                       [[ 1.0916e-02,  1.0892e-02,  1.4228e-02],\n                        [ 1.1874e-02, -6.4073e-03, -5.1940e-03],\n                        [-7.4828e-03, -7.4947e-03,  2.5183e-03]],\n              \n                       ...,\n              \n                       [[ 9.7132e-03,  2.0456e-03, -4.0253e-03],\n                        [ 1.9973e-03,  1.2258e-02, -1.3174e-03],\n                        [-9.0220e-03, -8.2095e-03,  1.4117e-02]],\n              \n                       [[-1.0827e-02,  1.4226e-02, -6.4879e-03],\n                        [ 1.2198e-02, -1.2647e-02,  8.6206e-03],\n                        [-2.7980e-03, -2.0266e-03,  5.7236e-03]],\n              \n                       [[-1.2030e-02,  1.2822e-02, -8.4252e-03],\n                        [ 1.1277e-02, -7.0514e-03, -7.5673e-03],\n                        [ 8.1968e-03, -1.2170e-02, -7.3895e-03]]],\n              \n              \n                      [[[ 8.0684e-03,  1.3598e-02, -7.9777e-03],\n                        [-1.4268e-02,  4.8484e-03, -1.1704e-02],\n                        [ 4.8766e-03,  2.9658e-03,  2.0288e-03]],\n              \n                       [[-1.1000e-03, -2.6417e-03,  3.1051e-03],\n                        [ 1.2253e-02, -7.2229e-03, -1.1037e-03],\n                        [ 1.0293e-02,  3.9444e-03, -8.0077e-03]],\n              \n                       [[ 3.6599e-03,  1.3138e-02, -1.0403e-03],\n                        [-1.0804e-02, -2.9224e-03, -7.3381e-04],\n                        [-8.4483e-03, -3.5656e-03,  1.0923e-02]],\n              \n                       ...,\n              \n                       [[ 1.0183e-02, -1.0656e-02,  2.5374e-03],\n                        [-2.4001e-03,  9.3434e-03,  8.0887e-03],\n                        [-3.1470e-03, -3.6860e-03,  6.9349e-03]],\n              \n                       [[-1.4212e-02,  4.7419e-03,  2.2588e-03],\n                        [ 1.2572e-02,  2.5563e-03, -8.1275e-03],\n                        [-3.7703e-03,  2.5945e-03,  5.5602e-03]],\n              \n                       [[-1.2830e-02, -1.0370e-02,  9.9764e-03],\n                        [-1.0848e-02, -9.6209e-03,  8.2907e-03],\n                        [ 4.6423e-03, -4.9777e-03, -8.6183e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.9552e-03,  1.0103e-02, -4.7408e-03],\n                        [-1.3407e-02,  6.5927e-03, -7.2890e-03],\n                        [ 1.2902e-02, -7.3139e-03,  4.8173e-03]],\n              \n                       [[-8.6896e-03, -1.9172e-03,  5.9656e-03],\n                        [-7.3172e-05,  2.9933e-03, -1.1204e-02],\n                        [ 2.1456e-03,  2.6252e-03, -1.3978e-02]],\n              \n                       [[-8.2944e-03, -6.1581e-03,  1.3276e-02],\n                        [ 2.0285e-04, -6.9051e-03,  1.3585e-02],\n                        [-7.9958e-03,  5.1597e-03, -1.1482e-02]],\n              \n                       ...,\n              \n                       [[ 2.9236e-03,  8.6567e-03, -5.6918e-03],\n                        [ 1.2319e-02, -1.2173e-02, -1.1142e-02],\n                        [ 2.1955e-03,  2.1893e-03,  1.0226e-02]],\n              \n                       [[-1.3731e-02,  2.4001e-04,  1.0280e-02],\n                        [ 6.2036e-04,  9.4891e-03, -9.4363e-03],\n                        [ 7.7716e-03, -5.3223e-03, -1.1793e-02]],\n              \n                       [[ 9.0567e-03, -9.4963e-03,  1.2966e-02],\n                        [-3.5606e-03,  6.7127e-03,  9.2346e-03],\n                        [ 1.6610e-04,  9.7832e-04, -3.7458e-03]]],\n              \n              \n                      [[[ 1.8821e-03,  7.0609e-03, -9.9641e-03],\n                        [ 2.8442e-03, -3.4813e-04,  2.8147e-03],\n                        [-7.6718e-03,  1.4098e-03,  3.6991e-03]],\n              \n                       [[-7.4600e-03,  6.1319e-03, -6.6834e-03],\n                        [ 4.6137e-03, -9.7316e-03, -2.1926e-03],\n                        [-5.1150e-03,  8.5056e-03,  1.4168e-02]],\n              \n                       [[ 1.2746e-02,  8.4634e-03,  1.2394e-02],\n                        [ 6.5522e-03, -1.0927e-02, -1.4621e-02],\n                        [ 9.5033e-03,  3.9224e-03,  9.9719e-03]],\n              \n                       ...,\n              \n                       [[-4.0116e-03, -1.4190e-02, -2.6838e-03],\n                        [-1.9716e-04, -1.6087e-03, -2.2089e-03],\n                        [ 1.1347e-02,  5.0595e-04, -2.1228e-03]],\n              \n                       [[ 1.1465e-03,  6.0314e-03, -7.8767e-03],\n                        [-6.6732e-03, -5.0615e-03, -7.0481e-03],\n                        [-3.5145e-03, -1.4674e-02,  9.3690e-03]],\n              \n                       [[-2.1949e-03,  1.8604e-04, -3.8469e-04],\n                        [-6.0911e-03,  4.8625e-03,  9.1291e-04],\n                        [-4.2253e-03, -9.7373e-03,  3.0233e-03]]],\n              \n              \n                      [[[ 1.3092e-02, -9.1652e-03, -1.4018e-02],\n                        [-7.5290e-03, -1.1704e-02,  1.1918e-02],\n                        [-3.6753e-03,  8.3012e-03, -7.8185e-03]],\n              \n                       [[ 1.3660e-02, -1.0051e-04, -4.8537e-03],\n                        [ 4.5250e-03,  1.1501e-02, -1.2260e-02],\n                        [-1.2088e-02, -1.1217e-02, -8.9023e-03]],\n              \n                       [[ 3.9087e-03, -1.1512e-03, -1.3955e-02],\n                        [-2.1982e-03,  1.0120e-02, -5.0558e-03],\n                        [-1.3255e-02,  2.8492e-03, -4.1524e-03]],\n              \n                       ...,\n              \n                       [[-1.2921e-02, -1.8075e-03,  3.1186e-03],\n                        [ 4.0110e-03,  5.9678e-03, -1.5871e-03],\n                        [ 4.0160e-03,  4.9175e-04,  2.2130e-03]],\n              \n                       [[-3.4039e-03, -1.2438e-02,  6.7231e-03],\n                        [ 1.2851e-02, -5.3675e-03,  1.6797e-03],\n                        [-1.3136e-02, -2.5658e-03, -5.8660e-03]],\n              \n                       [[-2.0538e-03,  7.5002e-04,  6.9986e-03],\n                        [ 1.3422e-02, -9.2835e-04,  4.6620e-03],\n                        [-1.3815e-02,  5.7040e-03, -6.6107e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('up1.conv.double_conv.0.weight',\n              tensor([[[[ 6.0052e-03, -6.1578e-03, -8.6970e-03],\n                        [ 1.6955e-03, -7.3866e-03,  5.3448e-03],\n                        [ 5.5082e-03,  9.1673e-03,  1.0191e-02]],\n              \n                       [[-3.7926e-03,  5.7925e-03,  1.0316e-02],\n                        [ 9.6915e-03,  8.8699e-03,  5.3047e-03],\n                        [ 5.0500e-03,  4.6066e-03,  1.0278e-02]],\n              \n                       [[-7.2442e-04, -7.9003e-03, -9.7175e-03],\n                        [ 4.6586e-04, -3.6655e-03, -9.5510e-03],\n                        [-9.1740e-03, -7.8502e-03, -5.3606e-03]],\n              \n                       ...,\n              \n                       [[ 2.1322e-03, -9.4887e-05, -4.9738e-03],\n                        [-6.1662e-03,  1.3903e-03, -7.2019e-03],\n                        [ 5.4206e-03,  8.7880e-03,  4.3695e-03]],\n              \n                       [[ 3.3114e-03, -4.8001e-03, -2.7326e-03],\n                        [-3.7524e-03,  7.7908e-03, -8.4219e-03],\n                        [ 2.0721e-03,  7.5771e-03,  6.9718e-03]],\n              \n                       [[-9.9150e-03, -2.1330e-03,  7.4038e-03],\n                        [-6.3372e-03, -8.1195e-03,  1.6034e-03],\n                        [ 5.8172e-03, -1.3327e-03, -7.0786e-03]]],\n              \n              \n                      [[[-4.7313e-03, -2.5325e-03, -6.1366e-03],\n                        [ 1.1530e-03, -5.3506e-03, -6.1344e-04],\n                        [ 2.7635e-03, -6.2766e-03,  4.6419e-03]],\n              \n                       [[ 4.3768e-03, -4.0070e-03,  8.7607e-03],\n                        [-8.9397e-03, -9.8516e-03, -2.8273e-03],\n                        [-3.7660e-03,  3.6542e-03,  1.0126e-02]],\n              \n                       [[-6.7512e-03,  6.0833e-03,  2.7166e-03],\n                        [ 9.3578e-04,  5.1147e-03,  6.3890e-03],\n                        [ 1.5687e-04,  7.4274e-03, -8.3365e-03]],\n              \n                       ...,\n              \n                       [[-4.8921e-03, -5.4093e-03,  5.6688e-03],\n                        [ 3.1983e-03,  3.9314e-03, -8.9410e-03],\n                        [ 6.5762e-03, -9.7403e-03, -4.1459e-03]],\n              \n                       [[ 8.1715e-03,  5.4453e-03, -7.9296e-03],\n                        [ 1.6348e-03, -1.7733e-04,  1.1809e-03],\n                        [-6.2941e-03,  6.1941e-03,  1.7227e-03]],\n              \n                       [[ 9.5111e-03, -8.0376e-03, -3.7345e-03],\n                        [ 5.4716e-03, -3.7542e-03,  2.9980e-03],\n                        [-7.5362e-03,  8.4094e-03,  8.9098e-03]]],\n              \n              \n                      [[[-9.6740e-03, -8.1277e-03,  3.9857e-03],\n                        [-3.5163e-03,  8.6464e-03,  4.2643e-03],\n                        [-5.0144e-03, -9.8802e-04,  4.8284e-04]],\n              \n                       [[-6.5739e-03,  9.1206e-03,  5.8876e-03],\n                        [-4.3970e-03,  3.9926e-04,  4.9571e-03],\n                        [-3.2965e-03,  4.1399e-04, -2.7867e-03]],\n              \n                       [[-4.9022e-03, -7.1855e-04,  5.2022e-04],\n                        [-3.8415e-03,  7.9072e-03,  1.0071e-02],\n                        [-6.5128e-03, -3.6828e-03, -8.3628e-03]],\n              \n                       ...,\n              \n                       [[ 8.5856e-03, -7.1988e-03,  9.1629e-03],\n                        [ 9.4906e-03, -6.0381e-03,  6.3775e-04],\n                        [ 3.2705e-03, -4.2573e-03,  7.2144e-03]],\n              \n                       [[-2.7434e-03, -5.6575e-03,  7.0926e-03],\n                        [ 6.5038e-03,  1.0222e-02,  7.6083e-03],\n                        [ 8.3256e-03,  7.9641e-03, -6.8926e-03]],\n              \n                       [[ 3.2581e-03, -3.4153e-03,  1.7781e-04],\n                        [-4.7329e-03, -2.7371e-03, -7.9243e-03],\n                        [-7.3951e-03, -3.6213e-03,  3.8721e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.3754e-03,  1.0256e-02, -9.6938e-03],\n                        [-5.2090e-03,  1.1899e-03,  6.6328e-03],\n                        [-6.4318e-03,  7.6097e-03,  3.2797e-03]],\n              \n                       [[-7.0052e-03,  4.5905e-03, -8.9286e-03],\n                        [-8.2543e-03, -5.1691e-03, -5.8590e-03],\n                        [ 8.7791e-03,  5.7680e-03, -8.9067e-03]],\n              \n                       [[-7.6416e-03, -9.3266e-03,  9.4770e-03],\n                        [ 1.4398e-03,  4.5831e-03, -3.4448e-03],\n                        [-4.5923e-03, -5.7610e-03, -4.3103e-03]],\n              \n                       ...,\n              \n                       [[-2.0614e-03, -8.5129e-03, -8.4951e-03],\n                        [ 2.6566e-03,  9.1776e-03,  2.6760e-03],\n                        [-1.7022e-04,  3.6392e-03,  5.0875e-03]],\n              \n                       [[-2.9073e-03, -7.8702e-03, -1.2811e-03],\n                        [-8.3429e-03, -8.4082e-03,  4.3443e-03],\n                        [-6.5337e-03,  3.0448e-03, -3.2978e-03]],\n              \n                       [[-6.3634e-03, -6.4584e-03, -9.4520e-03],\n                        [ 6.3613e-03,  1.3895e-03,  6.7184e-03],\n                        [ 1.9717e-04,  3.0919e-03, -9.3850e-03]]],\n              \n              \n                      [[[-7.3347e-03,  3.7111e-03, -1.4600e-03],\n                        [-8.9929e-03, -1.0001e-02, -9.7608e-03],\n                        [ 4.9672e-03, -5.1917e-03, -9.9102e-03]],\n              \n                       [[ 7.6933e-03, -4.9824e-03, -8.9469e-03],\n                        [ 4.8704e-03, -1.6437e-03,  8.8097e-03],\n                        [-3.0993e-03, -5.9778e-03, -3.1651e-03]],\n              \n                       [[ 8.6893e-03,  9.8990e-03,  7.1665e-03],\n                        [ 7.6924e-03, -1.0816e-03,  9.3137e-03],\n                        [-4.7224e-03, -3.9862e-03, -7.0841e-03]],\n              \n                       ...,\n              \n                       [[ 7.1673e-03,  5.2882e-03,  5.8690e-03],\n                        [ 4.2807e-04, -4.7009e-04,  9.8658e-03],\n                        [-3.6831e-03, -3.5520e-03,  4.0485e-03]],\n              \n                       [[-5.5522e-03,  9.4766e-03,  8.2692e-03],\n                        [-3.1187e-03, -8.5105e-03,  8.7861e-03],\n                        [-7.3462e-03,  5.8684e-03,  9.6273e-03]],\n              \n                       [[-3.7102e-03,  7.7810e-03, -1.4194e-03],\n                        [-4.0797e-03, -8.0059e-03,  8.5199e-03],\n                        [-9.1947e-03,  3.5915e-03, -4.6602e-03]]],\n              \n              \n                      [[[-1.3775e-03,  6.0666e-04, -6.9796e-04],\n                        [ 6.7400e-03,  6.6210e-03,  2.7429e-03],\n                        [-8.8243e-03, -9.8390e-03,  2.4116e-03]],\n              \n                       [[ 4.7119e-03,  3.2005e-03,  5.9726e-03],\n                        [ 9.5476e-03,  1.6969e-03,  9.7832e-03],\n                        [-2.6481e-03,  7.0522e-03, -7.9863e-03]],\n              \n                       [[ 4.9707e-03,  9.5256e-04, -1.3029e-03],\n                        [-6.9370e-03, -1.0068e-02,  1.0652e-03],\n                        [-2.0503e-03,  8.6360e-03, -1.5661e-03]],\n              \n                       ...,\n              \n                       [[-6.5328e-03, -9.1420e-04,  5.5855e-03],\n                        [ 8.4739e-03, -4.1916e-03,  1.0212e-02],\n                        [ 1.0342e-02, -8.0135e-03, -1.1019e-04]],\n              \n                       [[ 4.2931e-03,  4.7278e-03,  8.9549e-03],\n                        [ 7.2504e-03,  4.6937e-03, -6.7444e-03],\n                        [-1.0244e-02,  2.1343e-03, -3.2979e-03]],\n              \n                       [[ 9.3904e-03, -7.6412e-03,  2.0035e-03],\n                        [-6.8808e-03,  1.0404e-02,  9.5906e-03],\n                        [ 5.1486e-03,  1.8948e-03, -1.0138e-03]]]])),\n             ('up1.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up1.conv.double_conv.3.weight',\n              tensor([[[[ 4.6532e-03, -7.6019e-03, -2.2726e-03],\n                        [ 4.6818e-03,  1.2958e-02,  7.4474e-03],\n                        [ 1.0656e-02,  7.3169e-03,  1.4385e-02]],\n              \n                       [[-7.1003e-03,  5.6198e-03,  1.1528e-02],\n                        [ 1.2165e-02,  2.7467e-03,  1.2221e-02],\n                        [ 1.0123e-02, -7.3388e-04, -1.3558e-02]],\n              \n                       [[ 6.1051e-04, -1.0071e-02,  1.0367e-02],\n                        [ 5.4181e-03,  3.2388e-03,  8.1533e-04],\n                        [ 9.9759e-03, -8.9243e-03, -1.0614e-02]],\n              \n                       ...,\n              \n                       [[-1.1593e-02,  4.4562e-03, -1.2794e-02],\n                        [-2.0847e-03,  8.4393e-03, -3.0718e-03],\n                        [ 1.2095e-02,  9.6634e-03, -6.1204e-03]],\n              \n                       [[-8.5692e-03, -5.3203e-03, -6.0301e-03],\n                        [-1.3060e-02, -4.9878e-03,  1.3536e-02],\n                        [-3.0446e-03, -3.7271e-03,  1.8943e-03]],\n              \n                       [[ 9.1236e-03,  6.2085e-03, -5.2066e-03],\n                        [ 7.0768e-03,  5.8855e-03, -1.3525e-02],\n                        [ 1.2969e-02, -3.1656e-03, -9.7805e-03]]],\n              \n              \n                      [[[-1.3448e-02, -1.4380e-02,  3.3876e-03],\n                        [-6.9893e-03, -8.7593e-03,  3.4935e-03],\n                        [ 6.0252e-03,  6.2473e-03, -7.2960e-04]],\n              \n                       [[ 1.2521e-03, -1.2604e-02, -1.4122e-02],\n                        [-7.8812e-03,  1.2843e-03,  3.4510e-03],\n                        [-8.0826e-03, -6.0928e-03,  1.4071e-02]],\n              \n                       [[ 1.2236e-02, -2.2066e-03,  7.5802e-03],\n                        [-3.4579e-03, -8.4028e-03,  1.2992e-02],\n                        [ 1.5273e-03,  9.6915e-03, -2.7779e-03]],\n              \n                       ...,\n              \n                       [[-9.7299e-03,  7.2240e-03,  3.2073e-04],\n                        [ 5.1952e-03,  1.3993e-02,  5.8187e-03],\n                        [-3.9472e-03,  9.5075e-03,  9.9508e-03]],\n              \n                       [[ 3.8860e-03, -7.5956e-03, -6.7716e-03],\n                        [-6.3491e-03,  1.1731e-02, -4.6717e-03],\n                        [ 5.6204e-04, -4.5982e-03, -1.3072e-03]],\n              \n                       [[-9.9374e-03, -1.4691e-03,  9.6274e-03],\n                        [-3.4154e-03, -9.9765e-03,  4.7587e-03],\n                        [ 1.1309e-02,  1.2087e-03,  1.1953e-02]]],\n              \n              \n                      [[[ 1.2883e-02, -7.2949e-03, -4.8458e-03],\n                        [ 9.7466e-03,  1.1054e-02,  1.2237e-02],\n                        [ 9.9405e-03,  1.4726e-02,  2.0744e-03]],\n              \n                       [[ 1.0789e-02,  1.3618e-02,  1.4625e-02],\n                        [-1.9228e-03,  5.1298e-03,  5.3312e-04],\n                        [ 1.4351e-02,  8.0309e-03, -1.3372e-02]],\n              \n                       [[-3.1131e-03, -6.5674e-04, -1.0796e-02],\n                        [-9.3562e-03,  6.5610e-03, -1.3210e-02],\n                        [ 7.9644e-03,  1.0064e-03,  6.2818e-04]],\n              \n                       ...,\n              \n                       [[-2.9593e-03, -3.4946e-03, -4.1973e-03],\n                        [ 1.2073e-02,  7.9237e-03,  9.7770e-05],\n                        [-4.5093e-03, -8.0024e-03, -3.3877e-03]],\n              \n                       [[ 4.1504e-04, -6.3685e-03,  2.9286e-04],\n                        [-1.4368e-02,  5.2549e-04, -1.2686e-02],\n                        [ 1.6020e-03,  4.4607e-03,  7.5159e-03]],\n              \n                       [[-6.6873e-03,  5.1561e-05,  8.2160e-03],\n                        [-7.2157e-03, -9.4008e-04, -9.3220e-03],\n                        [ 1.3272e-03,  1.3943e-03, -1.0126e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.3756e-03,  1.2603e-02,  1.0009e-02],\n                        [ 1.3332e-02,  2.2436e-03, -2.6538e-03],\n                        [ 1.2150e-02, -6.4561e-03, -1.2219e-02]],\n              \n                       [[-8.2563e-03,  1.4514e-02, -6.5334e-03],\n                        [ 1.0584e-02,  7.2743e-03, -7.7184e-03],\n                        [-1.3945e-02, -3.9507e-04, -1.3207e-02]],\n              \n                       [[-1.1936e-02,  1.2723e-02,  1.4794e-03],\n                        [-9.2238e-03,  1.2513e-02, -1.2755e-02],\n                        [-2.3135e-04, -1.2050e-02,  1.0637e-02]],\n              \n                       ...,\n              \n                       [[-1.7315e-03, -1.1583e-02, -6.2004e-03],\n                        [-3.6829e-03, -7.5475e-03, -1.1467e-02],\n                        [-1.2565e-04, -1.6956e-03,  7.3251e-03]],\n              \n                       [[ 4.5195e-03,  9.6949e-03, -1.1593e-02],\n                        [-1.0726e-02, -4.3706e-03, -1.0075e-02],\n                        [-1.1938e-02, -6.4125e-03,  5.7692e-04]],\n              \n                       [[-1.1380e-02, -9.5971e-03, -1.3420e-02],\n                        [ 1.0888e-02, -1.0871e-02,  4.6657e-05],\n                        [-2.8069e-03, -1.0725e-02,  2.2430e-03]]],\n              \n              \n                      [[[ 1.1839e-02,  1.3359e-02, -2.2681e-03],\n                        [ 1.8450e-03,  5.9289e-04, -1.2829e-02],\n                        [ 1.4203e-02,  2.5810e-03, -1.1913e-02]],\n              \n                       [[-1.3077e-02, -1.4014e-02, -4.2100e-03],\n                        [-9.9503e-03,  1.1108e-02, -3.2723e-03],\n                        [ 2.0312e-03,  4.5349e-03,  1.3859e-02]],\n              \n                       [[-1.4575e-02,  1.1122e-02, -7.5780e-03],\n                        [-3.8330e-03, -9.8024e-04,  5.9586e-03],\n                        [ 9.8220e-03, -6.8341e-03,  1.2393e-02]],\n              \n                       ...,\n              \n                       [[-3.4048e-03,  1.3819e-02, -2.6837e-03],\n                        [ 1.1734e-02,  1.4311e-03, -1.2245e-02],\n                        [-8.3261e-03,  1.3495e-02,  2.9223e-03]],\n              \n                       [[-1.2962e-02, -7.3929e-03, -7.3878e-03],\n                        [-1.7338e-03, -6.7076e-03, -7.7754e-03],\n                        [ 1.4972e-03, -6.4253e-03, -1.4126e-02]],\n              \n                       [[ 1.4451e-02, -4.8099e-03,  5.7255e-03],\n                        [-5.8516e-03,  4.0733e-03,  1.0094e-02],\n                        [ 8.1309e-04,  5.1471e-03,  5.1509e-03]]],\n              \n              \n                      [[[ 9.8223e-04,  1.1245e-02,  1.1552e-02],\n                        [-7.6653e-03,  6.1365e-04, -4.2670e-03],\n                        [ 5.1350e-03,  1.4145e-02, -8.8357e-04]],\n              \n                       [[ 1.2253e-02,  1.0491e-02, -1.4184e-02],\n                        [ 2.6855e-03,  7.4216e-03, -4.6636e-03],\n                        [-1.0291e-02, -1.2930e-02, -3.5078e-04]],\n              \n                       [[ 4.5516e-03, -9.4295e-03,  9.7718e-03],\n                        [-7.6455e-03,  1.0235e-02,  1.2030e-03],\n                        [-2.7815e-03,  6.6763e-03, -8.7617e-03]],\n              \n                       ...,\n              \n                       [[-9.8976e-03,  1.2484e-02, -2.8897e-03],\n                        [ 4.3479e-03,  8.9747e-03,  8.7985e-04],\n                        [ 1.2341e-02,  4.2616e-04,  4.2251e-03]],\n              \n                       [[ 1.2692e-02, -1.7026e-03,  7.1434e-03],\n                        [ 1.1852e-02, -1.1433e-02, -1.3874e-02],\n                        [ 1.2581e-02, -3.8352e-03, -7.5201e-04]],\n              \n                       [[-4.7592e-04, -3.9157e-03,  3.5884e-03],\n                        [-3.2631e-03, -1.6258e-03, -1.0496e-02],\n                        [ 1.3847e-03, -5.7536e-04, -1.0432e-02]]]])),\n             ('up1.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.0.weight',\n              tensor([[[[-2.1518e-03,  1.0631e-02,  1.2601e-02],\n                        [ 9.9365e-03,  8.6478e-03, -1.2200e-02],\n                        [-8.7199e-03, -1.3551e-04,  2.7872e-03]],\n              \n                       [[ 1.0136e-02,  5.1465e-03, -7.2739e-03],\n                        [-1.0549e-02, -4.3726e-03, -1.0110e-02],\n                        [-1.2202e-02,  8.1444e-03,  1.2508e-02]],\n              \n                       [[-1.1105e-02, -3.2792e-03,  1.1186e-02],\n                        [-8.2915e-03,  8.8182e-03,  1.1263e-02],\n                        [-4.4057e-03,  8.6805e-03, -9.5922e-03]],\n              \n                       ...,\n              \n                       [[ 6.3221e-03, -1.2953e-02,  5.1380e-03],\n                        [ 2.9260e-04, -1.0260e-02,  6.4162e-03],\n                        [-5.8944e-03,  4.6316e-03,  1.4742e-03]],\n              \n                       [[-1.0956e-02, -3.5614e-03, -3.6777e-03],\n                        [ 1.2266e-02, -3.7897e-05, -1.1044e-02],\n                        [ 5.1852e-03,  8.2570e-03,  1.3097e-03]],\n              \n                       [[-2.4492e-03, -3.5821e-03, -1.4560e-02],\n                        [ 9.1054e-03, -4.1931e-03,  9.5132e-03],\n                        [ 5.1267e-03,  1.1881e-02,  5.6942e-04]]],\n              \n              \n                      [[[ 1.0638e-02, -5.4433e-03, -3.7759e-03],\n                        [ 1.1677e-02, -4.1737e-03, -1.0637e-02],\n                        [-1.6576e-03, -2.1487e-03, -1.1114e-02]],\n              \n                       [[ 1.8396e-03,  1.3266e-02,  6.8261e-03],\n                        [ 3.9165e-03, -8.8550e-03,  1.4806e-03],\n                        [ 7.0773e-04,  1.1756e-02, -1.0292e-02]],\n              \n                       [[ 1.3127e-02,  4.8850e-03,  2.1176e-03],\n                        [ 2.1249e-03, -5.7832e-03, -1.3140e-02],\n                        [ 8.5454e-03, -8.9114e-03, -1.3402e-02]],\n              \n                       ...,\n              \n                       [[ 1.1088e-02,  7.2383e-03,  1.2047e-02],\n                        [ 9.5457e-03,  1.3826e-02, -2.5452e-03],\n                        [ 9.1783e-03,  1.0598e-02, -8.6740e-04]],\n              \n                       [[ 4.5989e-03, -1.4716e-03, -1.2077e-02],\n                        [-9.6809e-04, -1.2336e-02,  9.3714e-04],\n                        [ 3.9654e-03, -7.3955e-03, -1.2232e-02]],\n              \n                       [[ 5.6303e-03, -8.0869e-03, -2.5287e-03],\n                        [ 1.8057e-03, -1.1487e-02, -2.8659e-03],\n                        [ 4.0015e-03, -1.2479e-02, -1.1998e-02]]],\n              \n              \n                      [[[ 9.4689e-03, -7.2081e-03,  1.4072e-03],\n                        [ 1.2932e-02, -3.2592e-03, -8.7485e-03],\n                        [ 9.2945e-03,  4.6018e-03,  4.0055e-03]],\n              \n                       [[-1.3764e-02, -4.2907e-03,  3.2547e-03],\n                        [ 3.3341e-03,  1.1304e-03, -1.2234e-02],\n                        [-1.3467e-02, -5.6734e-03,  7.4354e-03]],\n              \n                       [[-5.6023e-03, -2.8761e-03, -1.4718e-02],\n                        [ 1.0713e-02, -1.6779e-03, -1.1996e-02],\n                        [-1.2827e-02,  1.0703e-02, -9.7047e-03]],\n              \n                       ...,\n              \n                       [[ 3.2607e-03, -8.0475e-03,  6.1829e-03],\n                        [-2.9395e-03,  3.3496e-03,  5.1071e-03],\n                        [ 5.9723e-03,  4.7608e-03, -1.6388e-03]],\n              \n                       [[-4.3904e-03,  7.7792e-03, -1.2428e-02],\n                        [-3.2456e-03,  5.5866e-03, -1.4352e-02],\n                        [-1.1821e-02,  2.6534e-03,  7.5290e-03]],\n              \n                       [[ 4.6186e-03, -6.2310e-03,  1.1741e-02],\n                        [-1.4587e-02,  9.7592e-03,  1.2688e-02],\n                        [ 4.2982e-03,  5.2313e-03, -1.2822e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1165e-02,  7.8691e-04, -9.3187e-03],\n                        [-7.7603e-03, -3.0258e-03, -9.7707e-03],\n                        [ 7.5438e-03,  1.4036e-02,  1.0273e-02]],\n              \n                       [[-1.3591e-02,  7.4804e-03, -4.6866e-04],\n                        [-1.3815e-02,  1.2045e-02, -9.8406e-03],\n                        [ 1.0759e-02,  6.9177e-03, -1.3892e-02]],\n              \n                       [[ 1.2857e-02, -4.8749e-04,  9.5570e-03],\n                        [ 2.7064e-03, -8.0672e-03,  1.0471e-02],\n                        [ 5.2177e-03,  1.2281e-02, -6.2795e-03]],\n              \n                       ...,\n              \n                       [[ 1.0430e-03,  1.3958e-02, -1.1441e-02],\n                        [-1.0572e-02,  4.8599e-04, -8.1871e-03],\n                        [ 8.7779e-03,  8.1478e-03, -3.1877e-03]],\n              \n                       [[ 7.4461e-03,  2.9228e-03, -1.0984e-02],\n                        [ 9.8613e-03,  1.3081e-02,  1.2413e-02],\n                        [ 1.2035e-02, -3.1168e-03, -7.5135e-03]],\n              \n                       [[ 8.0283e-03, -4.2646e-03, -7.9841e-03],\n                        [-1.9161e-05, -6.6800e-03, -1.6066e-04],\n                        [ 9.5017e-03, -1.7248e-03,  7.0304e-03]]],\n              \n              \n                      [[[ 3.5356e-03, -7.6512e-03, -8.9665e-03],\n                        [-4.8910e-03,  2.0278e-03,  7.1160e-03],\n                        [-3.0881e-03, -4.1455e-03,  1.1920e-02]],\n              \n                       [[ 3.7466e-03, -3.9381e-03,  1.4420e-02],\n                        [-1.3107e-02, -5.7352e-03,  6.8331e-03],\n                        [-6.0296e-03,  1.2593e-02,  8.2828e-03]],\n              \n                       [[-9.1421e-03,  1.2051e-02,  9.1719e-03],\n                        [-2.3811e-03, -1.4370e-02, -1.1317e-02],\n                        [-5.8528e-03,  5.9658e-03, -7.2074e-03]],\n              \n                       ...,\n              \n                       [[ 1.4338e-02,  1.0304e-02, -6.8373e-03],\n                        [ 2.6406e-03, -2.9580e-03, -2.9774e-03],\n                        [-6.9043e-03,  1.4699e-02, -7.5011e-03]],\n              \n                       [[ 9.0359e-03, -7.4744e-03,  2.7057e-03],\n                        [-1.0241e-03, -9.2485e-03, -3.4580e-03],\n                        [ 3.8833e-03,  7.4134e-03, -1.1881e-02]],\n              \n                       [[-1.9624e-03,  2.7043e-03, -4.4755e-04],\n                        [-1.1581e-02, -1.3765e-02, -8.7221e-03],\n                        [ 1.3774e-02, -1.1876e-02, -1.0575e-02]]],\n              \n              \n                      [[[-1.7063e-04,  6.7622e-04,  8.8984e-03],\n                        [-5.9551e-03,  1.2280e-02, -1.2928e-02],\n                        [-1.2386e-02,  1.3566e-02,  3.3778e-03]],\n              \n                       [[-4.9461e-03, -1.1765e-03, -5.0370e-03],\n                        [-3.2352e-03,  8.2034e-03,  1.2355e-02],\n                        [ 3.5783e-03,  1.1220e-02, -1.3388e-02]],\n              \n                       [[-1.8399e-03,  5.9302e-03,  9.6810e-03],\n                        [ 5.0733e-03,  1.0453e-02, -4.8722e-03],\n                        [-1.3514e-02, -1.1929e-03,  1.7507e-03]],\n              \n                       ...,\n              \n                       [[-1.4605e-03,  2.2461e-03, -8.0156e-03],\n                        [ 1.0985e-02,  5.1273e-03, -1.1668e-02],\n                        [ 1.4627e-02,  2.7758e-03,  7.2483e-03]],\n              \n                       [[ 1.3621e-02, -4.5283e-03,  6.4443e-04],\n                        [ 1.0748e-02,  1.1094e-02,  1.4675e-02],\n                        [-9.0625e-03, -6.1689e-03, -2.2046e-03]],\n              \n                       [[-1.4035e-03, -1.3366e-02,  5.8688e-03],\n                        [ 2.4954e-04,  7.3011e-03,  8.3442e-03],\n                        [-2.7433e-04, -1.0389e-02,  3.1839e-03]]]])),\n             ('up2.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.3.weight',\n              tensor([[[[ 7.9497e-03, -1.7790e-02, -1.7096e-02],\n                        [-1.6327e-02,  4.0280e-03, -1.9224e-02],\n                        [-4.1614e-03,  2.0345e-02, -1.3011e-02]],\n              \n                       [[-1.1634e-02,  5.5307e-03, -1.6266e-02],\n                        [-1.1103e-02,  8.3270e-03, -1.5757e-02],\n                        [ 1.5221e-02, -1.2837e-02,  9.6909e-04]],\n              \n                       [[-1.6213e-02,  6.1893e-03,  1.9967e-02],\n                        [-1.0630e-02,  2.0123e-02,  6.5128e-03],\n                        [-2.0276e-02,  2.0401e-02,  1.5855e-02]],\n              \n                       ...,\n              \n                       [[ 1.4602e-02, -9.3187e-03,  1.2791e-02],\n                        [ 3.5288e-03,  8.2964e-03,  1.7589e-02],\n                        [ 4.4983e-03, -4.8159e-04, -3.6260e-03]],\n              \n                       [[-8.9474e-05,  1.3904e-02,  1.9019e-02],\n                        [-1.9988e-02, -1.3111e-02,  6.4248e-04],\n                        [ 6.8580e-04,  1.7128e-03,  5.4387e-03]],\n              \n                       [[ 1.4890e-02, -9.2215e-03, -5.8313e-03],\n                        [ 1.1482e-02, -1.2943e-02,  1.7208e-02],\n                        [-2.3544e-03,  8.3377e-04, -1.4550e-02]]],\n              \n              \n                      [[[-2.5915e-03, -3.9138e-03, -1.6308e-02],\n                        [-1.9927e-02, -9.3398e-03, -1.9362e-02],\n                        [-1.4066e-02,  9.7209e-03,  1.6551e-02]],\n              \n                       [[-1.9409e-02, -1.3963e-02,  6.9585e-03],\n                        [-5.1612e-04, -1.9914e-02,  1.8270e-02],\n                        [-7.2831e-03,  1.2477e-02, -2.8120e-04]],\n              \n                       [[-1.5371e-02,  9.3540e-04,  9.9296e-03],\n                        [-1.0750e-02, -3.9004e-03,  1.7460e-02],\n                        [-1.9144e-02,  2.0190e-02, -1.1884e-02]],\n              \n                       ...,\n              \n                       [[ 7.7697e-03,  1.9071e-02, -3.6815e-03],\n                        [ 5.6426e-03, -8.5833e-03,  1.6836e-02],\n                        [ 1.8768e-03, -2.5059e-04,  8.1764e-03]],\n              \n                       [[ 5.9330e-03, -1.4364e-02, -3.9514e-03],\n                        [ 1.9684e-02, -1.4239e-02, -2.0091e-02],\n                        [ 2.0407e-02,  1.8737e-02, -5.8489e-03]],\n              \n                       [[ 5.4501e-03,  1.1028e-02, -1.9625e-02],\n                        [-1.3838e-02, -8.5165e-03,  2.6146e-03],\n                        [-6.4134e-03,  1.4367e-02,  1.4903e-02]]],\n              \n              \n                      [[[-1.1303e-03,  3.3091e-03, -6.1916e-03],\n                        [-1.5099e-02, -2.1207e-04,  4.5621e-03],\n                        [ 1.7857e-02, -2.7128e-03, -5.4803e-03]],\n              \n                       [[ 5.9743e-03,  2.0597e-02,  6.6697e-03],\n                        [ 9.8200e-03,  1.3099e-02,  1.7841e-03],\n                        [-1.6089e-02,  1.5824e-02,  8.0234e-04]],\n              \n                       [[-7.2984e-03,  1.2674e-02,  1.8605e-02],\n                        [ 3.9323e-03,  8.1922e-03, -9.3463e-04],\n                        [-1.9702e-02,  1.4019e-02,  1.6300e-02]],\n              \n                       ...,\n              \n                       [[ 1.6479e-02,  1.6218e-02, -1.5242e-02],\n                        [-3.6273e-03,  5.0512e-03,  1.1426e-02],\n                        [ 7.1217e-03,  7.2147e-03, -2.5175e-03]],\n              \n                       [[ 1.5327e-02,  1.4072e-02, -1.7085e-02],\n                        [ 4.0818e-04, -1.7114e-02, -3.8038e-03],\n                        [-1.5342e-02, -2.0213e-02, -1.3697e-02]],\n              \n                       [[-2.0410e-02, -1.5656e-02,  5.8427e-03],\n                        [-3.8405e-03,  1.0923e-02, -1.2858e-02],\n                        [ 1.8628e-02,  4.0466e-03, -2.0422e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.9150e-02,  1.2267e-02,  1.7782e-02],\n                        [ 1.3684e-02, -1.9804e-02, -9.2421e-03],\n                        [ 1.7435e-02,  1.7343e-02, -1.8515e-02]],\n              \n                       [[ 1.8531e-02, -6.2842e-03, -2.1436e-03],\n                        [-6.2577e-03,  1.8332e-02,  1.9857e-02],\n                        [-1.0869e-02, -5.4065e-03,  1.8648e-02]],\n              \n                       [[-9.8150e-03, -1.9312e-02, -5.3483e-04],\n                        [ 2.2209e-03,  2.0530e-02, -6.2797e-03],\n                        [ 3.1732e-03,  1.7359e-02,  1.0300e-02]],\n              \n                       ...,\n              \n                       [[ 5.3619e-03, -8.6172e-03,  1.9207e-02],\n                        [ 1.2767e-02, -3.0699e-03, -9.6391e-03],\n                        [-8.9599e-04,  6.0747e-03,  4.0384e-03]],\n              \n                       [[-5.2875e-03,  6.5115e-04,  5.4017e-03],\n                        [ 1.5804e-03,  8.6046e-03,  1.7447e-02],\n                        [ 7.5348e-03,  1.8965e-02,  1.9957e-02]],\n              \n                       [[-1.0331e-02, -1.1320e-02,  1.5131e-02],\n                        [ 2.9035e-03,  1.1799e-02, -1.5353e-03],\n                        [-8.3366e-03,  9.3031e-03, -1.7604e-02]]],\n              \n              \n                      [[[ 1.4307e-02,  1.1860e-02,  5.1069e-03],\n                        [-1.5284e-02,  8.2293e-03, -9.5887e-03],\n                        [ 5.3585e-03,  2.0224e-03,  1.5437e-02]],\n              \n                       [[ 1.2629e-03,  9.5884e-03,  1.5362e-02],\n                        [-4.8209e-03,  1.4933e-02, -1.2048e-02],\n                        [-3.0520e-05, -1.3378e-02, -2.1463e-03]],\n              \n                       [[-1.1527e-02,  7.7163e-03, -1.2359e-02],\n                        [-2.0476e-02, -1.7779e-02, -6.4546e-03],\n                        [ 3.1536e-03, -1.0851e-04, -1.9629e-02]],\n              \n                       ...,\n              \n                       [[-3.6267e-03, -1.7496e-02, -1.8531e-02],\n                        [ 3.0812e-03, -4.4989e-03, -5.3328e-03],\n                        [-3.5008e-03, -1.0352e-02,  2.0659e-02]],\n              \n                       [[-4.5241e-03,  6.3328e-03,  8.7361e-03],\n                        [-6.1625e-03, -1.3019e-02,  1.6934e-02],\n                        [-3.4158e-03,  8.9188e-03, -1.3646e-02]],\n              \n                       [[ 1.7996e-02,  1.7854e-02, -1.5007e-02],\n                        [ 2.2617e-04,  1.8391e-02,  2.0008e-02],\n                        [-1.4899e-03,  1.6801e-02,  2.3108e-03]]],\n              \n              \n                      [[[-1.5664e-02,  4.3163e-03,  1.2885e-02],\n                        [ 2.6682e-03,  1.6914e-02,  3.5899e-03],\n                        [ 1.9674e-02, -1.1662e-02, -1.2853e-02]],\n              \n                       [[-3.9540e-04, -1.7787e-02,  9.8214e-03],\n                        [ 1.3250e-02, -2.1693e-03, -4.9136e-03],\n                        [ 1.9610e-02,  1.1362e-03,  2.0132e-02]],\n              \n                       [[ 1.0343e-03,  8.4445e-03,  1.5850e-02],\n                        [ 1.1820e-02,  1.0775e-03, -1.8296e-02],\n                        [-1.1273e-02,  2.6236e-03,  1.3343e-02]],\n              \n                       ...,\n              \n                       [[ 1.6003e-02,  5.4038e-03, -3.7506e-03],\n                        [-2.4944e-03, -8.0193e-03, -6.6061e-03],\n                        [-1.2857e-02,  1.3497e-02,  8.1090e-03]],\n              \n                       [[-1.8006e-02, -8.5612e-03,  1.9954e-02],\n                        [-3.3323e-03, -7.7578e-04,  1.2751e-02],\n                        [ 8.0447e-03, -3.9115e-04,  2.0177e-02]],\n              \n                       [[-1.7435e-02, -8.4071e-03, -9.7204e-03],\n                        [ 1.8257e-02, -1.7279e-02, -1.8781e-02],\n                        [ 1.5807e-02, -1.8718e-02,  2.0478e-02]]]])),\n             ('up2.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.0.weight',\n              tensor([[[[ 6.5360e-04, -1.1478e-02, -1.2108e-02],\n                        [-1.3628e-02, -9.4881e-03,  4.5922e-03],\n                        [-1.3436e-03, -9.4868e-03, -4.5939e-03]],\n              \n                       [[ 1.0784e-02, -1.2223e-03, -1.5292e-02],\n                        [-5.8855e-03, -1.8780e-02, -8.7660e-03],\n                        [ 1.8609e-03,  1.2953e-02, -1.4010e-02]],\n              \n                       [[-6.7148e-03, -1.5341e-02,  1.2591e-02],\n                        [ 7.5377e-03,  1.1052e-02, -1.1975e-02],\n                        [-1.9517e-02, -1.9137e-02, -7.4886e-04]],\n              \n                       ...,\n              \n                       [[ 2.0512e-02, -3.9202e-03,  1.4523e-02],\n                        [ 1.2714e-02,  1.3007e-02,  6.8676e-04],\n                        [-1.7327e-02, -8.6569e-03,  1.2416e-03]],\n              \n                       [[-2.0188e-02, -1.2779e-02, -7.3068e-03],\n                        [-9.3873e-03,  1.3301e-02,  1.6646e-02],\n                        [-1.7413e-02,  1.7294e-03, -1.5510e-02]],\n              \n                       [[-1.4983e-02,  1.7590e-02,  1.2623e-02],\n                        [-2.8354e-03, -2.8116e-03,  1.7879e-02],\n                        [-1.7114e-02,  1.2573e-02,  1.0661e-02]]],\n              \n              \n                      [[[ 1.1610e-02, -1.0957e-02,  1.8087e-02],\n                        [ 1.2981e-02, -1.2237e-02, -1.3717e-02],\n                        [-8.9545e-03,  1.0519e-02, -1.8804e-02]],\n              \n                       [[-5.7298e-03,  1.7915e-02, -3.1621e-03],\n                        [ 7.9957e-03,  3.4881e-03, -1.5158e-02],\n                        [ 1.8798e-03,  1.6252e-02, -1.5315e-03]],\n              \n                       [[-4.2252e-03,  8.9630e-03, -7.0830e-03],\n                        [-1.0045e-02, -2.2602e-03,  7.8443e-03],\n                        [-2.6957e-03,  1.3411e-02,  4.8645e-03]],\n              \n                       ...,\n              \n                       [[-5.3712e-03, -1.0452e-02, -1.6330e-02],\n                        [-1.0432e-02, -1.9882e-02, -1.6169e-02],\n                        [-7.2622e-03, -1.8196e-02, -6.7982e-03]],\n              \n                       [[-7.0105e-05, -1.2175e-02, -1.0749e-02],\n                        [ 1.1441e-02,  3.5827e-03,  1.7456e-02],\n                        [-4.9655e-03,  1.9057e-03, -1.7193e-02]],\n              \n                       [[ 1.7013e-02,  3.1988e-04,  5.7411e-03],\n                        [-3.7235e-04, -1.8450e-03,  3.6671e-03],\n                        [ 1.6459e-02,  1.1565e-02,  1.9842e-02]]],\n              \n              \n                      [[[ 1.6914e-02, -1.2111e-02,  1.4786e-02],\n                        [ 7.7207e-03,  2.5537e-03,  4.0743e-03],\n                        [ 1.0419e-04,  1.0066e-02, -8.1808e-03]],\n              \n                       [[ 5.5924e-03,  3.0751e-03, -1.4255e-02],\n                        [ 1.4609e-02, -6.0797e-03,  1.8090e-02],\n                        [-2.0465e-02, -1.9647e-02,  1.9963e-02]],\n              \n                       [[ 1.7703e-02,  9.7912e-04, -1.7088e-02],\n                        [-3.0930e-03,  1.0013e-02,  1.5110e-02],\n                        [-1.5153e-02, -6.5340e-03,  1.6374e-02]],\n              \n                       ...,\n              \n                       [[-1.0198e-02,  1.8628e-02, -7.3407e-03],\n                        [-2.0066e-02,  1.8155e-02,  8.2106e-03],\n                        [-5.0477e-04, -5.1193e-03, -1.9685e-02]],\n              \n                       [[ 7.3187e-03, -1.8577e-02, -1.9180e-02],\n                        [ 1.3858e-02, -1.6733e-02, -5.7723e-04],\n                        [ 1.2103e-02,  8.6336e-03, -2.0067e-02]],\n              \n                       [[-3.8180e-03,  1.9922e-03, -1.2753e-02],\n                        [ 1.9889e-02,  1.9218e-02,  1.2516e-02],\n                        [-1.6966e-02, -1.9937e-02,  6.3545e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4647e-02,  1.3599e-02, -1.1497e-02],\n                        [ 1.0819e-02,  6.2655e-03,  8.2514e-03],\n                        [ 9.7814e-03,  1.5446e-03,  5.0288e-03]],\n              \n                       [[-3.7955e-03,  1.2494e-02, -7.8703e-03],\n                        [ 4.0349e-03,  1.4197e-02, -1.1018e-02],\n                        [ 1.2082e-02, -1.9828e-03,  1.1344e-02]],\n              \n                       [[-1.6060e-02,  5.2254e-03,  1.3679e-02],\n                        [ 2.3551e-03, -5.8034e-03, -1.0188e-02],\n                        [-7.8099e-03, -7.3378e-03, -1.6845e-02]],\n              \n                       ...,\n              \n                       [[ 4.8750e-03, -1.5202e-02, -8.3033e-03],\n                        [-1.4143e-02,  9.6245e-03,  1.0595e-03],\n                        [-6.6992e-03,  1.8018e-02,  1.4028e-02]],\n              \n                       [[-2.4361e-03,  8.2809e-03, -6.7384e-03],\n                        [-2.4594e-03,  4.9077e-03,  1.8375e-02],\n                        [-4.1593e-03, -3.5705e-03, -1.3529e-02]],\n              \n                       [[-1.7012e-02,  1.9748e-02,  1.9104e-02],\n                        [-1.4910e-02, -1.9546e-02,  1.1406e-02],\n                        [-1.7544e-04,  1.5866e-02,  3.8805e-03]]],\n              \n              \n                      [[[-4.2661e-03,  2.0544e-02, -2.0223e-02],\n                        [-1.7558e-02,  1.2315e-02, -1.1358e-03],\n                        [-9.5695e-03,  1.7591e-02, -1.8437e-02]],\n              \n                       [[-7.6622e-03,  1.3523e-02, -1.2805e-02],\n                        [ 4.2950e-03, -7.9838e-03, -8.6255e-03],\n                        [ 1.5282e-03, -8.8083e-03,  5.8126e-03]],\n              \n                       [[ 1.2428e-02,  1.6649e-03, -1.8423e-02],\n                        [ 3.3804e-03, -9.0342e-03, -2.8731e-03],\n                        [ 2.8868e-03, -4.1382e-03,  1.6776e-02]],\n              \n                       ...,\n              \n                       [[ 1.6678e-02, -4.2476e-03, -9.8835e-03],\n                        [-9.7655e-03, -3.7623e-03,  5.0571e-03],\n                        [ 1.0131e-02, -7.6768e-03, -5.4080e-04]],\n              \n                       [[ 1.7999e-02,  5.0342e-03, -2.2092e-03],\n                        [ 1.2079e-02, -8.4492e-03, -1.6282e-02],\n                        [-2.0245e-02,  4.7685e-03, -9.7620e-03]],\n              \n                       [[-4.6216e-03, -1.1652e-02, -1.2818e-02],\n                        [ 1.2088e-02, -9.3832e-03, -4.1677e-03],\n                        [ 1.1476e-02, -4.4116e-03, -2.0018e-02]]],\n              \n              \n                      [[[ 3.7413e-03, -1.8938e-02, -1.2220e-02],\n                        [ 1.7449e-02,  9.5147e-03,  2.5178e-03],\n                        [-6.6552e-03,  2.6520e-03, -2.0583e-02]],\n              \n                       [[ 1.9046e-02,  1.7330e-03,  3.4585e-03],\n                        [ 1.6316e-02, -1.8740e-02,  1.6343e-02],\n                        [-8.1862e-03, -1.9654e-02,  6.7754e-04]],\n              \n                       [[-7.8348e-03, -1.0483e-02, -1.1580e-02],\n                        [ 2.0537e-02, -1.2595e-02,  4.6942e-03],\n                        [ 5.1139e-04, -8.2631e-04, -1.3213e-03]],\n              \n                       ...,\n              \n                       [[ 2.0120e-02, -1.8718e-02,  7.1457e-03],\n                        [ 8.7498e-03, -8.0881e-03, -8.0977e-03],\n                        [-1.8490e-02, -2.0089e-02,  2.6450e-04]],\n              \n                       [[ 3.0537e-03, -8.0446e-03, -9.7033e-03],\n                        [ 2.9420e-03,  1.5974e-02, -8.4568e-03],\n                        [-4.6306e-03,  7.5076e-03, -9.9498e-04]],\n              \n                       [[-1.7441e-02, -4.8928e-03,  2.0088e-02],\n                        [ 1.1744e-02, -1.9409e-02, -1.2495e-02],\n                        [ 1.6826e-02, -6.6388e-03, -1.3236e-03]]]])),\n             ('up3.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.3.weight',\n              tensor([[[[-6.2617e-03,  5.1519e-03,  1.0535e-02],\n                        [ 2.2614e-02,  2.3770e-02,  7.1172e-03],\n                        [-9.0252e-04, -2.0448e-02, -2.0432e-02]],\n              \n                       [[-5.3073e-03,  2.0543e-03, -1.9999e-02],\n                        [ 1.7058e-02,  4.4323e-03,  2.0256e-02],\n                        [ 1.6059e-02,  7.8848e-03,  2.6898e-02]],\n              \n                       [[ 2.4905e-02, -9.5489e-04, -4.0310e-05],\n                        [ 2.6839e-02,  1.0395e-02, -1.1824e-02],\n                        [ 1.3696e-02, -4.7753e-03,  4.4547e-03]],\n              \n                       ...,\n              \n                       [[-4.0551e-03, -2.0774e-02,  5.0831e-03],\n                        [ 8.9578e-03, -2.4251e-02, -2.7485e-02],\n                        [-1.1212e-02, -3.5667e-03, -2.9207e-02]],\n              \n                       [[-2.5817e-02,  2.8529e-02, -2.4398e-02],\n                        [ 2.0831e-02,  1.4292e-02, -1.8673e-02],\n                        [-8.5094e-04, -1.2406e-03,  3.7525e-04]],\n              \n                       [[ 2.1931e-03,  6.2044e-03, -9.8672e-03],\n                        [-6.0165e-03,  7.0416e-03, -3.2293e-03],\n                        [-1.1025e-02, -1.1666e-02, -1.8839e-02]]],\n              \n              \n                      [[[-1.9571e-02,  1.3345e-02, -3.1977e-03],\n                        [-2.4555e-02, -3.5323e-03, -2.8703e-02],\n                        [-1.5313e-02,  2.1116e-02, -1.0758e-03]],\n              \n                       [[-1.0014e-02,  1.1471e-02, -2.2742e-02],\n                        [ 2.5164e-02,  1.5579e-02, -2.2211e-02],\n                        [ 2.7174e-02,  1.9207e-02, -1.7626e-02]],\n              \n                       [[ 2.7689e-02, -5.7403e-03, -1.0863e-02],\n                        [ 5.0870e-03,  6.7373e-03, -2.0150e-02],\n                        [ 2.9319e-02, -9.6329e-03, -2.0385e-02]],\n              \n                       ...,\n              \n                       [[-2.4959e-02,  1.2766e-03,  2.4264e-03],\n                        [ 2.1160e-02, -2.1553e-02,  1.6825e-02],\n                        [ 2.6579e-02,  6.6060e-03,  2.5650e-02]],\n              \n                       [[ 4.5595e-03,  1.9319e-03, -2.5173e-02],\n                        [-2.3925e-02, -8.3372e-03, -9.0146e-03],\n                        [ 1.7461e-02, -2.5896e-02, -1.8144e-02]],\n              \n                       [[ 2.5831e-02, -2.1761e-02, -2.9396e-02],\n                        [ 2.7635e-02, -1.2928e-02,  5.8588e-03],\n                        [-2.0192e-02,  4.7528e-03,  2.8390e-02]]],\n              \n              \n                      [[[ 1.8739e-03, -1.3140e-02,  2.6128e-02],\n                        [ 1.1566e-02,  3.5446e-03, -5.1995e-03],\n                        [ 5.5016e-03, -4.5294e-03,  1.9544e-02]],\n              \n                       [[-9.9646e-03,  2.7664e-02,  1.1371e-02],\n                        [ 1.2055e-02,  1.6825e-02, -1.1272e-02],\n                        [ 1.3120e-02,  1.7465e-02,  1.1575e-02]],\n              \n                       [[-4.8596e-03,  9.3461e-03,  2.0105e-02],\n                        [ 1.2126e-02, -2.2240e-03,  1.3572e-02],\n                        [-2.8769e-02, -7.9955e-03, -1.2733e-02]],\n              \n                       ...,\n              \n                       [[ 2.5646e-02,  1.6559e-02, -2.2198e-02],\n                        [-3.0433e-03,  2.7646e-02,  2.8915e-02],\n                        [ 2.3706e-02, -2.5853e-02, -8.8919e-05]],\n              \n                       [[ 1.9385e-02,  9.4940e-03, -1.7507e-02],\n                        [-1.0995e-02, -1.9027e-02,  2.6517e-02],\n                        [ 6.5096e-03,  8.3432e-03,  4.3078e-03]],\n              \n                       [[-1.2435e-02, -1.2040e-02,  6.4921e-03],\n                        [-1.9559e-02,  2.2276e-02,  1.2324e-02],\n                        [ 7.4537e-03,  5.5965e-03, -2.4149e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.9395e-02,  2.0365e-02, -1.6215e-02],\n                        [ 1.8015e-02,  1.1132e-02, -5.3747e-03],\n                        [ 4.5775e-03,  1.9513e-02,  5.4436e-03]],\n              \n                       [[ 2.0589e-02,  4.0204e-03, -7.1212e-03],\n                        [-1.7708e-02, -2.7610e-02,  2.9521e-03],\n                        [ 1.4294e-02, -6.5115e-03, -1.4379e-03]],\n              \n                       [[ 2.8011e-02,  1.6216e-02,  2.5210e-02],\n                        [-1.6498e-02,  1.0523e-02,  2.6155e-02],\n                        [ 1.6074e-02, -8.3713e-03,  2.2026e-02]],\n              \n                       ...,\n              \n                       [[-1.3617e-02, -1.4065e-02, -2.3103e-02],\n                        [ 2.4879e-02, -8.9402e-03,  3.0990e-03],\n                        [ 1.3965e-03, -2.5021e-02, -2.0546e-02]],\n              \n                       [[ 2.0246e-03, -7.9078e-03, -2.6747e-02],\n                        [ 2.9376e-02, -6.2544e-03, -1.8549e-02],\n                        [ 1.5150e-02, -3.9595e-03,  2.3443e-03]],\n              \n                       [[-3.6495e-03, -1.0052e-02,  1.2397e-03],\n                        [ 3.8338e-03, -2.8786e-02, -5.1455e-03],\n                        [-1.5915e-02,  2.8991e-02,  6.3032e-03]]],\n              \n              \n                      [[[-2.0503e-02, -2.8574e-02,  1.7111e-02],\n                        [-1.5106e-02,  2.2639e-02,  3.2666e-03],\n                        [ 1.1444e-02, -9.7533e-03,  1.8418e-02]],\n              \n                       [[-2.8729e-02, -1.7639e-02,  1.5558e-02],\n                        [ 2.1907e-02,  2.6665e-02, -2.0398e-02],\n                        [ 4.7236e-03,  2.2406e-02, -1.1982e-03]],\n              \n                       [[-6.9613e-03,  1.6444e-02,  1.0986e-04],\n                        [-2.5102e-02,  2.7951e-02,  1.8224e-02],\n                        [-9.3261e-03, -2.2952e-02, -1.9339e-02]],\n              \n                       ...,\n              \n                       [[ 6.3333e-03, -8.1322e-03,  3.5560e-03],\n                        [-2.3900e-02, -2.8754e-02, -2.0715e-02],\n                        [ 1.3923e-02,  1.0834e-02, -1.1983e-02]],\n              \n                       [[-1.2872e-02,  6.1885e-03, -1.2684e-02],\n                        [ 8.5061e-03, -1.3273e-03, -1.6401e-03],\n                        [ 3.5566e-03,  1.4142e-02,  7.0110e-03]],\n              \n                       [[ 1.2880e-02,  6.1687e-03, -9.6315e-03],\n                        [ 1.5918e-02,  2.2629e-03, -2.7104e-03],\n                        [-8.4794e-04,  2.0819e-02, -2.2515e-02]]],\n              \n              \n                      [[[ 8.6197e-03,  2.3163e-02,  1.9551e-02],\n                        [ 2.2528e-02,  1.8106e-02,  1.0401e-02],\n                        [-1.7955e-03, -5.1270e-03,  9.9206e-03]],\n              \n                       [[ 2.3529e-02,  1.5074e-02, -1.5779e-02],\n                        [-2.8125e-02, -1.9706e-02, -2.7739e-02],\n                        [ 1.2969e-02, -6.8372e-03, -1.8700e-02]],\n              \n                       [[-1.6456e-02, -1.9319e-02,  2.9451e-02],\n                        [-4.3081e-03,  1.6394e-02,  2.0039e-02],\n                        [-2.6109e-02,  1.8154e-02, -4.1342e-03]],\n              \n                       ...,\n              \n                       [[ 1.4506e-02, -2.9666e-03,  3.6261e-03],\n                        [ 1.6303e-02, -4.9343e-03, -1.7006e-02],\n                        [ 2.6239e-02, -2.3413e-02,  1.2565e-02]],\n              \n                       [[-7.7776e-03,  2.6909e-02,  1.0444e-02],\n                        [-8.7274e-03, -8.3104e-03,  2.3266e-03],\n                        [-2.4073e-02, -1.0433e-02, -1.1619e-02]],\n              \n                       [[-1.0362e-02, -2.3291e-02, -1.0579e-02],\n                        [ 1.6419e-02,  2.0854e-02,  2.4889e-02],\n                        [ 1.3606e-03, -9.4291e-03, -1.6355e-03]]]])),\n             ('up3.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.0.weight',\n              tensor([[[[-2.4477e-02, -1.7234e-02,  2.2003e-03],\n                        [-7.8829e-03,  6.1736e-03,  1.4644e-02],\n                        [ 9.7539e-03,  5.7497e-04, -2.1407e-02]],\n              \n                       [[ 2.5615e-02,  6.0152e-03, -2.8486e-02],\n                        [ 2.1189e-02,  6.7674e-03, -1.4792e-03],\n                        [ 2.2734e-02,  1.7544e-03, -1.0535e-02]],\n              \n                       [[ 2.1016e-02,  3.9310e-03,  5.9241e-03],\n                        [-9.3318e-04,  1.3821e-02,  2.8222e-02],\n                        [ 7.3732e-03,  2.3611e-03,  2.2986e-02]],\n              \n                       ...,\n              \n                       [[-2.6076e-02,  9.7759e-03,  1.7446e-02],\n                        [-4.6081e-03, -7.8919e-03, -1.3171e-02],\n                        [ 3.6483e-03,  5.5107e-04, -2.6154e-02]],\n              \n                       [[ 2.4815e-02,  6.5554e-04, -2.6840e-02],\n                        [-5.4893e-03, -1.2978e-02, -7.7000e-03],\n                        [ 1.7822e-02, -2.0376e-02,  1.8151e-02]],\n              \n                       [[-1.3709e-02, -2.1298e-02,  1.4319e-02],\n                        [-1.1540e-02,  2.9451e-03,  4.6603e-03],\n                        [ 1.6498e-02, -2.2247e-02, -2.6400e-02]]],\n              \n              \n                      [[[-2.9053e-02,  6.6088e-03,  2.8600e-02],\n                        [-8.5117e-03,  3.7488e-03,  2.5909e-02],\n                        [-6.6344e-03, -1.8867e-02,  2.1232e-02]],\n              \n                       [[ 2.7659e-02, -1.5675e-02, -1.2514e-02],\n                        [ 6.8806e-03, -2.4540e-02, -2.0591e-02],\n                        [-6.2750e-03, -2.9055e-02,  2.7674e-02]],\n              \n                       [[ 6.6344e-03, -2.5097e-02, -2.7987e-02],\n                        [-1.9412e-02, -1.7099e-02,  2.4543e-02],\n                        [-6.0892e-03, -1.9663e-02, -2.1830e-02]],\n              \n                       ...,\n              \n                       [[-2.4330e-02, -5.3355e-04,  1.6593e-02],\n                        [-1.5296e-02, -1.2302e-02, -2.1773e-02],\n                        [-2.4805e-02, -2.7568e-02, -5.2265e-03]],\n              \n                       [[ 1.4438e-02, -1.1498e-02, -5.8588e-03],\n                        [ 2.3541e-02,  2.8545e-02, -2.1781e-02],\n                        [ 2.1298e-02, -1.4740e-02,  2.0063e-02]],\n              \n                       [[-1.4228e-02,  2.7397e-02,  1.9363e-03],\n                        [ 1.3088e-02,  1.8878e-02,  2.5326e-02],\n                        [-2.7118e-02,  1.8095e-02,  1.5554e-02]]],\n              \n              \n                      [[[-2.7807e-02,  2.8756e-02, -2.4947e-02],\n                        [ 2.8239e-03,  6.4158e-03,  1.7847e-02],\n                        [-2.1316e-02, -1.1236e-02, -7.1000e-03]],\n              \n                       [[-2.2642e-02, -2.9162e-02, -2.7960e-02],\n                        [ 2.2822e-02,  2.6365e-02, -2.2013e-02],\n                        [-4.3668e-03,  5.9663e-03, -2.2929e-02]],\n              \n                       [[ 2.6231e-02,  6.2513e-04, -1.5292e-02],\n                        [-2.3744e-02,  1.0287e-02, -1.7989e-02],\n                        [ 1.4567e-02, -5.4238e-04, -1.8888e-03]],\n              \n                       ...,\n              \n                       [[ 8.2702e-03, -3.9680e-03,  4.4591e-03],\n                        [ 1.2113e-02,  1.9210e-02, -2.1732e-02],\n                        [ 1.8309e-02, -2.5562e-02, -3.4519e-03]],\n              \n                       [[ 2.0920e-02,  5.1383e-03, -2.8351e-02],\n                        [ 2.4168e-02,  2.4032e-03,  4.4554e-03],\n                        [-9.5799e-03, -4.6795e-03,  2.1697e-02]],\n              \n                       [[ 5.9437e-03,  1.4123e-03, -8.3815e-03],\n                        [ 2.3132e-02, -2.6785e-02, -1.6763e-02],\n                        [-9.6515e-03, -2.1222e-02,  2.4000e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.3391e-02,  2.3395e-02, -2.1791e-02],\n                        [ 1.8008e-02,  5.3447e-03,  2.3465e-02],\n                        [ 1.7817e-02, -3.0541e-04,  1.8585e-02]],\n              \n                       [[-1.8773e-02,  9.5143e-03, -9.0805e-03],\n                        [-1.1845e-02, -2.0910e-02,  7.6076e-03],\n                        [-1.9462e-03,  2.5138e-02, -2.8411e-02]],\n              \n                       [[ 1.2022e-02, -1.4268e-02,  1.6846e-02],\n                        [-1.5587e-02, -2.2586e-02,  1.7113e-03],\n                        [-2.0474e-02,  2.1718e-02,  2.6473e-02]],\n              \n                       ...,\n              \n                       [[-9.5288e-04, -2.0567e-02, -5.8081e-03],\n                        [-9.2609e-03,  2.2689e-02,  7.9880e-03],\n                        [-2.3267e-02, -2.2080e-03, -3.7323e-04]],\n              \n                       [[ 7.0031e-03,  1.5936e-02, -1.7355e-02],\n                        [ 9.1528e-03,  6.0140e-04, -4.6582e-03],\n                        [-2.2403e-03,  1.1589e-02,  1.3004e-02]],\n              \n                       [[ 7.5902e-03, -2.7939e-02,  1.6827e-02],\n                        [-1.1944e-02, -2.1053e-02,  7.7404e-03],\n                        [-2.4648e-02,  1.0781e-02,  1.6477e-02]]],\n              \n              \n                      [[[ 2.8526e-02, -8.3310e-03, -3.3514e-03],\n                        [ 8.7738e-03,  3.3132e-03, -2.3501e-03],\n                        [-1.5227e-02, -6.8209e-03,  7.2189e-03]],\n              \n                       [[ 3.2429e-03,  2.9305e-02,  7.2086e-03],\n                        [-2.8544e-02, -2.1567e-02, -7.0302e-03],\n                        [-1.2484e-02,  4.2848e-03, -1.5662e-02]],\n              \n                       [[ 1.4185e-03,  6.2046e-03,  2.1498e-02],\n                        [ 1.4784e-02, -2.4929e-02, -2.7400e-02],\n                        [-2.6303e-05,  2.4616e-02, -1.2550e-02]],\n              \n                       ...,\n              \n                       [[-1.1245e-02, -6.3400e-03, -1.4372e-02],\n                        [-2.6327e-02, -9.7659e-03, -1.9709e-03],\n                        [-2.4333e-03,  5.2920e-03,  1.3149e-02]],\n              \n                       [[ 2.8700e-03,  7.3612e-03,  2.3691e-03],\n                        [-2.7523e-02,  1.5241e-02,  1.3450e-02],\n                        [ 2.5740e-03, -3.4698e-03, -1.3424e-02]],\n              \n                       [[-1.4515e-02, -2.1749e-02,  1.3343e-02],\n                        [ 2.5754e-02,  3.5074e-03,  1.9747e-02],\n                        [ 2.7382e-03,  1.4910e-02, -2.2954e-02]]],\n              \n              \n                      [[[-4.3458e-03, -1.3681e-02,  1.8517e-02],\n                        [-1.4100e-02,  2.4556e-02, -1.6581e-03],\n                        [-2.7384e-02,  1.7085e-02,  1.9694e-02]],\n              \n                       [[ 5.4223e-03, -1.7057e-02, -6.0624e-03],\n                        [ 2.8144e-02, -1.2404e-02, -9.2200e-05],\n                        [ 8.0187e-03, -2.4534e-02, -6.1641e-03]],\n              \n                       [[ 4.4628e-03, -2.3212e-02,  1.8625e-02],\n                        [ 2.0626e-03, -1.1065e-02,  2.2116e-02],\n                        [-2.3691e-02,  7.7271e-03,  2.3667e-02]],\n              \n                       ...,\n              \n                       [[ 1.6437e-02,  1.7844e-02,  4.2858e-03],\n                        [ 1.8507e-02, -1.4175e-02,  6.2452e-03],\n                        [-2.2591e-02, -1.6163e-02,  2.8446e-02]],\n              \n                       [[ 7.0578e-03,  8.5772e-03,  1.2336e-03],\n                        [-2.7270e-02, -4.7153e-03,  1.8364e-02],\n                        [-1.7723e-02, -6.1744e-03, -2.6519e-02]],\n              \n                       [[ 2.6981e-03,  2.3110e-02, -1.9544e-02],\n                        [ 2.8593e-02,  2.6731e-02,  2.1887e-02],\n                        [-9.6571e-04,  1.7459e-02,  3.4465e-03]]]])),\n             ('up4.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.3.weight',\n              tensor([[[[ 3.1426e-03, -3.7804e-02, -1.9636e-03],\n                        [-3.3168e-02,  2.4599e-03, -2.5361e-02],\n                        [ 2.0291e-02, -3.1659e-02, -2.2596e-02]],\n              \n                       [[-8.4917e-03, -3.0465e-04, -2.1817e-02],\n                        [ 2.9646e-03,  2.4069e-02, -2.6871e-02],\n                        [ 2.7976e-02, -2.9426e-02, -1.9063e-02]],\n              \n                       [[ 3.4714e-02,  2.5515e-02,  2.2645e-03],\n                        [ 1.1169e-02, -1.5637e-02, -3.2919e-02],\n                        [-1.3760e-02,  1.0523e-03,  3.2319e-02]],\n              \n                       ...,\n              \n                       [[-2.6632e-02,  1.5643e-02, -3.1304e-03],\n                        [-6.5018e-03,  1.7912e-02, -1.7220e-02],\n                        [ 3.1036e-02,  3.4784e-02, -1.4025e-02]],\n              \n                       [[ 3.3626e-02, -2.4100e-02,  3.6708e-02],\n                        [-2.1758e-02, -1.4161e-02, -2.8572e-02],\n                        [ 5.2657e-03,  2.2184e-02, -1.2249e-02]],\n              \n                       [[ 3.9889e-02, -9.9724e-03,  1.4062e-03],\n                        [ 1.6991e-02, -5.8726e-03, -1.2741e-02],\n                        [-2.3483e-02,  3.6793e-02,  1.0728e-03]]],\n              \n              \n                      [[[-1.1431e-02,  2.8004e-03, -2.1472e-02],\n                        [-4.7250e-03,  3.1195e-02, -3.4145e-02],\n                        [-3.9074e-02, -9.0451e-03,  3.6595e-02]],\n              \n                       [[-3.4954e-02, -2.8686e-02,  7.4445e-03],\n                        [-3.4594e-02, -1.5361e-02,  3.2916e-02],\n                        [ 7.3619e-03, -2.8733e-02, -2.8171e-02]],\n              \n                       [[-1.6132e-02,  9.1593e-03, -1.5983e-03],\n                        [ 1.9147e-02, -3.0231e-02,  3.5481e-02],\n                        [-2.8131e-02, -1.5797e-02,  1.4560e-02]],\n              \n                       ...,\n              \n                       [[-2.0996e-03, -2.3411e-02, -1.1860e-02],\n                        [ 3.8093e-02,  3.5264e-02,  3.0247e-02],\n                        [ 1.3708e-02, -2.7209e-02,  3.5293e-02]],\n              \n                       [[-1.4823e-02, -1.3127e-02, -1.8602e-02],\n                        [ 3.1382e-02, -2.8936e-02, -3.5547e-02],\n                        [ 2.8250e-02,  2.5477e-02, -1.1684e-02]],\n              \n                       [[-3.4762e-03, -2.8827e-02,  2.2720e-02],\n                        [ 1.9048e-02,  1.9151e-02,  4.8282e-03],\n                        [ 3.6979e-02,  1.1263e-02,  1.4983e-02]]],\n              \n              \n                      [[[ 4.0528e-02, -1.5267e-02,  4.1640e-02],\n                        [ 1.4580e-02,  2.1254e-03,  2.1454e-02],\n                        [ 2.3367e-02,  2.4535e-02, -2.9547e-02]],\n              \n                       [[ 1.2478e-02, -3.2175e-02,  3.1261e-02],\n                        [-2.5070e-02,  1.0443e-02, -1.7667e-02],\n                        [-3.9835e-03, -1.4524e-02,  2.9181e-02]],\n              \n                       [[ 8.7496e-03,  1.6791e-02, -3.3366e-02],\n                        [ 3.9007e-02,  1.0403e-02,  3.8254e-02],\n                        [-1.2029e-02,  1.1168e-02, -1.9442e-02]],\n              \n                       ...,\n              \n                       [[ 2.2030e-02,  1.0903e-02, -1.4863e-02],\n                        [-1.3346e-02, -3.5193e-02,  3.2643e-02],\n                        [-3.8632e-02, -8.3370e-03,  1.8904e-02]],\n              \n                       [[-3.9616e-02, -2.5855e-02,  3.3651e-02],\n                        [ 3.9193e-02,  2.7768e-02,  1.4065e-02],\n                        [-8.8412e-03, -2.1744e-02, -2.0466e-02]],\n              \n                       [[-9.5175e-03, -3.2115e-02,  2.8135e-02],\n                        [-3.5135e-02, -3.5658e-02, -1.6859e-02],\n                        [ 3.8371e-02,  4.0490e-03,  2.5179e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6391e-02,  5.2747e-03,  3.4211e-02],\n                        [-3.6951e-02, -2.0392e-02,  1.9124e-02],\n                        [-4.0592e-03, -2.1158e-02, -5.6858e-03]],\n              \n                       [[-1.2450e-02, -7.7264e-03, -2.7716e-02],\n                        [ 3.4721e-02,  2.8399e-02,  3.7686e-02],\n                        [ 3.6166e-02,  1.7743e-02, -3.3313e-02]],\n              \n                       [[-2.4009e-03,  2.7938e-02,  8.2821e-03],\n                        [-1.0567e-02, -1.0721e-02,  3.9096e-02],\n                        [-1.0329e-02,  3.5188e-04,  1.9992e-02]],\n              \n                       ...,\n              \n                       [[ 4.0091e-02,  2.7190e-02, -3.8786e-02],\n                        [ 3.7762e-02,  1.6390e-02, -4.1539e-02],\n                        [ 2.8608e-02, -3.4842e-02, -1.5290e-02]],\n              \n                       [[ 2.5458e-02,  3.8800e-02,  1.8157e-02],\n                        [-3.0404e-02, -2.8858e-02, -3.7904e-02],\n                        [-1.7384e-02,  1.3624e-02, -3.8238e-02]],\n              \n                       [[-3.4968e-02, -2.1631e-02,  1.8572e-02],\n                        [ 3.9958e-02,  3.1534e-02, -2.6919e-03],\n                        [ 2.9025e-02, -2.5323e-02,  1.8108e-02]]],\n              \n              \n                      [[[ 1.4118e-02,  1.3075e-02,  7.9425e-04],\n                        [-1.5709e-02,  2.2579e-02, -3.4406e-03],\n                        [ 3.9156e-02, -5.3889e-03, -4.1343e-02]],\n              \n                       [[-1.1825e-03, -7.4790e-03,  3.0482e-02],\n                        [-4.0314e-02, -1.9415e-02, -5.4573e-05],\n                        [-3.6205e-03, -4.0538e-02,  1.6526e-02]],\n              \n                       [[ 3.1517e-02,  1.2538e-02,  1.7676e-03],\n                        [ 2.2461e-02, -2.9065e-02,  3.1906e-02],\n                        [-3.9866e-02, -2.3473e-02,  4.0793e-02]],\n              \n                       ...,\n              \n                       [[-2.2015e-02, -1.4035e-03, -3.4191e-02],\n                        [ 3.4649e-02,  2.7996e-02,  2.5186e-02],\n                        [-2.6122e-02, -3.7787e-02, -3.5784e-02]],\n              \n                       [[-3.5926e-03, -1.5855e-02, -2.4558e-02],\n                        [-3.5714e-02,  4.0327e-02,  3.9204e-02],\n                        [ 1.6102e-03, -2.2671e-02,  3.9940e-02]],\n              \n                       [[-4.1120e-02,  6.4742e-03,  1.8772e-02],\n                        [ 3.4173e-02,  5.7441e-04, -1.9311e-02],\n                        [-1.4727e-02,  1.7990e-02, -1.8958e-02]]],\n              \n              \n                      [[[ 2.9624e-02, -8.9972e-03,  4.0076e-02],\n                        [ 1.4882e-02, -1.9439e-02,  8.6693e-03],\n                        [-4.0603e-02,  1.5571e-02, -2.9153e-02]],\n              \n                       [[-3.5557e-02,  1.8946e-04,  2.2721e-02],\n                        [ 2.9935e-03,  8.9930e-03, -2.0757e-02],\n                        [ 2.0412e-02,  5.7608e-03,  2.6245e-02]],\n              \n                       [[-6.2162e-03, -7.0439e-04,  1.3922e-02],\n                        [-9.8026e-03,  2.8211e-02, -3.7612e-03],\n                        [-3.1022e-02, -2.4241e-02,  2.0704e-03]],\n              \n                       ...,\n              \n                       [[ 1.8656e-05, -3.5449e-02, -1.9142e-02],\n                        [-3.7448e-02, -3.8316e-02,  3.6445e-02],\n                        [ 1.8268e-02, -3.2087e-02, -3.0568e-02]],\n              \n                       [[-2.6703e-02, -7.0255e-04,  1.3062e-02],\n                        [ 9.2566e-03,  3.0957e-02, -3.9456e-02],\n                        [ 2.6741e-02,  1.7924e-02,  2.6267e-02]],\n              \n                       [[-3.0110e-02, -1.6314e-03, -2.8098e-02],\n                        [ 2.0860e-02,  1.5562e-02,  2.9175e-02],\n                        [ 9.1814e-03,  2.6883e-02,  2.8830e-02]]]])),\n             ('up4.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('outc.conv.weight',\n              tensor([[[[ 0.0984]],\n              \n                       [[-0.0668]],\n              \n                       [[-0.0782]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0089]],\n              \n                       [[-0.0501]],\n              \n                       [[-0.0261]],\n              \n                       [[ 0.0791]],\n              \n                       [[-0.1128]],\n              \n                       [[ 0.0102]],\n              \n                       [[ 0.0258]],\n              \n                       [[-0.0357]],\n              \n                       [[-0.0674]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.0549]],\n              \n                       [[-0.0972]],\n              \n                       [[-0.1207]],\n              \n                       [[ 0.1104]],\n              \n                       [[ 0.0293]],\n              \n                       [[-0.1182]],\n              \n                       [[ 0.1166]],\n              \n                       [[ 0.1038]],\n              \n                       [[-0.0085]],\n              \n                       [[-0.0039]],\n              \n                       [[ 0.0621]],\n              \n                       [[ 0.0331]],\n              \n                       [[ 0.0618]],\n              \n                       [[ 0.0310]],\n              \n                       [[ 0.1245]],\n              \n                       [[-0.1027]],\n              \n                       [[ 0.0523]],\n              \n                       [[ 0.0731]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0495]],\n              \n                       [[ 0.1218]],\n              \n                       [[ 0.1106]],\n              \n                       [[ 0.0079]],\n              \n                       [[-0.1117]],\n              \n                       [[ 0.1123]],\n              \n                       [[-0.0453]],\n              \n                       [[ 0.0750]],\n              \n                       [[ 0.0378]],\n              \n                       [[ 0.1220]],\n              \n                       [[-0.1052]],\n              \n                       [[-0.0909]],\n              \n                       [[-0.0841]],\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0207]],\n              \n                       [[-0.0161]],\n              \n                       [[-0.0815]],\n              \n                       [[ 0.0737]],\n              \n                       [[-0.0565]],\n              \n                       [[-0.0620]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.1087]],\n              \n                       [[ 0.0442]],\n              \n                       [[-0.0377]],\n              \n                       [[-0.0474]],\n              \n                       [[ 0.0807]],\n              \n                       [[ 0.0298]],\n              \n                       [[ 0.0700]],\n              \n                       [[ 0.0749]],\n              \n                       [[ 0.0847]],\n              \n                       [[-0.1145]]]])),\n             ('outc.conv.bias', tensor([-0.0712]))])</pre> In\u00a0[29]: Copied! <pre>## CPU\u200b\u6216\u200b\u5355\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\ntorch.save(unet, \"./unet_example.pth\")\nloaded_unet = torch.load(\"./unet_example.pth\")\nloaded_unet.state_dict()\n</pre> ## CPU\u200b\u6216\u200b\u5355\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b torch.save(unet, \"./unet_example.pth\") loaded_unet = torch.load(\"./unet_example.pth\") loaded_unet.state_dict() Out[29]: <pre>OrderedDict([('inc.double_conv.0.weight',\n              tensor([[[[-0.1569, -0.0516,  0.1381],\n                        [-0.0167,  0.1114, -0.1482],\n                        [-0.1659, -0.0492, -0.1526]],\n              \n                       [[ 0.0871,  0.1102, -0.1270],\n                        [ 0.1058,  0.0541, -0.0767],\n                        [ 0.1247,  0.1813,  0.1895]],\n              \n                       [[ 0.0929, -0.1305,  0.0531],\n                        [-0.0972, -0.1668, -0.0183],\n                        [-0.1754, -0.0862,  0.0373]]],\n              \n              \n                      [[[-0.0014,  0.1440, -0.0519],\n                        [ 0.1643,  0.1829,  0.1713],\n                        [-0.0702, -0.0426,  0.0083]],\n              \n                       [[ 0.1057,  0.0303,  0.0280],\n                        [-0.0306, -0.0898,  0.1635],\n                        [-0.1388, -0.0430,  0.0839]],\n              \n                       [[ 0.0840,  0.1753,  0.0916],\n                        [ 0.0819,  0.1624,  0.1901],\n                        [ 0.1914,  0.0483, -0.0875]]],\n              \n              \n                      [[[ 0.1197, -0.1618, -0.1778],\n                        [ 0.0866, -0.0638, -0.1615],\n                        [ 0.1437, -0.1523, -0.1007]],\n              \n                       [[-0.1395, -0.0602, -0.0457],\n                        [ 0.0582, -0.1701,  0.0586],\n                        [-0.1828,  0.0463,  0.1460]],\n              \n                       [[ 0.0735,  0.0299, -0.0629],\n                        [-0.0345, -0.0038,  0.0794],\n                        [-0.0958, -0.1519, -0.0411]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1095,  0.0703, -0.0860],\n                        [-0.1243, -0.0596, -0.1636],\n                        [ 0.0819,  0.0457,  0.1248]],\n              \n                       [[-0.1077, -0.1394,  0.0295],\n                        [ 0.1442, -0.1271,  0.1462],\n                        [-0.1011,  0.1301, -0.1294]],\n              \n                       [[-0.1653, -0.1431, -0.1031],\n                        [ 0.0511,  0.1370,  0.0210],\n                        [-0.1709,  0.0438, -0.0352]]],\n              \n              \n                      [[[-0.0893,  0.1826, -0.0856],\n                        [-0.1679,  0.0620,  0.1056],\n                        [-0.0206, -0.1745, -0.0500]],\n              \n                       [[ 0.0784,  0.0502,  0.1084],\n                        [-0.0746, -0.1213,  0.0849],\n                        [-0.1682, -0.1131, -0.1769]],\n              \n                       [[ 0.1111, -0.0814,  0.1804],\n                        [-0.0183,  0.0950, -0.0082],\n                        [-0.0761, -0.0757, -0.1657]]],\n              \n              \n                      [[[ 0.0543, -0.0157, -0.1387],\n                        [ 0.1503,  0.1388,  0.0653],\n                        [ 0.1474, -0.0991, -0.1478]],\n              \n                       [[ 0.0953, -0.1215,  0.1848],\n                        [-0.0360,  0.0052, -0.1841],\n                        [-0.1859, -0.0946,  0.1727]],\n              \n                       [[-0.0668, -0.0142,  0.1517],\n                        [-0.1101,  0.0217, -0.1021],\n                        [-0.1509,  0.0912,  0.1346]]]])),\n             ('inc.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.num_batches_tracked', tensor(0)),\n             ('inc.double_conv.3.weight',\n              tensor([[[[-4.1079e-02,  2.4625e-02, -5.8618e-03],\n                        [-3.6583e-02, -1.7239e-02,  2.4723e-02],\n                        [-2.0914e-03,  3.0168e-02, -2.0448e-02]],\n              \n                       [[ 4.1381e-03, -2.0328e-02, -2.9454e-02],\n                        [ 1.0681e-02, -3.6947e-02, -1.4246e-02],\n                        [-3.8679e-03,  2.3515e-02,  7.0796e-03]],\n              \n                       [[-3.3515e-02,  2.3345e-02, -5.7584e-04],\n                        [ 3.0752e-02, -3.5342e-02, -3.0192e-02],\n                        [ 3.0137e-02,  4.9735e-03,  3.0268e-02]],\n              \n                       ...,\n              \n                       [[ 2.6247e-02,  3.5036e-02, -2.7703e-02],\n                        [ 1.2037e-02, -1.1631e-02, -3.5691e-02],\n                        [ 1.8343e-02,  2.3172e-02, -2.3284e-02]],\n              \n                       [[ 3.9720e-02, -2.9578e-02, -3.8113e-02],\n                        [ 6.7576e-04, -4.0048e-02, -6.3216e-05],\n                        [ 1.9008e-02,  3.8545e-02,  3.0812e-02]],\n              \n                       [[-6.7981e-03, -1.5902e-03,  3.7965e-02],\n                        [ 8.6753e-03, -1.4569e-03, -1.9033e-02],\n                        [-2.0683e-02, -2.7206e-02,  2.5007e-02]]],\n              \n              \n                      [[[-1.3453e-02,  4.8410e-03,  6.3604e-03],\n                        [ 1.4860e-02, -1.9902e-04, -3.7245e-02],\n                        [ 1.2965e-02,  9.0473e-03,  2.3664e-02]],\n              \n                       [[-3.6142e-02, -2.9932e-02, -2.7691e-02],\n                        [ 2.6747e-02,  2.1051e-02, -6.9610e-03],\n                        [ 1.6672e-02,  2.4121e-02,  3.9934e-02]],\n              \n                       [[ 1.8793e-02,  3.8492e-02, -1.8463e-02],\n                        [ 2.4193e-02,  1.2931e-02, -2.9170e-02],\n                        [-2.2503e-02,  7.4183e-03, -9.9386e-03]],\n              \n                       ...,\n              \n                       [[-3.5583e-02,  1.0415e-02,  2.6884e-03],\n                        [-2.4120e-02, -1.6516e-02, -3.5117e-02],\n                        [-1.1389e-02, -3.2349e-02, -5.4190e-03]],\n              \n                       [[ 1.0794e-02, -1.4699e-02, -3.9218e-02],\n                        [ 7.2620e-03,  2.3942e-02, -9.0866e-03],\n                        [-3.9156e-02, -2.2665e-02,  3.0706e-02]],\n              \n                       [[ 2.5315e-02,  3.8635e-02, -1.4174e-03],\n                        [ 4.2061e-03, -3.3006e-02, -2.6736e-02],\n                        [-1.2201e-02,  2.4348e-02, -2.8096e-02]]],\n              \n              \n                      [[[-2.9801e-02,  1.3935e-02, -2.9342e-02],\n                        [-4.2913e-03,  9.5715e-03,  3.7494e-02],\n                        [ 2.2639e-02,  1.3474e-02,  2.3872e-02]],\n              \n                       [[ 1.6016e-03,  2.9424e-02,  2.3341e-02],\n                        [-1.2055e-02, -3.9560e-02, -1.5007e-02],\n                        [ 2.5384e-02, -4.1246e-02,  2.9730e-02]],\n              \n                       [[ 2.2965e-02, -2.7511e-02, -1.2306e-02],\n                        [-1.4792e-02,  2.7210e-03, -3.1689e-02],\n                        [ 3.1452e-02, -2.1154e-02,  3.2495e-02]],\n              \n                       ...,\n              \n                       [[ 6.1211e-03, -1.7085e-03,  1.0614e-02],\n                        [-1.3250e-03,  2.0869e-02,  7.6367e-03],\n                        [-3.3447e-02, -3.5193e-02, -3.4296e-02]],\n              \n                       [[ 2.6182e-02, -9.0026e-03,  4.3130e-03],\n                        [-1.9488e-02,  3.6438e-02, -2.9620e-02],\n                        [-4.0476e-02,  8.5702e-03,  2.2612e-02]],\n              \n                       [[ 1.9338e-03, -1.3990e-02,  8.3609e-03],\n                        [-1.3580e-02, -3.6543e-02,  2.8900e-02],\n                        [ 2.8948e-02, -2.2145e-03, -2.4276e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.0462e-03,  3.9649e-02,  1.0557e-02],\n                        [ 3.1926e-02,  3.8248e-02,  9.8494e-03],\n                        [ 1.2289e-03, -1.9980e-02, -3.3557e-02]],\n              \n                       [[-4.0275e-02,  1.1621e-02,  1.1366e-02],\n                        [-1.9881e-02,  6.3696e-03,  4.0948e-02],\n                        [-1.5219e-02, -1.6628e-02,  2.8343e-03]],\n              \n                       [[ 2.7490e-02,  3.5501e-02,  3.2039e-02],\n                        [ 3.5091e-03,  1.1285e-02,  1.5338e-02],\n                        [ 1.9410e-02, -5.1183e-03, -2.9545e-02]],\n              \n                       ...,\n              \n                       [[-2.0173e-02,  3.1788e-02,  8.5245e-03],\n                        [ 1.2969e-02,  1.4843e-02,  1.5726e-02],\n                        [ 3.1018e-02, -2.0554e-02,  1.6326e-02]],\n              \n                       [[-3.5004e-02,  3.6636e-02,  5.2004e-03],\n                        [ 2.9926e-02,  3.7449e-02,  6.1300e-04],\n                        [-5.1867e-04, -4.0083e-02, -3.0298e-02]],\n              \n                       [[-1.5009e-02,  4.1003e-02,  7.9811e-03],\n                        [ 6.5824e-03, -2.2011e-02,  8.9981e-03],\n                        [ 1.5385e-02, -3.9503e-02,  4.1086e-02]]],\n              \n              \n                      [[[-2.8993e-02, -3.7376e-02,  1.1231e-02],\n                        [ 1.7329e-02, -5.8507e-03,  1.9821e-02],\n                        [ 2.0648e-02, -3.9886e-02,  1.6316e-02]],\n              \n                       [[ 3.2519e-02,  1.6676e-02,  1.2690e-03],\n                        [ 1.6236e-03,  4.4074e-03, -2.0494e-02],\n                        [-3.6117e-02,  1.2012e-02, -2.8950e-02]],\n              \n                       [[-3.4818e-02, -1.8692e-02, -6.5148e-03],\n                        [-3.8199e-02, -2.1533e-03, -2.6669e-02],\n                        [ 2.0359e-03, -1.0877e-02,  3.2552e-02]],\n              \n                       ...,\n              \n                       [[ 2.6173e-03, -3.7495e-02,  8.6743e-03],\n                        [ 4.8354e-04,  4.1075e-02, -6.5880e-03],\n                        [ 3.3915e-02,  3.9410e-03, -1.2893e-02]],\n              \n                       [[ 2.6528e-02, -4.0759e-02,  1.9229e-02],\n                        [ 2.2432e-02, -3.9180e-03,  2.6232e-02],\n                        [ 1.2603e-02, -3.1149e-03, -1.4234e-02]],\n              \n                       [[-2.9655e-03,  1.3039e-03, -2.7197e-02],\n                        [ 3.9957e-02, -1.5892e-02,  2.0109e-02],\n                        [ 1.4106e-03,  6.4586e-04,  8.9162e-03]]],\n              \n              \n                      [[[ 3.1019e-02,  3.9165e-02, -2.7102e-02],\n                        [-3.8747e-02, -2.9976e-02, -8.2251e-04],\n                        [ 3.1431e-02, -9.7356e-03,  1.1533e-02]],\n              \n                       [[-8.6869e-03,  3.6680e-02,  1.8349e-02],\n                        [-3.1113e-02, -2.5772e-02, -1.2013e-02],\n                        [ 2.4810e-02,  2.1669e-02, -3.3620e-02]],\n              \n                       [[-3.0419e-02,  7.3520e-03, -1.9823e-02],\n                        [ 3.8660e-02,  2.6089e-02,  3.0254e-02],\n                        [ 1.4994e-02,  1.0452e-02,  3.4261e-02]],\n              \n                       ...,\n              \n                       [[-3.2601e-02, -3.6214e-02,  3.6512e-02],\n                        [-3.7527e-02, -2.9699e-02,  1.5305e-02],\n                        [-2.4764e-02,  2.2672e-02,  2.2486e-02]],\n              \n                       [[ 1.1033e-02,  3.0824e-02,  2.4714e-02],\n                        [-2.1154e-02,  2.5543e-02,  1.0087e-02],\n                        [ 2.3082e-02, -3.0461e-02,  3.4150e-02]],\n              \n                       [[-1.8519e-02, -7.6047e-03,  2.7975e-02],\n                        [-6.4077e-03, -2.6562e-02,  9.9592e-03],\n                        [-2.9076e-02, -2.5703e-02, -2.9623e-02]]]])),\n             ('inc.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.num_batches_tracked', tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[ 0.0357, -0.0264,  0.0201],\n                        [ 0.0235, -0.0205,  0.0169],\n                        [ 0.0325, -0.0087, -0.0301]],\n              \n                       [[-0.0252,  0.0130,  0.0105],\n                        [ 0.0278,  0.0094, -0.0272],\n                        [ 0.0324,  0.0047,  0.0045]],\n              \n                       [[-0.0352, -0.0399, -0.0170],\n                        [ 0.0144,  0.0158, -0.0144],\n                        [-0.0233,  0.0018, -0.0334]],\n              \n                       ...,\n              \n                       [[ 0.0116, -0.0235, -0.0296],\n                        [-0.0242,  0.0119,  0.0299],\n                        [ 0.0114,  0.0182,  0.0288]],\n              \n                       [[-0.0316, -0.0088, -0.0152],\n                        [-0.0325, -0.0183, -0.0030],\n                        [-0.0355, -0.0339,  0.0363]],\n              \n                       [[-0.0135,  0.0221,  0.0305],\n                        [-0.0268,  0.0040, -0.0396],\n                        [-0.0201,  0.0218, -0.0349]]],\n              \n              \n                      [[[ 0.0126,  0.0043, -0.0306],\n                        [-0.0146,  0.0352,  0.0244],\n                        [ 0.0250,  0.0273,  0.0250]],\n              \n                       [[-0.0412,  0.0087,  0.0332],\n                        [ 0.0187, -0.0076, -0.0089],\n                        [-0.0151, -0.0058, -0.0293]],\n              \n                       [[-0.0167, -0.0200,  0.0142],\n                        [-0.0356,  0.0294,  0.0118],\n                        [-0.0244, -0.0215,  0.0074]],\n              \n                       ...,\n              \n                       [[-0.0035,  0.0137, -0.0314],\n                        [ 0.0138, -0.0057,  0.0048],\n                        [ 0.0214, -0.0232, -0.0108]],\n              \n                       [[-0.0412, -0.0090, -0.0090],\n                        [-0.0287,  0.0126,  0.0135],\n                        [ 0.0138,  0.0354, -0.0151]],\n              \n                       [[ 0.0006, -0.0026,  0.0229],\n                        [ 0.0340,  0.0215,  0.0193],\n                        [-0.0062,  0.0044,  0.0232]]],\n              \n              \n                      [[[ 0.0393,  0.0131, -0.0272],\n                        [-0.0268, -0.0212,  0.0276],\n                        [-0.0300,  0.0367, -0.0406]],\n              \n                       [[ 0.0010, -0.0226, -0.0340],\n                        [ 0.0188,  0.0097, -0.0116],\n                        [ 0.0346, -0.0155,  0.0074]],\n              \n                       [[ 0.0277, -0.0405,  0.0331],\n                        [ 0.0064,  0.0333,  0.0368],\n                        [ 0.0375,  0.0212, -0.0242]],\n              \n                       ...,\n              \n                       [[-0.0069,  0.0186, -0.0329],\n                        [ 0.0099, -0.0293,  0.0133],\n                        [ 0.0385,  0.0099,  0.0152]],\n              \n                       [[ 0.0165,  0.0133,  0.0077],\n                        [-0.0347, -0.0064,  0.0321],\n                        [-0.0038, -0.0347,  0.0405]],\n              \n                       [[ 0.0055, -0.0044, -0.0135],\n                        [ 0.0195,  0.0027,  0.0329],\n                        [-0.0107,  0.0344, -0.0313]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0298, -0.0407, -0.0166],\n                        [-0.0002, -0.0221,  0.0067],\n                        [ 0.0178,  0.0013, -0.0193]],\n              \n                       [[-0.0238,  0.0293,  0.0269],\n                        [ 0.0277,  0.0384,  0.0140],\n                        [-0.0363, -0.0101,  0.0253]],\n              \n                       [[ 0.0334, -0.0225, -0.0067],\n                        [-0.0341,  0.0260, -0.0054],\n                        [ 0.0118,  0.0148,  0.0336]],\n              \n                       ...,\n              \n                       [[-0.0390,  0.0067, -0.0146],\n                        [-0.0058, -0.0076,  0.0248],\n                        [-0.0309, -0.0162, -0.0044]],\n              \n                       [[ 0.0156,  0.0133, -0.0077],\n                        [-0.0084, -0.0258,  0.0351],\n                        [ 0.0133, -0.0063,  0.0344]],\n              \n                       [[ 0.0333,  0.0093, -0.0372],\n                        [-0.0002,  0.0405, -0.0157],\n                        [-0.0018, -0.0008,  0.0080]]],\n              \n              \n                      [[[ 0.0330, -0.0097, -0.0083],\n                        [-0.0216,  0.0057, -0.0085],\n                        [ 0.0082,  0.0023,  0.0381]],\n              \n                       [[-0.0320,  0.0131, -0.0137],\n                        [-0.0037,  0.0201, -0.0339],\n                        [ 0.0327,  0.0375, -0.0072]],\n              \n                       [[-0.0085, -0.0173,  0.0102],\n                        [ 0.0381,  0.0038,  0.0299],\n                        [ 0.0261,  0.0366,  0.0206]],\n              \n                       ...,\n              \n                       [[-0.0330, -0.0098, -0.0026],\n                        [ 0.0038,  0.0086,  0.0258],\n                        [-0.0036,  0.0356, -0.0383]],\n              \n                       [[ 0.0014,  0.0289, -0.0069],\n                        [-0.0358, -0.0261, -0.0318],\n                        [-0.0223, -0.0333,  0.0221]],\n              \n                       [[ 0.0099, -0.0044,  0.0356],\n                        [-0.0416,  0.0245,  0.0219],\n                        [-0.0125, -0.0308, -0.0395]]],\n              \n              \n                      [[[-0.0059, -0.0348, -0.0104],\n                        [-0.0281, -0.0408,  0.0101],\n                        [-0.0012,  0.0124, -0.0115]],\n              \n                       [[-0.0382, -0.0336,  0.0156],\n                        [-0.0337,  0.0008,  0.0405],\n                        [-0.0058, -0.0384, -0.0303]],\n              \n                       [[-0.0357,  0.0154,  0.0037],\n                        [ 0.0079,  0.0382, -0.0023],\n                        [-0.0099,  0.0091, -0.0170]],\n              \n                       ...,\n              \n                       [[-0.0194,  0.0131, -0.0097],\n                        [-0.0112, -0.0016, -0.0009],\n                        [-0.0198, -0.0326, -0.0109]],\n              \n                       [[ 0.0248, -0.0348, -0.0202],\n                        [-0.0041, -0.0386, -0.0109],\n                        [-0.0228, -0.0399,  0.0372]],\n              \n                       [[-0.0010, -0.0073,  0.0204],\n                        [-0.0288,  0.0141,  0.0010],\n                        [-0.0160, -0.0138,  0.0360]]]])),\n             ('down1.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.1305e-02, -1.2684e-03,  2.4892e-02],\n                        [-2.6919e-02, -1.1080e-02,  6.1028e-04],\n                        [-6.9626e-03,  2.4179e-02,  7.0370e-03]],\n              \n                       [[-8.0535e-03, -1.8495e-04, -2.7226e-02],\n                        [-1.6500e-02,  3.6307e-03,  2.3883e-02],\n                        [-7.6892e-03,  2.6147e-02,  1.8880e-02]],\n              \n                       [[-6.3356e-04, -7.4601e-03, -7.9877e-03],\n                        [ 1.3430e-02, -1.9490e-02,  3.8737e-03],\n                        [-1.6122e-02, -1.8464e-02,  2.0742e-02]],\n              \n                       ...,\n              \n                       [[ 1.8362e-03, -1.1564e-02, -2.8767e-02],\n                        [ 5.5608e-03,  6.5534e-03,  1.5489e-02],\n                        [-1.3676e-02, -2.4228e-02,  1.2859e-02]],\n              \n                       [[ 1.7046e-02,  3.1059e-03, -1.3043e-02],\n                        [-1.1144e-02,  8.5697e-03, -9.9781e-03],\n                        [ 6.2510e-03, -2.7031e-02, -8.6106e-03]],\n              \n                       [[ 2.8901e-02,  1.9356e-02, -2.5723e-02],\n                        [-2.0941e-02,  1.2509e-02,  2.8496e-02],\n                        [-1.6640e-02, -3.5848e-03, -1.0853e-02]]],\n              \n              \n                      [[[ 1.2726e-02, -1.6195e-02,  1.4709e-02],\n                        [-2.0562e-02, -2.8356e-02,  1.0373e-02],\n                        [ 1.6941e-02, -1.7723e-02,  2.5551e-02]],\n              \n                       [[-1.9462e-02,  2.7471e-02, -1.6930e-02],\n                        [-2.7676e-03, -1.4025e-03,  1.7487e-02],\n                        [ 1.6080e-02,  2.9447e-02, -1.8378e-02]],\n              \n                       [[ 2.8415e-03, -1.0617e-02, -1.0754e-03],\n                        [ 2.2315e-02, -1.2144e-02, -1.7454e-02],\n                        [-2.4725e-02, -1.4872e-02,  1.2383e-02]],\n              \n                       ...,\n              \n                       [[ 2.1383e-02, -2.6270e-02, -1.2159e-02],\n                        [-2.1438e-02, -2.4603e-02, -1.3974e-02],\n                        [-2.2166e-02,  2.9069e-02,  1.0996e-02]],\n              \n                       [[ 2.6262e-02, -3.3151e-03,  2.6866e-02],\n                        [-1.1902e-02,  2.3779e-03,  2.6081e-02],\n                        [ 5.4771e-03,  7.5126e-04, -8.3137e-03]],\n              \n                       [[ 2.5385e-02,  7.2457e-03, -1.6735e-02],\n                        [-4.7629e-03, -1.2607e-02, -4.5772e-03],\n                        [ 1.6854e-02,  1.9901e-02,  2.8703e-02]]],\n              \n              \n                      [[[-2.8001e-02, -4.4546e-04, -2.0191e-02],\n                        [ 2.4830e-02, -2.2498e-02, -2.0728e-02],\n                        [-1.0464e-02,  2.7569e-02,  2.9056e-02]],\n              \n                       [[-2.7124e-02, -7.6276e-03,  2.4910e-02],\n                        [-5.0865e-03, -1.3039e-02, -1.9636e-02],\n                        [-2.0727e-02, -2.3310e-02, -1.5865e-02]],\n              \n                       [[ 7.5711e-03,  7.3599e-03, -2.2980e-02],\n                        [-2.5551e-02,  2.2718e-02,  1.5489e-02],\n                        [-3.0655e-04,  1.2903e-02, -2.2033e-02]],\n              \n                       ...,\n              \n                       [[-1.5014e-02, -7.5347e-04,  1.6599e-03],\n                        [-5.4850e-03,  1.3427e-02,  2.9824e-03],\n                        [ 2.4041e-02,  1.7558e-03,  1.0491e-02]],\n              \n                       [[-1.7517e-02,  2.2218e-02,  2.1117e-02],\n                        [-8.5116e-05,  2.7633e-02,  1.1950e-03],\n                        [ 2.3484e-02, -2.0629e-02, -7.9562e-03]],\n              \n                       [[ 6.6841e-03, -2.7769e-02, -2.2987e-02],\n                        [-2.4637e-02,  2.2629e-02, -1.2457e-02],\n                        [-1.0986e-02, -1.6586e-02, -4.0791e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.6628e-03,  2.6667e-02,  6.7481e-03],\n                        [-1.4348e-02, -1.9016e-02,  2.1977e-02],\n                        [ 1.1526e-02,  2.0264e-03, -1.9429e-02]],\n              \n                       [[-1.5399e-02,  2.4140e-02,  1.7281e-02],\n                        [-5.1553e-05,  2.7146e-03, -2.2730e-02],\n                        [-2.2137e-02,  1.5756e-02,  9.6129e-03]],\n              \n                       [[-5.2356e-03,  1.8795e-02,  1.4753e-02],\n                        [-2.9235e-02, -2.4725e-02, -9.9595e-03],\n                        [-2.5816e-02, -1.2593e-02, -1.4906e-02]],\n              \n                       ...,\n              \n                       [[-5.1329e-04,  2.4464e-02,  1.0491e-02],\n                        [ 1.6588e-03, -1.9864e-02, -2.4729e-02],\n                        [-5.7917e-03,  1.2495e-02,  7.5220e-03]],\n              \n                       [[ 1.5368e-02, -2.5456e-02, -1.4819e-02],\n                        [-2.5614e-02, -2.3670e-03,  2.6447e-02],\n                        [-5.4125e-03, -4.6167e-03, -7.2054e-04]],\n              \n                       [[-1.7071e-02, -2.6587e-03,  2.1725e-02],\n                        [-2.8988e-02,  3.1809e-03,  1.3815e-03],\n                        [ 6.4158e-03, -2.6444e-04,  1.8910e-02]]],\n              \n              \n                      [[[ 2.5009e-02,  4.4661e-03, -2.5017e-02],\n                        [ 6.8237e-03,  1.3778e-02,  6.8838e-03],\n                        [-1.5440e-02, -1.2293e-03,  2.2054e-02]],\n              \n                       [[-1.6465e-02,  1.3906e-02,  2.9242e-02],\n                        [ 2.2392e-02, -6.8427e-03, -2.1006e-02],\n                        [ 2.3828e-02, -1.8528e-02,  4.6238e-03]],\n              \n                       [[ 2.6324e-02, -3.9792e-03, -2.8550e-02],\n                        [ 9.2739e-03,  8.2617e-03, -2.5574e-02],\n                        [ 1.6078e-02,  1.6129e-02,  6.8392e-03]],\n              \n                       ...,\n              \n                       [[ 2.7127e-02, -1.3369e-02,  8.5266e-03],\n                        [-1.0530e-02, -2.0817e-02, -8.6817e-03],\n                        [-2.9038e-02, -2.4825e-03,  1.3813e-02]],\n              \n                       [[ 1.2809e-02, -2.7485e-02, -2.8767e-02],\n                        [-5.6553e-03,  1.9724e-02,  1.1964e-02],\n                        [ 5.6818e-03,  1.9974e-02, -1.8658e-02]],\n              \n                       [[ 2.8031e-02, -2.4776e-02, -3.0622e-03],\n                        [ 1.4898e-02,  2.7475e-03, -2.2119e-02],\n                        [ 5.8204e-03,  6.9012e-03, -2.6735e-02]]],\n              \n              \n                      [[[ 9.7910e-03,  1.7056e-02, -4.8750e-03],\n                        [ 3.8653e-03,  9.2350e-03, -2.7748e-02],\n                        [ 2.4542e-02, -9.4870e-03,  2.7431e-02]],\n              \n                       [[ 1.5725e-03,  5.4433e-03,  6.2727e-03],\n                        [ 2.9122e-02,  1.9450e-02, -1.4450e-02],\n                        [ 7.3775e-03,  2.3615e-02, -1.2452e-02]],\n              \n                       [[-7.7901e-04,  5.2408e-03,  1.3440e-02],\n                        [ 1.1745e-02, -2.4794e-02,  5.6418e-03],\n                        [ 1.4150e-02, -1.9262e-02, -6.3717e-04]],\n              \n                       ...,\n              \n                       [[ 4.6180e-03,  2.1094e-03, -2.5070e-02],\n                        [-1.9577e-02,  2.3995e-02, -1.5351e-02],\n                        [-2.1875e-02, -2.0034e-03,  3.7910e-03]],\n              \n                       [[ 2.1114e-03,  2.1738e-02,  1.3168e-03],\n                        [-9.2969e-03,  1.9882e-02,  5.0677e-03],\n                        [ 6.9171e-03,  2.1555e-02, -1.1559e-02]],\n              \n                       [[-2.8176e-02, -2.6783e-02,  2.4445e-02],\n                        [ 1.4733e-02,  4.4278e-03,  7.2822e-03],\n                        [-2.4972e-02, -1.4935e-02,  2.7857e-02]]]])),\n             ('down1.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-2.0874e-03,  2.8328e-02,  3.8197e-03],\n                        [ 2.0103e-02, -2.4530e-02,  3.5383e-03],\n                        [ 1.2657e-02,  2.5045e-02,  5.3281e-03]],\n              \n                       [[ 9.3871e-03,  2.5844e-02, -1.4631e-02],\n                        [ 2.7466e-02, -1.0389e-02,  1.5178e-02],\n                        [ 2.8453e-02,  1.3451e-02, -1.1607e-03]],\n              \n                       [[ 2.0450e-02,  1.3948e-02, -1.8822e-02],\n                        [-1.6178e-03,  2.4138e-02,  1.6494e-02],\n                        [-2.7684e-02, -1.6600e-02,  2.5942e-03]],\n              \n                       ...,\n              \n                       [[-2.5010e-03,  2.1981e-02,  1.0307e-02],\n                        [ 1.0725e-02,  2.8690e-02, -1.7391e-02],\n                        [ 3.5500e-03,  2.0341e-03,  5.9864e-03]],\n              \n                       [[-8.7539e-03,  1.3636e-02,  2.7444e-02],\n                        [-5.3241e-03,  1.4782e-02, -1.6061e-02],\n                        [ 2.8436e-02, -2.6700e-02, -5.3704e-03]],\n              \n                       [[-2.3932e-02,  6.0354e-03,  2.0279e-02],\n                        [-2.7523e-02, -2.8895e-02,  2.0104e-02],\n                        [-6.3520e-03,  8.0765e-03,  2.4935e-03]]],\n              \n              \n                      [[[-1.0771e-02, -3.8036e-03, -2.3648e-02],\n                        [-1.3159e-02,  2.4382e-02,  2.5068e-02],\n                        [-1.8793e-02, -2.5927e-02,  1.6405e-02]],\n              \n                       [[ 4.6219e-03,  2.3189e-02, -1.0743e-02],\n                        [ 2.8896e-02, -2.2556e-02,  5.3712e-03],\n                        [-8.8788e-03, -8.3982e-03, -9.5629e-03]],\n              \n                       [[-2.3292e-02,  1.9044e-02,  1.8797e-03],\n                        [-1.7992e-02, -2.8691e-02,  1.8576e-03],\n                        [-2.4593e-02,  8.3165e-03, -5.6803e-03]],\n              \n                       ...,\n              \n                       [[-2.7325e-02, -1.6579e-02, -2.7656e-02],\n                        [-1.4223e-02,  6.2641e-03, -2.7416e-02],\n                        [-1.8046e-02,  1.1367e-02, -1.2150e-02]],\n              \n                       [[-3.4729e-03,  5.4115e-04, -1.9539e-02],\n                        [ 1.6914e-02, -1.1351e-02,  2.0686e-02],\n                        [-1.0540e-02, -2.7865e-02,  3.4599e-03]],\n              \n                       [[-1.5403e-02, -5.0929e-03, -2.0951e-02],\n                        [ 1.8758e-02, -1.5846e-02, -2.6030e-02],\n                        [ 2.3687e-02, -2.6410e-02,  5.7963e-03]]],\n              \n              \n                      [[[-2.6278e-02, -1.2930e-02, -1.6344e-02],\n                        [ 8.9017e-03, -1.8674e-02, -1.6698e-02],\n                        [-1.0313e-02,  9.8180e-03,  1.0110e-02]],\n              \n                       [[-2.1049e-02,  1.4577e-02, -1.8113e-02],\n                        [-2.0648e-02, -1.4387e-02, -2.4280e-04],\n                        [-2.0775e-02, -4.0661e-03,  2.7782e-02]],\n              \n                       [[-2.7178e-02,  4.2496e-03, -2.3201e-02],\n                        [ 1.0937e-02, -6.5350e-03, -2.3540e-02],\n                        [-2.9455e-02,  2.3027e-02, -2.7718e-02]],\n              \n                       ...,\n              \n                       [[-2.1814e-02,  1.5335e-02, -2.3714e-02],\n                        [-2.8257e-02,  2.3738e-02, -1.3762e-02],\n                        [-3.1294e-03,  9.6518e-03,  6.7151e-03]],\n              \n                       [[-2.5689e-02,  4.9199e-03,  1.6813e-02],\n                        [ 2.7413e-02, -2.5757e-02, -2.6320e-02],\n                        [ 2.8428e-02, -1.9982e-02, -6.2184e-03]],\n              \n                       [[-4.9595e-03, -2.2561e-02,  2.1508e-02],\n                        [ 6.1043e-03, -1.9141e-02, -1.6917e-02],\n                        [-2.2802e-02, -7.2276e-03,  1.1010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8587e-04,  2.5234e-02,  1.2862e-02],\n                        [ 6.4087e-03,  2.9456e-03, -6.2891e-03],\n                        [ 1.3295e-02,  1.1122e-02, -3.8489e-03]],\n              \n                       [[ 2.4627e-02, -8.6374e-03,  9.6317e-03],\n                        [-4.4341e-03, -2.0696e-03,  5.3607e-05],\n                        [ 2.7382e-02, -1.1736e-03, -2.8442e-03]],\n              \n                       [[ 7.9895e-03, -6.4228e-03,  9.2783e-03],\n                        [ 1.0661e-03, -2.7210e-02,  2.9449e-02],\n                        [ 2.8375e-03, -2.2452e-02, -3.4423e-03]],\n              \n                       ...,\n              \n                       [[ 7.1594e-03, -2.7026e-02, -6.7921e-03],\n                        [-1.5202e-02, -7.0004e-04, -6.5862e-03],\n                        [ 2.7967e-02,  2.5300e-02,  5.7218e-03]],\n              \n                       [[ 1.9714e-02,  2.5212e-02,  2.6632e-02],\n                        [ 3.6115e-03, -2.2397e-02, -1.0878e-02],\n                        [-1.3762e-02,  4.6104e-04,  1.6057e-02]],\n              \n                       [[ 2.5034e-02, -2.9420e-02, -1.7739e-02],\n                        [ 1.0064e-02, -2.8722e-02, -1.6836e-02],\n                        [ 1.7448e-02,  2.8111e-02,  1.4150e-03]]],\n              \n              \n                      [[[-1.5742e-02, -1.3421e-02,  2.7663e-02],\n                        [-1.5744e-02,  2.0141e-03,  1.1419e-03],\n                        [ 2.5981e-02,  1.0222e-02, -1.5587e-02]],\n              \n                       [[ 1.3669e-02,  5.2103e-03, -7.6013e-03],\n                        [-1.6173e-02,  5.6269e-04,  2.4350e-03],\n                        [ 2.4261e-03,  2.5788e-02, -2.8097e-02]],\n              \n                       [[-1.4888e-02, -1.7731e-02, -6.4337e-03],\n                        [ 2.2471e-02,  2.3679e-04, -1.1437e-02],\n                        [-5.8912e-03,  1.0241e-02,  1.8909e-02]],\n              \n                       ...,\n              \n                       [[-1.4776e-02,  2.1398e-02,  8.8336e-04],\n                        [-3.3876e-03,  9.3768e-03, -5.3336e-03],\n                        [-4.4843e-03, -5.7139e-03, -6.8183e-03]],\n              \n                       [[-2.0888e-02, -2.4299e-02, -1.6261e-02],\n                        [-2.0847e-02,  1.3012e-02,  2.1894e-02],\n                        [-4.3075e-03,  2.1090e-02,  2.2750e-02]],\n              \n                       [[-1.7861e-02, -2.5487e-02, -9.7013e-03],\n                        [-2.8849e-03, -2.6374e-02, -2.2423e-02],\n                        [ 3.2294e-03,  1.0469e-02, -2.7943e-02]]],\n              \n              \n                      [[[ 4.1885e-03, -2.7628e-02, -2.5770e-02],\n                        [ 1.4383e-02, -3.2527e-03, -2.1710e-02],\n                        [-1.4146e-02,  7.5708e-03, -1.2968e-02]],\n              \n                       [[ 6.4110e-03,  1.5356e-02, -1.1846e-02],\n                        [ 2.1303e-02,  6.4434e-03, -2.6370e-02],\n                        [ 1.7484e-02,  1.9423e-02,  2.9357e-02]],\n              \n                       [[ 3.5598e-03,  2.6142e-02, -2.6987e-02],\n                        [ 9.4496e-03,  1.8193e-02,  1.0256e-02],\n                        [ 3.0655e-03,  2.6695e-03, -9.7217e-04]],\n              \n                       ...,\n              \n                       [[ 1.2180e-02,  2.1096e-02, -2.4789e-02],\n                        [ 6.3251e-03,  3.0475e-03, -6.8353e-03],\n                        [ 1.8787e-02, -9.2431e-03,  1.7185e-02]],\n              \n                       [[-1.1940e-02,  1.8412e-02,  1.7622e-02],\n                        [ 2.1504e-02,  2.3440e-02,  1.1492e-02],\n                        [-1.6089e-02, -1.5441e-02,  2.1249e-02]],\n              \n                       [[-2.3543e-02, -2.0001e-02, -2.0346e-02],\n                        [ 2.0520e-02,  2.9473e-03, -1.2873e-02],\n                        [ 1.3080e-02, -1.3335e-02,  2.4488e-02]]]])),\n             ('down2.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-0.0199, -0.0207, -0.0025],\n                        [-0.0202,  0.0202, -0.0180],\n                        [-0.0126,  0.0164, -0.0123]],\n              \n                       [[ 0.0062, -0.0141,  0.0168],\n                        [ 0.0078,  0.0006, -0.0096],\n                        [ 0.0036, -0.0188,  0.0195]],\n              \n                       [[-0.0073, -0.0065, -0.0040],\n                        [ 0.0086,  0.0105,  0.0089],\n                        [-0.0055,  0.0144, -0.0161]],\n              \n                       ...,\n              \n                       [[ 0.0131, -0.0028, -0.0143],\n                        [-0.0057, -0.0096, -0.0171],\n                        [-0.0130, -0.0047, -0.0005]],\n              \n                       [[-0.0046, -0.0177,  0.0125],\n                        [-0.0102,  0.0154,  0.0072],\n                        [ 0.0206,  0.0169, -0.0156]],\n              \n                       [[ 0.0036,  0.0074,  0.0056],\n                        [ 0.0112, -0.0127, -0.0147],\n                        [ 0.0001,  0.0135,  0.0017]]],\n              \n              \n                      [[[-0.0075, -0.0151,  0.0206],\n                        [ 0.0001, -0.0105, -0.0072],\n                        [ 0.0066,  0.0189,  0.0178]],\n              \n                       [[ 0.0086, -0.0003,  0.0005],\n                        [ 0.0185, -0.0089, -0.0045],\n                        [ 0.0166, -0.0010,  0.0182]],\n              \n                       [[-0.0107, -0.0202,  0.0050],\n                        [-0.0029, -0.0139,  0.0134],\n                        [ 0.0037,  0.0136, -0.0140]],\n              \n                       ...,\n              \n                       [[ 0.0171,  0.0028,  0.0002],\n                        [ 0.0165,  0.0112,  0.0014],\n                        [-0.0089, -0.0016,  0.0104]],\n              \n                       [[-0.0161, -0.0097, -0.0042],\n                        [ 0.0174,  0.0107,  0.0100],\n                        [-0.0053, -0.0070,  0.0113]],\n              \n                       [[-0.0016, -0.0070,  0.0061],\n                        [ 0.0017,  0.0160,  0.0013],\n                        [ 0.0057,  0.0200, -0.0160]]],\n              \n              \n                      [[[-0.0060, -0.0105, -0.0198],\n                        [-0.0150, -0.0083,  0.0156],\n                        [-0.0090,  0.0120, -0.0199]],\n              \n                       [[ 0.0127,  0.0145, -0.0122],\n                        [ 0.0110, -0.0001, -0.0018],\n                        [ 0.0039,  0.0206, -0.0076]],\n              \n                       [[ 0.0101,  0.0061, -0.0136],\n                        [ 0.0194, -0.0136,  0.0016],\n                        [-0.0007,  0.0173,  0.0011]],\n              \n                       ...,\n              \n                       [[-0.0134, -0.0127, -0.0165],\n                        [ 0.0041, -0.0118,  0.0110],\n                        [ 0.0044,  0.0060,  0.0036]],\n              \n                       [[ 0.0056, -0.0185,  0.0055],\n                        [ 0.0114, -0.0050, -0.0185],\n                        [ 0.0116, -0.0140, -0.0148]],\n              \n                       [[ 0.0145,  0.0188, -0.0130],\n                        [ 0.0065, -0.0171,  0.0036],\n                        [-0.0037, -0.0078,  0.0077]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0090,  0.0069, -0.0124],\n                        [-0.0150, -0.0065,  0.0094],\n                        [-0.0195, -0.0163, -0.0144]],\n              \n                       [[-0.0142,  0.0055, -0.0013],\n                        [-0.0149, -0.0092,  0.0063],\n                        [ 0.0007,  0.0089,  0.0060]],\n              \n                       [[-0.0055, -0.0047, -0.0065],\n                        [-0.0140,  0.0113, -0.0194],\n                        [-0.0049,  0.0079,  0.0079]],\n              \n                       ...,\n              \n                       [[-0.0111, -0.0127,  0.0139],\n                        [ 0.0075, -0.0173, -0.0109],\n                        [ 0.0204, -0.0063, -0.0174]],\n              \n                       [[ 0.0198,  0.0142,  0.0200],\n                        [ 0.0188,  0.0201, -0.0102],\n                        [ 0.0027, -0.0103, -0.0160]],\n              \n                       [[ 0.0090,  0.0116,  0.0114],\n                        [-0.0037, -0.0078,  0.0121],\n                        [-0.0192, -0.0149, -0.0202]]],\n              \n              \n                      [[[ 0.0045, -0.0102,  0.0195],\n                        [-0.0163, -0.0012,  0.0005],\n                        [ 0.0079, -0.0045,  0.0198]],\n              \n                       [[ 0.0181,  0.0146, -0.0039],\n                        [ 0.0095,  0.0106, -0.0055],\n                        [ 0.0028,  0.0103,  0.0006]],\n              \n                       [[ 0.0039, -0.0051, -0.0071],\n                        [-0.0123, -0.0141,  0.0050],\n                        [-0.0146,  0.0068,  0.0163]],\n              \n                       ...,\n              \n                       [[-0.0144,  0.0072, -0.0097],\n                        [-0.0070,  0.0141,  0.0089],\n                        [-0.0034,  0.0030,  0.0124]],\n              \n                       [[ 0.0143, -0.0146, -0.0182],\n                        [-0.0080,  0.0061, -0.0181],\n                        [ 0.0166,  0.0175, -0.0116]],\n              \n                       [[-0.0095, -0.0014, -0.0191],\n                        [ 0.0184, -0.0074, -0.0144],\n                        [ 0.0201, -0.0136, -0.0001]]],\n              \n              \n                      [[[-0.0022, -0.0024,  0.0035],\n                        [-0.0075, -0.0206,  0.0173],\n                        [-0.0160,  0.0207,  0.0060]],\n              \n                       [[-0.0073,  0.0075, -0.0149],\n                        [-0.0112,  0.0081, -0.0034],\n                        [-0.0176, -0.0169,  0.0041]],\n              \n                       [[-0.0040,  0.0199, -0.0174],\n                        [ 0.0103,  0.0153, -0.0109],\n                        [-0.0044, -0.0160, -0.0072]],\n              \n                       ...,\n              \n                       [[ 0.0142, -0.0045,  0.0044],\n                        [-0.0134, -0.0153, -0.0110],\n                        [-0.0178,  0.0051, -0.0051]],\n              \n                       [[ 0.0090,  0.0175,  0.0111],\n                        [ 0.0201, -0.0061,  0.0081],\n                        [-0.0037,  0.0166,  0.0074]],\n              \n                       [[-0.0069,  0.0019, -0.0200],\n                        [-0.0047, -0.0145,  0.0192],\n                        [-0.0100,  0.0121, -0.0193]]]])),\n             ('down2.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-4.6348e-03,  9.8509e-03,  1.6142e-02],\n                        [ 2.6825e-05, -8.4992e-03,  3.6535e-04],\n                        [-2.0749e-02, -2.7181e-03,  1.4475e-02]],\n              \n                       [[ 1.0194e-02,  6.9748e-03,  1.3849e-02],\n                        [ 1.4200e-03,  2.5024e-03,  1.5259e-02],\n                        [ 1.1671e-02,  4.0497e-03,  8.7697e-03]],\n              \n                       [[-4.4309e-03, -1.1845e-02, -1.6037e-02],\n                        [-7.8910e-03, -9.7038e-03,  5.6008e-03],\n                        [-1.6987e-02,  7.1697e-03,  1.7236e-02]],\n              \n                       ...,\n              \n                       [[-1.1635e-02,  1.8610e-02,  1.4086e-02],\n                        [-1.1576e-02, -1.9610e-03, -1.8455e-02],\n                        [-8.6874e-03, -1.1485e-02, -5.8817e-03]],\n              \n                       [[-1.3743e-02,  1.2879e-02,  2.2404e-03],\n                        [-6.8730e-03,  1.0492e-02,  8.4602e-03],\n                        [ 1.9366e-03, -1.0892e-02,  9.0133e-03]],\n              \n                       [[-6.9619e-03, -1.7941e-02, -1.1306e-02],\n                        [-6.8960e-03, -6.8894e-03, -6.9923e-04],\n                        [ 1.0807e-02,  1.8476e-02,  1.9441e-02]]],\n              \n              \n                      [[[ 6.4426e-03,  7.5100e-03,  6.7503e-03],\n                        [-1.8439e-02,  1.4277e-02, -1.0381e-02],\n                        [-1.7296e-02, -1.2204e-02,  5.2923e-03]],\n              \n                       [[-6.8046e-03,  6.3742e-03, -1.1632e-02],\n                        [ 4.2213e-03,  2.0774e-02, -3.7589e-03],\n                        [ 1.6312e-02,  7.4283e-04,  1.2614e-02]],\n              \n                       [[-6.7564e-03, -1.0808e-02, -1.6746e-02],\n                        [-6.2140e-03,  9.3120e-03, -9.2284e-03],\n                        [ 2.8789e-03,  1.2397e-03,  1.5193e-02]],\n              \n                       ...,\n              \n                       [[-1.4065e-02, -4.0645e-03, -1.4819e-02],\n                        [ 7.9262e-03, -1.4440e-02, -1.3676e-02],\n                        [ 8.2918e-04,  1.0951e-02,  6.6675e-03]],\n              \n                       [[ 1.8929e-02, -1.6932e-02,  7.8811e-03],\n                        [ 1.6661e-02, -1.4852e-02, -6.1440e-03],\n                        [-4.3739e-03,  1.0890e-02,  1.2552e-03]],\n              \n                       [[ 1.6674e-02,  8.4053e-03, -5.2151e-03],\n                        [-1.8711e-02, -6.0464e-04,  4.8782e-03],\n                        [-1.0599e-02, -8.5500e-03, -4.4493e-04]]],\n              \n              \n                      [[[ 7.4150e-03, -1.7817e-02, -9.8810e-03],\n                        [ 1.5139e-02, -5.4702e-03,  3.1069e-03],\n                        [ 1.6121e-02, -2.4298e-03, -3.4243e-03]],\n              \n                       [[ 5.2642e-03, -1.7880e-02, -1.8678e-02],\n                        [ 2.9048e-03,  1.0568e-02, -2.8701e-04],\n                        [-4.0345e-05, -2.8312e-03,  6.9242e-03]],\n              \n                       [[ 1.2557e-02,  1.3475e-02, -1.1946e-02],\n                        [ 1.0504e-02, -1.1848e-02,  1.4417e-02],\n                        [-1.8312e-02,  1.1722e-02, -6.9120e-03]],\n              \n                       ...,\n              \n                       [[ 1.9895e-02,  1.5509e-02,  1.9991e-02],\n                        [-1.5190e-02, -1.9972e-02, -1.3091e-02],\n                        [-1.1537e-02, -6.8988e-03,  1.1122e-02]],\n              \n                       [[ 1.0277e-02, -9.5677e-03,  1.4165e-02],\n                        [ 5.0890e-03,  1.1992e-02,  2.0542e-02],\n                        [-9.9942e-04,  1.1082e-02, -5.1328e-03]],\n              \n                       [[ 1.0213e-02, -4.6551e-03, -5.2989e-03],\n                        [ 1.5165e-02, -1.7655e-02,  5.5892e-03],\n                        [ 1.1311e-02, -1.2807e-02, -1.2253e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4459e-02,  4.5380e-04, -2.9677e-03],\n                        [ 1.8889e-02, -1.6052e-02, -1.5562e-02],\n                        [ 1.3935e-03, -1.6170e-02,  2.0204e-02]],\n              \n                       [[ 1.0080e-02, -3.7539e-03, -1.5059e-02],\n                        [ 6.8971e-03, -8.5807e-03,  1.5525e-02],\n                        [ 1.4992e-03, -7.8594e-03,  7.5005e-03]],\n              \n                       [[ 3.7703e-03,  9.6159e-03,  1.6808e-02],\n                        [-1.1511e-02, -1.9614e-02, -1.7621e-02],\n                        [ 6.5007e-03, -1.5883e-02, -1.3063e-02]],\n              \n                       ...,\n              \n                       [[ 1.1717e-02,  1.3965e-03, -5.3536e-03],\n                        [ 1.4582e-02, -1.8533e-03, -1.5276e-02],\n                        [-2.0322e-02, -1.0361e-02, -6.1722e-03]],\n              \n                       [[ 5.0393e-04,  3.0661e-03, -9.3391e-03],\n                        [-5.0653e-03,  1.3716e-02,  9.7900e-03],\n                        [-2.0547e-02,  1.3067e-02,  1.6991e-03]],\n              \n                       [[-8.7317e-03,  1.5140e-02, -9.8445e-03],\n                        [-2.9895e-03,  1.0854e-02, -7.8243e-03],\n                        [ 1.5019e-03,  1.9270e-02,  9.2994e-03]]],\n              \n              \n                      [[[-3.2868e-03, -1.6655e-03,  1.3082e-02],\n                        [ 7.1859e-03, -1.9157e-03, -3.5394e-03],\n                        [-1.9397e-02,  5.5216e-03, -1.8486e-02]],\n              \n                       [[ 9.8068e-03,  2.6197e-03,  4.8447e-04],\n                        [ 1.5565e-02,  1.1252e-02,  1.8660e-02],\n                        [ 3.1310e-03,  6.5078e-03, -1.4506e-02]],\n              \n                       [[-1.5900e-02, -3.8698e-03,  4.6403e-03],\n                        [ 1.0163e-02,  1.0891e-02,  1.9025e-02],\n                        [-7.0364e-03,  1.0454e-02,  7.3635e-03]],\n              \n                       ...,\n              \n                       [[ 1.5563e-02, -1.9394e-02,  1.5875e-03],\n                        [-4.1397e-03, -7.3719e-04, -8.6707e-03],\n                        [-1.5182e-02,  1.4803e-02, -1.7555e-02]],\n              \n                       [[-7.9233e-04,  1.1101e-03,  1.7634e-03],\n                        [ 1.5103e-02, -1.4403e-02,  1.4855e-02],\n                        [-7.4607e-03,  7.4488e-03, -1.7282e-02]],\n              \n                       [[ 1.4080e-02,  1.6888e-02,  1.6374e-02],\n                        [ 7.7976e-03, -6.2802e-03, -3.1626e-03],\n                        [ 2.0682e-02, -1.9079e-02,  1.3276e-02]]],\n              \n              \n                      [[[ 1.8058e-02, -9.1462e-03, -7.2015e-03],\n                        [-6.4691e-03, -2.9027e-03,  9.6589e-03],\n                        [-1.3747e-02,  1.9787e-02,  1.9956e-02]],\n              \n                       [[-1.1408e-02, -2.4681e-05,  7.7289e-03],\n                        [ 1.9633e-02, -8.2515e-03,  1.3016e-02],\n                        [-1.8417e-02,  1.8677e-02, -1.1818e-02]],\n              \n                       [[ 1.9430e-02,  1.0222e-02, -5.9156e-03],\n                        [ 1.5036e-02,  9.4860e-03,  2.0289e-03],\n                        [-6.1385e-03, -6.8786e-03, -1.0498e-02]],\n              \n                       ...,\n              \n                       [[ 1.8626e-02, -4.7810e-03,  1.8702e-02],\n                        [-7.9554e-03, -1.7242e-02, -1.2626e-03],\n                        [ 1.9328e-02, -5.6285e-03, -1.1736e-02]],\n              \n                       [[-4.1653e-04, -1.8020e-02, -1.2647e-02],\n                        [-4.7124e-03,  3.7225e-03,  3.3474e-03],\n                        [-2.6790e-03,  6.2666e-03,  3.8707e-03]],\n              \n                       [[ 1.9958e-03, -6.2181e-03, -1.5993e-02],\n                        [ 4.3567e-03,  2.8269e-03,  2.0313e-02],\n                        [-1.6953e-02, -1.2477e-02, -6.3685e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.3495e-02,  1.1336e-02,  3.2999e-03],\n                        [ 1.0248e-02,  4.9058e-03,  1.6721e-03],\n                        [ 1.4577e-02,  1.2254e-02, -1.0996e-02]],\n              \n                       [[ 2.8387e-03, -1.2857e-02, -6.3248e-04],\n                        [ 1.0179e-02, -7.9369e-03,  9.4359e-03],\n                        [ 2.8751e-03, -1.1316e-02, -2.7018e-03]],\n              \n                       [[ 1.3239e-02,  1.3039e-03, -1.3213e-02],\n                        [-8.4236e-03,  2.3438e-03, -1.4353e-02],\n                        [ 9.7540e-03,  7.3673e-03,  9.9629e-04]],\n              \n                       ...,\n              \n                       [[-1.2715e-02, -5.7416e-03,  8.1590e-04],\n                        [ 1.2467e-02,  5.0082e-03, -9.3793e-03],\n                        [-1.0866e-02,  6.1197e-03,  2.4678e-03]],\n              \n                       [[-1.3211e-02, -6.7648e-03,  1.4521e-02],\n                        [-5.5102e-03, -5.2198e-03,  1.0626e-02],\n                        [-1.1742e-02, -6.2968e-03, -3.1413e-03]],\n              \n                       [[ 5.9503e-04, -9.2838e-03,  2.2524e-03],\n                        [ 4.4587e-03, -6.3728e-04, -1.4285e-02],\n                        [-5.1423e-03, -5.7166e-03,  1.2934e-02]]],\n              \n              \n                      [[[ 1.8463e-03, -5.4794e-04, -1.8946e-03],\n                        [ 9.7586e-04,  3.5177e-03, -4.0504e-03],\n                        [-6.2299e-03,  5.2996e-03,  1.3720e-02]],\n              \n                       [[-5.9090e-03,  1.6445e-03,  2.7570e-03],\n                        [-9.9673e-04, -1.0245e-02,  5.6605e-03],\n                        [ 1.1391e-02, -1.1658e-02, -1.1734e-02]],\n              \n                       [[-1.1735e-02,  2.4595e-03,  5.7827e-03],\n                        [ 7.1670e-03, -1.6270e-03,  1.0687e-02],\n                        [ 6.0396e-03, -7.3033e-04, -8.5946e-03]],\n              \n                       ...,\n              \n                       [[ 1.1671e-02,  1.3118e-02, -1.3291e-02],\n                        [ 6.1538e-03, -6.0592e-04,  6.6185e-03],\n                        [ 1.2829e-03, -1.3731e-02,  1.4932e-03]],\n              \n                       [[-7.4605e-03,  6.8828e-04, -1.2302e-04],\n                        [-8.1735e-03,  1.2001e-02,  7.8193e-03],\n                        [ 2.0528e-03, -6.3210e-03,  1.3449e-02]],\n              \n                       [[ 2.9136e-03,  6.6908e-03, -3.7520e-03],\n                        [ 9.3340e-03, -4.1290e-03, -1.4161e-02],\n                        [-5.5939e-03,  5.1468e-03,  7.5768e-05]]],\n              \n              \n                      [[[ 7.9902e-03,  8.0955e-03,  1.0381e-02],\n                        [ 6.6680e-03,  2.9378e-03,  6.6944e-03],\n                        [-2.3877e-03, -4.8883e-03,  8.5533e-03]],\n              \n                       [[-1.2371e-02, -1.2348e-02,  4.0223e-03],\n                        [-6.9362e-03, -1.0553e-02,  5.3495e-03],\n                        [ 4.4429e-04,  5.7790e-03, -2.5581e-03]],\n              \n                       [[ 2.1132e-03, -1.0715e-02,  3.1263e-03],\n                        [ 1.4578e-02, -4.7421e-03, -4.1220e-03],\n                        [ 7.7216e-03, -7.0857e-03, -4.0999e-03]],\n              \n                       ...,\n              \n                       [[-1.2722e-02,  4.8952e-03,  3.1216e-03],\n                        [-3.6589e-03,  3.9157e-03,  7.6172e-05],\n                        [ 6.6556e-03,  1.3619e-02, -1.0715e-02]],\n              \n                       [[-8.3624e-03,  2.8966e-03,  7.7819e-03],\n                        [ 9.6693e-03, -1.3035e-02, -1.2682e-02],\n                        [-1.2393e-02,  1.4095e-02, -9.9444e-03]],\n              \n                       [[-2.6372e-03, -9.4880e-03, -4.2093e-03],\n                        [ 2.4768e-03,  5.2376e-03, -1.6081e-03],\n                        [ 1.4001e-03,  8.7849e-03, -6.4915e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.1331e-03, -1.0245e-02,  5.5679e-03],\n                        [-1.3925e-02, -5.4960e-03, -6.4326e-03],\n                        [ 1.0665e-03,  9.3625e-03, -1.0900e-02]],\n              \n                       [[-1.2820e-02, -1.4185e-02,  7.6603e-03],\n                        [ 5.5901e-03, -7.7663e-03, -1.3632e-02],\n                        [-7.8664e-03,  3.8328e-03, -6.1660e-03]],\n              \n                       [[ 2.2009e-03,  1.2656e-02, -5.1460e-03],\n                        [-7.3644e-03, -1.2076e-03,  1.9836e-03],\n                        [-1.4580e-03, -8.4020e-04,  1.0106e-02]],\n              \n                       ...,\n              \n                       [[ 7.8239e-03,  8.2156e-03,  5.3135e-03],\n                        [ 7.6519e-03,  2.5644e-03,  9.5596e-03],\n                        [ 1.2521e-02,  7.5805e-03, -1.3987e-02]],\n              \n                       [[ 1.0951e-02,  7.9635e-04, -6.1090e-03],\n                        [ 7.5488e-03,  1.2158e-02, -1.4382e-02],\n                        [-3.4198e-03, -3.9887e-03, -3.8113e-03]],\n              \n                       [[-1.1689e-02,  9.5688e-03, -5.1517e-03],\n                        [-1.1460e-02, -4.0730e-03, -5.6413e-03],\n                        [ 7.0657e-03,  2.6805e-03, -5.1478e-03]]],\n              \n              \n                      [[[-9.6095e-03, -1.3585e-03, -7.0119e-03],\n                        [ 9.6654e-03,  1.0712e-02,  1.0401e-02],\n                        [-3.5123e-03,  1.3850e-02,  1.0464e-02]],\n              \n                       [[-1.1702e-02, -7.7455e-03, -5.3939e-03],\n                        [-1.2093e-02, -8.4871e-03, -3.2977e-03],\n                        [-1.0420e-02,  8.9802e-03, -4.9594e-03]],\n              \n                       [[-1.2320e-02,  2.4707e-03, -2.3200e-03],\n                        [-3.9590e-03,  1.1381e-02, -3.2109e-03],\n                        [-1.9178e-03, -1.3853e-02, -4.3691e-03]],\n              \n                       ...,\n              \n                       [[ 1.0142e-02,  1.3061e-02,  1.1623e-02],\n                        [-5.8694e-03, -6.4008e-04,  1.3774e-02],\n                        [ 6.2873e-03,  3.2907e-03, -8.4393e-03]],\n              \n                       [[ 3.5045e-03,  4.6928e-03,  1.1195e-02],\n                        [ 5.2034e-03, -9.1595e-03,  1.1639e-02],\n                        [-7.8218e-03,  7.5058e-03, -1.4309e-02]],\n              \n                       [[-2.4525e-03, -3.6981e-03,  1.1964e-02],\n                        [-1.2757e-02, -5.8314e-03, -1.1045e-02],\n                        [ 6.1323e-03,  1.4707e-02, -9.2333e-03]]],\n              \n              \n                      [[[ 5.0627e-03,  1.4049e-02,  7.1501e-03],\n                        [-1.3210e-02,  1.1269e-02,  2.2428e-03],\n                        [-9.7158e-03,  5.5631e-03, -1.2279e-02]],\n              \n                       [[-9.5874e-03, -5.4147e-04,  1.4689e-02],\n                        [ 4.4917e-03, -1.3910e-02, -3.7383e-04],\n                        [-7.5597e-03,  9.3203e-03, -7.5512e-03]],\n              \n                       [[-1.4322e-02, -1.1102e-02,  1.1979e-02],\n                        [ 6.4091e-03, -1.3175e-02,  2.6744e-04],\n                        [ 1.1095e-03,  6.2741e-03,  5.1492e-04]],\n              \n                       ...,\n              \n                       [[ 1.3908e-02,  9.8417e-03,  9.4988e-03],\n                        [ 1.1376e-02,  1.9947e-04, -8.0265e-03],\n                        [-1.1771e-02, -1.0298e-02, -2.5397e-03]],\n              \n                       [[-2.3932e-03,  1.3351e-02,  1.0970e-02],\n                        [ 1.2986e-02,  3.9482e-03, -8.2351e-03],\n                        [-1.0508e-02, -3.3115e-03, -8.0658e-03]],\n              \n                       [[-2.9153e-03,  1.4376e-02, -3.0430e-03],\n                        [ 1.3600e-02, -2.1507e-03, -4.3007e-03],\n                        [-3.6526e-03,  8.3328e-03,  8.7380e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-1.3104e-02,  9.6535e-03,  7.0547e-03],\n                        [ 6.8489e-03,  5.6884e-03, -3.3797e-03],\n                        [-1.3077e-02,  1.1413e-02, -8.2186e-03]],\n              \n                       [[-6.4877e-03,  1.2398e-02,  1.4672e-02],\n                        [-2.8377e-03,  2.9911e-03,  8.6744e-03],\n                        [ 4.6708e-03, -1.9309e-03, -1.3963e-02]],\n              \n                       [[-8.8996e-04, -1.3098e-02, -1.2099e-02],\n                        [ 1.1789e-02, -6.3457e-03,  8.4533e-03],\n                        [ 6.9120e-04,  3.7103e-03, -3.9384e-03]],\n              \n                       ...,\n              \n                       [[-1.4631e-02,  7.6187e-03,  1.3055e-02],\n                        [ 8.7348e-03,  2.2455e-03,  1.4252e-02],\n                        [-7.8609e-03,  6.6497e-03,  1.2674e-02]],\n              \n                       [[ 1.0928e-02,  8.1940e-03,  1.4620e-03],\n                        [ 1.1112e-03, -7.0720e-03, -1.2397e-02],\n                        [ 1.3073e-02,  2.2528e-03,  6.1473e-03]],\n              \n                       [[-1.1589e-02, -9.5213e-03, -5.2496e-03],\n                        [-1.1412e-02, -1.3629e-02,  7.4268e-03],\n                        [-6.4922e-03,  1.1146e-02, -9.5554e-03]]],\n              \n              \n                      [[[ 2.3625e-05, -1.3995e-02, -7.6334e-03],\n                        [-9.4009e-03, -9.2042e-03,  5.7072e-03],\n                        [ 9.9287e-03, -5.7740e-03,  8.9586e-03]],\n              \n                       [[ 1.4008e-02, -1.0200e-02,  1.3237e-02],\n                        [ 1.4621e-02, -1.2051e-02,  6.9597e-03],\n                        [ 1.2422e-02, -8.4337e-03, -7.5494e-03]],\n              \n                       [[ 5.7422e-04, -8.9031e-03,  1.4246e-02],\n                        [-3.9909e-03, -1.2648e-05,  7.5228e-03],\n                        [ 4.5517e-03, -8.1091e-03, -2.5926e-03]],\n              \n                       ...,\n              \n                       [[ 1.7802e-03,  1.2118e-02, -8.6626e-04],\n                        [-6.0965e-04, -5.6477e-03, -4.7239e-03],\n                        [-1.4231e-03, -1.1298e-02,  4.0613e-03]],\n              \n                       [[ 2.4961e-05,  4.4265e-03,  1.4223e-02],\n                        [ 2.2458e-03,  1.3728e-02, -1.1796e-02],\n                        [-7.2479e-03,  1.2696e-02,  4.3921e-03]],\n              \n                       [[ 1.4457e-02, -1.0118e-02,  1.3083e-02],\n                        [-7.3051e-03,  1.3544e-02, -1.2357e-02],\n                        [ 3.5746e-03, -1.3268e-02, -9.3003e-03]]],\n              \n              \n                      [[[-3.1621e-03,  1.4471e-02,  1.0941e-02],\n                        [ 1.2192e-02,  5.9600e-03,  7.0732e-03],\n                        [ 1.6198e-03, -1.1914e-02, -1.1316e-02]],\n              \n                       [[-8.1733e-03, -4.6493e-03,  1.3078e-02],\n                        [-5.0052e-03, -1.0437e-02,  9.8975e-03],\n                        [-1.3412e-02, -8.9157e-03,  1.3293e-02]],\n              \n                       [[-5.0194e-03,  6.6695e-03,  3.4234e-04],\n                        [-1.3336e-02,  1.4430e-03,  7.5926e-03],\n                        [-1.0269e-03,  1.0630e-02, -8.4293e-03]],\n              \n                       ...,\n              \n                       [[ 1.0040e-02, -9.6519e-03,  1.1701e-02],\n                        [ 6.5308e-05,  3.5704e-03, -1.2048e-02],\n                        [-9.5033e-03, -1.2604e-02, -1.2307e-02]],\n              \n                       [[-6.6415e-03, -1.0024e-02,  1.3435e-02],\n                        [-6.3868e-03, -1.4265e-02, -2.8581e-03],\n                        [-1.3789e-02,  1.1855e-02,  7.1601e-03]],\n              \n                       [[-9.1238e-03,  4.7032e-05, -2.2387e-03],\n                        [ 4.9879e-04,  7.7738e-03,  5.1973e-03],\n                        [ 3.4793e-03,  9.1406e-03, -9.1121e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.2879e-03,  1.1191e-03, -6.0251e-03],\n                        [-3.2071e-03,  5.4502e-03,  1.2839e-04],\n                        [ 5.8309e-03, -1.3948e-02,  3.9841e-03]],\n              \n                       [[ 1.0795e-02,  5.7343e-03,  3.2873e-03],\n                        [ 5.4282e-03, -1.0134e-02,  3.3486e-03],\n                        [ 5.0658e-03, -1.4290e-02,  3.9768e-03]],\n              \n                       [[-1.4718e-02, -4.8749e-03,  8.8550e-03],\n                        [-1.2116e-02,  3.9706e-03, -1.5341e-04],\n                        [-5.6044e-03,  9.2914e-03,  2.6309e-03]],\n              \n                       ...,\n              \n                       [[ 1.1578e-02,  4.7662e-03,  1.0865e-02],\n                        [-9.9621e-03,  7.2204e-03,  6.7652e-03],\n                        [ 6.1930e-03,  5.5036e-03, -4.8385e-03]],\n              \n                       [[-1.1982e-02,  9.0713e-03, -6.7553e-03],\n                        [ 1.0392e-02, -6.3635e-03, -1.1598e-03],\n                        [ 1.0464e-02,  4.0243e-03,  1.4345e-03]],\n              \n                       [[ 3.2504e-03,  1.4237e-02, -7.7320e-03],\n                        [-1.0245e-02, -8.5657e-03, -1.2735e-02],\n                        [-3.5816e-03,  1.3560e-02, -1.2678e-02]]],\n              \n              \n                      [[[-1.4336e-02, -4.6926e-03,  1.3425e-02],\n                        [ 1.3409e-02, -6.8928e-03, -9.7946e-03],\n                        [-1.4182e-02, -8.6928e-03, -1.4202e-02]],\n              \n                       [[-5.0576e-03, -9.8077e-03,  5.6572e-03],\n                        [-1.4611e-02,  4.4676e-03, -1.3235e-02],\n                        [ 3.6478e-03,  4.1773e-04,  1.4504e-02]],\n              \n                       [[-8.5665e-03, -6.6888e-03, -5.9852e-03],\n                        [ 1.8548e-03,  1.2795e-02, -6.3900e-03],\n                        [-1.3038e-02,  7.2169e-03,  9.2560e-03]],\n              \n                       ...,\n              \n                       [[-5.8375e-03,  8.9250e-03,  1.2109e-02],\n                        [-1.3653e-02,  1.3453e-02, -6.7649e-03],\n                        [-1.2166e-02, -1.3578e-02, -1.2037e-03]],\n              \n                       [[-5.5372e-03, -3.9234e-03, -2.1640e-03],\n                        [-8.1456e-03, -8.1486e-03,  4.8608e-05],\n                        [-7.9746e-03,  3.5861e-03, -5.4110e-03]],\n              \n                       [[ 9.0684e-03, -4.6523e-03,  8.6029e-03],\n                        [-3.5470e-03, -2.6329e-03,  4.1187e-03],\n                        [-1.7698e-03,  3.1339e-03, -1.3087e-02]]],\n              \n              \n                      [[[ 1.3993e-02,  1.0210e-02, -9.8379e-03],\n                        [-3.6017e-03,  1.5505e-03, -7.5702e-03],\n                        [-1.3827e-03, -1.4429e-02, -1.3696e-02]],\n              \n                       [[ 1.2335e-02,  8.3124e-03, -4.6792e-03],\n                        [ 4.8468e-03,  1.3626e-04,  9.8758e-03],\n                        [-2.6817e-03,  3.2997e-03, -9.7415e-04]],\n              \n                       [[ 3.1673e-03, -7.1938e-03, -1.4500e-03],\n                        [-9.1013e-03,  8.4705e-03, -9.5864e-03],\n                        [ 1.6714e-03, -1.4101e-02,  1.1644e-02]],\n              \n                       ...,\n              \n                       [[ 1.4320e-02,  4.4366e-03, -5.8747e-03],\n                        [-8.1688e-03, -6.9629e-03,  3.0317e-04],\n                        [-1.2110e-02, -1.3646e-02, -6.0113e-03]],\n              \n                       [[-3.7647e-04,  7.6979e-03,  3.3129e-03],\n                        [ 7.6917e-03, -1.9005e-03,  6.3914e-03],\n                        [-2.9271e-03,  1.0327e-02, -9.8557e-03]],\n              \n                       [[ 1.1749e-02,  3.9048e-03, -7.2822e-03],\n                        [ 1.4049e-02,  1.3569e-02,  2.5594e-03],\n                        [ 1.2890e-02,  5.6545e-03,  6.2168e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-1.0162e-02, -7.9513e-03, -1.4126e-02],\n                        [-6.2557e-03, -9.7779e-03,  1.0858e-02],\n                        [ 9.1498e-03,  3.0958e-04,  9.0409e-03]],\n              \n                       [[-7.6646e-03, -9.0559e-03, -8.4516e-04],\n                        [-1.2277e-02,  2.7770e-03,  2.4928e-03],\n                        [ 2.1196e-03, -2.7451e-03, -1.3663e-02]],\n              \n                       [[-8.4018e-03,  3.2803e-03, -6.1505e-03],\n                        [ 1.3116e-02,  8.8065e-03,  4.6064e-03],\n                        [ 9.4382e-03, -7.7282e-03,  1.0306e-02]],\n              \n                       ...,\n              \n                       [[ 6.6357e-03, -2.2279e-03, -8.7835e-03],\n                        [-5.1093e-03,  3.9618e-03,  8.8206e-03],\n                        [ 1.4141e-02,  1.3784e-02,  1.1771e-02]],\n              \n                       [[-5.9949e-03, -1.3745e-04,  7.4454e-03],\n                        [-9.2404e-03,  1.3126e-02,  9.9188e-03],\n                        [-6.8859e-03, -1.4138e-02, -9.2198e-03]],\n              \n                       [[-1.4438e-02,  1.1573e-02,  1.1146e-02],\n                        [-8.7031e-03, -4.6383e-03,  7.3338e-03],\n                        [ 1.1381e-02, -9.0583e-03, -2.5293e-03]]],\n              \n              \n                      [[[-1.3852e-02, -6.8651e-03,  2.3293e-03],\n                        [ 1.2269e-02,  6.5710e-03,  3.9793e-03],\n                        [-7.3067e-03, -5.9318e-03, -6.7658e-03]],\n              \n                       [[ 9.5927e-03, -7.6682e-03, -1.3819e-02],\n                        [-9.0626e-03,  3.5546e-03, -8.5062e-03],\n                        [ 1.7261e-03, -2.6030e-03, -1.4632e-02]],\n              \n                       [[ 1.0916e-02,  1.0892e-02,  1.4228e-02],\n                        [ 1.1874e-02, -6.4073e-03, -5.1940e-03],\n                        [-7.4828e-03, -7.4947e-03,  2.5183e-03]],\n              \n                       ...,\n              \n                       [[ 9.7132e-03,  2.0456e-03, -4.0253e-03],\n                        [ 1.9973e-03,  1.2258e-02, -1.3174e-03],\n                        [-9.0220e-03, -8.2095e-03,  1.4117e-02]],\n              \n                       [[-1.0827e-02,  1.4226e-02, -6.4879e-03],\n                        [ 1.2198e-02, -1.2647e-02,  8.6206e-03],\n                        [-2.7980e-03, -2.0266e-03,  5.7236e-03]],\n              \n                       [[-1.2030e-02,  1.2822e-02, -8.4252e-03],\n                        [ 1.1277e-02, -7.0514e-03, -7.5673e-03],\n                        [ 8.1968e-03, -1.2170e-02, -7.3895e-03]]],\n              \n              \n                      [[[ 8.0684e-03,  1.3598e-02, -7.9777e-03],\n                        [-1.4268e-02,  4.8484e-03, -1.1704e-02],\n                        [ 4.8766e-03,  2.9658e-03,  2.0288e-03]],\n              \n                       [[-1.1000e-03, -2.6417e-03,  3.1051e-03],\n                        [ 1.2253e-02, -7.2229e-03, -1.1037e-03],\n                        [ 1.0293e-02,  3.9444e-03, -8.0077e-03]],\n              \n                       [[ 3.6599e-03,  1.3138e-02, -1.0403e-03],\n                        [-1.0804e-02, -2.9224e-03, -7.3381e-04],\n                        [-8.4483e-03, -3.5656e-03,  1.0923e-02]],\n              \n                       ...,\n              \n                       [[ 1.0183e-02, -1.0656e-02,  2.5374e-03],\n                        [-2.4001e-03,  9.3434e-03,  8.0887e-03],\n                        [-3.1470e-03, -3.6860e-03,  6.9349e-03]],\n              \n                       [[-1.4212e-02,  4.7419e-03,  2.2588e-03],\n                        [ 1.2572e-02,  2.5563e-03, -8.1275e-03],\n                        [-3.7703e-03,  2.5945e-03,  5.5602e-03]],\n              \n                       [[-1.2830e-02, -1.0370e-02,  9.9764e-03],\n                        [-1.0848e-02, -9.6209e-03,  8.2907e-03],\n                        [ 4.6423e-03, -4.9777e-03, -8.6183e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.9552e-03,  1.0103e-02, -4.7408e-03],\n                        [-1.3407e-02,  6.5927e-03, -7.2890e-03],\n                        [ 1.2902e-02, -7.3139e-03,  4.8173e-03]],\n              \n                       [[-8.6896e-03, -1.9172e-03,  5.9656e-03],\n                        [-7.3172e-05,  2.9933e-03, -1.1204e-02],\n                        [ 2.1456e-03,  2.6252e-03, -1.3978e-02]],\n              \n                       [[-8.2944e-03, -6.1581e-03,  1.3276e-02],\n                        [ 2.0285e-04, -6.9051e-03,  1.3585e-02],\n                        [-7.9958e-03,  5.1597e-03, -1.1482e-02]],\n              \n                       ...,\n              \n                       [[ 2.9236e-03,  8.6567e-03, -5.6918e-03],\n                        [ 1.2319e-02, -1.2173e-02, -1.1142e-02],\n                        [ 2.1955e-03,  2.1893e-03,  1.0226e-02]],\n              \n                       [[-1.3731e-02,  2.4001e-04,  1.0280e-02],\n                        [ 6.2036e-04,  9.4891e-03, -9.4363e-03],\n                        [ 7.7716e-03, -5.3223e-03, -1.1793e-02]],\n              \n                       [[ 9.0567e-03, -9.4963e-03,  1.2966e-02],\n                        [-3.5606e-03,  6.7127e-03,  9.2346e-03],\n                        [ 1.6610e-04,  9.7832e-04, -3.7458e-03]]],\n              \n              \n                      [[[ 1.8821e-03,  7.0609e-03, -9.9641e-03],\n                        [ 2.8442e-03, -3.4813e-04,  2.8147e-03],\n                        [-7.6718e-03,  1.4098e-03,  3.6991e-03]],\n              \n                       [[-7.4600e-03,  6.1319e-03, -6.6834e-03],\n                        [ 4.6137e-03, -9.7316e-03, -2.1926e-03],\n                        [-5.1150e-03,  8.5056e-03,  1.4168e-02]],\n              \n                       [[ 1.2746e-02,  8.4634e-03,  1.2394e-02],\n                        [ 6.5522e-03, -1.0927e-02, -1.4621e-02],\n                        [ 9.5033e-03,  3.9224e-03,  9.9719e-03]],\n              \n                       ...,\n              \n                       [[-4.0116e-03, -1.4190e-02, -2.6838e-03],\n                        [-1.9716e-04, -1.6087e-03, -2.2089e-03],\n                        [ 1.1347e-02,  5.0595e-04, -2.1228e-03]],\n              \n                       [[ 1.1465e-03,  6.0314e-03, -7.8767e-03],\n                        [-6.6732e-03, -5.0615e-03, -7.0481e-03],\n                        [-3.5145e-03, -1.4674e-02,  9.3690e-03]],\n              \n                       [[-2.1949e-03,  1.8604e-04, -3.8469e-04],\n                        [-6.0911e-03,  4.8625e-03,  9.1291e-04],\n                        [-4.2253e-03, -9.7373e-03,  3.0233e-03]]],\n              \n              \n                      [[[ 1.3092e-02, -9.1652e-03, -1.4018e-02],\n                        [-7.5290e-03, -1.1704e-02,  1.1918e-02],\n                        [-3.6753e-03,  8.3012e-03, -7.8185e-03]],\n              \n                       [[ 1.3660e-02, -1.0051e-04, -4.8537e-03],\n                        [ 4.5250e-03,  1.1501e-02, -1.2260e-02],\n                        [-1.2088e-02, -1.1217e-02, -8.9023e-03]],\n              \n                       [[ 3.9087e-03, -1.1512e-03, -1.3955e-02],\n                        [-2.1982e-03,  1.0120e-02, -5.0558e-03],\n                        [-1.3255e-02,  2.8492e-03, -4.1524e-03]],\n              \n                       ...,\n              \n                       [[-1.2921e-02, -1.8075e-03,  3.1186e-03],\n                        [ 4.0110e-03,  5.9678e-03, -1.5871e-03],\n                        [ 4.0160e-03,  4.9175e-04,  2.2130e-03]],\n              \n                       [[-3.4039e-03, -1.2438e-02,  6.7231e-03],\n                        [ 1.2851e-02, -5.3675e-03,  1.6797e-03],\n                        [-1.3136e-02, -2.5658e-03, -5.8660e-03]],\n              \n                       [[-2.0538e-03,  7.5002e-04,  6.9986e-03],\n                        [ 1.3422e-02, -9.2835e-04,  4.6620e-03],\n                        [-1.3815e-02,  5.7040e-03, -6.6107e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('up1.conv.double_conv.0.weight',\n              tensor([[[[ 6.0052e-03, -6.1578e-03, -8.6970e-03],\n                        [ 1.6955e-03, -7.3866e-03,  5.3448e-03],\n                        [ 5.5082e-03,  9.1673e-03,  1.0191e-02]],\n              \n                       [[-3.7926e-03,  5.7925e-03,  1.0316e-02],\n                        [ 9.6915e-03,  8.8699e-03,  5.3047e-03],\n                        [ 5.0500e-03,  4.6066e-03,  1.0278e-02]],\n              \n                       [[-7.2442e-04, -7.9003e-03, -9.7175e-03],\n                        [ 4.6586e-04, -3.6655e-03, -9.5510e-03],\n                        [-9.1740e-03, -7.8502e-03, -5.3606e-03]],\n              \n                       ...,\n              \n                       [[ 2.1322e-03, -9.4887e-05, -4.9738e-03],\n                        [-6.1662e-03,  1.3903e-03, -7.2019e-03],\n                        [ 5.4206e-03,  8.7880e-03,  4.3695e-03]],\n              \n                       [[ 3.3114e-03, -4.8001e-03, -2.7326e-03],\n                        [-3.7524e-03,  7.7908e-03, -8.4219e-03],\n                        [ 2.0721e-03,  7.5771e-03,  6.9718e-03]],\n              \n                       [[-9.9150e-03, -2.1330e-03,  7.4038e-03],\n                        [-6.3372e-03, -8.1195e-03,  1.6034e-03],\n                        [ 5.8172e-03, -1.3327e-03, -7.0786e-03]]],\n              \n              \n                      [[[-4.7313e-03, -2.5325e-03, -6.1366e-03],\n                        [ 1.1530e-03, -5.3506e-03, -6.1344e-04],\n                        [ 2.7635e-03, -6.2766e-03,  4.6419e-03]],\n              \n                       [[ 4.3768e-03, -4.0070e-03,  8.7607e-03],\n                        [-8.9397e-03, -9.8516e-03, -2.8273e-03],\n                        [-3.7660e-03,  3.6542e-03,  1.0126e-02]],\n              \n                       [[-6.7512e-03,  6.0833e-03,  2.7166e-03],\n                        [ 9.3578e-04,  5.1147e-03,  6.3890e-03],\n                        [ 1.5687e-04,  7.4274e-03, -8.3365e-03]],\n              \n                       ...,\n              \n                       [[-4.8921e-03, -5.4093e-03,  5.6688e-03],\n                        [ 3.1983e-03,  3.9314e-03, -8.9410e-03],\n                        [ 6.5762e-03, -9.7403e-03, -4.1459e-03]],\n              \n                       [[ 8.1715e-03,  5.4453e-03, -7.9296e-03],\n                        [ 1.6348e-03, -1.7733e-04,  1.1809e-03],\n                        [-6.2941e-03,  6.1941e-03,  1.7227e-03]],\n              \n                       [[ 9.5111e-03, -8.0376e-03, -3.7345e-03],\n                        [ 5.4716e-03, -3.7542e-03,  2.9980e-03],\n                        [-7.5362e-03,  8.4094e-03,  8.9098e-03]]],\n              \n              \n                      [[[-9.6740e-03, -8.1277e-03,  3.9857e-03],\n                        [-3.5163e-03,  8.6464e-03,  4.2643e-03],\n                        [-5.0144e-03, -9.8802e-04,  4.8284e-04]],\n              \n                       [[-6.5739e-03,  9.1206e-03,  5.8876e-03],\n                        [-4.3970e-03,  3.9926e-04,  4.9571e-03],\n                        [-3.2965e-03,  4.1399e-04, -2.7867e-03]],\n              \n                       [[-4.9022e-03, -7.1855e-04,  5.2022e-04],\n                        [-3.8415e-03,  7.9072e-03,  1.0071e-02],\n                        [-6.5128e-03, -3.6828e-03, -8.3628e-03]],\n              \n                       ...,\n              \n                       [[ 8.5856e-03, -7.1988e-03,  9.1629e-03],\n                        [ 9.4906e-03, -6.0381e-03,  6.3775e-04],\n                        [ 3.2705e-03, -4.2573e-03,  7.2144e-03]],\n              \n                       [[-2.7434e-03, -5.6575e-03,  7.0926e-03],\n                        [ 6.5038e-03,  1.0222e-02,  7.6083e-03],\n                        [ 8.3256e-03,  7.9641e-03, -6.8926e-03]],\n              \n                       [[ 3.2581e-03, -3.4153e-03,  1.7781e-04],\n                        [-4.7329e-03, -2.7371e-03, -7.9243e-03],\n                        [-7.3951e-03, -3.6213e-03,  3.8721e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.3754e-03,  1.0256e-02, -9.6938e-03],\n                        [-5.2090e-03,  1.1899e-03,  6.6328e-03],\n                        [-6.4318e-03,  7.6097e-03,  3.2797e-03]],\n              \n                       [[-7.0052e-03,  4.5905e-03, -8.9286e-03],\n                        [-8.2543e-03, -5.1691e-03, -5.8590e-03],\n                        [ 8.7791e-03,  5.7680e-03, -8.9067e-03]],\n              \n                       [[-7.6416e-03, -9.3266e-03,  9.4770e-03],\n                        [ 1.4398e-03,  4.5831e-03, -3.4448e-03],\n                        [-4.5923e-03, -5.7610e-03, -4.3103e-03]],\n              \n                       ...,\n              \n                       [[-2.0614e-03, -8.5129e-03, -8.4951e-03],\n                        [ 2.6566e-03,  9.1776e-03,  2.6760e-03],\n                        [-1.7022e-04,  3.6392e-03,  5.0875e-03]],\n              \n                       [[-2.9073e-03, -7.8702e-03, -1.2811e-03],\n                        [-8.3429e-03, -8.4082e-03,  4.3443e-03],\n                        [-6.5337e-03,  3.0448e-03, -3.2978e-03]],\n              \n                       [[-6.3634e-03, -6.4584e-03, -9.4520e-03],\n                        [ 6.3613e-03,  1.3895e-03,  6.7184e-03],\n                        [ 1.9717e-04,  3.0919e-03, -9.3850e-03]]],\n              \n              \n                      [[[-7.3347e-03,  3.7111e-03, -1.4600e-03],\n                        [-8.9929e-03, -1.0001e-02, -9.7608e-03],\n                        [ 4.9672e-03, -5.1917e-03, -9.9102e-03]],\n              \n                       [[ 7.6933e-03, -4.9824e-03, -8.9469e-03],\n                        [ 4.8704e-03, -1.6437e-03,  8.8097e-03],\n                        [-3.0993e-03, -5.9778e-03, -3.1651e-03]],\n              \n                       [[ 8.6893e-03,  9.8990e-03,  7.1665e-03],\n                        [ 7.6924e-03, -1.0816e-03,  9.3137e-03],\n                        [-4.7224e-03, -3.9862e-03, -7.0841e-03]],\n              \n                       ...,\n              \n                       [[ 7.1673e-03,  5.2882e-03,  5.8690e-03],\n                        [ 4.2807e-04, -4.7009e-04,  9.8658e-03],\n                        [-3.6831e-03, -3.5520e-03,  4.0485e-03]],\n              \n                       [[-5.5522e-03,  9.4766e-03,  8.2692e-03],\n                        [-3.1187e-03, -8.5105e-03,  8.7861e-03],\n                        [-7.3462e-03,  5.8684e-03,  9.6273e-03]],\n              \n                       [[-3.7102e-03,  7.7810e-03, -1.4194e-03],\n                        [-4.0797e-03, -8.0059e-03,  8.5199e-03],\n                        [-9.1947e-03,  3.5915e-03, -4.6602e-03]]],\n              \n              \n                      [[[-1.3775e-03,  6.0666e-04, -6.9796e-04],\n                        [ 6.7400e-03,  6.6210e-03,  2.7429e-03],\n                        [-8.8243e-03, -9.8390e-03,  2.4116e-03]],\n              \n                       [[ 4.7119e-03,  3.2005e-03,  5.9726e-03],\n                        [ 9.5476e-03,  1.6969e-03,  9.7832e-03],\n                        [-2.6481e-03,  7.0522e-03, -7.9863e-03]],\n              \n                       [[ 4.9707e-03,  9.5256e-04, -1.3029e-03],\n                        [-6.9370e-03, -1.0068e-02,  1.0652e-03],\n                        [-2.0503e-03,  8.6360e-03, -1.5661e-03]],\n              \n                       ...,\n              \n                       [[-6.5328e-03, -9.1420e-04,  5.5855e-03],\n                        [ 8.4739e-03, -4.1916e-03,  1.0212e-02],\n                        [ 1.0342e-02, -8.0135e-03, -1.1019e-04]],\n              \n                       [[ 4.2931e-03,  4.7278e-03,  8.9549e-03],\n                        [ 7.2504e-03,  4.6937e-03, -6.7444e-03],\n                        [-1.0244e-02,  2.1343e-03, -3.2979e-03]],\n              \n                       [[ 9.3904e-03, -7.6412e-03,  2.0035e-03],\n                        [-6.8808e-03,  1.0404e-02,  9.5906e-03],\n                        [ 5.1486e-03,  1.8948e-03, -1.0138e-03]]]])),\n             ('up1.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up1.conv.double_conv.3.weight',\n              tensor([[[[ 4.6532e-03, -7.6019e-03, -2.2726e-03],\n                        [ 4.6818e-03,  1.2958e-02,  7.4474e-03],\n                        [ 1.0656e-02,  7.3169e-03,  1.4385e-02]],\n              \n                       [[-7.1003e-03,  5.6198e-03,  1.1528e-02],\n                        [ 1.2165e-02,  2.7467e-03,  1.2221e-02],\n                        [ 1.0123e-02, -7.3388e-04, -1.3558e-02]],\n              \n                       [[ 6.1051e-04, -1.0071e-02,  1.0367e-02],\n                        [ 5.4181e-03,  3.2388e-03,  8.1533e-04],\n                        [ 9.9759e-03, -8.9243e-03, -1.0614e-02]],\n              \n                       ...,\n              \n                       [[-1.1593e-02,  4.4562e-03, -1.2794e-02],\n                        [-2.0847e-03,  8.4393e-03, -3.0718e-03],\n                        [ 1.2095e-02,  9.6634e-03, -6.1204e-03]],\n              \n                       [[-8.5692e-03, -5.3203e-03, -6.0301e-03],\n                        [-1.3060e-02, -4.9878e-03,  1.3536e-02],\n                        [-3.0446e-03, -3.7271e-03,  1.8943e-03]],\n              \n                       [[ 9.1236e-03,  6.2085e-03, -5.2066e-03],\n                        [ 7.0768e-03,  5.8855e-03, -1.3525e-02],\n                        [ 1.2969e-02, -3.1656e-03, -9.7805e-03]]],\n              \n              \n                      [[[-1.3448e-02, -1.4380e-02,  3.3876e-03],\n                        [-6.9893e-03, -8.7593e-03,  3.4935e-03],\n                        [ 6.0252e-03,  6.2473e-03, -7.2960e-04]],\n              \n                       [[ 1.2521e-03, -1.2604e-02, -1.4122e-02],\n                        [-7.8812e-03,  1.2843e-03,  3.4510e-03],\n                        [-8.0826e-03, -6.0928e-03,  1.4071e-02]],\n              \n                       [[ 1.2236e-02, -2.2066e-03,  7.5802e-03],\n                        [-3.4579e-03, -8.4028e-03,  1.2992e-02],\n                        [ 1.5273e-03,  9.6915e-03, -2.7779e-03]],\n              \n                       ...,\n              \n                       [[-9.7299e-03,  7.2240e-03,  3.2073e-04],\n                        [ 5.1952e-03,  1.3993e-02,  5.8187e-03],\n                        [-3.9472e-03,  9.5075e-03,  9.9508e-03]],\n              \n                       [[ 3.8860e-03, -7.5956e-03, -6.7716e-03],\n                        [-6.3491e-03,  1.1731e-02, -4.6717e-03],\n                        [ 5.6204e-04, -4.5982e-03, -1.3072e-03]],\n              \n                       [[-9.9374e-03, -1.4691e-03,  9.6274e-03],\n                        [-3.4154e-03, -9.9765e-03,  4.7587e-03],\n                        [ 1.1309e-02,  1.2087e-03,  1.1953e-02]]],\n              \n              \n                      [[[ 1.2883e-02, -7.2949e-03, -4.8458e-03],\n                        [ 9.7466e-03,  1.1054e-02,  1.2237e-02],\n                        [ 9.9405e-03,  1.4726e-02,  2.0744e-03]],\n              \n                       [[ 1.0789e-02,  1.3618e-02,  1.4625e-02],\n                        [-1.9228e-03,  5.1298e-03,  5.3312e-04],\n                        [ 1.4351e-02,  8.0309e-03, -1.3372e-02]],\n              \n                       [[-3.1131e-03, -6.5674e-04, -1.0796e-02],\n                        [-9.3562e-03,  6.5610e-03, -1.3210e-02],\n                        [ 7.9644e-03,  1.0064e-03,  6.2818e-04]],\n              \n                       ...,\n              \n                       [[-2.9593e-03, -3.4946e-03, -4.1973e-03],\n                        [ 1.2073e-02,  7.9237e-03,  9.7770e-05],\n                        [-4.5093e-03, -8.0024e-03, -3.3877e-03]],\n              \n                       [[ 4.1504e-04, -6.3685e-03,  2.9286e-04],\n                        [-1.4368e-02,  5.2549e-04, -1.2686e-02],\n                        [ 1.6020e-03,  4.4607e-03,  7.5159e-03]],\n              \n                       [[-6.6873e-03,  5.1561e-05,  8.2160e-03],\n                        [-7.2157e-03, -9.4008e-04, -9.3220e-03],\n                        [ 1.3272e-03,  1.3943e-03, -1.0126e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.3756e-03,  1.2603e-02,  1.0009e-02],\n                        [ 1.3332e-02,  2.2436e-03, -2.6538e-03],\n                        [ 1.2150e-02, -6.4561e-03, -1.2219e-02]],\n              \n                       [[-8.2563e-03,  1.4514e-02, -6.5334e-03],\n                        [ 1.0584e-02,  7.2743e-03, -7.7184e-03],\n                        [-1.3945e-02, -3.9507e-04, -1.3207e-02]],\n              \n                       [[-1.1936e-02,  1.2723e-02,  1.4794e-03],\n                        [-9.2238e-03,  1.2513e-02, -1.2755e-02],\n                        [-2.3135e-04, -1.2050e-02,  1.0637e-02]],\n              \n                       ...,\n              \n                       [[-1.7315e-03, -1.1583e-02, -6.2004e-03],\n                        [-3.6829e-03, -7.5475e-03, -1.1467e-02],\n                        [-1.2565e-04, -1.6956e-03,  7.3251e-03]],\n              \n                       [[ 4.5195e-03,  9.6949e-03, -1.1593e-02],\n                        [-1.0726e-02, -4.3706e-03, -1.0075e-02],\n                        [-1.1938e-02, -6.4125e-03,  5.7692e-04]],\n              \n                       [[-1.1380e-02, -9.5971e-03, -1.3420e-02],\n                        [ 1.0888e-02, -1.0871e-02,  4.6657e-05],\n                        [-2.8069e-03, -1.0725e-02,  2.2430e-03]]],\n              \n              \n                      [[[ 1.1839e-02,  1.3359e-02, -2.2681e-03],\n                        [ 1.8450e-03,  5.9289e-04, -1.2829e-02],\n                        [ 1.4203e-02,  2.5810e-03, -1.1913e-02]],\n              \n                       [[-1.3077e-02, -1.4014e-02, -4.2100e-03],\n                        [-9.9503e-03,  1.1108e-02, -3.2723e-03],\n                        [ 2.0312e-03,  4.5349e-03,  1.3859e-02]],\n              \n                       [[-1.4575e-02,  1.1122e-02, -7.5780e-03],\n                        [-3.8330e-03, -9.8024e-04,  5.9586e-03],\n                        [ 9.8220e-03, -6.8341e-03,  1.2393e-02]],\n              \n                       ...,\n              \n                       [[-3.4048e-03,  1.3819e-02, -2.6837e-03],\n                        [ 1.1734e-02,  1.4311e-03, -1.2245e-02],\n                        [-8.3261e-03,  1.3495e-02,  2.9223e-03]],\n              \n                       [[-1.2962e-02, -7.3929e-03, -7.3878e-03],\n                        [-1.7338e-03, -6.7076e-03, -7.7754e-03],\n                        [ 1.4972e-03, -6.4253e-03, -1.4126e-02]],\n              \n                       [[ 1.4451e-02, -4.8099e-03,  5.7255e-03],\n                        [-5.8516e-03,  4.0733e-03,  1.0094e-02],\n                        [ 8.1309e-04,  5.1471e-03,  5.1509e-03]]],\n              \n              \n                      [[[ 9.8223e-04,  1.1245e-02,  1.1552e-02],\n                        [-7.6653e-03,  6.1365e-04, -4.2670e-03],\n                        [ 5.1350e-03,  1.4145e-02, -8.8357e-04]],\n              \n                       [[ 1.2253e-02,  1.0491e-02, -1.4184e-02],\n                        [ 2.6855e-03,  7.4216e-03, -4.6636e-03],\n                        [-1.0291e-02, -1.2930e-02, -3.5078e-04]],\n              \n                       [[ 4.5516e-03, -9.4295e-03,  9.7718e-03],\n                        [-7.6455e-03,  1.0235e-02,  1.2030e-03],\n                        [-2.7815e-03,  6.6763e-03, -8.7617e-03]],\n              \n                       ...,\n              \n                       [[-9.8976e-03,  1.2484e-02, -2.8897e-03],\n                        [ 4.3479e-03,  8.9747e-03,  8.7985e-04],\n                        [ 1.2341e-02,  4.2616e-04,  4.2251e-03]],\n              \n                       [[ 1.2692e-02, -1.7026e-03,  7.1434e-03],\n                        [ 1.1852e-02, -1.1433e-02, -1.3874e-02],\n                        [ 1.2581e-02, -3.8352e-03, -7.5201e-04]],\n              \n                       [[-4.7592e-04, -3.9157e-03,  3.5884e-03],\n                        [-3.2631e-03, -1.6258e-03, -1.0496e-02],\n                        [ 1.3847e-03, -5.7536e-04, -1.0432e-02]]]])),\n             ('up1.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.0.weight',\n              tensor([[[[-2.1518e-03,  1.0631e-02,  1.2601e-02],\n                        [ 9.9365e-03,  8.6478e-03, -1.2200e-02],\n                        [-8.7199e-03, -1.3551e-04,  2.7872e-03]],\n              \n                       [[ 1.0136e-02,  5.1465e-03, -7.2739e-03],\n                        [-1.0549e-02, -4.3726e-03, -1.0110e-02],\n                        [-1.2202e-02,  8.1444e-03,  1.2508e-02]],\n              \n                       [[-1.1105e-02, -3.2792e-03,  1.1186e-02],\n                        [-8.2915e-03,  8.8182e-03,  1.1263e-02],\n                        [-4.4057e-03,  8.6805e-03, -9.5922e-03]],\n              \n                       ...,\n              \n                       [[ 6.3221e-03, -1.2953e-02,  5.1380e-03],\n                        [ 2.9260e-04, -1.0260e-02,  6.4162e-03],\n                        [-5.8944e-03,  4.6316e-03,  1.4742e-03]],\n              \n                       [[-1.0956e-02, -3.5614e-03, -3.6777e-03],\n                        [ 1.2266e-02, -3.7897e-05, -1.1044e-02],\n                        [ 5.1852e-03,  8.2570e-03,  1.3097e-03]],\n              \n                       [[-2.4492e-03, -3.5821e-03, -1.4560e-02],\n                        [ 9.1054e-03, -4.1931e-03,  9.5132e-03],\n                        [ 5.1267e-03,  1.1881e-02,  5.6942e-04]]],\n              \n              \n                      [[[ 1.0638e-02, -5.4433e-03, -3.7759e-03],\n                        [ 1.1677e-02, -4.1737e-03, -1.0637e-02],\n                        [-1.6576e-03, -2.1487e-03, -1.1114e-02]],\n              \n                       [[ 1.8396e-03,  1.3266e-02,  6.8261e-03],\n                        [ 3.9165e-03, -8.8550e-03,  1.4806e-03],\n                        [ 7.0773e-04,  1.1756e-02, -1.0292e-02]],\n              \n                       [[ 1.3127e-02,  4.8850e-03,  2.1176e-03],\n                        [ 2.1249e-03, -5.7832e-03, -1.3140e-02],\n                        [ 8.5454e-03, -8.9114e-03, -1.3402e-02]],\n              \n                       ...,\n              \n                       [[ 1.1088e-02,  7.2383e-03,  1.2047e-02],\n                        [ 9.5457e-03,  1.3826e-02, -2.5452e-03],\n                        [ 9.1783e-03,  1.0598e-02, -8.6740e-04]],\n              \n                       [[ 4.5989e-03, -1.4716e-03, -1.2077e-02],\n                        [-9.6809e-04, -1.2336e-02,  9.3714e-04],\n                        [ 3.9654e-03, -7.3955e-03, -1.2232e-02]],\n              \n                       [[ 5.6303e-03, -8.0869e-03, -2.5287e-03],\n                        [ 1.8057e-03, -1.1487e-02, -2.8659e-03],\n                        [ 4.0015e-03, -1.2479e-02, -1.1998e-02]]],\n              \n              \n                      [[[ 9.4689e-03, -7.2081e-03,  1.4072e-03],\n                        [ 1.2932e-02, -3.2592e-03, -8.7485e-03],\n                        [ 9.2945e-03,  4.6018e-03,  4.0055e-03]],\n              \n                       [[-1.3764e-02, -4.2907e-03,  3.2547e-03],\n                        [ 3.3341e-03,  1.1304e-03, -1.2234e-02],\n                        [-1.3467e-02, -5.6734e-03,  7.4354e-03]],\n              \n                       [[-5.6023e-03, -2.8761e-03, -1.4718e-02],\n                        [ 1.0713e-02, -1.6779e-03, -1.1996e-02],\n                        [-1.2827e-02,  1.0703e-02, -9.7047e-03]],\n              \n                       ...,\n              \n                       [[ 3.2607e-03, -8.0475e-03,  6.1829e-03],\n                        [-2.9395e-03,  3.3496e-03,  5.1071e-03],\n                        [ 5.9723e-03,  4.7608e-03, -1.6388e-03]],\n              \n                       [[-4.3904e-03,  7.7792e-03, -1.2428e-02],\n                        [-3.2456e-03,  5.5866e-03, -1.4352e-02],\n                        [-1.1821e-02,  2.6534e-03,  7.5290e-03]],\n              \n                       [[ 4.6186e-03, -6.2310e-03,  1.1741e-02],\n                        [-1.4587e-02,  9.7592e-03,  1.2688e-02],\n                        [ 4.2982e-03,  5.2313e-03, -1.2822e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1165e-02,  7.8691e-04, -9.3187e-03],\n                        [-7.7603e-03, -3.0258e-03, -9.7707e-03],\n                        [ 7.5438e-03,  1.4036e-02,  1.0273e-02]],\n              \n                       [[-1.3591e-02,  7.4804e-03, -4.6866e-04],\n                        [-1.3815e-02,  1.2045e-02, -9.8406e-03],\n                        [ 1.0759e-02,  6.9177e-03, -1.3892e-02]],\n              \n                       [[ 1.2857e-02, -4.8749e-04,  9.5570e-03],\n                        [ 2.7064e-03, -8.0672e-03,  1.0471e-02],\n                        [ 5.2177e-03,  1.2281e-02, -6.2795e-03]],\n              \n                       ...,\n              \n                       [[ 1.0430e-03,  1.3958e-02, -1.1441e-02],\n                        [-1.0572e-02,  4.8599e-04, -8.1871e-03],\n                        [ 8.7779e-03,  8.1478e-03, -3.1877e-03]],\n              \n                       [[ 7.4461e-03,  2.9228e-03, -1.0984e-02],\n                        [ 9.8613e-03,  1.3081e-02,  1.2413e-02],\n                        [ 1.2035e-02, -3.1168e-03, -7.5135e-03]],\n              \n                       [[ 8.0283e-03, -4.2646e-03, -7.9841e-03],\n                        [-1.9161e-05, -6.6800e-03, -1.6066e-04],\n                        [ 9.5017e-03, -1.7248e-03,  7.0304e-03]]],\n              \n              \n                      [[[ 3.5356e-03, -7.6512e-03, -8.9665e-03],\n                        [-4.8910e-03,  2.0278e-03,  7.1160e-03],\n                        [-3.0881e-03, -4.1455e-03,  1.1920e-02]],\n              \n                       [[ 3.7466e-03, -3.9381e-03,  1.4420e-02],\n                        [-1.3107e-02, -5.7352e-03,  6.8331e-03],\n                        [-6.0296e-03,  1.2593e-02,  8.2828e-03]],\n              \n                       [[-9.1421e-03,  1.2051e-02,  9.1719e-03],\n                        [-2.3811e-03, -1.4370e-02, -1.1317e-02],\n                        [-5.8528e-03,  5.9658e-03, -7.2074e-03]],\n              \n                       ...,\n              \n                       [[ 1.4338e-02,  1.0304e-02, -6.8373e-03],\n                        [ 2.6406e-03, -2.9580e-03, -2.9774e-03],\n                        [-6.9043e-03,  1.4699e-02, -7.5011e-03]],\n              \n                       [[ 9.0359e-03, -7.4744e-03,  2.7057e-03],\n                        [-1.0241e-03, -9.2485e-03, -3.4580e-03],\n                        [ 3.8833e-03,  7.4134e-03, -1.1881e-02]],\n              \n                       [[-1.9624e-03,  2.7043e-03, -4.4755e-04],\n                        [-1.1581e-02, -1.3765e-02, -8.7221e-03],\n                        [ 1.3774e-02, -1.1876e-02, -1.0575e-02]]],\n              \n              \n                      [[[-1.7063e-04,  6.7622e-04,  8.8984e-03],\n                        [-5.9551e-03,  1.2280e-02, -1.2928e-02],\n                        [-1.2386e-02,  1.3566e-02,  3.3778e-03]],\n              \n                       [[-4.9461e-03, -1.1765e-03, -5.0370e-03],\n                        [-3.2352e-03,  8.2034e-03,  1.2355e-02],\n                        [ 3.5783e-03,  1.1220e-02, -1.3388e-02]],\n              \n                       [[-1.8399e-03,  5.9302e-03,  9.6810e-03],\n                        [ 5.0733e-03,  1.0453e-02, -4.8722e-03],\n                        [-1.3514e-02, -1.1929e-03,  1.7507e-03]],\n              \n                       ...,\n              \n                       [[-1.4605e-03,  2.2461e-03, -8.0156e-03],\n                        [ 1.0985e-02,  5.1273e-03, -1.1668e-02],\n                        [ 1.4627e-02,  2.7758e-03,  7.2483e-03]],\n              \n                       [[ 1.3621e-02, -4.5283e-03,  6.4443e-04],\n                        [ 1.0748e-02,  1.1094e-02,  1.4675e-02],\n                        [-9.0625e-03, -6.1689e-03, -2.2046e-03]],\n              \n                       [[-1.4035e-03, -1.3366e-02,  5.8688e-03],\n                        [ 2.4954e-04,  7.3011e-03,  8.3442e-03],\n                        [-2.7433e-04, -1.0389e-02,  3.1839e-03]]]])),\n             ('up2.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.3.weight',\n              tensor([[[[ 7.9497e-03, -1.7790e-02, -1.7096e-02],\n                        [-1.6327e-02,  4.0280e-03, -1.9224e-02],\n                        [-4.1614e-03,  2.0345e-02, -1.3011e-02]],\n              \n                       [[-1.1634e-02,  5.5307e-03, -1.6266e-02],\n                        [-1.1103e-02,  8.3270e-03, -1.5757e-02],\n                        [ 1.5221e-02, -1.2837e-02,  9.6909e-04]],\n              \n                       [[-1.6213e-02,  6.1893e-03,  1.9967e-02],\n                        [-1.0630e-02,  2.0123e-02,  6.5128e-03],\n                        [-2.0276e-02,  2.0401e-02,  1.5855e-02]],\n              \n                       ...,\n              \n                       [[ 1.4602e-02, -9.3187e-03,  1.2791e-02],\n                        [ 3.5288e-03,  8.2964e-03,  1.7589e-02],\n                        [ 4.4983e-03, -4.8159e-04, -3.6260e-03]],\n              \n                       [[-8.9474e-05,  1.3904e-02,  1.9019e-02],\n                        [-1.9988e-02, -1.3111e-02,  6.4248e-04],\n                        [ 6.8580e-04,  1.7128e-03,  5.4387e-03]],\n              \n                       [[ 1.4890e-02, -9.2215e-03, -5.8313e-03],\n                        [ 1.1482e-02, -1.2943e-02,  1.7208e-02],\n                        [-2.3544e-03,  8.3377e-04, -1.4550e-02]]],\n              \n              \n                      [[[-2.5915e-03, -3.9138e-03, -1.6308e-02],\n                        [-1.9927e-02, -9.3398e-03, -1.9362e-02],\n                        [-1.4066e-02,  9.7209e-03,  1.6551e-02]],\n              \n                       [[-1.9409e-02, -1.3963e-02,  6.9585e-03],\n                        [-5.1612e-04, -1.9914e-02,  1.8270e-02],\n                        [-7.2831e-03,  1.2477e-02, -2.8120e-04]],\n              \n                       [[-1.5371e-02,  9.3540e-04,  9.9296e-03],\n                        [-1.0750e-02, -3.9004e-03,  1.7460e-02],\n                        [-1.9144e-02,  2.0190e-02, -1.1884e-02]],\n              \n                       ...,\n              \n                       [[ 7.7697e-03,  1.9071e-02, -3.6815e-03],\n                        [ 5.6426e-03, -8.5833e-03,  1.6836e-02],\n                        [ 1.8768e-03, -2.5059e-04,  8.1764e-03]],\n              \n                       [[ 5.9330e-03, -1.4364e-02, -3.9514e-03],\n                        [ 1.9684e-02, -1.4239e-02, -2.0091e-02],\n                        [ 2.0407e-02,  1.8737e-02, -5.8489e-03]],\n              \n                       [[ 5.4501e-03,  1.1028e-02, -1.9625e-02],\n                        [-1.3838e-02, -8.5165e-03,  2.6146e-03],\n                        [-6.4134e-03,  1.4367e-02,  1.4903e-02]]],\n              \n              \n                      [[[-1.1303e-03,  3.3091e-03, -6.1916e-03],\n                        [-1.5099e-02, -2.1207e-04,  4.5621e-03],\n                        [ 1.7857e-02, -2.7128e-03, -5.4803e-03]],\n              \n                       [[ 5.9743e-03,  2.0597e-02,  6.6697e-03],\n                        [ 9.8200e-03,  1.3099e-02,  1.7841e-03],\n                        [-1.6089e-02,  1.5824e-02,  8.0234e-04]],\n              \n                       [[-7.2984e-03,  1.2674e-02,  1.8605e-02],\n                        [ 3.9323e-03,  8.1922e-03, -9.3463e-04],\n                        [-1.9702e-02,  1.4019e-02,  1.6300e-02]],\n              \n                       ...,\n              \n                       [[ 1.6479e-02,  1.6218e-02, -1.5242e-02],\n                        [-3.6273e-03,  5.0512e-03,  1.1426e-02],\n                        [ 7.1217e-03,  7.2147e-03, -2.5175e-03]],\n              \n                       [[ 1.5327e-02,  1.4072e-02, -1.7085e-02],\n                        [ 4.0818e-04, -1.7114e-02, -3.8038e-03],\n                        [-1.5342e-02, -2.0213e-02, -1.3697e-02]],\n              \n                       [[-2.0410e-02, -1.5656e-02,  5.8427e-03],\n                        [-3.8405e-03,  1.0923e-02, -1.2858e-02],\n                        [ 1.8628e-02,  4.0466e-03, -2.0422e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.9150e-02,  1.2267e-02,  1.7782e-02],\n                        [ 1.3684e-02, -1.9804e-02, -9.2421e-03],\n                        [ 1.7435e-02,  1.7343e-02, -1.8515e-02]],\n              \n                       [[ 1.8531e-02, -6.2842e-03, -2.1436e-03],\n                        [-6.2577e-03,  1.8332e-02,  1.9857e-02],\n                        [-1.0869e-02, -5.4065e-03,  1.8648e-02]],\n              \n                       [[-9.8150e-03, -1.9312e-02, -5.3483e-04],\n                        [ 2.2209e-03,  2.0530e-02, -6.2797e-03],\n                        [ 3.1732e-03,  1.7359e-02,  1.0300e-02]],\n              \n                       ...,\n              \n                       [[ 5.3619e-03, -8.6172e-03,  1.9207e-02],\n                        [ 1.2767e-02, -3.0699e-03, -9.6391e-03],\n                        [-8.9599e-04,  6.0747e-03,  4.0384e-03]],\n              \n                       [[-5.2875e-03,  6.5115e-04,  5.4017e-03],\n                        [ 1.5804e-03,  8.6046e-03,  1.7447e-02],\n                        [ 7.5348e-03,  1.8965e-02,  1.9957e-02]],\n              \n                       [[-1.0331e-02, -1.1320e-02,  1.5131e-02],\n                        [ 2.9035e-03,  1.1799e-02, -1.5353e-03],\n                        [-8.3366e-03,  9.3031e-03, -1.7604e-02]]],\n              \n              \n                      [[[ 1.4307e-02,  1.1860e-02,  5.1069e-03],\n                        [-1.5284e-02,  8.2293e-03, -9.5887e-03],\n                        [ 5.3585e-03,  2.0224e-03,  1.5437e-02]],\n              \n                       [[ 1.2629e-03,  9.5884e-03,  1.5362e-02],\n                        [-4.8209e-03,  1.4933e-02, -1.2048e-02],\n                        [-3.0520e-05, -1.3378e-02, -2.1463e-03]],\n              \n                       [[-1.1527e-02,  7.7163e-03, -1.2359e-02],\n                        [-2.0476e-02, -1.7779e-02, -6.4546e-03],\n                        [ 3.1536e-03, -1.0851e-04, -1.9629e-02]],\n              \n                       ...,\n              \n                       [[-3.6267e-03, -1.7496e-02, -1.8531e-02],\n                        [ 3.0812e-03, -4.4989e-03, -5.3328e-03],\n                        [-3.5008e-03, -1.0352e-02,  2.0659e-02]],\n              \n                       [[-4.5241e-03,  6.3328e-03,  8.7361e-03],\n                        [-6.1625e-03, -1.3019e-02,  1.6934e-02],\n                        [-3.4158e-03,  8.9188e-03, -1.3646e-02]],\n              \n                       [[ 1.7996e-02,  1.7854e-02, -1.5007e-02],\n                        [ 2.2617e-04,  1.8391e-02,  2.0008e-02],\n                        [-1.4899e-03,  1.6801e-02,  2.3108e-03]]],\n              \n              \n                      [[[-1.5664e-02,  4.3163e-03,  1.2885e-02],\n                        [ 2.6682e-03,  1.6914e-02,  3.5899e-03],\n                        [ 1.9674e-02, -1.1662e-02, -1.2853e-02]],\n              \n                       [[-3.9540e-04, -1.7787e-02,  9.8214e-03],\n                        [ 1.3250e-02, -2.1693e-03, -4.9136e-03],\n                        [ 1.9610e-02,  1.1362e-03,  2.0132e-02]],\n              \n                       [[ 1.0343e-03,  8.4445e-03,  1.5850e-02],\n                        [ 1.1820e-02,  1.0775e-03, -1.8296e-02],\n                        [-1.1273e-02,  2.6236e-03,  1.3343e-02]],\n              \n                       ...,\n              \n                       [[ 1.6003e-02,  5.4038e-03, -3.7506e-03],\n                        [-2.4944e-03, -8.0193e-03, -6.6061e-03],\n                        [-1.2857e-02,  1.3497e-02,  8.1090e-03]],\n              \n                       [[-1.8006e-02, -8.5612e-03,  1.9954e-02],\n                        [-3.3323e-03, -7.7578e-04,  1.2751e-02],\n                        [ 8.0447e-03, -3.9115e-04,  2.0177e-02]],\n              \n                       [[-1.7435e-02, -8.4071e-03, -9.7204e-03],\n                        [ 1.8257e-02, -1.7279e-02, -1.8781e-02],\n                        [ 1.5807e-02, -1.8718e-02,  2.0478e-02]]]])),\n             ('up2.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.0.weight',\n              tensor([[[[ 6.5360e-04, -1.1478e-02, -1.2108e-02],\n                        [-1.3628e-02, -9.4881e-03,  4.5922e-03],\n                        [-1.3436e-03, -9.4868e-03, -4.5939e-03]],\n              \n                       [[ 1.0784e-02, -1.2223e-03, -1.5292e-02],\n                        [-5.8855e-03, -1.8780e-02, -8.7660e-03],\n                        [ 1.8609e-03,  1.2953e-02, -1.4010e-02]],\n              \n                       [[-6.7148e-03, -1.5341e-02,  1.2591e-02],\n                        [ 7.5377e-03,  1.1052e-02, -1.1975e-02],\n                        [-1.9517e-02, -1.9137e-02, -7.4886e-04]],\n              \n                       ...,\n              \n                       [[ 2.0512e-02, -3.9202e-03,  1.4523e-02],\n                        [ 1.2714e-02,  1.3007e-02,  6.8676e-04],\n                        [-1.7327e-02, -8.6569e-03,  1.2416e-03]],\n              \n                       [[-2.0188e-02, -1.2779e-02, -7.3068e-03],\n                        [-9.3873e-03,  1.3301e-02,  1.6646e-02],\n                        [-1.7413e-02,  1.7294e-03, -1.5510e-02]],\n              \n                       [[-1.4983e-02,  1.7590e-02,  1.2623e-02],\n                        [-2.8354e-03, -2.8116e-03,  1.7879e-02],\n                        [-1.7114e-02,  1.2573e-02,  1.0661e-02]]],\n              \n              \n                      [[[ 1.1610e-02, -1.0957e-02,  1.8087e-02],\n                        [ 1.2981e-02, -1.2237e-02, -1.3717e-02],\n                        [-8.9545e-03,  1.0519e-02, -1.8804e-02]],\n              \n                       [[-5.7298e-03,  1.7915e-02, -3.1621e-03],\n                        [ 7.9957e-03,  3.4881e-03, -1.5158e-02],\n                        [ 1.8798e-03,  1.6252e-02, -1.5315e-03]],\n              \n                       [[-4.2252e-03,  8.9630e-03, -7.0830e-03],\n                        [-1.0045e-02, -2.2602e-03,  7.8443e-03],\n                        [-2.6957e-03,  1.3411e-02,  4.8645e-03]],\n              \n                       ...,\n              \n                       [[-5.3712e-03, -1.0452e-02, -1.6330e-02],\n                        [-1.0432e-02, -1.9882e-02, -1.6169e-02],\n                        [-7.2622e-03, -1.8196e-02, -6.7982e-03]],\n              \n                       [[-7.0105e-05, -1.2175e-02, -1.0749e-02],\n                        [ 1.1441e-02,  3.5827e-03,  1.7456e-02],\n                        [-4.9655e-03,  1.9057e-03, -1.7193e-02]],\n              \n                       [[ 1.7013e-02,  3.1988e-04,  5.7411e-03],\n                        [-3.7235e-04, -1.8450e-03,  3.6671e-03],\n                        [ 1.6459e-02,  1.1565e-02,  1.9842e-02]]],\n              \n              \n                      [[[ 1.6914e-02, -1.2111e-02,  1.4786e-02],\n                        [ 7.7207e-03,  2.5537e-03,  4.0743e-03],\n                        [ 1.0419e-04,  1.0066e-02, -8.1808e-03]],\n              \n                       [[ 5.5924e-03,  3.0751e-03, -1.4255e-02],\n                        [ 1.4609e-02, -6.0797e-03,  1.8090e-02],\n                        [-2.0465e-02, -1.9647e-02,  1.9963e-02]],\n              \n                       [[ 1.7703e-02,  9.7912e-04, -1.7088e-02],\n                        [-3.0930e-03,  1.0013e-02,  1.5110e-02],\n                        [-1.5153e-02, -6.5340e-03,  1.6374e-02]],\n              \n                       ...,\n              \n                       [[-1.0198e-02,  1.8628e-02, -7.3407e-03],\n                        [-2.0066e-02,  1.8155e-02,  8.2106e-03],\n                        [-5.0477e-04, -5.1193e-03, -1.9685e-02]],\n              \n                       [[ 7.3187e-03, -1.8577e-02, -1.9180e-02],\n                        [ 1.3858e-02, -1.6733e-02, -5.7723e-04],\n                        [ 1.2103e-02,  8.6336e-03, -2.0067e-02]],\n              \n                       [[-3.8180e-03,  1.9922e-03, -1.2753e-02],\n                        [ 1.9889e-02,  1.9218e-02,  1.2516e-02],\n                        [-1.6966e-02, -1.9937e-02,  6.3545e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4647e-02,  1.3599e-02, -1.1497e-02],\n                        [ 1.0819e-02,  6.2655e-03,  8.2514e-03],\n                        [ 9.7814e-03,  1.5446e-03,  5.0288e-03]],\n              \n                       [[-3.7955e-03,  1.2494e-02, -7.8703e-03],\n                        [ 4.0349e-03,  1.4197e-02, -1.1018e-02],\n                        [ 1.2082e-02, -1.9828e-03,  1.1344e-02]],\n              \n                       [[-1.6060e-02,  5.2254e-03,  1.3679e-02],\n                        [ 2.3551e-03, -5.8034e-03, -1.0188e-02],\n                        [-7.8099e-03, -7.3378e-03, -1.6845e-02]],\n              \n                       ...,\n              \n                       [[ 4.8750e-03, -1.5202e-02, -8.3033e-03],\n                        [-1.4143e-02,  9.6245e-03,  1.0595e-03],\n                        [-6.6992e-03,  1.8018e-02,  1.4028e-02]],\n              \n                       [[-2.4361e-03,  8.2809e-03, -6.7384e-03],\n                        [-2.4594e-03,  4.9077e-03,  1.8375e-02],\n                        [-4.1593e-03, -3.5705e-03, -1.3529e-02]],\n              \n                       [[-1.7012e-02,  1.9748e-02,  1.9104e-02],\n                        [-1.4910e-02, -1.9546e-02,  1.1406e-02],\n                        [-1.7544e-04,  1.5866e-02,  3.8805e-03]]],\n              \n              \n                      [[[-4.2661e-03,  2.0544e-02, -2.0223e-02],\n                        [-1.7558e-02,  1.2315e-02, -1.1358e-03],\n                        [-9.5695e-03,  1.7591e-02, -1.8437e-02]],\n              \n                       [[-7.6622e-03,  1.3523e-02, -1.2805e-02],\n                        [ 4.2950e-03, -7.9838e-03, -8.6255e-03],\n                        [ 1.5282e-03, -8.8083e-03,  5.8126e-03]],\n              \n                       [[ 1.2428e-02,  1.6649e-03, -1.8423e-02],\n                        [ 3.3804e-03, -9.0342e-03, -2.8731e-03],\n                        [ 2.8868e-03, -4.1382e-03,  1.6776e-02]],\n              \n                       ...,\n              \n                       [[ 1.6678e-02, -4.2476e-03, -9.8835e-03],\n                        [-9.7655e-03, -3.7623e-03,  5.0571e-03],\n                        [ 1.0131e-02, -7.6768e-03, -5.4080e-04]],\n              \n                       [[ 1.7999e-02,  5.0342e-03, -2.2092e-03],\n                        [ 1.2079e-02, -8.4492e-03, -1.6282e-02],\n                        [-2.0245e-02,  4.7685e-03, -9.7620e-03]],\n              \n                       [[-4.6216e-03, -1.1652e-02, -1.2818e-02],\n                        [ 1.2088e-02, -9.3832e-03, -4.1677e-03],\n                        [ 1.1476e-02, -4.4116e-03, -2.0018e-02]]],\n              \n              \n                      [[[ 3.7413e-03, -1.8938e-02, -1.2220e-02],\n                        [ 1.7449e-02,  9.5147e-03,  2.5178e-03],\n                        [-6.6552e-03,  2.6520e-03, -2.0583e-02]],\n              \n                       [[ 1.9046e-02,  1.7330e-03,  3.4585e-03],\n                        [ 1.6316e-02, -1.8740e-02,  1.6343e-02],\n                        [-8.1862e-03, -1.9654e-02,  6.7754e-04]],\n              \n                       [[-7.8348e-03, -1.0483e-02, -1.1580e-02],\n                        [ 2.0537e-02, -1.2595e-02,  4.6942e-03],\n                        [ 5.1139e-04, -8.2631e-04, -1.3213e-03]],\n              \n                       ...,\n              \n                       [[ 2.0120e-02, -1.8718e-02,  7.1457e-03],\n                        [ 8.7498e-03, -8.0881e-03, -8.0977e-03],\n                        [-1.8490e-02, -2.0089e-02,  2.6450e-04]],\n              \n                       [[ 3.0537e-03, -8.0446e-03, -9.7033e-03],\n                        [ 2.9420e-03,  1.5974e-02, -8.4568e-03],\n                        [-4.6306e-03,  7.5076e-03, -9.9498e-04]],\n              \n                       [[-1.7441e-02, -4.8928e-03,  2.0088e-02],\n                        [ 1.1744e-02, -1.9409e-02, -1.2495e-02],\n                        [ 1.6826e-02, -6.6388e-03, -1.3236e-03]]]])),\n             ('up3.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.3.weight',\n              tensor([[[[-6.2617e-03,  5.1519e-03,  1.0535e-02],\n                        [ 2.2614e-02,  2.3770e-02,  7.1172e-03],\n                        [-9.0252e-04, -2.0448e-02, -2.0432e-02]],\n              \n                       [[-5.3073e-03,  2.0543e-03, -1.9999e-02],\n                        [ 1.7058e-02,  4.4323e-03,  2.0256e-02],\n                        [ 1.6059e-02,  7.8848e-03,  2.6898e-02]],\n              \n                       [[ 2.4905e-02, -9.5489e-04, -4.0310e-05],\n                        [ 2.6839e-02,  1.0395e-02, -1.1824e-02],\n                        [ 1.3696e-02, -4.7753e-03,  4.4547e-03]],\n              \n                       ...,\n              \n                       [[-4.0551e-03, -2.0774e-02,  5.0831e-03],\n                        [ 8.9578e-03, -2.4251e-02, -2.7485e-02],\n                        [-1.1212e-02, -3.5667e-03, -2.9207e-02]],\n              \n                       [[-2.5817e-02,  2.8529e-02, -2.4398e-02],\n                        [ 2.0831e-02,  1.4292e-02, -1.8673e-02],\n                        [-8.5094e-04, -1.2406e-03,  3.7525e-04]],\n              \n                       [[ 2.1931e-03,  6.2044e-03, -9.8672e-03],\n                        [-6.0165e-03,  7.0416e-03, -3.2293e-03],\n                        [-1.1025e-02, -1.1666e-02, -1.8839e-02]]],\n              \n              \n                      [[[-1.9571e-02,  1.3345e-02, -3.1977e-03],\n                        [-2.4555e-02, -3.5323e-03, -2.8703e-02],\n                        [-1.5313e-02,  2.1116e-02, -1.0758e-03]],\n              \n                       [[-1.0014e-02,  1.1471e-02, -2.2742e-02],\n                        [ 2.5164e-02,  1.5579e-02, -2.2211e-02],\n                        [ 2.7174e-02,  1.9207e-02, -1.7626e-02]],\n              \n                       [[ 2.7689e-02, -5.7403e-03, -1.0863e-02],\n                        [ 5.0870e-03,  6.7373e-03, -2.0150e-02],\n                        [ 2.9319e-02, -9.6329e-03, -2.0385e-02]],\n              \n                       ...,\n              \n                       [[-2.4959e-02,  1.2766e-03,  2.4264e-03],\n                        [ 2.1160e-02, -2.1553e-02,  1.6825e-02],\n                        [ 2.6579e-02,  6.6060e-03,  2.5650e-02]],\n              \n                       [[ 4.5595e-03,  1.9319e-03, -2.5173e-02],\n                        [-2.3925e-02, -8.3372e-03, -9.0146e-03],\n                        [ 1.7461e-02, -2.5896e-02, -1.8144e-02]],\n              \n                       [[ 2.5831e-02, -2.1761e-02, -2.9396e-02],\n                        [ 2.7635e-02, -1.2928e-02,  5.8588e-03],\n                        [-2.0192e-02,  4.7528e-03,  2.8390e-02]]],\n              \n              \n                      [[[ 1.8739e-03, -1.3140e-02,  2.6128e-02],\n                        [ 1.1566e-02,  3.5446e-03, -5.1995e-03],\n                        [ 5.5016e-03, -4.5294e-03,  1.9544e-02]],\n              \n                       [[-9.9646e-03,  2.7664e-02,  1.1371e-02],\n                        [ 1.2055e-02,  1.6825e-02, -1.1272e-02],\n                        [ 1.3120e-02,  1.7465e-02,  1.1575e-02]],\n              \n                       [[-4.8596e-03,  9.3461e-03,  2.0105e-02],\n                        [ 1.2126e-02, -2.2240e-03,  1.3572e-02],\n                        [-2.8769e-02, -7.9955e-03, -1.2733e-02]],\n              \n                       ...,\n              \n                       [[ 2.5646e-02,  1.6559e-02, -2.2198e-02],\n                        [-3.0433e-03,  2.7646e-02,  2.8915e-02],\n                        [ 2.3706e-02, -2.5853e-02, -8.8919e-05]],\n              \n                       [[ 1.9385e-02,  9.4940e-03, -1.7507e-02],\n                        [-1.0995e-02, -1.9027e-02,  2.6517e-02],\n                        [ 6.5096e-03,  8.3432e-03,  4.3078e-03]],\n              \n                       [[-1.2435e-02, -1.2040e-02,  6.4921e-03],\n                        [-1.9559e-02,  2.2276e-02,  1.2324e-02],\n                        [ 7.4537e-03,  5.5965e-03, -2.4149e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.9395e-02,  2.0365e-02, -1.6215e-02],\n                        [ 1.8015e-02,  1.1132e-02, -5.3747e-03],\n                        [ 4.5775e-03,  1.9513e-02,  5.4436e-03]],\n              \n                       [[ 2.0589e-02,  4.0204e-03, -7.1212e-03],\n                        [-1.7708e-02, -2.7610e-02,  2.9521e-03],\n                        [ 1.4294e-02, -6.5115e-03, -1.4379e-03]],\n              \n                       [[ 2.8011e-02,  1.6216e-02,  2.5210e-02],\n                        [-1.6498e-02,  1.0523e-02,  2.6155e-02],\n                        [ 1.6074e-02, -8.3713e-03,  2.2026e-02]],\n              \n                       ...,\n              \n                       [[-1.3617e-02, -1.4065e-02, -2.3103e-02],\n                        [ 2.4879e-02, -8.9402e-03,  3.0990e-03],\n                        [ 1.3965e-03, -2.5021e-02, -2.0546e-02]],\n              \n                       [[ 2.0246e-03, -7.9078e-03, -2.6747e-02],\n                        [ 2.9376e-02, -6.2544e-03, -1.8549e-02],\n                        [ 1.5150e-02, -3.9595e-03,  2.3443e-03]],\n              \n                       [[-3.6495e-03, -1.0052e-02,  1.2397e-03],\n                        [ 3.8338e-03, -2.8786e-02, -5.1455e-03],\n                        [-1.5915e-02,  2.8991e-02,  6.3032e-03]]],\n              \n              \n                      [[[-2.0503e-02, -2.8574e-02,  1.7111e-02],\n                        [-1.5106e-02,  2.2639e-02,  3.2666e-03],\n                        [ 1.1444e-02, -9.7533e-03,  1.8418e-02]],\n              \n                       [[-2.8729e-02, -1.7639e-02,  1.5558e-02],\n                        [ 2.1907e-02,  2.6665e-02, -2.0398e-02],\n                        [ 4.7236e-03,  2.2406e-02, -1.1982e-03]],\n              \n                       [[-6.9613e-03,  1.6444e-02,  1.0986e-04],\n                        [-2.5102e-02,  2.7951e-02,  1.8224e-02],\n                        [-9.3261e-03, -2.2952e-02, -1.9339e-02]],\n              \n                       ...,\n              \n                       [[ 6.3333e-03, -8.1322e-03,  3.5560e-03],\n                        [-2.3900e-02, -2.8754e-02, -2.0715e-02],\n                        [ 1.3923e-02,  1.0834e-02, -1.1983e-02]],\n              \n                       [[-1.2872e-02,  6.1885e-03, -1.2684e-02],\n                        [ 8.5061e-03, -1.3273e-03, -1.6401e-03],\n                        [ 3.5566e-03,  1.4142e-02,  7.0110e-03]],\n              \n                       [[ 1.2880e-02,  6.1687e-03, -9.6315e-03],\n                        [ 1.5918e-02,  2.2629e-03, -2.7104e-03],\n                        [-8.4794e-04,  2.0819e-02, -2.2515e-02]]],\n              \n              \n                      [[[ 8.6197e-03,  2.3163e-02,  1.9551e-02],\n                        [ 2.2528e-02,  1.8106e-02,  1.0401e-02],\n                        [-1.7955e-03, -5.1270e-03,  9.9206e-03]],\n              \n                       [[ 2.3529e-02,  1.5074e-02, -1.5779e-02],\n                        [-2.8125e-02, -1.9706e-02, -2.7739e-02],\n                        [ 1.2969e-02, -6.8372e-03, -1.8700e-02]],\n              \n                       [[-1.6456e-02, -1.9319e-02,  2.9451e-02],\n                        [-4.3081e-03,  1.6394e-02,  2.0039e-02],\n                        [-2.6109e-02,  1.8154e-02, -4.1342e-03]],\n              \n                       ...,\n              \n                       [[ 1.4506e-02, -2.9666e-03,  3.6261e-03],\n                        [ 1.6303e-02, -4.9343e-03, -1.7006e-02],\n                        [ 2.6239e-02, -2.3413e-02,  1.2565e-02]],\n              \n                       [[-7.7776e-03,  2.6909e-02,  1.0444e-02],\n                        [-8.7274e-03, -8.3104e-03,  2.3266e-03],\n                        [-2.4073e-02, -1.0433e-02, -1.1619e-02]],\n              \n                       [[-1.0362e-02, -2.3291e-02, -1.0579e-02],\n                        [ 1.6419e-02,  2.0854e-02,  2.4889e-02],\n                        [ 1.3606e-03, -9.4291e-03, -1.6355e-03]]]])),\n             ('up3.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.0.weight',\n              tensor([[[[-2.4477e-02, -1.7234e-02,  2.2003e-03],\n                        [-7.8829e-03,  6.1736e-03,  1.4644e-02],\n                        [ 9.7539e-03,  5.7497e-04, -2.1407e-02]],\n              \n                       [[ 2.5615e-02,  6.0152e-03, -2.8486e-02],\n                        [ 2.1189e-02,  6.7674e-03, -1.4792e-03],\n                        [ 2.2734e-02,  1.7544e-03, -1.0535e-02]],\n              \n                       [[ 2.1016e-02,  3.9310e-03,  5.9241e-03],\n                        [-9.3318e-04,  1.3821e-02,  2.8222e-02],\n                        [ 7.3732e-03,  2.3611e-03,  2.2986e-02]],\n              \n                       ...,\n              \n                       [[-2.6076e-02,  9.7759e-03,  1.7446e-02],\n                        [-4.6081e-03, -7.8919e-03, -1.3171e-02],\n                        [ 3.6483e-03,  5.5107e-04, -2.6154e-02]],\n              \n                       [[ 2.4815e-02,  6.5554e-04, -2.6840e-02],\n                        [-5.4893e-03, -1.2978e-02, -7.7000e-03],\n                        [ 1.7822e-02, -2.0376e-02,  1.8151e-02]],\n              \n                       [[-1.3709e-02, -2.1298e-02,  1.4319e-02],\n                        [-1.1540e-02,  2.9451e-03,  4.6603e-03],\n                        [ 1.6498e-02, -2.2247e-02, -2.6400e-02]]],\n              \n              \n                      [[[-2.9053e-02,  6.6088e-03,  2.8600e-02],\n                        [-8.5117e-03,  3.7488e-03,  2.5909e-02],\n                        [-6.6344e-03, -1.8867e-02,  2.1232e-02]],\n              \n                       [[ 2.7659e-02, -1.5675e-02, -1.2514e-02],\n                        [ 6.8806e-03, -2.4540e-02, -2.0591e-02],\n                        [-6.2750e-03, -2.9055e-02,  2.7674e-02]],\n              \n                       [[ 6.6344e-03, -2.5097e-02, -2.7987e-02],\n                        [-1.9412e-02, -1.7099e-02,  2.4543e-02],\n                        [-6.0892e-03, -1.9663e-02, -2.1830e-02]],\n              \n                       ...,\n              \n                       [[-2.4330e-02, -5.3355e-04,  1.6593e-02],\n                        [-1.5296e-02, -1.2302e-02, -2.1773e-02],\n                        [-2.4805e-02, -2.7568e-02, -5.2265e-03]],\n              \n                       [[ 1.4438e-02, -1.1498e-02, -5.8588e-03],\n                        [ 2.3541e-02,  2.8545e-02, -2.1781e-02],\n                        [ 2.1298e-02, -1.4740e-02,  2.0063e-02]],\n              \n                       [[-1.4228e-02,  2.7397e-02,  1.9363e-03],\n                        [ 1.3088e-02,  1.8878e-02,  2.5326e-02],\n                        [-2.7118e-02,  1.8095e-02,  1.5554e-02]]],\n              \n              \n                      [[[-2.7807e-02,  2.8756e-02, -2.4947e-02],\n                        [ 2.8239e-03,  6.4158e-03,  1.7847e-02],\n                        [-2.1316e-02, -1.1236e-02, -7.1000e-03]],\n              \n                       [[-2.2642e-02, -2.9162e-02, -2.7960e-02],\n                        [ 2.2822e-02,  2.6365e-02, -2.2013e-02],\n                        [-4.3668e-03,  5.9663e-03, -2.2929e-02]],\n              \n                       [[ 2.6231e-02,  6.2513e-04, -1.5292e-02],\n                        [-2.3744e-02,  1.0287e-02, -1.7989e-02],\n                        [ 1.4567e-02, -5.4238e-04, -1.8888e-03]],\n              \n                       ...,\n              \n                       [[ 8.2702e-03, -3.9680e-03,  4.4591e-03],\n                        [ 1.2113e-02,  1.9210e-02, -2.1732e-02],\n                        [ 1.8309e-02, -2.5562e-02, -3.4519e-03]],\n              \n                       [[ 2.0920e-02,  5.1383e-03, -2.8351e-02],\n                        [ 2.4168e-02,  2.4032e-03,  4.4554e-03],\n                        [-9.5799e-03, -4.6795e-03,  2.1697e-02]],\n              \n                       [[ 5.9437e-03,  1.4123e-03, -8.3815e-03],\n                        [ 2.3132e-02, -2.6785e-02, -1.6763e-02],\n                        [-9.6515e-03, -2.1222e-02,  2.4000e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.3391e-02,  2.3395e-02, -2.1791e-02],\n                        [ 1.8008e-02,  5.3447e-03,  2.3465e-02],\n                        [ 1.7817e-02, -3.0541e-04,  1.8585e-02]],\n              \n                       [[-1.8773e-02,  9.5143e-03, -9.0805e-03],\n                        [-1.1845e-02, -2.0910e-02,  7.6076e-03],\n                        [-1.9462e-03,  2.5138e-02, -2.8411e-02]],\n              \n                       [[ 1.2022e-02, -1.4268e-02,  1.6846e-02],\n                        [-1.5587e-02, -2.2586e-02,  1.7113e-03],\n                        [-2.0474e-02,  2.1718e-02,  2.6473e-02]],\n              \n                       ...,\n              \n                       [[-9.5288e-04, -2.0567e-02, -5.8081e-03],\n                        [-9.2609e-03,  2.2689e-02,  7.9880e-03],\n                        [-2.3267e-02, -2.2080e-03, -3.7323e-04]],\n              \n                       [[ 7.0031e-03,  1.5936e-02, -1.7355e-02],\n                        [ 9.1528e-03,  6.0140e-04, -4.6582e-03],\n                        [-2.2403e-03,  1.1589e-02,  1.3004e-02]],\n              \n                       [[ 7.5902e-03, -2.7939e-02,  1.6827e-02],\n                        [-1.1944e-02, -2.1053e-02,  7.7404e-03],\n                        [-2.4648e-02,  1.0781e-02,  1.6477e-02]]],\n              \n              \n                      [[[ 2.8526e-02, -8.3310e-03, -3.3514e-03],\n                        [ 8.7738e-03,  3.3132e-03, -2.3501e-03],\n                        [-1.5227e-02, -6.8209e-03,  7.2189e-03]],\n              \n                       [[ 3.2429e-03,  2.9305e-02,  7.2086e-03],\n                        [-2.8544e-02, -2.1567e-02, -7.0302e-03],\n                        [-1.2484e-02,  4.2848e-03, -1.5662e-02]],\n              \n                       [[ 1.4185e-03,  6.2046e-03,  2.1498e-02],\n                        [ 1.4784e-02, -2.4929e-02, -2.7400e-02],\n                        [-2.6303e-05,  2.4616e-02, -1.2550e-02]],\n              \n                       ...,\n              \n                       [[-1.1245e-02, -6.3400e-03, -1.4372e-02],\n                        [-2.6327e-02, -9.7659e-03, -1.9709e-03],\n                        [-2.4333e-03,  5.2920e-03,  1.3149e-02]],\n              \n                       [[ 2.8700e-03,  7.3612e-03,  2.3691e-03],\n                        [-2.7523e-02,  1.5241e-02,  1.3450e-02],\n                        [ 2.5740e-03, -3.4698e-03, -1.3424e-02]],\n              \n                       [[-1.4515e-02, -2.1749e-02,  1.3343e-02],\n                        [ 2.5754e-02,  3.5074e-03,  1.9747e-02],\n                        [ 2.7382e-03,  1.4910e-02, -2.2954e-02]]],\n              \n              \n                      [[[-4.3458e-03, -1.3681e-02,  1.8517e-02],\n                        [-1.4100e-02,  2.4556e-02, -1.6581e-03],\n                        [-2.7384e-02,  1.7085e-02,  1.9694e-02]],\n              \n                       [[ 5.4223e-03, -1.7057e-02, -6.0624e-03],\n                        [ 2.8144e-02, -1.2404e-02, -9.2200e-05],\n                        [ 8.0187e-03, -2.4534e-02, -6.1641e-03]],\n              \n                       [[ 4.4628e-03, -2.3212e-02,  1.8625e-02],\n                        [ 2.0626e-03, -1.1065e-02,  2.2116e-02],\n                        [-2.3691e-02,  7.7271e-03,  2.3667e-02]],\n              \n                       ...,\n              \n                       [[ 1.6437e-02,  1.7844e-02,  4.2858e-03],\n                        [ 1.8507e-02, -1.4175e-02,  6.2452e-03],\n                        [-2.2591e-02, -1.6163e-02,  2.8446e-02]],\n              \n                       [[ 7.0578e-03,  8.5772e-03,  1.2336e-03],\n                        [-2.7270e-02, -4.7153e-03,  1.8364e-02],\n                        [-1.7723e-02, -6.1744e-03, -2.6519e-02]],\n              \n                       [[ 2.6981e-03,  2.3110e-02, -1.9544e-02],\n                        [ 2.8593e-02,  2.6731e-02,  2.1887e-02],\n                        [-9.6571e-04,  1.7459e-02,  3.4465e-03]]]])),\n             ('up4.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.3.weight',\n              tensor([[[[ 3.1426e-03, -3.7804e-02, -1.9636e-03],\n                        [-3.3168e-02,  2.4599e-03, -2.5361e-02],\n                        [ 2.0291e-02, -3.1659e-02, -2.2596e-02]],\n              \n                       [[-8.4917e-03, -3.0465e-04, -2.1817e-02],\n                        [ 2.9646e-03,  2.4069e-02, -2.6871e-02],\n                        [ 2.7976e-02, -2.9426e-02, -1.9063e-02]],\n              \n                       [[ 3.4714e-02,  2.5515e-02,  2.2645e-03],\n                        [ 1.1169e-02, -1.5637e-02, -3.2919e-02],\n                        [-1.3760e-02,  1.0523e-03,  3.2319e-02]],\n              \n                       ...,\n              \n                       [[-2.6632e-02,  1.5643e-02, -3.1304e-03],\n                        [-6.5018e-03,  1.7912e-02, -1.7220e-02],\n                        [ 3.1036e-02,  3.4784e-02, -1.4025e-02]],\n              \n                       [[ 3.3626e-02, -2.4100e-02,  3.6708e-02],\n                        [-2.1758e-02, -1.4161e-02, -2.8572e-02],\n                        [ 5.2657e-03,  2.2184e-02, -1.2249e-02]],\n              \n                       [[ 3.9889e-02, -9.9724e-03,  1.4062e-03],\n                        [ 1.6991e-02, -5.8726e-03, -1.2741e-02],\n                        [-2.3483e-02,  3.6793e-02,  1.0728e-03]]],\n              \n              \n                      [[[-1.1431e-02,  2.8004e-03, -2.1472e-02],\n                        [-4.7250e-03,  3.1195e-02, -3.4145e-02],\n                        [-3.9074e-02, -9.0451e-03,  3.6595e-02]],\n              \n                       [[-3.4954e-02, -2.8686e-02,  7.4445e-03],\n                        [-3.4594e-02, -1.5361e-02,  3.2916e-02],\n                        [ 7.3619e-03, -2.8733e-02, -2.8171e-02]],\n              \n                       [[-1.6132e-02,  9.1593e-03, -1.5983e-03],\n                        [ 1.9147e-02, -3.0231e-02,  3.5481e-02],\n                        [-2.8131e-02, -1.5797e-02,  1.4560e-02]],\n              \n                       ...,\n              \n                       [[-2.0996e-03, -2.3411e-02, -1.1860e-02],\n                        [ 3.8093e-02,  3.5264e-02,  3.0247e-02],\n                        [ 1.3708e-02, -2.7209e-02,  3.5293e-02]],\n              \n                       [[-1.4823e-02, -1.3127e-02, -1.8602e-02],\n                        [ 3.1382e-02, -2.8936e-02, -3.5547e-02],\n                        [ 2.8250e-02,  2.5477e-02, -1.1684e-02]],\n              \n                       [[-3.4762e-03, -2.8827e-02,  2.2720e-02],\n                        [ 1.9048e-02,  1.9151e-02,  4.8282e-03],\n                        [ 3.6979e-02,  1.1263e-02,  1.4983e-02]]],\n              \n              \n                      [[[ 4.0528e-02, -1.5267e-02,  4.1640e-02],\n                        [ 1.4580e-02,  2.1254e-03,  2.1454e-02],\n                        [ 2.3367e-02,  2.4535e-02, -2.9547e-02]],\n              \n                       [[ 1.2478e-02, -3.2175e-02,  3.1261e-02],\n                        [-2.5070e-02,  1.0443e-02, -1.7667e-02],\n                        [-3.9835e-03, -1.4524e-02,  2.9181e-02]],\n              \n                       [[ 8.7496e-03,  1.6791e-02, -3.3366e-02],\n                        [ 3.9007e-02,  1.0403e-02,  3.8254e-02],\n                        [-1.2029e-02,  1.1168e-02, -1.9442e-02]],\n              \n                       ...,\n              \n                       [[ 2.2030e-02,  1.0903e-02, -1.4863e-02],\n                        [-1.3346e-02, -3.5193e-02,  3.2643e-02],\n                        [-3.8632e-02, -8.3370e-03,  1.8904e-02]],\n              \n                       [[-3.9616e-02, -2.5855e-02,  3.3651e-02],\n                        [ 3.9193e-02,  2.7768e-02,  1.4065e-02],\n                        [-8.8412e-03, -2.1744e-02, -2.0466e-02]],\n              \n                       [[-9.5175e-03, -3.2115e-02,  2.8135e-02],\n                        [-3.5135e-02, -3.5658e-02, -1.6859e-02],\n                        [ 3.8371e-02,  4.0490e-03,  2.5179e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6391e-02,  5.2747e-03,  3.4211e-02],\n                        [-3.6951e-02, -2.0392e-02,  1.9124e-02],\n                        [-4.0592e-03, -2.1158e-02, -5.6858e-03]],\n              \n                       [[-1.2450e-02, -7.7264e-03, -2.7716e-02],\n                        [ 3.4721e-02,  2.8399e-02,  3.7686e-02],\n                        [ 3.6166e-02,  1.7743e-02, -3.3313e-02]],\n              \n                       [[-2.4009e-03,  2.7938e-02,  8.2821e-03],\n                        [-1.0567e-02, -1.0721e-02,  3.9096e-02],\n                        [-1.0329e-02,  3.5188e-04,  1.9992e-02]],\n              \n                       ...,\n              \n                       [[ 4.0091e-02,  2.7190e-02, -3.8786e-02],\n                        [ 3.7762e-02,  1.6390e-02, -4.1539e-02],\n                        [ 2.8608e-02, -3.4842e-02, -1.5290e-02]],\n              \n                       [[ 2.5458e-02,  3.8800e-02,  1.8157e-02],\n                        [-3.0404e-02, -2.8858e-02, -3.7904e-02],\n                        [-1.7384e-02,  1.3624e-02, -3.8238e-02]],\n              \n                       [[-3.4968e-02, -2.1631e-02,  1.8572e-02],\n                        [ 3.9958e-02,  3.1534e-02, -2.6919e-03],\n                        [ 2.9025e-02, -2.5323e-02,  1.8108e-02]]],\n              \n              \n                      [[[ 1.4118e-02,  1.3075e-02,  7.9425e-04],\n                        [-1.5709e-02,  2.2579e-02, -3.4406e-03],\n                        [ 3.9156e-02, -5.3889e-03, -4.1343e-02]],\n              \n                       [[-1.1825e-03, -7.4790e-03,  3.0482e-02],\n                        [-4.0314e-02, -1.9415e-02, -5.4573e-05],\n                        [-3.6205e-03, -4.0538e-02,  1.6526e-02]],\n              \n                       [[ 3.1517e-02,  1.2538e-02,  1.7676e-03],\n                        [ 2.2461e-02, -2.9065e-02,  3.1906e-02],\n                        [-3.9866e-02, -2.3473e-02,  4.0793e-02]],\n              \n                       ...,\n              \n                       [[-2.2015e-02, -1.4035e-03, -3.4191e-02],\n                        [ 3.4649e-02,  2.7996e-02,  2.5186e-02],\n                        [-2.6122e-02, -3.7787e-02, -3.5784e-02]],\n              \n                       [[-3.5926e-03, -1.5855e-02, -2.4558e-02],\n                        [-3.5714e-02,  4.0327e-02,  3.9204e-02],\n                        [ 1.6102e-03, -2.2671e-02,  3.9940e-02]],\n              \n                       [[-4.1120e-02,  6.4742e-03,  1.8772e-02],\n                        [ 3.4173e-02,  5.7441e-04, -1.9311e-02],\n                        [-1.4727e-02,  1.7990e-02, -1.8958e-02]]],\n              \n              \n                      [[[ 2.9624e-02, -8.9972e-03,  4.0076e-02],\n                        [ 1.4882e-02, -1.9439e-02,  8.6693e-03],\n                        [-4.0603e-02,  1.5571e-02, -2.9153e-02]],\n              \n                       [[-3.5557e-02,  1.8946e-04,  2.2721e-02],\n                        [ 2.9935e-03,  8.9930e-03, -2.0757e-02],\n                        [ 2.0412e-02,  5.7608e-03,  2.6245e-02]],\n              \n                       [[-6.2162e-03, -7.0439e-04,  1.3922e-02],\n                        [-9.8026e-03,  2.8211e-02, -3.7612e-03],\n                        [-3.1022e-02, -2.4241e-02,  2.0704e-03]],\n              \n                       ...,\n              \n                       [[ 1.8656e-05, -3.5449e-02, -1.9142e-02],\n                        [-3.7448e-02, -3.8316e-02,  3.6445e-02],\n                        [ 1.8268e-02, -3.2087e-02, -3.0568e-02]],\n              \n                       [[-2.6703e-02, -7.0255e-04,  1.3062e-02],\n                        [ 9.2566e-03,  3.0957e-02, -3.9456e-02],\n                        [ 2.6741e-02,  1.7924e-02,  2.6267e-02]],\n              \n                       [[-3.0110e-02, -1.6314e-03, -2.8098e-02],\n                        [ 2.0860e-02,  1.5562e-02,  2.9175e-02],\n                        [ 9.1814e-03,  2.6883e-02,  2.8830e-02]]]])),\n             ('up4.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('outc.conv.weight',\n              tensor([[[[ 0.0984]],\n              \n                       [[-0.0668]],\n              \n                       [[-0.0782]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0089]],\n              \n                       [[-0.0501]],\n              \n                       [[-0.0261]],\n              \n                       [[ 0.0791]],\n              \n                       [[-0.1128]],\n              \n                       [[ 0.0102]],\n              \n                       [[ 0.0258]],\n              \n                       [[-0.0357]],\n              \n                       [[-0.0674]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.0549]],\n              \n                       [[-0.0972]],\n              \n                       [[-0.1207]],\n              \n                       [[ 0.1104]],\n              \n                       [[ 0.0293]],\n              \n                       [[-0.1182]],\n              \n                       [[ 0.1166]],\n              \n                       [[ 0.1038]],\n              \n                       [[-0.0085]],\n              \n                       [[-0.0039]],\n              \n                       [[ 0.0621]],\n              \n                       [[ 0.0331]],\n              \n                       [[ 0.0618]],\n              \n                       [[ 0.0310]],\n              \n                       [[ 0.1245]],\n              \n                       [[-0.1027]],\n              \n                       [[ 0.0523]],\n              \n                       [[ 0.0731]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0495]],\n              \n                       [[ 0.1218]],\n              \n                       [[ 0.1106]],\n              \n                       [[ 0.0079]],\n              \n                       [[-0.1117]],\n              \n                       [[ 0.1123]],\n              \n                       [[-0.0453]],\n              \n                       [[ 0.0750]],\n              \n                       [[ 0.0378]],\n              \n                       [[ 0.1220]],\n              \n                       [[-0.1052]],\n              \n                       [[-0.0909]],\n              \n                       [[-0.0841]],\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0207]],\n              \n                       [[-0.0161]],\n              \n                       [[-0.0815]],\n              \n                       [[ 0.0737]],\n              \n                       [[-0.0565]],\n              \n                       [[-0.0620]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.1087]],\n              \n                       [[ 0.0442]],\n              \n                       [[-0.0377]],\n              \n                       [[-0.0474]],\n              \n                       [[ 0.0807]],\n              \n                       [[ 0.0298]],\n              \n                       [[ 0.0700]],\n              \n                       [[ 0.0749]],\n              \n                       [[ 0.0847]],\n              \n                       [[-0.1145]]]])),\n             ('outc.conv.bias', tensor([-0.0712]))])</pre> In\u00a0[30]: Copied! <pre>## CPU\u200b\u6216\u200b\u5355\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\ntorch.save(unet.state_dict(), \"./unet_weight_example.pth\")\nloaded_unet_weights = torch.load(\"./unet_weight_example.pth\")\nunet.load_state_dict(loaded_unet_weights)\nunet.state_dict()\n</pre> ## CPU\u200b\u6216\u200b\u5355\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b torch.save(unet.state_dict(), \"./unet_weight_example.pth\") loaded_unet_weights = torch.load(\"./unet_weight_example.pth\") unet.load_state_dict(loaded_unet_weights) unet.state_dict() Out[30]: <pre>OrderedDict([('inc.double_conv.0.weight',\n              tensor([[[[-0.1569, -0.0516,  0.1381],\n                        [-0.0167,  0.1114, -0.1482],\n                        [-0.1659, -0.0492, -0.1526]],\n              \n                       [[ 0.0871,  0.1102, -0.1270],\n                        [ 0.1058,  0.0541, -0.0767],\n                        [ 0.1247,  0.1813,  0.1895]],\n              \n                       [[ 0.0929, -0.1305,  0.0531],\n                        [-0.0972, -0.1668, -0.0183],\n                        [-0.1754, -0.0862,  0.0373]]],\n              \n              \n                      [[[-0.0014,  0.1440, -0.0519],\n                        [ 0.1643,  0.1829,  0.1713],\n                        [-0.0702, -0.0426,  0.0083]],\n              \n                       [[ 0.1057,  0.0303,  0.0280],\n                        [-0.0306, -0.0898,  0.1635],\n                        [-0.1388, -0.0430,  0.0839]],\n              \n                       [[ 0.0840,  0.1753,  0.0916],\n                        [ 0.0819,  0.1624,  0.1901],\n                        [ 0.1914,  0.0483, -0.0875]]],\n              \n              \n                      [[[ 0.1197, -0.1618, -0.1778],\n                        [ 0.0866, -0.0638, -0.1615],\n                        [ 0.1437, -0.1523, -0.1007]],\n              \n                       [[-0.1395, -0.0602, -0.0457],\n                        [ 0.0582, -0.1701,  0.0586],\n                        [-0.1828,  0.0463,  0.1460]],\n              \n                       [[ 0.0735,  0.0299, -0.0629],\n                        [-0.0345, -0.0038,  0.0794],\n                        [-0.0958, -0.1519, -0.0411]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1095,  0.0703, -0.0860],\n                        [-0.1243, -0.0596, -0.1636],\n                        [ 0.0819,  0.0457,  0.1248]],\n              \n                       [[-0.1077, -0.1394,  0.0295],\n                        [ 0.1442, -0.1271,  0.1462],\n                        [-0.1011,  0.1301, -0.1294]],\n              \n                       [[-0.1653, -0.1431, -0.1031],\n                        [ 0.0511,  0.1370,  0.0210],\n                        [-0.1709,  0.0438, -0.0352]]],\n              \n              \n                      [[[-0.0893,  0.1826, -0.0856],\n                        [-0.1679,  0.0620,  0.1056],\n                        [-0.0206, -0.1745, -0.0500]],\n              \n                       [[ 0.0784,  0.0502,  0.1084],\n                        [-0.0746, -0.1213,  0.0849],\n                        [-0.1682, -0.1131, -0.1769]],\n              \n                       [[ 0.1111, -0.0814,  0.1804],\n                        [-0.0183,  0.0950, -0.0082],\n                        [-0.0761, -0.0757, -0.1657]]],\n              \n              \n                      [[[ 0.0543, -0.0157, -0.1387],\n                        [ 0.1503,  0.1388,  0.0653],\n                        [ 0.1474, -0.0991, -0.1478]],\n              \n                       [[ 0.0953, -0.1215,  0.1848],\n                        [-0.0360,  0.0052, -0.1841],\n                        [-0.1859, -0.0946,  0.1727]],\n              \n                       [[-0.0668, -0.0142,  0.1517],\n                        [-0.1101,  0.0217, -0.1021],\n                        [-0.1509,  0.0912,  0.1346]]]])),\n             ('inc.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.1.num_batches_tracked', tensor(0)),\n             ('inc.double_conv.3.weight',\n              tensor([[[[-4.1079e-02,  2.4625e-02, -5.8618e-03],\n                        [-3.6583e-02, -1.7239e-02,  2.4723e-02],\n                        [-2.0914e-03,  3.0168e-02, -2.0448e-02]],\n              \n                       [[ 4.1381e-03, -2.0328e-02, -2.9454e-02],\n                        [ 1.0681e-02, -3.6947e-02, -1.4246e-02],\n                        [-3.8679e-03,  2.3515e-02,  7.0796e-03]],\n              \n                       [[-3.3515e-02,  2.3345e-02, -5.7584e-04],\n                        [ 3.0752e-02, -3.5342e-02, -3.0192e-02],\n                        [ 3.0137e-02,  4.9735e-03,  3.0268e-02]],\n              \n                       ...,\n              \n                       [[ 2.6247e-02,  3.5036e-02, -2.7703e-02],\n                        [ 1.2037e-02, -1.1631e-02, -3.5691e-02],\n                        [ 1.8343e-02,  2.3172e-02, -2.3284e-02]],\n              \n                       [[ 3.9720e-02, -2.9578e-02, -3.8113e-02],\n                        [ 6.7576e-04, -4.0048e-02, -6.3216e-05],\n                        [ 1.9008e-02,  3.8545e-02,  3.0812e-02]],\n              \n                       [[-6.7981e-03, -1.5902e-03,  3.7965e-02],\n                        [ 8.6753e-03, -1.4569e-03, -1.9033e-02],\n                        [-2.0683e-02, -2.7206e-02,  2.5007e-02]]],\n              \n              \n                      [[[-1.3453e-02,  4.8410e-03,  6.3604e-03],\n                        [ 1.4860e-02, -1.9902e-04, -3.7245e-02],\n                        [ 1.2965e-02,  9.0473e-03,  2.3664e-02]],\n              \n                       [[-3.6142e-02, -2.9932e-02, -2.7691e-02],\n                        [ 2.6747e-02,  2.1051e-02, -6.9610e-03],\n                        [ 1.6672e-02,  2.4121e-02,  3.9934e-02]],\n              \n                       [[ 1.8793e-02,  3.8492e-02, -1.8463e-02],\n                        [ 2.4193e-02,  1.2931e-02, -2.9170e-02],\n                        [-2.2503e-02,  7.4183e-03, -9.9386e-03]],\n              \n                       ...,\n              \n                       [[-3.5583e-02,  1.0415e-02,  2.6884e-03],\n                        [-2.4120e-02, -1.6516e-02, -3.5117e-02],\n                        [-1.1389e-02, -3.2349e-02, -5.4190e-03]],\n              \n                       [[ 1.0794e-02, -1.4699e-02, -3.9218e-02],\n                        [ 7.2620e-03,  2.3942e-02, -9.0866e-03],\n                        [-3.9156e-02, -2.2665e-02,  3.0706e-02]],\n              \n                       [[ 2.5315e-02,  3.8635e-02, -1.4174e-03],\n                        [ 4.2061e-03, -3.3006e-02, -2.6736e-02],\n                        [-1.2201e-02,  2.4348e-02, -2.8096e-02]]],\n              \n              \n                      [[[-2.9801e-02,  1.3935e-02, -2.9342e-02],\n                        [-4.2913e-03,  9.5715e-03,  3.7494e-02],\n                        [ 2.2639e-02,  1.3474e-02,  2.3872e-02]],\n              \n                       [[ 1.6016e-03,  2.9424e-02,  2.3341e-02],\n                        [-1.2055e-02, -3.9560e-02, -1.5007e-02],\n                        [ 2.5384e-02, -4.1246e-02,  2.9730e-02]],\n              \n                       [[ 2.2965e-02, -2.7511e-02, -1.2306e-02],\n                        [-1.4792e-02,  2.7210e-03, -3.1689e-02],\n                        [ 3.1452e-02, -2.1154e-02,  3.2495e-02]],\n              \n                       ...,\n              \n                       [[ 6.1211e-03, -1.7085e-03,  1.0614e-02],\n                        [-1.3250e-03,  2.0869e-02,  7.6367e-03],\n                        [-3.3447e-02, -3.5193e-02, -3.4296e-02]],\n              \n                       [[ 2.6182e-02, -9.0026e-03,  4.3130e-03],\n                        [-1.9488e-02,  3.6438e-02, -2.9620e-02],\n                        [-4.0476e-02,  8.5702e-03,  2.2612e-02]],\n              \n                       [[ 1.9338e-03, -1.3990e-02,  8.3609e-03],\n                        [-1.3580e-02, -3.6543e-02,  2.8900e-02],\n                        [ 2.8948e-02, -2.2145e-03, -2.4276e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.0462e-03,  3.9649e-02,  1.0557e-02],\n                        [ 3.1926e-02,  3.8248e-02,  9.8494e-03],\n                        [ 1.2289e-03, -1.9980e-02, -3.3557e-02]],\n              \n                       [[-4.0275e-02,  1.1621e-02,  1.1366e-02],\n                        [-1.9881e-02,  6.3696e-03,  4.0948e-02],\n                        [-1.5219e-02, -1.6628e-02,  2.8343e-03]],\n              \n                       [[ 2.7490e-02,  3.5501e-02,  3.2039e-02],\n                        [ 3.5091e-03,  1.1285e-02,  1.5338e-02],\n                        [ 1.9410e-02, -5.1183e-03, -2.9545e-02]],\n              \n                       ...,\n              \n                       [[-2.0173e-02,  3.1788e-02,  8.5245e-03],\n                        [ 1.2969e-02,  1.4843e-02,  1.5726e-02],\n                        [ 3.1018e-02, -2.0554e-02,  1.6326e-02]],\n              \n                       [[-3.5004e-02,  3.6636e-02,  5.2004e-03],\n                        [ 2.9926e-02,  3.7449e-02,  6.1300e-04],\n                        [-5.1867e-04, -4.0083e-02, -3.0298e-02]],\n              \n                       [[-1.5009e-02,  4.1003e-02,  7.9811e-03],\n                        [ 6.5824e-03, -2.2011e-02,  8.9981e-03],\n                        [ 1.5385e-02, -3.9503e-02,  4.1086e-02]]],\n              \n              \n                      [[[-2.8993e-02, -3.7376e-02,  1.1231e-02],\n                        [ 1.7329e-02, -5.8507e-03,  1.9821e-02],\n                        [ 2.0648e-02, -3.9886e-02,  1.6316e-02]],\n              \n                       [[ 3.2519e-02,  1.6676e-02,  1.2690e-03],\n                        [ 1.6236e-03,  4.4074e-03, -2.0494e-02],\n                        [-3.6117e-02,  1.2012e-02, -2.8950e-02]],\n              \n                       [[-3.4818e-02, -1.8692e-02, -6.5148e-03],\n                        [-3.8199e-02, -2.1533e-03, -2.6669e-02],\n                        [ 2.0359e-03, -1.0877e-02,  3.2552e-02]],\n              \n                       ...,\n              \n                       [[ 2.6173e-03, -3.7495e-02,  8.6743e-03],\n                        [ 4.8354e-04,  4.1075e-02, -6.5880e-03],\n                        [ 3.3915e-02,  3.9410e-03, -1.2893e-02]],\n              \n                       [[ 2.6528e-02, -4.0759e-02,  1.9229e-02],\n                        [ 2.2432e-02, -3.9180e-03,  2.6232e-02],\n                        [ 1.2603e-02, -3.1149e-03, -1.4234e-02]],\n              \n                       [[-2.9655e-03,  1.3039e-03, -2.7197e-02],\n                        [ 3.9957e-02, -1.5892e-02,  2.0109e-02],\n                        [ 1.4106e-03,  6.4586e-04,  8.9162e-03]]],\n              \n              \n                      [[[ 3.1019e-02,  3.9165e-02, -2.7102e-02],\n                        [-3.8747e-02, -2.9976e-02, -8.2251e-04],\n                        [ 3.1431e-02, -9.7356e-03,  1.1533e-02]],\n              \n                       [[-8.6869e-03,  3.6680e-02,  1.8349e-02],\n                        [-3.1113e-02, -2.5772e-02, -1.2013e-02],\n                        [ 2.4810e-02,  2.1669e-02, -3.3620e-02]],\n              \n                       [[-3.0419e-02,  7.3520e-03, -1.9823e-02],\n                        [ 3.8660e-02,  2.6089e-02,  3.0254e-02],\n                        [ 1.4994e-02,  1.0452e-02,  3.4261e-02]],\n              \n                       ...,\n              \n                       [[-3.2601e-02, -3.6214e-02,  3.6512e-02],\n                        [-3.7527e-02, -2.9699e-02,  1.5305e-02],\n                        [-2.4764e-02,  2.2672e-02,  2.2486e-02]],\n              \n                       [[ 1.1033e-02,  3.0824e-02,  2.4714e-02],\n                        [-2.1154e-02,  2.5543e-02,  1.0087e-02],\n                        [ 2.3082e-02, -3.0461e-02,  3.4150e-02]],\n              \n                       [[-1.8519e-02, -7.6047e-03,  2.7975e-02],\n                        [-6.4077e-03, -2.6562e-02,  9.9592e-03],\n                        [-2.9076e-02, -2.5703e-02, -2.9623e-02]]]])),\n             ('inc.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('inc.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('inc.double_conv.4.num_batches_tracked', tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[ 0.0357, -0.0264,  0.0201],\n                        [ 0.0235, -0.0205,  0.0169],\n                        [ 0.0325, -0.0087, -0.0301]],\n              \n                       [[-0.0252,  0.0130,  0.0105],\n                        [ 0.0278,  0.0094, -0.0272],\n                        [ 0.0324,  0.0047,  0.0045]],\n              \n                       [[-0.0352, -0.0399, -0.0170],\n                        [ 0.0144,  0.0158, -0.0144],\n                        [-0.0233,  0.0018, -0.0334]],\n              \n                       ...,\n              \n                       [[ 0.0116, -0.0235, -0.0296],\n                        [-0.0242,  0.0119,  0.0299],\n                        [ 0.0114,  0.0182,  0.0288]],\n              \n                       [[-0.0316, -0.0088, -0.0152],\n                        [-0.0325, -0.0183, -0.0030],\n                        [-0.0355, -0.0339,  0.0363]],\n              \n                       [[-0.0135,  0.0221,  0.0305],\n                        [-0.0268,  0.0040, -0.0396],\n                        [-0.0201,  0.0218, -0.0349]]],\n              \n              \n                      [[[ 0.0126,  0.0043, -0.0306],\n                        [-0.0146,  0.0352,  0.0244],\n                        [ 0.0250,  0.0273,  0.0250]],\n              \n                       [[-0.0412,  0.0087,  0.0332],\n                        [ 0.0187, -0.0076, -0.0089],\n                        [-0.0151, -0.0058, -0.0293]],\n              \n                       [[-0.0167, -0.0200,  0.0142],\n                        [-0.0356,  0.0294,  0.0118],\n                        [-0.0244, -0.0215,  0.0074]],\n              \n                       ...,\n              \n                       [[-0.0035,  0.0137, -0.0314],\n                        [ 0.0138, -0.0057,  0.0048],\n                        [ 0.0214, -0.0232, -0.0108]],\n              \n                       [[-0.0412, -0.0090, -0.0090],\n                        [-0.0287,  0.0126,  0.0135],\n                        [ 0.0138,  0.0354, -0.0151]],\n              \n                       [[ 0.0006, -0.0026,  0.0229],\n                        [ 0.0340,  0.0215,  0.0193],\n                        [-0.0062,  0.0044,  0.0232]]],\n              \n              \n                      [[[ 0.0393,  0.0131, -0.0272],\n                        [-0.0268, -0.0212,  0.0276],\n                        [-0.0300,  0.0367, -0.0406]],\n              \n                       [[ 0.0010, -0.0226, -0.0340],\n                        [ 0.0188,  0.0097, -0.0116],\n                        [ 0.0346, -0.0155,  0.0074]],\n              \n                       [[ 0.0277, -0.0405,  0.0331],\n                        [ 0.0064,  0.0333,  0.0368],\n                        [ 0.0375,  0.0212, -0.0242]],\n              \n                       ...,\n              \n                       [[-0.0069,  0.0186, -0.0329],\n                        [ 0.0099, -0.0293,  0.0133],\n                        [ 0.0385,  0.0099,  0.0152]],\n              \n                       [[ 0.0165,  0.0133,  0.0077],\n                        [-0.0347, -0.0064,  0.0321],\n                        [-0.0038, -0.0347,  0.0405]],\n              \n                       [[ 0.0055, -0.0044, -0.0135],\n                        [ 0.0195,  0.0027,  0.0329],\n                        [-0.0107,  0.0344, -0.0313]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0298, -0.0407, -0.0166],\n                        [-0.0002, -0.0221,  0.0067],\n                        [ 0.0178,  0.0013, -0.0193]],\n              \n                       [[-0.0238,  0.0293,  0.0269],\n                        [ 0.0277,  0.0384,  0.0140],\n                        [-0.0363, -0.0101,  0.0253]],\n              \n                       [[ 0.0334, -0.0225, -0.0067],\n                        [-0.0341,  0.0260, -0.0054],\n                        [ 0.0118,  0.0148,  0.0336]],\n              \n                       ...,\n              \n                       [[-0.0390,  0.0067, -0.0146],\n                        [-0.0058, -0.0076,  0.0248],\n                        [-0.0309, -0.0162, -0.0044]],\n              \n                       [[ 0.0156,  0.0133, -0.0077],\n                        [-0.0084, -0.0258,  0.0351],\n                        [ 0.0133, -0.0063,  0.0344]],\n              \n                       [[ 0.0333,  0.0093, -0.0372],\n                        [-0.0002,  0.0405, -0.0157],\n                        [-0.0018, -0.0008,  0.0080]]],\n              \n              \n                      [[[ 0.0330, -0.0097, -0.0083],\n                        [-0.0216,  0.0057, -0.0085],\n                        [ 0.0082,  0.0023,  0.0381]],\n              \n                       [[-0.0320,  0.0131, -0.0137],\n                        [-0.0037,  0.0201, -0.0339],\n                        [ 0.0327,  0.0375, -0.0072]],\n              \n                       [[-0.0085, -0.0173,  0.0102],\n                        [ 0.0381,  0.0038,  0.0299],\n                        [ 0.0261,  0.0366,  0.0206]],\n              \n                       ...,\n              \n                       [[-0.0330, -0.0098, -0.0026],\n                        [ 0.0038,  0.0086,  0.0258],\n                        [-0.0036,  0.0356, -0.0383]],\n              \n                       [[ 0.0014,  0.0289, -0.0069],\n                        [-0.0358, -0.0261, -0.0318],\n                        [-0.0223, -0.0333,  0.0221]],\n              \n                       [[ 0.0099, -0.0044,  0.0356],\n                        [-0.0416,  0.0245,  0.0219],\n                        [-0.0125, -0.0308, -0.0395]]],\n              \n              \n                      [[[-0.0059, -0.0348, -0.0104],\n                        [-0.0281, -0.0408,  0.0101],\n                        [-0.0012,  0.0124, -0.0115]],\n              \n                       [[-0.0382, -0.0336,  0.0156],\n                        [-0.0337,  0.0008,  0.0405],\n                        [-0.0058, -0.0384, -0.0303]],\n              \n                       [[-0.0357,  0.0154,  0.0037],\n                        [ 0.0079,  0.0382, -0.0023],\n                        [-0.0099,  0.0091, -0.0170]],\n              \n                       ...,\n              \n                       [[-0.0194,  0.0131, -0.0097],\n                        [-0.0112, -0.0016, -0.0009],\n                        [-0.0198, -0.0326, -0.0109]],\n              \n                       [[ 0.0248, -0.0348, -0.0202],\n                        [-0.0041, -0.0386, -0.0109],\n                        [-0.0228, -0.0399,  0.0372]],\n              \n                       [[-0.0010, -0.0073,  0.0204],\n                        [-0.0288,  0.0141,  0.0010],\n                        [-0.0160, -0.0138,  0.0360]]]])),\n             ('down1.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down1.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.1305e-02, -1.2684e-03,  2.4892e-02],\n                        [-2.6919e-02, -1.1080e-02,  6.1028e-04],\n                        [-6.9626e-03,  2.4179e-02,  7.0370e-03]],\n              \n                       [[-8.0535e-03, -1.8495e-04, -2.7226e-02],\n                        [-1.6500e-02,  3.6307e-03,  2.3883e-02],\n                        [-7.6892e-03,  2.6147e-02,  1.8880e-02]],\n              \n                       [[-6.3356e-04, -7.4601e-03, -7.9877e-03],\n                        [ 1.3430e-02, -1.9490e-02,  3.8737e-03],\n                        [-1.6122e-02, -1.8464e-02,  2.0742e-02]],\n              \n                       ...,\n              \n                       [[ 1.8362e-03, -1.1564e-02, -2.8767e-02],\n                        [ 5.5608e-03,  6.5534e-03,  1.5489e-02],\n                        [-1.3676e-02, -2.4228e-02,  1.2859e-02]],\n              \n                       [[ 1.7046e-02,  3.1059e-03, -1.3043e-02],\n                        [-1.1144e-02,  8.5697e-03, -9.9781e-03],\n                        [ 6.2510e-03, -2.7031e-02, -8.6106e-03]],\n              \n                       [[ 2.8901e-02,  1.9356e-02, -2.5723e-02],\n                        [-2.0941e-02,  1.2509e-02,  2.8496e-02],\n                        [-1.6640e-02, -3.5848e-03, -1.0853e-02]]],\n              \n              \n                      [[[ 1.2726e-02, -1.6195e-02,  1.4709e-02],\n                        [-2.0562e-02, -2.8356e-02,  1.0373e-02],\n                        [ 1.6941e-02, -1.7723e-02,  2.5551e-02]],\n              \n                       [[-1.9462e-02,  2.7471e-02, -1.6930e-02],\n                        [-2.7676e-03, -1.4025e-03,  1.7487e-02],\n                        [ 1.6080e-02,  2.9447e-02, -1.8378e-02]],\n              \n                       [[ 2.8415e-03, -1.0617e-02, -1.0754e-03],\n                        [ 2.2315e-02, -1.2144e-02, -1.7454e-02],\n                        [-2.4725e-02, -1.4872e-02,  1.2383e-02]],\n              \n                       ...,\n              \n                       [[ 2.1383e-02, -2.6270e-02, -1.2159e-02],\n                        [-2.1438e-02, -2.4603e-02, -1.3974e-02],\n                        [-2.2166e-02,  2.9069e-02,  1.0996e-02]],\n              \n                       [[ 2.6262e-02, -3.3151e-03,  2.6866e-02],\n                        [-1.1902e-02,  2.3779e-03,  2.6081e-02],\n                        [ 5.4771e-03,  7.5126e-04, -8.3137e-03]],\n              \n                       [[ 2.5385e-02,  7.2457e-03, -1.6735e-02],\n                        [-4.7629e-03, -1.2607e-02, -4.5772e-03],\n                        [ 1.6854e-02,  1.9901e-02,  2.8703e-02]]],\n              \n              \n                      [[[-2.8001e-02, -4.4546e-04, -2.0191e-02],\n                        [ 2.4830e-02, -2.2498e-02, -2.0728e-02],\n                        [-1.0464e-02,  2.7569e-02,  2.9056e-02]],\n              \n                       [[-2.7124e-02, -7.6276e-03,  2.4910e-02],\n                        [-5.0865e-03, -1.3039e-02, -1.9636e-02],\n                        [-2.0727e-02, -2.3310e-02, -1.5865e-02]],\n              \n                       [[ 7.5711e-03,  7.3599e-03, -2.2980e-02],\n                        [-2.5551e-02,  2.2718e-02,  1.5489e-02],\n                        [-3.0655e-04,  1.2903e-02, -2.2033e-02]],\n              \n                       ...,\n              \n                       [[-1.5014e-02, -7.5347e-04,  1.6599e-03],\n                        [-5.4850e-03,  1.3427e-02,  2.9824e-03],\n                        [ 2.4041e-02,  1.7558e-03,  1.0491e-02]],\n              \n                       [[-1.7517e-02,  2.2218e-02,  2.1117e-02],\n                        [-8.5116e-05,  2.7633e-02,  1.1950e-03],\n                        [ 2.3484e-02, -2.0629e-02, -7.9562e-03]],\n              \n                       [[ 6.6841e-03, -2.7769e-02, -2.2987e-02],\n                        [-2.4637e-02,  2.2629e-02, -1.2457e-02],\n                        [-1.0986e-02, -1.6586e-02, -4.0791e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.6628e-03,  2.6667e-02,  6.7481e-03],\n                        [-1.4348e-02, -1.9016e-02,  2.1977e-02],\n                        [ 1.1526e-02,  2.0264e-03, -1.9429e-02]],\n              \n                       [[-1.5399e-02,  2.4140e-02,  1.7281e-02],\n                        [-5.1553e-05,  2.7146e-03, -2.2730e-02],\n                        [-2.2137e-02,  1.5756e-02,  9.6129e-03]],\n              \n                       [[-5.2356e-03,  1.8795e-02,  1.4753e-02],\n                        [-2.9235e-02, -2.4725e-02, -9.9595e-03],\n                        [-2.5816e-02, -1.2593e-02, -1.4906e-02]],\n              \n                       ...,\n              \n                       [[-5.1329e-04,  2.4464e-02,  1.0491e-02],\n                        [ 1.6588e-03, -1.9864e-02, -2.4729e-02],\n                        [-5.7917e-03,  1.2495e-02,  7.5220e-03]],\n              \n                       [[ 1.5368e-02, -2.5456e-02, -1.4819e-02],\n                        [-2.5614e-02, -2.3670e-03,  2.6447e-02],\n                        [-5.4125e-03, -4.6167e-03, -7.2054e-04]],\n              \n                       [[-1.7071e-02, -2.6587e-03,  2.1725e-02],\n                        [-2.8988e-02,  3.1809e-03,  1.3815e-03],\n                        [ 6.4158e-03, -2.6444e-04,  1.8910e-02]]],\n              \n              \n                      [[[ 2.5009e-02,  4.4661e-03, -2.5017e-02],\n                        [ 6.8237e-03,  1.3778e-02,  6.8838e-03],\n                        [-1.5440e-02, -1.2293e-03,  2.2054e-02]],\n              \n                       [[-1.6465e-02,  1.3906e-02,  2.9242e-02],\n                        [ 2.2392e-02, -6.8427e-03, -2.1006e-02],\n                        [ 2.3828e-02, -1.8528e-02,  4.6238e-03]],\n              \n                       [[ 2.6324e-02, -3.9792e-03, -2.8550e-02],\n                        [ 9.2739e-03,  8.2617e-03, -2.5574e-02],\n                        [ 1.6078e-02,  1.6129e-02,  6.8392e-03]],\n              \n                       ...,\n              \n                       [[ 2.7127e-02, -1.3369e-02,  8.5266e-03],\n                        [-1.0530e-02, -2.0817e-02, -8.6817e-03],\n                        [-2.9038e-02, -2.4825e-03,  1.3813e-02]],\n              \n                       [[ 1.2809e-02, -2.7485e-02, -2.8767e-02],\n                        [-5.6553e-03,  1.9724e-02,  1.1964e-02],\n                        [ 5.6818e-03,  1.9974e-02, -1.8658e-02]],\n              \n                       [[ 2.8031e-02, -2.4776e-02, -3.0622e-03],\n                        [ 1.4898e-02,  2.7475e-03, -2.2119e-02],\n                        [ 5.8204e-03,  6.9012e-03, -2.6735e-02]]],\n              \n              \n                      [[[ 9.7910e-03,  1.7056e-02, -4.8750e-03],\n                        [ 3.8653e-03,  9.2350e-03, -2.7748e-02],\n                        [ 2.4542e-02, -9.4870e-03,  2.7431e-02]],\n              \n                       [[ 1.5725e-03,  5.4433e-03,  6.2727e-03],\n                        [ 2.9122e-02,  1.9450e-02, -1.4450e-02],\n                        [ 7.3775e-03,  2.3615e-02, -1.2452e-02]],\n              \n                       [[-7.7901e-04,  5.2408e-03,  1.3440e-02],\n                        [ 1.1745e-02, -2.4794e-02,  5.6418e-03],\n                        [ 1.4150e-02, -1.9262e-02, -6.3717e-04]],\n              \n                       ...,\n              \n                       [[ 4.6180e-03,  2.1094e-03, -2.5070e-02],\n                        [-1.9577e-02,  2.3995e-02, -1.5351e-02],\n                        [-2.1875e-02, -2.0034e-03,  3.7910e-03]],\n              \n                       [[ 2.1114e-03,  2.1738e-02,  1.3168e-03],\n                        [-9.2969e-03,  1.9882e-02,  5.0677e-03],\n                        [ 6.9171e-03,  2.1555e-02, -1.1559e-02]],\n              \n                       [[-2.8176e-02, -2.6783e-02,  2.4445e-02],\n                        [ 1.4733e-02,  4.4278e-03,  7.2822e-03],\n                        [-2.4972e-02, -1.4935e-02,  2.7857e-02]]]])),\n             ('down1.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down1.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('down1.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-2.0874e-03,  2.8328e-02,  3.8197e-03],\n                        [ 2.0103e-02, -2.4530e-02,  3.5383e-03],\n                        [ 1.2657e-02,  2.5045e-02,  5.3281e-03]],\n              \n                       [[ 9.3871e-03,  2.5844e-02, -1.4631e-02],\n                        [ 2.7466e-02, -1.0389e-02,  1.5178e-02],\n                        [ 2.8453e-02,  1.3451e-02, -1.1607e-03]],\n              \n                       [[ 2.0450e-02,  1.3948e-02, -1.8822e-02],\n                        [-1.6178e-03,  2.4138e-02,  1.6494e-02],\n                        [-2.7684e-02, -1.6600e-02,  2.5942e-03]],\n              \n                       ...,\n              \n                       [[-2.5010e-03,  2.1981e-02,  1.0307e-02],\n                        [ 1.0725e-02,  2.8690e-02, -1.7391e-02],\n                        [ 3.5500e-03,  2.0341e-03,  5.9864e-03]],\n              \n                       [[-8.7539e-03,  1.3636e-02,  2.7444e-02],\n                        [-5.3241e-03,  1.4782e-02, -1.6061e-02],\n                        [ 2.8436e-02, -2.6700e-02, -5.3704e-03]],\n              \n                       [[-2.3932e-02,  6.0354e-03,  2.0279e-02],\n                        [-2.7523e-02, -2.8895e-02,  2.0104e-02],\n                        [-6.3520e-03,  8.0765e-03,  2.4935e-03]]],\n              \n              \n                      [[[-1.0771e-02, -3.8036e-03, -2.3648e-02],\n                        [-1.3159e-02,  2.4382e-02,  2.5068e-02],\n                        [-1.8793e-02, -2.5927e-02,  1.6405e-02]],\n              \n                       [[ 4.6219e-03,  2.3189e-02, -1.0743e-02],\n                        [ 2.8896e-02, -2.2556e-02,  5.3712e-03],\n                        [-8.8788e-03, -8.3982e-03, -9.5629e-03]],\n              \n                       [[-2.3292e-02,  1.9044e-02,  1.8797e-03],\n                        [-1.7992e-02, -2.8691e-02,  1.8576e-03],\n                        [-2.4593e-02,  8.3165e-03, -5.6803e-03]],\n              \n                       ...,\n              \n                       [[-2.7325e-02, -1.6579e-02, -2.7656e-02],\n                        [-1.4223e-02,  6.2641e-03, -2.7416e-02],\n                        [-1.8046e-02,  1.1367e-02, -1.2150e-02]],\n              \n                       [[-3.4729e-03,  5.4115e-04, -1.9539e-02],\n                        [ 1.6914e-02, -1.1351e-02,  2.0686e-02],\n                        [-1.0540e-02, -2.7865e-02,  3.4599e-03]],\n              \n                       [[-1.5403e-02, -5.0929e-03, -2.0951e-02],\n                        [ 1.8758e-02, -1.5846e-02, -2.6030e-02],\n                        [ 2.3687e-02, -2.6410e-02,  5.7963e-03]]],\n              \n              \n                      [[[-2.6278e-02, -1.2930e-02, -1.6344e-02],\n                        [ 8.9017e-03, -1.8674e-02, -1.6698e-02],\n                        [-1.0313e-02,  9.8180e-03,  1.0110e-02]],\n              \n                       [[-2.1049e-02,  1.4577e-02, -1.8113e-02],\n                        [-2.0648e-02, -1.4387e-02, -2.4280e-04],\n                        [-2.0775e-02, -4.0661e-03,  2.7782e-02]],\n              \n                       [[-2.7178e-02,  4.2496e-03, -2.3201e-02],\n                        [ 1.0937e-02, -6.5350e-03, -2.3540e-02],\n                        [-2.9455e-02,  2.3027e-02, -2.7718e-02]],\n              \n                       ...,\n              \n                       [[-2.1814e-02,  1.5335e-02, -2.3714e-02],\n                        [-2.8257e-02,  2.3738e-02, -1.3762e-02],\n                        [-3.1294e-03,  9.6518e-03,  6.7151e-03]],\n              \n                       [[-2.5689e-02,  4.9199e-03,  1.6813e-02],\n                        [ 2.7413e-02, -2.5757e-02, -2.6320e-02],\n                        [ 2.8428e-02, -1.9982e-02, -6.2184e-03]],\n              \n                       [[-4.9595e-03, -2.2561e-02,  2.1508e-02],\n                        [ 6.1043e-03, -1.9141e-02, -1.6917e-02],\n                        [-2.2802e-02, -7.2276e-03,  1.1010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8587e-04,  2.5234e-02,  1.2862e-02],\n                        [ 6.4087e-03,  2.9456e-03, -6.2891e-03],\n                        [ 1.3295e-02,  1.1122e-02, -3.8489e-03]],\n              \n                       [[ 2.4627e-02, -8.6374e-03,  9.6317e-03],\n                        [-4.4341e-03, -2.0696e-03,  5.3607e-05],\n                        [ 2.7382e-02, -1.1736e-03, -2.8442e-03]],\n              \n                       [[ 7.9895e-03, -6.4228e-03,  9.2783e-03],\n                        [ 1.0661e-03, -2.7210e-02,  2.9449e-02],\n                        [ 2.8375e-03, -2.2452e-02, -3.4423e-03]],\n              \n                       ...,\n              \n                       [[ 7.1594e-03, -2.7026e-02, -6.7921e-03],\n                        [-1.5202e-02, -7.0004e-04, -6.5862e-03],\n                        [ 2.7967e-02,  2.5300e-02,  5.7218e-03]],\n              \n                       [[ 1.9714e-02,  2.5212e-02,  2.6632e-02],\n                        [ 3.6115e-03, -2.2397e-02, -1.0878e-02],\n                        [-1.3762e-02,  4.6104e-04,  1.6057e-02]],\n              \n                       [[ 2.5034e-02, -2.9420e-02, -1.7739e-02],\n                        [ 1.0064e-02, -2.8722e-02, -1.6836e-02],\n                        [ 1.7448e-02,  2.8111e-02,  1.4150e-03]]],\n              \n              \n                      [[[-1.5742e-02, -1.3421e-02,  2.7663e-02],\n                        [-1.5744e-02,  2.0141e-03,  1.1419e-03],\n                        [ 2.5981e-02,  1.0222e-02, -1.5587e-02]],\n              \n                       [[ 1.3669e-02,  5.2103e-03, -7.6013e-03],\n                        [-1.6173e-02,  5.6269e-04,  2.4350e-03],\n                        [ 2.4261e-03,  2.5788e-02, -2.8097e-02]],\n              \n                       [[-1.4888e-02, -1.7731e-02, -6.4337e-03],\n                        [ 2.2471e-02,  2.3679e-04, -1.1437e-02],\n                        [-5.8912e-03,  1.0241e-02,  1.8909e-02]],\n              \n                       ...,\n              \n                       [[-1.4776e-02,  2.1398e-02,  8.8336e-04],\n                        [-3.3876e-03,  9.3768e-03, -5.3336e-03],\n                        [-4.4843e-03, -5.7139e-03, -6.8183e-03]],\n              \n                       [[-2.0888e-02, -2.4299e-02, -1.6261e-02],\n                        [-2.0847e-02,  1.3012e-02,  2.1894e-02],\n                        [-4.3075e-03,  2.1090e-02,  2.2750e-02]],\n              \n                       [[-1.7861e-02, -2.5487e-02, -9.7013e-03],\n                        [-2.8849e-03, -2.6374e-02, -2.2423e-02],\n                        [ 3.2294e-03,  1.0469e-02, -2.7943e-02]]],\n              \n              \n                      [[[ 4.1885e-03, -2.7628e-02, -2.5770e-02],\n                        [ 1.4383e-02, -3.2527e-03, -2.1710e-02],\n                        [-1.4146e-02,  7.5708e-03, -1.2968e-02]],\n              \n                       [[ 6.4110e-03,  1.5356e-02, -1.1846e-02],\n                        [ 2.1303e-02,  6.4434e-03, -2.6370e-02],\n                        [ 1.7484e-02,  1.9423e-02,  2.9357e-02]],\n              \n                       [[ 3.5598e-03,  2.6142e-02, -2.6987e-02],\n                        [ 9.4496e-03,  1.8193e-02,  1.0256e-02],\n                        [ 3.0655e-03,  2.6695e-03, -9.7217e-04]],\n              \n                       ...,\n              \n                       [[ 1.2180e-02,  2.1096e-02, -2.4789e-02],\n                        [ 6.3251e-03,  3.0475e-03, -6.8353e-03],\n                        [ 1.8787e-02, -9.2431e-03,  1.7185e-02]],\n              \n                       [[-1.1940e-02,  1.8412e-02,  1.7622e-02],\n                        [ 2.1504e-02,  2.3440e-02,  1.1492e-02],\n                        [-1.6089e-02, -1.5441e-02,  2.1249e-02]],\n              \n                       [[-2.3543e-02, -2.0001e-02, -2.0346e-02],\n                        [ 2.0520e-02,  2.9473e-03, -1.2873e-02],\n                        [ 1.3080e-02, -1.3335e-02,  2.4488e-02]]]])),\n             ('down2.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down2.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-0.0199, -0.0207, -0.0025],\n                        [-0.0202,  0.0202, -0.0180],\n                        [-0.0126,  0.0164, -0.0123]],\n              \n                       [[ 0.0062, -0.0141,  0.0168],\n                        [ 0.0078,  0.0006, -0.0096],\n                        [ 0.0036, -0.0188,  0.0195]],\n              \n                       [[-0.0073, -0.0065, -0.0040],\n                        [ 0.0086,  0.0105,  0.0089],\n                        [-0.0055,  0.0144, -0.0161]],\n              \n                       ...,\n              \n                       [[ 0.0131, -0.0028, -0.0143],\n                        [-0.0057, -0.0096, -0.0171],\n                        [-0.0130, -0.0047, -0.0005]],\n              \n                       [[-0.0046, -0.0177,  0.0125],\n                        [-0.0102,  0.0154,  0.0072],\n                        [ 0.0206,  0.0169, -0.0156]],\n              \n                       [[ 0.0036,  0.0074,  0.0056],\n                        [ 0.0112, -0.0127, -0.0147],\n                        [ 0.0001,  0.0135,  0.0017]]],\n              \n              \n                      [[[-0.0075, -0.0151,  0.0206],\n                        [ 0.0001, -0.0105, -0.0072],\n                        [ 0.0066,  0.0189,  0.0178]],\n              \n                       [[ 0.0086, -0.0003,  0.0005],\n                        [ 0.0185, -0.0089, -0.0045],\n                        [ 0.0166, -0.0010,  0.0182]],\n              \n                       [[-0.0107, -0.0202,  0.0050],\n                        [-0.0029, -0.0139,  0.0134],\n                        [ 0.0037,  0.0136, -0.0140]],\n              \n                       ...,\n              \n                       [[ 0.0171,  0.0028,  0.0002],\n                        [ 0.0165,  0.0112,  0.0014],\n                        [-0.0089, -0.0016,  0.0104]],\n              \n                       [[-0.0161, -0.0097, -0.0042],\n                        [ 0.0174,  0.0107,  0.0100],\n                        [-0.0053, -0.0070,  0.0113]],\n              \n                       [[-0.0016, -0.0070,  0.0061],\n                        [ 0.0017,  0.0160,  0.0013],\n                        [ 0.0057,  0.0200, -0.0160]]],\n              \n              \n                      [[[-0.0060, -0.0105, -0.0198],\n                        [-0.0150, -0.0083,  0.0156],\n                        [-0.0090,  0.0120, -0.0199]],\n              \n                       [[ 0.0127,  0.0145, -0.0122],\n                        [ 0.0110, -0.0001, -0.0018],\n                        [ 0.0039,  0.0206, -0.0076]],\n              \n                       [[ 0.0101,  0.0061, -0.0136],\n                        [ 0.0194, -0.0136,  0.0016],\n                        [-0.0007,  0.0173,  0.0011]],\n              \n                       ...,\n              \n                       [[-0.0134, -0.0127, -0.0165],\n                        [ 0.0041, -0.0118,  0.0110],\n                        [ 0.0044,  0.0060,  0.0036]],\n              \n                       [[ 0.0056, -0.0185,  0.0055],\n                        [ 0.0114, -0.0050, -0.0185],\n                        [ 0.0116, -0.0140, -0.0148]],\n              \n                       [[ 0.0145,  0.0188, -0.0130],\n                        [ 0.0065, -0.0171,  0.0036],\n                        [-0.0037, -0.0078,  0.0077]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0090,  0.0069, -0.0124],\n                        [-0.0150, -0.0065,  0.0094],\n                        [-0.0195, -0.0163, -0.0144]],\n              \n                       [[-0.0142,  0.0055, -0.0013],\n                        [-0.0149, -0.0092,  0.0063],\n                        [ 0.0007,  0.0089,  0.0060]],\n              \n                       [[-0.0055, -0.0047, -0.0065],\n                        [-0.0140,  0.0113, -0.0194],\n                        [-0.0049,  0.0079,  0.0079]],\n              \n                       ...,\n              \n                       [[-0.0111, -0.0127,  0.0139],\n                        [ 0.0075, -0.0173, -0.0109],\n                        [ 0.0204, -0.0063, -0.0174]],\n              \n                       [[ 0.0198,  0.0142,  0.0200],\n                        [ 0.0188,  0.0201, -0.0102],\n                        [ 0.0027, -0.0103, -0.0160]],\n              \n                       [[ 0.0090,  0.0116,  0.0114],\n                        [-0.0037, -0.0078,  0.0121],\n                        [-0.0192, -0.0149, -0.0202]]],\n              \n              \n                      [[[ 0.0045, -0.0102,  0.0195],\n                        [-0.0163, -0.0012,  0.0005],\n                        [ 0.0079, -0.0045,  0.0198]],\n              \n                       [[ 0.0181,  0.0146, -0.0039],\n                        [ 0.0095,  0.0106, -0.0055],\n                        [ 0.0028,  0.0103,  0.0006]],\n              \n                       [[ 0.0039, -0.0051, -0.0071],\n                        [-0.0123, -0.0141,  0.0050],\n                        [-0.0146,  0.0068,  0.0163]],\n              \n                       ...,\n              \n                       [[-0.0144,  0.0072, -0.0097],\n                        [-0.0070,  0.0141,  0.0089],\n                        [-0.0034,  0.0030,  0.0124]],\n              \n                       [[ 0.0143, -0.0146, -0.0182],\n                        [-0.0080,  0.0061, -0.0181],\n                        [ 0.0166,  0.0175, -0.0116]],\n              \n                       [[-0.0095, -0.0014, -0.0191],\n                        [ 0.0184, -0.0074, -0.0144],\n                        [ 0.0201, -0.0136, -0.0001]]],\n              \n              \n                      [[[-0.0022, -0.0024,  0.0035],\n                        [-0.0075, -0.0206,  0.0173],\n                        [-0.0160,  0.0207,  0.0060]],\n              \n                       [[-0.0073,  0.0075, -0.0149],\n                        [-0.0112,  0.0081, -0.0034],\n                        [-0.0176, -0.0169,  0.0041]],\n              \n                       [[-0.0040,  0.0199, -0.0174],\n                        [ 0.0103,  0.0153, -0.0109],\n                        [-0.0044, -0.0160, -0.0072]],\n              \n                       ...,\n              \n                       [[ 0.0142, -0.0045,  0.0044],\n                        [-0.0134, -0.0153, -0.0110],\n                        [-0.0178,  0.0051, -0.0051]],\n              \n                       [[ 0.0090,  0.0175,  0.0111],\n                        [ 0.0201, -0.0061,  0.0081],\n                        [-0.0037,  0.0166,  0.0074]],\n              \n                       [[-0.0069,  0.0019, -0.0200],\n                        [-0.0047, -0.0145,  0.0192],\n                        [-0.0100,  0.0121, -0.0193]]]])),\n             ('down2.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down2.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('down2.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-4.6348e-03,  9.8509e-03,  1.6142e-02],\n                        [ 2.6825e-05, -8.4992e-03,  3.6535e-04],\n                        [-2.0749e-02, -2.7181e-03,  1.4475e-02]],\n              \n                       [[ 1.0194e-02,  6.9748e-03,  1.3849e-02],\n                        [ 1.4200e-03,  2.5024e-03,  1.5259e-02],\n                        [ 1.1671e-02,  4.0497e-03,  8.7697e-03]],\n              \n                       [[-4.4309e-03, -1.1845e-02, -1.6037e-02],\n                        [-7.8910e-03, -9.7038e-03,  5.6008e-03],\n                        [-1.6987e-02,  7.1697e-03,  1.7236e-02]],\n              \n                       ...,\n              \n                       [[-1.1635e-02,  1.8610e-02,  1.4086e-02],\n                        [-1.1576e-02, -1.9610e-03, -1.8455e-02],\n                        [-8.6874e-03, -1.1485e-02, -5.8817e-03]],\n              \n                       [[-1.3743e-02,  1.2879e-02,  2.2404e-03],\n                        [-6.8730e-03,  1.0492e-02,  8.4602e-03],\n                        [ 1.9366e-03, -1.0892e-02,  9.0133e-03]],\n              \n                       [[-6.9619e-03, -1.7941e-02, -1.1306e-02],\n                        [-6.8960e-03, -6.8894e-03, -6.9923e-04],\n                        [ 1.0807e-02,  1.8476e-02,  1.9441e-02]]],\n              \n              \n                      [[[ 6.4426e-03,  7.5100e-03,  6.7503e-03],\n                        [-1.8439e-02,  1.4277e-02, -1.0381e-02],\n                        [-1.7296e-02, -1.2204e-02,  5.2923e-03]],\n              \n                       [[-6.8046e-03,  6.3742e-03, -1.1632e-02],\n                        [ 4.2213e-03,  2.0774e-02, -3.7589e-03],\n                        [ 1.6312e-02,  7.4283e-04,  1.2614e-02]],\n              \n                       [[-6.7564e-03, -1.0808e-02, -1.6746e-02],\n                        [-6.2140e-03,  9.3120e-03, -9.2284e-03],\n                        [ 2.8789e-03,  1.2397e-03,  1.5193e-02]],\n              \n                       ...,\n              \n                       [[-1.4065e-02, -4.0645e-03, -1.4819e-02],\n                        [ 7.9262e-03, -1.4440e-02, -1.3676e-02],\n                        [ 8.2918e-04,  1.0951e-02,  6.6675e-03]],\n              \n                       [[ 1.8929e-02, -1.6932e-02,  7.8811e-03],\n                        [ 1.6661e-02, -1.4852e-02, -6.1440e-03],\n                        [-4.3739e-03,  1.0890e-02,  1.2552e-03]],\n              \n                       [[ 1.6674e-02,  8.4053e-03, -5.2151e-03],\n                        [-1.8711e-02, -6.0464e-04,  4.8782e-03],\n                        [-1.0599e-02, -8.5500e-03, -4.4493e-04]]],\n              \n              \n                      [[[ 7.4150e-03, -1.7817e-02, -9.8810e-03],\n                        [ 1.5139e-02, -5.4702e-03,  3.1069e-03],\n                        [ 1.6121e-02, -2.4298e-03, -3.4243e-03]],\n              \n                       [[ 5.2642e-03, -1.7880e-02, -1.8678e-02],\n                        [ 2.9048e-03,  1.0568e-02, -2.8701e-04],\n                        [-4.0345e-05, -2.8312e-03,  6.9242e-03]],\n              \n                       [[ 1.2557e-02,  1.3475e-02, -1.1946e-02],\n                        [ 1.0504e-02, -1.1848e-02,  1.4417e-02],\n                        [-1.8312e-02,  1.1722e-02, -6.9120e-03]],\n              \n                       ...,\n              \n                       [[ 1.9895e-02,  1.5509e-02,  1.9991e-02],\n                        [-1.5190e-02, -1.9972e-02, -1.3091e-02],\n                        [-1.1537e-02, -6.8988e-03,  1.1122e-02]],\n              \n                       [[ 1.0277e-02, -9.5677e-03,  1.4165e-02],\n                        [ 5.0890e-03,  1.1992e-02,  2.0542e-02],\n                        [-9.9942e-04,  1.1082e-02, -5.1328e-03]],\n              \n                       [[ 1.0213e-02, -4.6551e-03, -5.2989e-03],\n                        [ 1.5165e-02, -1.7655e-02,  5.5892e-03],\n                        [ 1.1311e-02, -1.2807e-02, -1.2253e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4459e-02,  4.5380e-04, -2.9677e-03],\n                        [ 1.8889e-02, -1.6052e-02, -1.5562e-02],\n                        [ 1.3935e-03, -1.6170e-02,  2.0204e-02]],\n              \n                       [[ 1.0080e-02, -3.7539e-03, -1.5059e-02],\n                        [ 6.8971e-03, -8.5807e-03,  1.5525e-02],\n                        [ 1.4992e-03, -7.8594e-03,  7.5005e-03]],\n              \n                       [[ 3.7703e-03,  9.6159e-03,  1.6808e-02],\n                        [-1.1511e-02, -1.9614e-02, -1.7621e-02],\n                        [ 6.5007e-03, -1.5883e-02, -1.3063e-02]],\n              \n                       ...,\n              \n                       [[ 1.1717e-02,  1.3965e-03, -5.3536e-03],\n                        [ 1.4582e-02, -1.8533e-03, -1.5276e-02],\n                        [-2.0322e-02, -1.0361e-02, -6.1722e-03]],\n              \n                       [[ 5.0393e-04,  3.0661e-03, -9.3391e-03],\n                        [-5.0653e-03,  1.3716e-02,  9.7900e-03],\n                        [-2.0547e-02,  1.3067e-02,  1.6991e-03]],\n              \n                       [[-8.7317e-03,  1.5140e-02, -9.8445e-03],\n                        [-2.9895e-03,  1.0854e-02, -7.8243e-03],\n                        [ 1.5019e-03,  1.9270e-02,  9.2994e-03]]],\n              \n              \n                      [[[-3.2868e-03, -1.6655e-03,  1.3082e-02],\n                        [ 7.1859e-03, -1.9157e-03, -3.5394e-03],\n                        [-1.9397e-02,  5.5216e-03, -1.8486e-02]],\n              \n                       [[ 9.8068e-03,  2.6197e-03,  4.8447e-04],\n                        [ 1.5565e-02,  1.1252e-02,  1.8660e-02],\n                        [ 3.1310e-03,  6.5078e-03, -1.4506e-02]],\n              \n                       [[-1.5900e-02, -3.8698e-03,  4.6403e-03],\n                        [ 1.0163e-02,  1.0891e-02,  1.9025e-02],\n                        [-7.0364e-03,  1.0454e-02,  7.3635e-03]],\n              \n                       ...,\n              \n                       [[ 1.5563e-02, -1.9394e-02,  1.5875e-03],\n                        [-4.1397e-03, -7.3719e-04, -8.6707e-03],\n                        [-1.5182e-02,  1.4803e-02, -1.7555e-02]],\n              \n                       [[-7.9233e-04,  1.1101e-03,  1.7634e-03],\n                        [ 1.5103e-02, -1.4403e-02,  1.4855e-02],\n                        [-7.4607e-03,  7.4488e-03, -1.7282e-02]],\n              \n                       [[ 1.4080e-02,  1.6888e-02,  1.6374e-02],\n                        [ 7.7976e-03, -6.2802e-03, -3.1626e-03],\n                        [ 2.0682e-02, -1.9079e-02,  1.3276e-02]]],\n              \n              \n                      [[[ 1.8058e-02, -9.1462e-03, -7.2015e-03],\n                        [-6.4691e-03, -2.9027e-03,  9.6589e-03],\n                        [-1.3747e-02,  1.9787e-02,  1.9956e-02]],\n              \n                       [[-1.1408e-02, -2.4681e-05,  7.7289e-03],\n                        [ 1.9633e-02, -8.2515e-03,  1.3016e-02],\n                        [-1.8417e-02,  1.8677e-02, -1.1818e-02]],\n              \n                       [[ 1.9430e-02,  1.0222e-02, -5.9156e-03],\n                        [ 1.5036e-02,  9.4860e-03,  2.0289e-03],\n                        [-6.1385e-03, -6.8786e-03, -1.0498e-02]],\n              \n                       ...,\n              \n                       [[ 1.8626e-02, -4.7810e-03,  1.8702e-02],\n                        [-7.9554e-03, -1.7242e-02, -1.2626e-03],\n                        [ 1.9328e-02, -5.6285e-03, -1.1736e-02]],\n              \n                       [[-4.1653e-04, -1.8020e-02, -1.2647e-02],\n                        [-4.7124e-03,  3.7225e-03,  3.3474e-03],\n                        [-2.6790e-03,  6.2666e-03,  3.8707e-03]],\n              \n                       [[ 1.9958e-03, -6.2181e-03, -1.5993e-02],\n                        [ 4.3567e-03,  2.8269e-03,  2.0313e-02],\n                        [-1.6953e-02, -1.2477e-02, -6.3685e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down3.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.3495e-02,  1.1336e-02,  3.2999e-03],\n                        [ 1.0248e-02,  4.9058e-03,  1.6721e-03],\n                        [ 1.4577e-02,  1.2254e-02, -1.0996e-02]],\n              \n                       [[ 2.8387e-03, -1.2857e-02, -6.3248e-04],\n                        [ 1.0179e-02, -7.9369e-03,  9.4359e-03],\n                        [ 2.8751e-03, -1.1316e-02, -2.7018e-03]],\n              \n                       [[ 1.3239e-02,  1.3039e-03, -1.3213e-02],\n                        [-8.4236e-03,  2.3438e-03, -1.4353e-02],\n                        [ 9.7540e-03,  7.3673e-03,  9.9629e-04]],\n              \n                       ...,\n              \n                       [[-1.2715e-02, -5.7416e-03,  8.1590e-04],\n                        [ 1.2467e-02,  5.0082e-03, -9.3793e-03],\n                        [-1.0866e-02,  6.1197e-03,  2.4678e-03]],\n              \n                       [[-1.3211e-02, -6.7648e-03,  1.4521e-02],\n                        [-5.5102e-03, -5.2198e-03,  1.0626e-02],\n                        [-1.1742e-02, -6.2968e-03, -3.1413e-03]],\n              \n                       [[ 5.9503e-04, -9.2838e-03,  2.2524e-03],\n                        [ 4.4587e-03, -6.3728e-04, -1.4285e-02],\n                        [-5.1423e-03, -5.7166e-03,  1.2934e-02]]],\n              \n              \n                      [[[ 1.8463e-03, -5.4794e-04, -1.8946e-03],\n                        [ 9.7586e-04,  3.5177e-03, -4.0504e-03],\n                        [-6.2299e-03,  5.2996e-03,  1.3720e-02]],\n              \n                       [[-5.9090e-03,  1.6445e-03,  2.7570e-03],\n                        [-9.9673e-04, -1.0245e-02,  5.6605e-03],\n                        [ 1.1391e-02, -1.1658e-02, -1.1734e-02]],\n              \n                       [[-1.1735e-02,  2.4595e-03,  5.7827e-03],\n                        [ 7.1670e-03, -1.6270e-03,  1.0687e-02],\n                        [ 6.0396e-03, -7.3033e-04, -8.5946e-03]],\n              \n                       ...,\n              \n                       [[ 1.1671e-02,  1.3118e-02, -1.3291e-02],\n                        [ 6.1538e-03, -6.0592e-04,  6.6185e-03],\n                        [ 1.2829e-03, -1.3731e-02,  1.4932e-03]],\n              \n                       [[-7.4605e-03,  6.8828e-04, -1.2302e-04],\n                        [-8.1735e-03,  1.2001e-02,  7.8193e-03],\n                        [ 2.0528e-03, -6.3210e-03,  1.3449e-02]],\n              \n                       [[ 2.9136e-03,  6.6908e-03, -3.7520e-03],\n                        [ 9.3340e-03, -4.1290e-03, -1.4161e-02],\n                        [-5.5939e-03,  5.1468e-03,  7.5768e-05]]],\n              \n              \n                      [[[ 7.9902e-03,  8.0955e-03,  1.0381e-02],\n                        [ 6.6680e-03,  2.9378e-03,  6.6944e-03],\n                        [-2.3877e-03, -4.8883e-03,  8.5533e-03]],\n              \n                       [[-1.2371e-02, -1.2348e-02,  4.0223e-03],\n                        [-6.9362e-03, -1.0553e-02,  5.3495e-03],\n                        [ 4.4429e-04,  5.7790e-03, -2.5581e-03]],\n              \n                       [[ 2.1132e-03, -1.0715e-02,  3.1263e-03],\n                        [ 1.4578e-02, -4.7421e-03, -4.1220e-03],\n                        [ 7.7216e-03, -7.0857e-03, -4.0999e-03]],\n              \n                       ...,\n              \n                       [[-1.2722e-02,  4.8952e-03,  3.1216e-03],\n                        [-3.6589e-03,  3.9157e-03,  7.6172e-05],\n                        [ 6.6556e-03,  1.3619e-02, -1.0715e-02]],\n              \n                       [[-8.3624e-03,  2.8966e-03,  7.7819e-03],\n                        [ 9.6693e-03, -1.3035e-02, -1.2682e-02],\n                        [-1.2393e-02,  1.4095e-02, -9.9444e-03]],\n              \n                       [[-2.6372e-03, -9.4880e-03, -4.2093e-03],\n                        [ 2.4768e-03,  5.2376e-03, -1.6081e-03],\n                        [ 1.4001e-03,  8.7849e-03, -6.4915e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.1331e-03, -1.0245e-02,  5.5679e-03],\n                        [-1.3925e-02, -5.4960e-03, -6.4326e-03],\n                        [ 1.0665e-03,  9.3625e-03, -1.0900e-02]],\n              \n                       [[-1.2820e-02, -1.4185e-02,  7.6603e-03],\n                        [ 5.5901e-03, -7.7663e-03, -1.3632e-02],\n                        [-7.8664e-03,  3.8328e-03, -6.1660e-03]],\n              \n                       [[ 2.2009e-03,  1.2656e-02, -5.1460e-03],\n                        [-7.3644e-03, -1.2076e-03,  1.9836e-03],\n                        [-1.4580e-03, -8.4020e-04,  1.0106e-02]],\n              \n                       ...,\n              \n                       [[ 7.8239e-03,  8.2156e-03,  5.3135e-03],\n                        [ 7.6519e-03,  2.5644e-03,  9.5596e-03],\n                        [ 1.2521e-02,  7.5805e-03, -1.3987e-02]],\n              \n                       [[ 1.0951e-02,  7.9635e-04, -6.1090e-03],\n                        [ 7.5488e-03,  1.2158e-02, -1.4382e-02],\n                        [-3.4198e-03, -3.9887e-03, -3.8113e-03]],\n              \n                       [[-1.1689e-02,  9.5688e-03, -5.1517e-03],\n                        [-1.1460e-02, -4.0730e-03, -5.6413e-03],\n                        [ 7.0657e-03,  2.6805e-03, -5.1478e-03]]],\n              \n              \n                      [[[-9.6095e-03, -1.3585e-03, -7.0119e-03],\n                        [ 9.6654e-03,  1.0712e-02,  1.0401e-02],\n                        [-3.5123e-03,  1.3850e-02,  1.0464e-02]],\n              \n                       [[-1.1702e-02, -7.7455e-03, -5.3939e-03],\n                        [-1.2093e-02, -8.4871e-03, -3.2977e-03],\n                        [-1.0420e-02,  8.9802e-03, -4.9594e-03]],\n              \n                       [[-1.2320e-02,  2.4707e-03, -2.3200e-03],\n                        [-3.9590e-03,  1.1381e-02, -3.2109e-03],\n                        [-1.9178e-03, -1.3853e-02, -4.3691e-03]],\n              \n                       ...,\n              \n                       [[ 1.0142e-02,  1.3061e-02,  1.1623e-02],\n                        [-5.8694e-03, -6.4008e-04,  1.3774e-02],\n                        [ 6.2873e-03,  3.2907e-03, -8.4393e-03]],\n              \n                       [[ 3.5045e-03,  4.6928e-03,  1.1195e-02],\n                        [ 5.2034e-03, -9.1595e-03,  1.1639e-02],\n                        [-7.8218e-03,  7.5058e-03, -1.4309e-02]],\n              \n                       [[-2.4525e-03, -3.6981e-03,  1.1964e-02],\n                        [-1.2757e-02, -5.8314e-03, -1.1045e-02],\n                        [ 6.1323e-03,  1.4707e-02, -9.2333e-03]]],\n              \n              \n                      [[[ 5.0627e-03,  1.4049e-02,  7.1501e-03],\n                        [-1.3210e-02,  1.1269e-02,  2.2428e-03],\n                        [-9.7158e-03,  5.5631e-03, -1.2279e-02]],\n              \n                       [[-9.5874e-03, -5.4147e-04,  1.4689e-02],\n                        [ 4.4917e-03, -1.3910e-02, -3.7383e-04],\n                        [-7.5597e-03,  9.3203e-03, -7.5512e-03]],\n              \n                       [[-1.4322e-02, -1.1102e-02,  1.1979e-02],\n                        [ 6.4091e-03, -1.3175e-02,  2.6744e-04],\n                        [ 1.1095e-03,  6.2741e-03,  5.1492e-04]],\n              \n                       ...,\n              \n                       [[ 1.3908e-02,  9.8417e-03,  9.4988e-03],\n                        [ 1.1376e-02,  1.9947e-04, -8.0265e-03],\n                        [-1.1771e-02, -1.0298e-02, -2.5397e-03]],\n              \n                       [[-2.3932e-03,  1.3351e-02,  1.0970e-02],\n                        [ 1.2986e-02,  3.9482e-03, -8.2351e-03],\n                        [-1.0508e-02, -3.3115e-03, -8.0658e-03]],\n              \n                       [[-2.9153e-03,  1.4376e-02, -3.0430e-03],\n                        [ 1.3600e-02, -2.1507e-03, -4.3007e-03],\n                        [-3.6526e-03,  8.3328e-03,  8.7380e-03]]]])),\n             ('down3.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down3.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down3.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-1.3104e-02,  9.6535e-03,  7.0547e-03],\n                        [ 6.8489e-03,  5.6884e-03, -3.3797e-03],\n                        [-1.3077e-02,  1.1413e-02, -8.2186e-03]],\n              \n                       [[-6.4877e-03,  1.2398e-02,  1.4672e-02],\n                        [-2.8377e-03,  2.9911e-03,  8.6744e-03],\n                        [ 4.6708e-03, -1.9309e-03, -1.3963e-02]],\n              \n                       [[-8.8996e-04, -1.3098e-02, -1.2099e-02],\n                        [ 1.1789e-02, -6.3457e-03,  8.4533e-03],\n                        [ 6.9120e-04,  3.7103e-03, -3.9384e-03]],\n              \n                       ...,\n              \n                       [[-1.4631e-02,  7.6187e-03,  1.3055e-02],\n                        [ 8.7348e-03,  2.2455e-03,  1.4252e-02],\n                        [-7.8609e-03,  6.6497e-03,  1.2674e-02]],\n              \n                       [[ 1.0928e-02,  8.1940e-03,  1.4620e-03],\n                        [ 1.1112e-03, -7.0720e-03, -1.2397e-02],\n                        [ 1.3073e-02,  2.2528e-03,  6.1473e-03]],\n              \n                       [[-1.1589e-02, -9.5213e-03, -5.2496e-03],\n                        [-1.1412e-02, -1.3629e-02,  7.4268e-03],\n                        [-6.4922e-03,  1.1146e-02, -9.5554e-03]]],\n              \n              \n                      [[[ 2.3625e-05, -1.3995e-02, -7.6334e-03],\n                        [-9.4009e-03, -9.2042e-03,  5.7072e-03],\n                        [ 9.9287e-03, -5.7740e-03,  8.9586e-03]],\n              \n                       [[ 1.4008e-02, -1.0200e-02,  1.3237e-02],\n                        [ 1.4621e-02, -1.2051e-02,  6.9597e-03],\n                        [ 1.2422e-02, -8.4337e-03, -7.5494e-03]],\n              \n                       [[ 5.7422e-04, -8.9031e-03,  1.4246e-02],\n                        [-3.9909e-03, -1.2648e-05,  7.5228e-03],\n                        [ 4.5517e-03, -8.1091e-03, -2.5926e-03]],\n              \n                       ...,\n              \n                       [[ 1.7802e-03,  1.2118e-02, -8.6626e-04],\n                        [-6.0965e-04, -5.6477e-03, -4.7239e-03],\n                        [-1.4231e-03, -1.1298e-02,  4.0613e-03]],\n              \n                       [[ 2.4961e-05,  4.4265e-03,  1.4223e-02],\n                        [ 2.2458e-03,  1.3728e-02, -1.1796e-02],\n                        [-7.2479e-03,  1.2696e-02,  4.3921e-03]],\n              \n                       [[ 1.4457e-02, -1.0118e-02,  1.3083e-02],\n                        [-7.3051e-03,  1.3544e-02, -1.2357e-02],\n                        [ 3.5746e-03, -1.3268e-02, -9.3003e-03]]],\n              \n              \n                      [[[-3.1621e-03,  1.4471e-02,  1.0941e-02],\n                        [ 1.2192e-02,  5.9600e-03,  7.0732e-03],\n                        [ 1.6198e-03, -1.1914e-02, -1.1316e-02]],\n              \n                       [[-8.1733e-03, -4.6493e-03,  1.3078e-02],\n                        [-5.0052e-03, -1.0437e-02,  9.8975e-03],\n                        [-1.3412e-02, -8.9157e-03,  1.3293e-02]],\n              \n                       [[-5.0194e-03,  6.6695e-03,  3.4234e-04],\n                        [-1.3336e-02,  1.4430e-03,  7.5926e-03],\n                        [-1.0269e-03,  1.0630e-02, -8.4293e-03]],\n              \n                       ...,\n              \n                       [[ 1.0040e-02, -9.6519e-03,  1.1701e-02],\n                        [ 6.5308e-05,  3.5704e-03, -1.2048e-02],\n                        [-9.5033e-03, -1.2604e-02, -1.2307e-02]],\n              \n                       [[-6.6415e-03, -1.0024e-02,  1.3435e-02],\n                        [-6.3868e-03, -1.4265e-02, -2.8581e-03],\n                        [-1.3789e-02,  1.1855e-02,  7.1601e-03]],\n              \n                       [[-9.1238e-03,  4.7032e-05, -2.2387e-03],\n                        [ 4.9879e-04,  7.7738e-03,  5.1973e-03],\n                        [ 3.4793e-03,  9.1406e-03, -9.1121e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.2879e-03,  1.1191e-03, -6.0251e-03],\n                        [-3.2071e-03,  5.4502e-03,  1.2839e-04],\n                        [ 5.8309e-03, -1.3948e-02,  3.9841e-03]],\n              \n                       [[ 1.0795e-02,  5.7343e-03,  3.2873e-03],\n                        [ 5.4282e-03, -1.0134e-02,  3.3486e-03],\n                        [ 5.0658e-03, -1.4290e-02,  3.9768e-03]],\n              \n                       [[-1.4718e-02, -4.8749e-03,  8.8550e-03],\n                        [-1.2116e-02,  3.9706e-03, -1.5341e-04],\n                        [-5.6044e-03,  9.2914e-03,  2.6309e-03]],\n              \n                       ...,\n              \n                       [[ 1.1578e-02,  4.7662e-03,  1.0865e-02],\n                        [-9.9621e-03,  7.2204e-03,  6.7652e-03],\n                        [ 6.1930e-03,  5.5036e-03, -4.8385e-03]],\n              \n                       [[-1.1982e-02,  9.0713e-03, -6.7553e-03],\n                        [ 1.0392e-02, -6.3635e-03, -1.1598e-03],\n                        [ 1.0464e-02,  4.0243e-03,  1.4345e-03]],\n              \n                       [[ 3.2504e-03,  1.4237e-02, -7.7320e-03],\n                        [-1.0245e-02, -8.5657e-03, -1.2735e-02],\n                        [-3.5816e-03,  1.3560e-02, -1.2678e-02]]],\n              \n              \n                      [[[-1.4336e-02, -4.6926e-03,  1.3425e-02],\n                        [ 1.3409e-02, -6.8928e-03, -9.7946e-03],\n                        [-1.4182e-02, -8.6928e-03, -1.4202e-02]],\n              \n                       [[-5.0576e-03, -9.8077e-03,  5.6572e-03],\n                        [-1.4611e-02,  4.4676e-03, -1.3235e-02],\n                        [ 3.6478e-03,  4.1773e-04,  1.4504e-02]],\n              \n                       [[-8.5665e-03, -6.6888e-03, -5.9852e-03],\n                        [ 1.8548e-03,  1.2795e-02, -6.3900e-03],\n                        [-1.3038e-02,  7.2169e-03,  9.2560e-03]],\n              \n                       ...,\n              \n                       [[-5.8375e-03,  8.9250e-03,  1.2109e-02],\n                        [-1.3653e-02,  1.3453e-02, -6.7649e-03],\n                        [-1.2166e-02, -1.3578e-02, -1.2037e-03]],\n              \n                       [[-5.5372e-03, -3.9234e-03, -2.1640e-03],\n                        [-8.1456e-03, -8.1486e-03,  4.8608e-05],\n                        [-7.9746e-03,  3.5861e-03, -5.4110e-03]],\n              \n                       [[ 9.0684e-03, -4.6523e-03,  8.6029e-03],\n                        [-3.5470e-03, -2.6329e-03,  4.1187e-03],\n                        [-1.7698e-03,  3.1339e-03, -1.3087e-02]]],\n              \n              \n                      [[[ 1.3993e-02,  1.0210e-02, -9.8379e-03],\n                        [-3.6017e-03,  1.5505e-03, -7.5702e-03],\n                        [-1.3827e-03, -1.4429e-02, -1.3696e-02]],\n              \n                       [[ 1.2335e-02,  8.3124e-03, -4.6792e-03],\n                        [ 4.8468e-03,  1.3626e-04,  9.8758e-03],\n                        [-2.6817e-03,  3.2997e-03, -9.7415e-04]],\n              \n                       [[ 3.1673e-03, -7.1938e-03, -1.4500e-03],\n                        [-9.1013e-03,  8.4705e-03, -9.5864e-03],\n                        [ 1.6714e-03, -1.4101e-02,  1.1644e-02]],\n              \n                       ...,\n              \n                       [[ 1.4320e-02,  4.4366e-03, -5.8747e-03],\n                        [-8.1688e-03, -6.9629e-03,  3.0317e-04],\n                        [-1.2110e-02, -1.3646e-02, -6.0113e-03]],\n              \n                       [[-3.7647e-04,  7.6979e-03,  3.3129e-03],\n                        [ 7.6917e-03, -1.9005e-03,  6.3914e-03],\n                        [-2.9271e-03,  1.0327e-02, -9.8557e-03]],\n              \n                       [[ 1.1749e-02,  3.9048e-03, -7.2822e-03],\n                        [ 1.4049e-02,  1.3569e-02,  2.5594e-03],\n                        [ 1.2890e-02,  5.6545e-03,  6.2168e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0)),\n             ('down4.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-1.0162e-02, -7.9513e-03, -1.4126e-02],\n                        [-6.2557e-03, -9.7779e-03,  1.0858e-02],\n                        [ 9.1498e-03,  3.0958e-04,  9.0409e-03]],\n              \n                       [[-7.6646e-03, -9.0559e-03, -8.4516e-04],\n                        [-1.2277e-02,  2.7770e-03,  2.4928e-03],\n                        [ 2.1196e-03, -2.7451e-03, -1.3663e-02]],\n              \n                       [[-8.4018e-03,  3.2803e-03, -6.1505e-03],\n                        [ 1.3116e-02,  8.8065e-03,  4.6064e-03],\n                        [ 9.4382e-03, -7.7282e-03,  1.0306e-02]],\n              \n                       ...,\n              \n                       [[ 6.6357e-03, -2.2279e-03, -8.7835e-03],\n                        [-5.1093e-03,  3.9618e-03,  8.8206e-03],\n                        [ 1.4141e-02,  1.3784e-02,  1.1771e-02]],\n              \n                       [[-5.9949e-03, -1.3745e-04,  7.4454e-03],\n                        [-9.2404e-03,  1.3126e-02,  9.9188e-03],\n                        [-6.8859e-03, -1.4138e-02, -9.2198e-03]],\n              \n                       [[-1.4438e-02,  1.1573e-02,  1.1146e-02],\n                        [-8.7031e-03, -4.6383e-03,  7.3338e-03],\n                        [ 1.1381e-02, -9.0583e-03, -2.5293e-03]]],\n              \n              \n                      [[[-1.3852e-02, -6.8651e-03,  2.3293e-03],\n                        [ 1.2269e-02,  6.5710e-03,  3.9793e-03],\n                        [-7.3067e-03, -5.9318e-03, -6.7658e-03]],\n              \n                       [[ 9.5927e-03, -7.6682e-03, -1.3819e-02],\n                        [-9.0626e-03,  3.5546e-03, -8.5062e-03],\n                        [ 1.7261e-03, -2.6030e-03, -1.4632e-02]],\n              \n                       [[ 1.0916e-02,  1.0892e-02,  1.4228e-02],\n                        [ 1.1874e-02, -6.4073e-03, -5.1940e-03],\n                        [-7.4828e-03, -7.4947e-03,  2.5183e-03]],\n              \n                       ...,\n              \n                       [[ 9.7132e-03,  2.0456e-03, -4.0253e-03],\n                        [ 1.9973e-03,  1.2258e-02, -1.3174e-03],\n                        [-9.0220e-03, -8.2095e-03,  1.4117e-02]],\n              \n                       [[-1.0827e-02,  1.4226e-02, -6.4879e-03],\n                        [ 1.2198e-02, -1.2647e-02,  8.6206e-03],\n                        [-2.7980e-03, -2.0266e-03,  5.7236e-03]],\n              \n                       [[-1.2030e-02,  1.2822e-02, -8.4252e-03],\n                        [ 1.1277e-02, -7.0514e-03, -7.5673e-03],\n                        [ 8.1968e-03, -1.2170e-02, -7.3895e-03]]],\n              \n              \n                      [[[ 8.0684e-03,  1.3598e-02, -7.9777e-03],\n                        [-1.4268e-02,  4.8484e-03, -1.1704e-02],\n                        [ 4.8766e-03,  2.9658e-03,  2.0288e-03]],\n              \n                       [[-1.1000e-03, -2.6417e-03,  3.1051e-03],\n                        [ 1.2253e-02, -7.2229e-03, -1.1037e-03],\n                        [ 1.0293e-02,  3.9444e-03, -8.0077e-03]],\n              \n                       [[ 3.6599e-03,  1.3138e-02, -1.0403e-03],\n                        [-1.0804e-02, -2.9224e-03, -7.3381e-04],\n                        [-8.4483e-03, -3.5656e-03,  1.0923e-02]],\n              \n                       ...,\n              \n                       [[ 1.0183e-02, -1.0656e-02,  2.5374e-03],\n                        [-2.4001e-03,  9.3434e-03,  8.0887e-03],\n                        [-3.1470e-03, -3.6860e-03,  6.9349e-03]],\n              \n                       [[-1.4212e-02,  4.7419e-03,  2.2588e-03],\n                        [ 1.2572e-02,  2.5563e-03, -8.1275e-03],\n                        [-3.7703e-03,  2.5945e-03,  5.5602e-03]],\n              \n                       [[-1.2830e-02, -1.0370e-02,  9.9764e-03],\n                        [-1.0848e-02, -9.6209e-03,  8.2907e-03],\n                        [ 4.6423e-03, -4.9777e-03, -8.6183e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.9552e-03,  1.0103e-02, -4.7408e-03],\n                        [-1.3407e-02,  6.5927e-03, -7.2890e-03],\n                        [ 1.2902e-02, -7.3139e-03,  4.8173e-03]],\n              \n                       [[-8.6896e-03, -1.9172e-03,  5.9656e-03],\n                        [-7.3172e-05,  2.9933e-03, -1.1204e-02],\n                        [ 2.1456e-03,  2.6252e-03, -1.3978e-02]],\n              \n                       [[-8.2944e-03, -6.1581e-03,  1.3276e-02],\n                        [ 2.0285e-04, -6.9051e-03,  1.3585e-02],\n                        [-7.9958e-03,  5.1597e-03, -1.1482e-02]],\n              \n                       ...,\n              \n                       [[ 2.9236e-03,  8.6567e-03, -5.6918e-03],\n                        [ 1.2319e-02, -1.2173e-02, -1.1142e-02],\n                        [ 2.1955e-03,  2.1893e-03,  1.0226e-02]],\n              \n                       [[-1.3731e-02,  2.4001e-04,  1.0280e-02],\n                        [ 6.2036e-04,  9.4891e-03, -9.4363e-03],\n                        [ 7.7716e-03, -5.3223e-03, -1.1793e-02]],\n              \n                       [[ 9.0567e-03, -9.4963e-03,  1.2966e-02],\n                        [-3.5606e-03,  6.7127e-03,  9.2346e-03],\n                        [ 1.6610e-04,  9.7832e-04, -3.7458e-03]]],\n              \n              \n                      [[[ 1.8821e-03,  7.0609e-03, -9.9641e-03],\n                        [ 2.8442e-03, -3.4813e-04,  2.8147e-03],\n                        [-7.6718e-03,  1.4098e-03,  3.6991e-03]],\n              \n                       [[-7.4600e-03,  6.1319e-03, -6.6834e-03],\n                        [ 4.6137e-03, -9.7316e-03, -2.1926e-03],\n                        [-5.1150e-03,  8.5056e-03,  1.4168e-02]],\n              \n                       [[ 1.2746e-02,  8.4634e-03,  1.2394e-02],\n                        [ 6.5522e-03, -1.0927e-02, -1.4621e-02],\n                        [ 9.5033e-03,  3.9224e-03,  9.9719e-03]],\n              \n                       ...,\n              \n                       [[-4.0116e-03, -1.4190e-02, -2.6838e-03],\n                        [-1.9716e-04, -1.6087e-03, -2.2089e-03],\n                        [ 1.1347e-02,  5.0595e-04, -2.1228e-03]],\n              \n                       [[ 1.1465e-03,  6.0314e-03, -7.8767e-03],\n                        [-6.6732e-03, -5.0615e-03, -7.0481e-03],\n                        [-3.5145e-03, -1.4674e-02,  9.3690e-03]],\n              \n                       [[-2.1949e-03,  1.8604e-04, -3.8469e-04],\n                        [-6.0911e-03,  4.8625e-03,  9.1291e-04],\n                        [-4.2253e-03, -9.7373e-03,  3.0233e-03]]],\n              \n              \n                      [[[ 1.3092e-02, -9.1652e-03, -1.4018e-02],\n                        [-7.5290e-03, -1.1704e-02,  1.1918e-02],\n                        [-3.6753e-03,  8.3012e-03, -7.8185e-03]],\n              \n                       [[ 1.3660e-02, -1.0051e-04, -4.8537e-03],\n                        [ 4.5250e-03,  1.1501e-02, -1.2260e-02],\n                        [-1.2088e-02, -1.1217e-02, -8.9023e-03]],\n              \n                       [[ 3.9087e-03, -1.1512e-03, -1.3955e-02],\n                        [-2.1982e-03,  1.0120e-02, -5.0558e-03],\n                        [-1.3255e-02,  2.8492e-03, -4.1524e-03]],\n              \n                       ...,\n              \n                       [[-1.2921e-02, -1.8075e-03,  3.1186e-03],\n                        [ 4.0110e-03,  5.9678e-03, -1.5871e-03],\n                        [ 4.0160e-03,  4.9175e-04,  2.2130e-03]],\n              \n                       [[-3.4039e-03, -1.2438e-02,  6.7231e-03],\n                        [ 1.2851e-02, -5.3675e-03,  1.6797e-03],\n                        [-1.3136e-02, -2.5658e-03, -5.8660e-03]],\n              \n                       [[-2.0538e-03,  7.5002e-04,  6.9986e-03],\n                        [ 1.3422e-02, -9.2835e-04,  4.6620e-03],\n                        [-1.3815e-02,  5.7040e-03, -6.6107e-03]]]])),\n             ('down4.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('down4.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('down4.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0)),\n             ('up1.conv.double_conv.0.weight',\n              tensor([[[[ 6.0052e-03, -6.1578e-03, -8.6970e-03],\n                        [ 1.6955e-03, -7.3866e-03,  5.3448e-03],\n                        [ 5.5082e-03,  9.1673e-03,  1.0191e-02]],\n              \n                       [[-3.7926e-03,  5.7925e-03,  1.0316e-02],\n                        [ 9.6915e-03,  8.8699e-03,  5.3047e-03],\n                        [ 5.0500e-03,  4.6066e-03,  1.0278e-02]],\n              \n                       [[-7.2442e-04, -7.9003e-03, -9.7175e-03],\n                        [ 4.6586e-04, -3.6655e-03, -9.5510e-03],\n                        [-9.1740e-03, -7.8502e-03, -5.3606e-03]],\n              \n                       ...,\n              \n                       [[ 2.1322e-03, -9.4887e-05, -4.9738e-03],\n                        [-6.1662e-03,  1.3903e-03, -7.2019e-03],\n                        [ 5.4206e-03,  8.7880e-03,  4.3695e-03]],\n              \n                       [[ 3.3114e-03, -4.8001e-03, -2.7326e-03],\n                        [-3.7524e-03,  7.7908e-03, -8.4219e-03],\n                        [ 2.0721e-03,  7.5771e-03,  6.9718e-03]],\n              \n                       [[-9.9150e-03, -2.1330e-03,  7.4038e-03],\n                        [-6.3372e-03, -8.1195e-03,  1.6034e-03],\n                        [ 5.8172e-03, -1.3327e-03, -7.0786e-03]]],\n              \n              \n                      [[[-4.7313e-03, -2.5325e-03, -6.1366e-03],\n                        [ 1.1530e-03, -5.3506e-03, -6.1344e-04],\n                        [ 2.7635e-03, -6.2766e-03,  4.6419e-03]],\n              \n                       [[ 4.3768e-03, -4.0070e-03,  8.7607e-03],\n                        [-8.9397e-03, -9.8516e-03, -2.8273e-03],\n                        [-3.7660e-03,  3.6542e-03,  1.0126e-02]],\n              \n                       [[-6.7512e-03,  6.0833e-03,  2.7166e-03],\n                        [ 9.3578e-04,  5.1147e-03,  6.3890e-03],\n                        [ 1.5687e-04,  7.4274e-03, -8.3365e-03]],\n              \n                       ...,\n              \n                       [[-4.8921e-03, -5.4093e-03,  5.6688e-03],\n                        [ 3.1983e-03,  3.9314e-03, -8.9410e-03],\n                        [ 6.5762e-03, -9.7403e-03, -4.1459e-03]],\n              \n                       [[ 8.1715e-03,  5.4453e-03, -7.9296e-03],\n                        [ 1.6348e-03, -1.7733e-04,  1.1809e-03],\n                        [-6.2941e-03,  6.1941e-03,  1.7227e-03]],\n              \n                       [[ 9.5111e-03, -8.0376e-03, -3.7345e-03],\n                        [ 5.4716e-03, -3.7542e-03,  2.9980e-03],\n                        [-7.5362e-03,  8.4094e-03,  8.9098e-03]]],\n              \n              \n                      [[[-9.6740e-03, -8.1277e-03,  3.9857e-03],\n                        [-3.5163e-03,  8.6464e-03,  4.2643e-03],\n                        [-5.0144e-03, -9.8802e-04,  4.8284e-04]],\n              \n                       [[-6.5739e-03,  9.1206e-03,  5.8876e-03],\n                        [-4.3970e-03,  3.9926e-04,  4.9571e-03],\n                        [-3.2965e-03,  4.1399e-04, -2.7867e-03]],\n              \n                       [[-4.9022e-03, -7.1855e-04,  5.2022e-04],\n                        [-3.8415e-03,  7.9072e-03,  1.0071e-02],\n                        [-6.5128e-03, -3.6828e-03, -8.3628e-03]],\n              \n                       ...,\n              \n                       [[ 8.5856e-03, -7.1988e-03,  9.1629e-03],\n                        [ 9.4906e-03, -6.0381e-03,  6.3775e-04],\n                        [ 3.2705e-03, -4.2573e-03,  7.2144e-03]],\n              \n                       [[-2.7434e-03, -5.6575e-03,  7.0926e-03],\n                        [ 6.5038e-03,  1.0222e-02,  7.6083e-03],\n                        [ 8.3256e-03,  7.9641e-03, -6.8926e-03]],\n              \n                       [[ 3.2581e-03, -3.4153e-03,  1.7781e-04],\n                        [-4.7329e-03, -2.7371e-03, -7.9243e-03],\n                        [-7.3951e-03, -3.6213e-03,  3.8721e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.3754e-03,  1.0256e-02, -9.6938e-03],\n                        [-5.2090e-03,  1.1899e-03,  6.6328e-03],\n                        [-6.4318e-03,  7.6097e-03,  3.2797e-03]],\n              \n                       [[-7.0052e-03,  4.5905e-03, -8.9286e-03],\n                        [-8.2543e-03, -5.1691e-03, -5.8590e-03],\n                        [ 8.7791e-03,  5.7680e-03, -8.9067e-03]],\n              \n                       [[-7.6416e-03, -9.3266e-03,  9.4770e-03],\n                        [ 1.4398e-03,  4.5831e-03, -3.4448e-03],\n                        [-4.5923e-03, -5.7610e-03, -4.3103e-03]],\n              \n                       ...,\n              \n                       [[-2.0614e-03, -8.5129e-03, -8.4951e-03],\n                        [ 2.6566e-03,  9.1776e-03,  2.6760e-03],\n                        [-1.7022e-04,  3.6392e-03,  5.0875e-03]],\n              \n                       [[-2.9073e-03, -7.8702e-03, -1.2811e-03],\n                        [-8.3429e-03, -8.4082e-03,  4.3443e-03],\n                        [-6.5337e-03,  3.0448e-03, -3.2978e-03]],\n              \n                       [[-6.3634e-03, -6.4584e-03, -9.4520e-03],\n                        [ 6.3613e-03,  1.3895e-03,  6.7184e-03],\n                        [ 1.9717e-04,  3.0919e-03, -9.3850e-03]]],\n              \n              \n                      [[[-7.3347e-03,  3.7111e-03, -1.4600e-03],\n                        [-8.9929e-03, -1.0001e-02, -9.7608e-03],\n                        [ 4.9672e-03, -5.1917e-03, -9.9102e-03]],\n              \n                       [[ 7.6933e-03, -4.9824e-03, -8.9469e-03],\n                        [ 4.8704e-03, -1.6437e-03,  8.8097e-03],\n                        [-3.0993e-03, -5.9778e-03, -3.1651e-03]],\n              \n                       [[ 8.6893e-03,  9.8990e-03,  7.1665e-03],\n                        [ 7.6924e-03, -1.0816e-03,  9.3137e-03],\n                        [-4.7224e-03, -3.9862e-03, -7.0841e-03]],\n              \n                       ...,\n              \n                       [[ 7.1673e-03,  5.2882e-03,  5.8690e-03],\n                        [ 4.2807e-04, -4.7009e-04,  9.8658e-03],\n                        [-3.6831e-03, -3.5520e-03,  4.0485e-03]],\n              \n                       [[-5.5522e-03,  9.4766e-03,  8.2692e-03],\n                        [-3.1187e-03, -8.5105e-03,  8.7861e-03],\n                        [-7.3462e-03,  5.8684e-03,  9.6273e-03]],\n              \n                       [[-3.7102e-03,  7.7810e-03, -1.4194e-03],\n                        [-4.0797e-03, -8.0059e-03,  8.5199e-03],\n                        [-9.1947e-03,  3.5915e-03, -4.6602e-03]]],\n              \n              \n                      [[[-1.3775e-03,  6.0666e-04, -6.9796e-04],\n                        [ 6.7400e-03,  6.6210e-03,  2.7429e-03],\n                        [-8.8243e-03, -9.8390e-03,  2.4116e-03]],\n              \n                       [[ 4.7119e-03,  3.2005e-03,  5.9726e-03],\n                        [ 9.5476e-03,  1.6969e-03,  9.7832e-03],\n                        [-2.6481e-03,  7.0522e-03, -7.9863e-03]],\n              \n                       [[ 4.9707e-03,  9.5256e-04, -1.3029e-03],\n                        [-6.9370e-03, -1.0068e-02,  1.0652e-03],\n                        [-2.0503e-03,  8.6360e-03, -1.5661e-03]],\n              \n                       ...,\n              \n                       [[-6.5328e-03, -9.1420e-04,  5.5855e-03],\n                        [ 8.4739e-03, -4.1916e-03,  1.0212e-02],\n                        [ 1.0342e-02, -8.0135e-03, -1.1019e-04]],\n              \n                       [[ 4.2931e-03,  4.7278e-03,  8.9549e-03],\n                        [ 7.2504e-03,  4.6937e-03, -6.7444e-03],\n                        [-1.0244e-02,  2.1343e-03, -3.2979e-03]],\n              \n                       [[ 9.3904e-03, -7.6412e-03,  2.0035e-03],\n                        [-6.8808e-03,  1.0404e-02,  9.5906e-03],\n                        [ 5.1486e-03,  1.8948e-03, -1.0138e-03]]]])),\n             ('up1.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up1.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up1.conv.double_conv.3.weight',\n              tensor([[[[ 4.6532e-03, -7.6019e-03, -2.2726e-03],\n                        [ 4.6818e-03,  1.2958e-02,  7.4474e-03],\n                        [ 1.0656e-02,  7.3169e-03,  1.4385e-02]],\n              \n                       [[-7.1003e-03,  5.6198e-03,  1.1528e-02],\n                        [ 1.2165e-02,  2.7467e-03,  1.2221e-02],\n                        [ 1.0123e-02, -7.3388e-04, -1.3558e-02]],\n              \n                       [[ 6.1051e-04, -1.0071e-02,  1.0367e-02],\n                        [ 5.4181e-03,  3.2388e-03,  8.1533e-04],\n                        [ 9.9759e-03, -8.9243e-03, -1.0614e-02]],\n              \n                       ...,\n              \n                       [[-1.1593e-02,  4.4562e-03, -1.2794e-02],\n                        [-2.0847e-03,  8.4393e-03, -3.0718e-03],\n                        [ 1.2095e-02,  9.6634e-03, -6.1204e-03]],\n              \n                       [[-8.5692e-03, -5.3203e-03, -6.0301e-03],\n                        [-1.3060e-02, -4.9878e-03,  1.3536e-02],\n                        [-3.0446e-03, -3.7271e-03,  1.8943e-03]],\n              \n                       [[ 9.1236e-03,  6.2085e-03, -5.2066e-03],\n                        [ 7.0768e-03,  5.8855e-03, -1.3525e-02],\n                        [ 1.2969e-02, -3.1656e-03, -9.7805e-03]]],\n              \n              \n                      [[[-1.3448e-02, -1.4380e-02,  3.3876e-03],\n                        [-6.9893e-03, -8.7593e-03,  3.4935e-03],\n                        [ 6.0252e-03,  6.2473e-03, -7.2960e-04]],\n              \n                       [[ 1.2521e-03, -1.2604e-02, -1.4122e-02],\n                        [-7.8812e-03,  1.2843e-03,  3.4510e-03],\n                        [-8.0826e-03, -6.0928e-03,  1.4071e-02]],\n              \n                       [[ 1.2236e-02, -2.2066e-03,  7.5802e-03],\n                        [-3.4579e-03, -8.4028e-03,  1.2992e-02],\n                        [ 1.5273e-03,  9.6915e-03, -2.7779e-03]],\n              \n                       ...,\n              \n                       [[-9.7299e-03,  7.2240e-03,  3.2073e-04],\n                        [ 5.1952e-03,  1.3993e-02,  5.8187e-03],\n                        [-3.9472e-03,  9.5075e-03,  9.9508e-03]],\n              \n                       [[ 3.8860e-03, -7.5956e-03, -6.7716e-03],\n                        [-6.3491e-03,  1.1731e-02, -4.6717e-03],\n                        [ 5.6204e-04, -4.5982e-03, -1.3072e-03]],\n              \n                       [[-9.9374e-03, -1.4691e-03,  9.6274e-03],\n                        [-3.4154e-03, -9.9765e-03,  4.7587e-03],\n                        [ 1.1309e-02,  1.2087e-03,  1.1953e-02]]],\n              \n              \n                      [[[ 1.2883e-02, -7.2949e-03, -4.8458e-03],\n                        [ 9.7466e-03,  1.1054e-02,  1.2237e-02],\n                        [ 9.9405e-03,  1.4726e-02,  2.0744e-03]],\n              \n                       [[ 1.0789e-02,  1.3618e-02,  1.4625e-02],\n                        [-1.9228e-03,  5.1298e-03,  5.3312e-04],\n                        [ 1.4351e-02,  8.0309e-03, -1.3372e-02]],\n              \n                       [[-3.1131e-03, -6.5674e-04, -1.0796e-02],\n                        [-9.3562e-03,  6.5610e-03, -1.3210e-02],\n                        [ 7.9644e-03,  1.0064e-03,  6.2818e-04]],\n              \n                       ...,\n              \n                       [[-2.9593e-03, -3.4946e-03, -4.1973e-03],\n                        [ 1.2073e-02,  7.9237e-03,  9.7770e-05],\n                        [-4.5093e-03, -8.0024e-03, -3.3877e-03]],\n              \n                       [[ 4.1504e-04, -6.3685e-03,  2.9286e-04],\n                        [-1.4368e-02,  5.2549e-04, -1.2686e-02],\n                        [ 1.6020e-03,  4.4607e-03,  7.5159e-03]],\n              \n                       [[-6.6873e-03,  5.1561e-05,  8.2160e-03],\n                        [-7.2157e-03, -9.4008e-04, -9.3220e-03],\n                        [ 1.3272e-03,  1.3943e-03, -1.0126e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.3756e-03,  1.2603e-02,  1.0009e-02],\n                        [ 1.3332e-02,  2.2436e-03, -2.6538e-03],\n                        [ 1.2150e-02, -6.4561e-03, -1.2219e-02]],\n              \n                       [[-8.2563e-03,  1.4514e-02, -6.5334e-03],\n                        [ 1.0584e-02,  7.2743e-03, -7.7184e-03],\n                        [-1.3945e-02, -3.9507e-04, -1.3207e-02]],\n              \n                       [[-1.1936e-02,  1.2723e-02,  1.4794e-03],\n                        [-9.2238e-03,  1.2513e-02, -1.2755e-02],\n                        [-2.3135e-04, -1.2050e-02,  1.0637e-02]],\n              \n                       ...,\n              \n                       [[-1.7315e-03, -1.1583e-02, -6.2004e-03],\n                        [-3.6829e-03, -7.5475e-03, -1.1467e-02],\n                        [-1.2565e-04, -1.6956e-03,  7.3251e-03]],\n              \n                       [[ 4.5195e-03,  9.6949e-03, -1.1593e-02],\n                        [-1.0726e-02, -4.3706e-03, -1.0075e-02],\n                        [-1.1938e-02, -6.4125e-03,  5.7692e-04]],\n              \n                       [[-1.1380e-02, -9.5971e-03, -1.3420e-02],\n                        [ 1.0888e-02, -1.0871e-02,  4.6657e-05],\n                        [-2.8069e-03, -1.0725e-02,  2.2430e-03]]],\n              \n              \n                      [[[ 1.1839e-02,  1.3359e-02, -2.2681e-03],\n                        [ 1.8450e-03,  5.9289e-04, -1.2829e-02],\n                        [ 1.4203e-02,  2.5810e-03, -1.1913e-02]],\n              \n                       [[-1.3077e-02, -1.4014e-02, -4.2100e-03],\n                        [-9.9503e-03,  1.1108e-02, -3.2723e-03],\n                        [ 2.0312e-03,  4.5349e-03,  1.3859e-02]],\n              \n                       [[-1.4575e-02,  1.1122e-02, -7.5780e-03],\n                        [-3.8330e-03, -9.8024e-04,  5.9586e-03],\n                        [ 9.8220e-03, -6.8341e-03,  1.2393e-02]],\n              \n                       ...,\n              \n                       [[-3.4048e-03,  1.3819e-02, -2.6837e-03],\n                        [ 1.1734e-02,  1.4311e-03, -1.2245e-02],\n                        [-8.3261e-03,  1.3495e-02,  2.9223e-03]],\n              \n                       [[-1.2962e-02, -7.3929e-03, -7.3878e-03],\n                        [-1.7338e-03, -6.7076e-03, -7.7754e-03],\n                        [ 1.4972e-03, -6.4253e-03, -1.4126e-02]],\n              \n                       [[ 1.4451e-02, -4.8099e-03,  5.7255e-03],\n                        [-5.8516e-03,  4.0733e-03,  1.0094e-02],\n                        [ 8.1309e-04,  5.1471e-03,  5.1509e-03]]],\n              \n              \n                      [[[ 9.8223e-04,  1.1245e-02,  1.1552e-02],\n                        [-7.6653e-03,  6.1365e-04, -4.2670e-03],\n                        [ 5.1350e-03,  1.4145e-02, -8.8357e-04]],\n              \n                       [[ 1.2253e-02,  1.0491e-02, -1.4184e-02],\n                        [ 2.6855e-03,  7.4216e-03, -4.6636e-03],\n                        [-1.0291e-02, -1.2930e-02, -3.5078e-04]],\n              \n                       [[ 4.5516e-03, -9.4295e-03,  9.7718e-03],\n                        [-7.6455e-03,  1.0235e-02,  1.2030e-03],\n                        [-2.7815e-03,  6.6763e-03, -8.7617e-03]],\n              \n                       ...,\n              \n                       [[-9.8976e-03,  1.2484e-02, -2.8897e-03],\n                        [ 4.3479e-03,  8.9747e-03,  8.7985e-04],\n                        [ 1.2341e-02,  4.2616e-04,  4.2251e-03]],\n              \n                       [[ 1.2692e-02, -1.7026e-03,  7.1434e-03],\n                        [ 1.1852e-02, -1.1433e-02, -1.3874e-02],\n                        [ 1.2581e-02, -3.8352e-03, -7.5201e-04]],\n              \n                       [[-4.7592e-04, -3.9157e-03,  3.5884e-03],\n                        [-3.2631e-03, -1.6258e-03, -1.0496e-02],\n                        [ 1.3847e-03, -5.7536e-04, -1.0432e-02]]]])),\n             ('up1.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up1.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up1.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.0.weight',\n              tensor([[[[-2.1518e-03,  1.0631e-02,  1.2601e-02],\n                        [ 9.9365e-03,  8.6478e-03, -1.2200e-02],\n                        [-8.7199e-03, -1.3551e-04,  2.7872e-03]],\n              \n                       [[ 1.0136e-02,  5.1465e-03, -7.2739e-03],\n                        [-1.0549e-02, -4.3726e-03, -1.0110e-02],\n                        [-1.2202e-02,  8.1444e-03,  1.2508e-02]],\n              \n                       [[-1.1105e-02, -3.2792e-03,  1.1186e-02],\n                        [-8.2915e-03,  8.8182e-03,  1.1263e-02],\n                        [-4.4057e-03,  8.6805e-03, -9.5922e-03]],\n              \n                       ...,\n              \n                       [[ 6.3221e-03, -1.2953e-02,  5.1380e-03],\n                        [ 2.9260e-04, -1.0260e-02,  6.4162e-03],\n                        [-5.8944e-03,  4.6316e-03,  1.4742e-03]],\n              \n                       [[-1.0956e-02, -3.5614e-03, -3.6777e-03],\n                        [ 1.2266e-02, -3.7897e-05, -1.1044e-02],\n                        [ 5.1852e-03,  8.2570e-03,  1.3097e-03]],\n              \n                       [[-2.4492e-03, -3.5821e-03, -1.4560e-02],\n                        [ 9.1054e-03, -4.1931e-03,  9.5132e-03],\n                        [ 5.1267e-03,  1.1881e-02,  5.6942e-04]]],\n              \n              \n                      [[[ 1.0638e-02, -5.4433e-03, -3.7759e-03],\n                        [ 1.1677e-02, -4.1737e-03, -1.0637e-02],\n                        [-1.6576e-03, -2.1487e-03, -1.1114e-02]],\n              \n                       [[ 1.8396e-03,  1.3266e-02,  6.8261e-03],\n                        [ 3.9165e-03, -8.8550e-03,  1.4806e-03],\n                        [ 7.0773e-04,  1.1756e-02, -1.0292e-02]],\n              \n                       [[ 1.3127e-02,  4.8850e-03,  2.1176e-03],\n                        [ 2.1249e-03, -5.7832e-03, -1.3140e-02],\n                        [ 8.5454e-03, -8.9114e-03, -1.3402e-02]],\n              \n                       ...,\n              \n                       [[ 1.1088e-02,  7.2383e-03,  1.2047e-02],\n                        [ 9.5457e-03,  1.3826e-02, -2.5452e-03],\n                        [ 9.1783e-03,  1.0598e-02, -8.6740e-04]],\n              \n                       [[ 4.5989e-03, -1.4716e-03, -1.2077e-02],\n                        [-9.6809e-04, -1.2336e-02,  9.3714e-04],\n                        [ 3.9654e-03, -7.3955e-03, -1.2232e-02]],\n              \n                       [[ 5.6303e-03, -8.0869e-03, -2.5287e-03],\n                        [ 1.8057e-03, -1.1487e-02, -2.8659e-03],\n                        [ 4.0015e-03, -1.2479e-02, -1.1998e-02]]],\n              \n              \n                      [[[ 9.4689e-03, -7.2081e-03,  1.4072e-03],\n                        [ 1.2932e-02, -3.2592e-03, -8.7485e-03],\n                        [ 9.2945e-03,  4.6018e-03,  4.0055e-03]],\n              \n                       [[-1.3764e-02, -4.2907e-03,  3.2547e-03],\n                        [ 3.3341e-03,  1.1304e-03, -1.2234e-02],\n                        [-1.3467e-02, -5.6734e-03,  7.4354e-03]],\n              \n                       [[-5.6023e-03, -2.8761e-03, -1.4718e-02],\n                        [ 1.0713e-02, -1.6779e-03, -1.1996e-02],\n                        [-1.2827e-02,  1.0703e-02, -9.7047e-03]],\n              \n                       ...,\n              \n                       [[ 3.2607e-03, -8.0475e-03,  6.1829e-03],\n                        [-2.9395e-03,  3.3496e-03,  5.1071e-03],\n                        [ 5.9723e-03,  4.7608e-03, -1.6388e-03]],\n              \n                       [[-4.3904e-03,  7.7792e-03, -1.2428e-02],\n                        [-3.2456e-03,  5.5866e-03, -1.4352e-02],\n                        [-1.1821e-02,  2.6534e-03,  7.5290e-03]],\n              \n                       [[ 4.6186e-03, -6.2310e-03,  1.1741e-02],\n                        [-1.4587e-02,  9.7592e-03,  1.2688e-02],\n                        [ 4.2982e-03,  5.2313e-03, -1.2822e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1165e-02,  7.8691e-04, -9.3187e-03],\n                        [-7.7603e-03, -3.0258e-03, -9.7707e-03],\n                        [ 7.5438e-03,  1.4036e-02,  1.0273e-02]],\n              \n                       [[-1.3591e-02,  7.4804e-03, -4.6866e-04],\n                        [-1.3815e-02,  1.2045e-02, -9.8406e-03],\n                        [ 1.0759e-02,  6.9177e-03, -1.3892e-02]],\n              \n                       [[ 1.2857e-02, -4.8749e-04,  9.5570e-03],\n                        [ 2.7064e-03, -8.0672e-03,  1.0471e-02],\n                        [ 5.2177e-03,  1.2281e-02, -6.2795e-03]],\n              \n                       ...,\n              \n                       [[ 1.0430e-03,  1.3958e-02, -1.1441e-02],\n                        [-1.0572e-02,  4.8599e-04, -8.1871e-03],\n                        [ 8.7779e-03,  8.1478e-03, -3.1877e-03]],\n              \n                       [[ 7.4461e-03,  2.9228e-03, -1.0984e-02],\n                        [ 9.8613e-03,  1.3081e-02,  1.2413e-02],\n                        [ 1.2035e-02, -3.1168e-03, -7.5135e-03]],\n              \n                       [[ 8.0283e-03, -4.2646e-03, -7.9841e-03],\n                        [-1.9161e-05, -6.6800e-03, -1.6066e-04],\n                        [ 9.5017e-03, -1.7248e-03,  7.0304e-03]]],\n              \n              \n                      [[[ 3.5356e-03, -7.6512e-03, -8.9665e-03],\n                        [-4.8910e-03,  2.0278e-03,  7.1160e-03],\n                        [-3.0881e-03, -4.1455e-03,  1.1920e-02]],\n              \n                       [[ 3.7466e-03, -3.9381e-03,  1.4420e-02],\n                        [-1.3107e-02, -5.7352e-03,  6.8331e-03],\n                        [-6.0296e-03,  1.2593e-02,  8.2828e-03]],\n              \n                       [[-9.1421e-03,  1.2051e-02,  9.1719e-03],\n                        [-2.3811e-03, -1.4370e-02, -1.1317e-02],\n                        [-5.8528e-03,  5.9658e-03, -7.2074e-03]],\n              \n                       ...,\n              \n                       [[ 1.4338e-02,  1.0304e-02, -6.8373e-03],\n                        [ 2.6406e-03, -2.9580e-03, -2.9774e-03],\n                        [-6.9043e-03,  1.4699e-02, -7.5011e-03]],\n              \n                       [[ 9.0359e-03, -7.4744e-03,  2.7057e-03],\n                        [-1.0241e-03, -9.2485e-03, -3.4580e-03],\n                        [ 3.8833e-03,  7.4134e-03, -1.1881e-02]],\n              \n                       [[-1.9624e-03,  2.7043e-03, -4.4755e-04],\n                        [-1.1581e-02, -1.3765e-02, -8.7221e-03],\n                        [ 1.3774e-02, -1.1876e-02, -1.0575e-02]]],\n              \n              \n                      [[[-1.7063e-04,  6.7622e-04,  8.8984e-03],\n                        [-5.9551e-03,  1.2280e-02, -1.2928e-02],\n                        [-1.2386e-02,  1.3566e-02,  3.3778e-03]],\n              \n                       [[-4.9461e-03, -1.1765e-03, -5.0370e-03],\n                        [-3.2352e-03,  8.2034e-03,  1.2355e-02],\n                        [ 3.5783e-03,  1.1220e-02, -1.3388e-02]],\n              \n                       [[-1.8399e-03,  5.9302e-03,  9.6810e-03],\n                        [ 5.0733e-03,  1.0453e-02, -4.8722e-03],\n                        [-1.3514e-02, -1.1929e-03,  1.7507e-03]],\n              \n                       ...,\n              \n                       [[-1.4605e-03,  2.2461e-03, -8.0156e-03],\n                        [ 1.0985e-02,  5.1273e-03, -1.1668e-02],\n                        [ 1.4627e-02,  2.7758e-03,  7.2483e-03]],\n              \n                       [[ 1.3621e-02, -4.5283e-03,  6.4443e-04],\n                        [ 1.0748e-02,  1.1094e-02,  1.4675e-02],\n                        [-9.0625e-03, -6.1689e-03, -2.2046e-03]],\n              \n                       [[-1.4035e-03, -1.3366e-02,  5.8688e-03],\n                        [ 2.4954e-04,  7.3011e-03,  8.3442e-03],\n                        [-2.7433e-04, -1.0389e-02,  3.1839e-03]]]])),\n             ('up2.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.])),\n             ('up2.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up2.conv.double_conv.3.weight',\n              tensor([[[[ 7.9497e-03, -1.7790e-02, -1.7096e-02],\n                        [-1.6327e-02,  4.0280e-03, -1.9224e-02],\n                        [-4.1614e-03,  2.0345e-02, -1.3011e-02]],\n              \n                       [[-1.1634e-02,  5.5307e-03, -1.6266e-02],\n                        [-1.1103e-02,  8.3270e-03, -1.5757e-02],\n                        [ 1.5221e-02, -1.2837e-02,  9.6909e-04]],\n              \n                       [[-1.6213e-02,  6.1893e-03,  1.9967e-02],\n                        [-1.0630e-02,  2.0123e-02,  6.5128e-03],\n                        [-2.0276e-02,  2.0401e-02,  1.5855e-02]],\n              \n                       ...,\n              \n                       [[ 1.4602e-02, -9.3187e-03,  1.2791e-02],\n                        [ 3.5288e-03,  8.2964e-03,  1.7589e-02],\n                        [ 4.4983e-03, -4.8159e-04, -3.6260e-03]],\n              \n                       [[-8.9474e-05,  1.3904e-02,  1.9019e-02],\n                        [-1.9988e-02, -1.3111e-02,  6.4248e-04],\n                        [ 6.8580e-04,  1.7128e-03,  5.4387e-03]],\n              \n                       [[ 1.4890e-02, -9.2215e-03, -5.8313e-03],\n                        [ 1.1482e-02, -1.2943e-02,  1.7208e-02],\n                        [-2.3544e-03,  8.3377e-04, -1.4550e-02]]],\n              \n              \n                      [[[-2.5915e-03, -3.9138e-03, -1.6308e-02],\n                        [-1.9927e-02, -9.3398e-03, -1.9362e-02],\n                        [-1.4066e-02,  9.7209e-03,  1.6551e-02]],\n              \n                       [[-1.9409e-02, -1.3963e-02,  6.9585e-03],\n                        [-5.1612e-04, -1.9914e-02,  1.8270e-02],\n                        [-7.2831e-03,  1.2477e-02, -2.8120e-04]],\n              \n                       [[-1.5371e-02,  9.3540e-04,  9.9296e-03],\n                        [-1.0750e-02, -3.9004e-03,  1.7460e-02],\n                        [-1.9144e-02,  2.0190e-02, -1.1884e-02]],\n              \n                       ...,\n              \n                       [[ 7.7697e-03,  1.9071e-02, -3.6815e-03],\n                        [ 5.6426e-03, -8.5833e-03,  1.6836e-02],\n                        [ 1.8768e-03, -2.5059e-04,  8.1764e-03]],\n              \n                       [[ 5.9330e-03, -1.4364e-02, -3.9514e-03],\n                        [ 1.9684e-02, -1.4239e-02, -2.0091e-02],\n                        [ 2.0407e-02,  1.8737e-02, -5.8489e-03]],\n              \n                       [[ 5.4501e-03,  1.1028e-02, -1.9625e-02],\n                        [-1.3838e-02, -8.5165e-03,  2.6146e-03],\n                        [-6.4134e-03,  1.4367e-02,  1.4903e-02]]],\n              \n              \n                      [[[-1.1303e-03,  3.3091e-03, -6.1916e-03],\n                        [-1.5099e-02, -2.1207e-04,  4.5621e-03],\n                        [ 1.7857e-02, -2.7128e-03, -5.4803e-03]],\n              \n                       [[ 5.9743e-03,  2.0597e-02,  6.6697e-03],\n                        [ 9.8200e-03,  1.3099e-02,  1.7841e-03],\n                        [-1.6089e-02,  1.5824e-02,  8.0234e-04]],\n              \n                       [[-7.2984e-03,  1.2674e-02,  1.8605e-02],\n                        [ 3.9323e-03,  8.1922e-03, -9.3463e-04],\n                        [-1.9702e-02,  1.4019e-02,  1.6300e-02]],\n              \n                       ...,\n              \n                       [[ 1.6479e-02,  1.6218e-02, -1.5242e-02],\n                        [-3.6273e-03,  5.0512e-03,  1.1426e-02],\n                        [ 7.1217e-03,  7.2147e-03, -2.5175e-03]],\n              \n                       [[ 1.5327e-02,  1.4072e-02, -1.7085e-02],\n                        [ 4.0818e-04, -1.7114e-02, -3.8038e-03],\n                        [-1.5342e-02, -2.0213e-02, -1.3697e-02]],\n              \n                       [[-2.0410e-02, -1.5656e-02,  5.8427e-03],\n                        [-3.8405e-03,  1.0923e-02, -1.2858e-02],\n                        [ 1.8628e-02,  4.0466e-03, -2.0422e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.9150e-02,  1.2267e-02,  1.7782e-02],\n                        [ 1.3684e-02, -1.9804e-02, -9.2421e-03],\n                        [ 1.7435e-02,  1.7343e-02, -1.8515e-02]],\n              \n                       [[ 1.8531e-02, -6.2842e-03, -2.1436e-03],\n                        [-6.2577e-03,  1.8332e-02,  1.9857e-02],\n                        [-1.0869e-02, -5.4065e-03,  1.8648e-02]],\n              \n                       [[-9.8150e-03, -1.9312e-02, -5.3483e-04],\n                        [ 2.2209e-03,  2.0530e-02, -6.2797e-03],\n                        [ 3.1732e-03,  1.7359e-02,  1.0300e-02]],\n              \n                       ...,\n              \n                       [[ 5.3619e-03, -8.6172e-03,  1.9207e-02],\n                        [ 1.2767e-02, -3.0699e-03, -9.6391e-03],\n                        [-8.9599e-04,  6.0747e-03,  4.0384e-03]],\n              \n                       [[-5.2875e-03,  6.5115e-04,  5.4017e-03],\n                        [ 1.5804e-03,  8.6046e-03,  1.7447e-02],\n                        [ 7.5348e-03,  1.8965e-02,  1.9957e-02]],\n              \n                       [[-1.0331e-02, -1.1320e-02,  1.5131e-02],\n                        [ 2.9035e-03,  1.1799e-02, -1.5353e-03],\n                        [-8.3366e-03,  9.3031e-03, -1.7604e-02]]],\n              \n              \n                      [[[ 1.4307e-02,  1.1860e-02,  5.1069e-03],\n                        [-1.5284e-02,  8.2293e-03, -9.5887e-03],\n                        [ 5.3585e-03,  2.0224e-03,  1.5437e-02]],\n              \n                       [[ 1.2629e-03,  9.5884e-03,  1.5362e-02],\n                        [-4.8209e-03,  1.4933e-02, -1.2048e-02],\n                        [-3.0520e-05, -1.3378e-02, -2.1463e-03]],\n              \n                       [[-1.1527e-02,  7.7163e-03, -1.2359e-02],\n                        [-2.0476e-02, -1.7779e-02, -6.4546e-03],\n                        [ 3.1536e-03, -1.0851e-04, -1.9629e-02]],\n              \n                       ...,\n              \n                       [[-3.6267e-03, -1.7496e-02, -1.8531e-02],\n                        [ 3.0812e-03, -4.4989e-03, -5.3328e-03],\n                        [-3.5008e-03, -1.0352e-02,  2.0659e-02]],\n              \n                       [[-4.5241e-03,  6.3328e-03,  8.7361e-03],\n                        [-6.1625e-03, -1.3019e-02,  1.6934e-02],\n                        [-3.4158e-03,  8.9188e-03, -1.3646e-02]],\n              \n                       [[ 1.7996e-02,  1.7854e-02, -1.5007e-02],\n                        [ 2.2617e-04,  1.8391e-02,  2.0008e-02],\n                        [-1.4899e-03,  1.6801e-02,  2.3108e-03]]],\n              \n              \n                      [[[-1.5664e-02,  4.3163e-03,  1.2885e-02],\n                        [ 2.6682e-03,  1.6914e-02,  3.5899e-03],\n                        [ 1.9674e-02, -1.1662e-02, -1.2853e-02]],\n              \n                       [[-3.9540e-04, -1.7787e-02,  9.8214e-03],\n                        [ 1.3250e-02, -2.1693e-03, -4.9136e-03],\n                        [ 1.9610e-02,  1.1362e-03,  2.0132e-02]],\n              \n                       [[ 1.0343e-03,  8.4445e-03,  1.5850e-02],\n                        [ 1.1820e-02,  1.0775e-03, -1.8296e-02],\n                        [-1.1273e-02,  2.6236e-03,  1.3343e-02]],\n              \n                       ...,\n              \n                       [[ 1.6003e-02,  5.4038e-03, -3.7506e-03],\n                        [-2.4944e-03, -8.0193e-03, -6.6061e-03],\n                        [-1.2857e-02,  1.3497e-02,  8.1090e-03]],\n              \n                       [[-1.8006e-02, -8.5612e-03,  1.9954e-02],\n                        [-3.3323e-03, -7.7578e-04,  1.2751e-02],\n                        [ 8.0447e-03, -3.9115e-04,  2.0177e-02]],\n              \n                       [[-1.7435e-02, -8.4071e-03, -9.7204e-03],\n                        [ 1.8257e-02, -1.7279e-02, -1.8781e-02],\n                        [ 1.5807e-02, -1.8718e-02,  2.0478e-02]]]])),\n             ('up2.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up2.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up2.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.0.weight',\n              tensor([[[[ 6.5360e-04, -1.1478e-02, -1.2108e-02],\n                        [-1.3628e-02, -9.4881e-03,  4.5922e-03],\n                        [-1.3436e-03, -9.4868e-03, -4.5939e-03]],\n              \n                       [[ 1.0784e-02, -1.2223e-03, -1.5292e-02],\n                        [-5.8855e-03, -1.8780e-02, -8.7660e-03],\n                        [ 1.8609e-03,  1.2953e-02, -1.4010e-02]],\n              \n                       [[-6.7148e-03, -1.5341e-02,  1.2591e-02],\n                        [ 7.5377e-03,  1.1052e-02, -1.1975e-02],\n                        [-1.9517e-02, -1.9137e-02, -7.4886e-04]],\n              \n                       ...,\n              \n                       [[ 2.0512e-02, -3.9202e-03,  1.4523e-02],\n                        [ 1.2714e-02,  1.3007e-02,  6.8676e-04],\n                        [-1.7327e-02, -8.6569e-03,  1.2416e-03]],\n              \n                       [[-2.0188e-02, -1.2779e-02, -7.3068e-03],\n                        [-9.3873e-03,  1.3301e-02,  1.6646e-02],\n                        [-1.7413e-02,  1.7294e-03, -1.5510e-02]],\n              \n                       [[-1.4983e-02,  1.7590e-02,  1.2623e-02],\n                        [-2.8354e-03, -2.8116e-03,  1.7879e-02],\n                        [-1.7114e-02,  1.2573e-02,  1.0661e-02]]],\n              \n              \n                      [[[ 1.1610e-02, -1.0957e-02,  1.8087e-02],\n                        [ 1.2981e-02, -1.2237e-02, -1.3717e-02],\n                        [-8.9545e-03,  1.0519e-02, -1.8804e-02]],\n              \n                       [[-5.7298e-03,  1.7915e-02, -3.1621e-03],\n                        [ 7.9957e-03,  3.4881e-03, -1.5158e-02],\n                        [ 1.8798e-03,  1.6252e-02, -1.5315e-03]],\n              \n                       [[-4.2252e-03,  8.9630e-03, -7.0830e-03],\n                        [-1.0045e-02, -2.2602e-03,  7.8443e-03],\n                        [-2.6957e-03,  1.3411e-02,  4.8645e-03]],\n              \n                       ...,\n              \n                       [[-5.3712e-03, -1.0452e-02, -1.6330e-02],\n                        [-1.0432e-02, -1.9882e-02, -1.6169e-02],\n                        [-7.2622e-03, -1.8196e-02, -6.7982e-03]],\n              \n                       [[-7.0105e-05, -1.2175e-02, -1.0749e-02],\n                        [ 1.1441e-02,  3.5827e-03,  1.7456e-02],\n                        [-4.9655e-03,  1.9057e-03, -1.7193e-02]],\n              \n                       [[ 1.7013e-02,  3.1988e-04,  5.7411e-03],\n                        [-3.7235e-04, -1.8450e-03,  3.6671e-03],\n                        [ 1.6459e-02,  1.1565e-02,  1.9842e-02]]],\n              \n              \n                      [[[ 1.6914e-02, -1.2111e-02,  1.4786e-02],\n                        [ 7.7207e-03,  2.5537e-03,  4.0743e-03],\n                        [ 1.0419e-04,  1.0066e-02, -8.1808e-03]],\n              \n                       [[ 5.5924e-03,  3.0751e-03, -1.4255e-02],\n                        [ 1.4609e-02, -6.0797e-03,  1.8090e-02],\n                        [-2.0465e-02, -1.9647e-02,  1.9963e-02]],\n              \n                       [[ 1.7703e-02,  9.7912e-04, -1.7088e-02],\n                        [-3.0930e-03,  1.0013e-02,  1.5110e-02],\n                        [-1.5153e-02, -6.5340e-03,  1.6374e-02]],\n              \n                       ...,\n              \n                       [[-1.0198e-02,  1.8628e-02, -7.3407e-03],\n                        [-2.0066e-02,  1.8155e-02,  8.2106e-03],\n                        [-5.0477e-04, -5.1193e-03, -1.9685e-02]],\n              \n                       [[ 7.3187e-03, -1.8577e-02, -1.9180e-02],\n                        [ 1.3858e-02, -1.6733e-02, -5.7723e-04],\n                        [ 1.2103e-02,  8.6336e-03, -2.0067e-02]],\n              \n                       [[-3.8180e-03,  1.9922e-03, -1.2753e-02],\n                        [ 1.9889e-02,  1.9218e-02,  1.2516e-02],\n                        [-1.6966e-02, -1.9937e-02,  6.3545e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4647e-02,  1.3599e-02, -1.1497e-02],\n                        [ 1.0819e-02,  6.2655e-03,  8.2514e-03],\n                        [ 9.7814e-03,  1.5446e-03,  5.0288e-03]],\n              \n                       [[-3.7955e-03,  1.2494e-02, -7.8703e-03],\n                        [ 4.0349e-03,  1.4197e-02, -1.1018e-02],\n                        [ 1.2082e-02, -1.9828e-03,  1.1344e-02]],\n              \n                       [[-1.6060e-02,  5.2254e-03,  1.3679e-02],\n                        [ 2.3551e-03, -5.8034e-03, -1.0188e-02],\n                        [-7.8099e-03, -7.3378e-03, -1.6845e-02]],\n              \n                       ...,\n              \n                       [[ 4.8750e-03, -1.5202e-02, -8.3033e-03],\n                        [-1.4143e-02,  9.6245e-03,  1.0595e-03],\n                        [-6.6992e-03,  1.8018e-02,  1.4028e-02]],\n              \n                       [[-2.4361e-03,  8.2809e-03, -6.7384e-03],\n                        [-2.4594e-03,  4.9077e-03,  1.8375e-02],\n                        [-4.1593e-03, -3.5705e-03, -1.3529e-02]],\n              \n                       [[-1.7012e-02,  1.9748e-02,  1.9104e-02],\n                        [-1.4910e-02, -1.9546e-02,  1.1406e-02],\n                        [-1.7544e-04,  1.5866e-02,  3.8805e-03]]],\n              \n              \n                      [[[-4.2661e-03,  2.0544e-02, -2.0223e-02],\n                        [-1.7558e-02,  1.2315e-02, -1.1358e-03],\n                        [-9.5695e-03,  1.7591e-02, -1.8437e-02]],\n              \n                       [[-7.6622e-03,  1.3523e-02, -1.2805e-02],\n                        [ 4.2950e-03, -7.9838e-03, -8.6255e-03],\n                        [ 1.5282e-03, -8.8083e-03,  5.8126e-03]],\n              \n                       [[ 1.2428e-02,  1.6649e-03, -1.8423e-02],\n                        [ 3.3804e-03, -9.0342e-03, -2.8731e-03],\n                        [ 2.8868e-03, -4.1382e-03,  1.6776e-02]],\n              \n                       ...,\n              \n                       [[ 1.6678e-02, -4.2476e-03, -9.8835e-03],\n                        [-9.7655e-03, -3.7623e-03,  5.0571e-03],\n                        [ 1.0131e-02, -7.6768e-03, -5.4080e-04]],\n              \n                       [[ 1.7999e-02,  5.0342e-03, -2.2092e-03],\n                        [ 1.2079e-02, -8.4492e-03, -1.6282e-02],\n                        [-2.0245e-02,  4.7685e-03, -9.7620e-03]],\n              \n                       [[-4.6216e-03, -1.1652e-02, -1.2818e-02],\n                        [ 1.2088e-02, -9.3832e-03, -4.1677e-03],\n                        [ 1.1476e-02, -4.4116e-03, -2.0018e-02]]],\n              \n              \n                      [[[ 3.7413e-03, -1.8938e-02, -1.2220e-02],\n                        [ 1.7449e-02,  9.5147e-03,  2.5178e-03],\n                        [-6.6552e-03,  2.6520e-03, -2.0583e-02]],\n              \n                       [[ 1.9046e-02,  1.7330e-03,  3.4585e-03],\n                        [ 1.6316e-02, -1.8740e-02,  1.6343e-02],\n                        [-8.1862e-03, -1.9654e-02,  6.7754e-04]],\n              \n                       [[-7.8348e-03, -1.0483e-02, -1.1580e-02],\n                        [ 2.0537e-02, -1.2595e-02,  4.6942e-03],\n                        [ 5.1139e-04, -8.2631e-04, -1.3213e-03]],\n              \n                       ...,\n              \n                       [[ 2.0120e-02, -1.8718e-02,  7.1457e-03],\n                        [ 8.7498e-03, -8.0881e-03, -8.0977e-03],\n                        [-1.8490e-02, -2.0089e-02,  2.6450e-04]],\n              \n                       [[ 3.0537e-03, -8.0446e-03, -9.7033e-03],\n                        [ 2.9420e-03,  1.5974e-02, -8.4568e-03],\n                        [-4.6306e-03,  7.5076e-03, -9.9498e-04]],\n              \n                       [[-1.7441e-02, -4.8928e-03,  2.0088e-02],\n                        [ 1.1744e-02, -1.9409e-02, -1.2495e-02],\n                        [ 1.6826e-02, -6.6388e-03, -1.3236e-03]]]])),\n             ('up3.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.])),\n             ('up3.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up3.conv.double_conv.3.weight',\n              tensor([[[[-6.2617e-03,  5.1519e-03,  1.0535e-02],\n                        [ 2.2614e-02,  2.3770e-02,  7.1172e-03],\n                        [-9.0252e-04, -2.0448e-02, -2.0432e-02]],\n              \n                       [[-5.3073e-03,  2.0543e-03, -1.9999e-02],\n                        [ 1.7058e-02,  4.4323e-03,  2.0256e-02],\n                        [ 1.6059e-02,  7.8848e-03,  2.6898e-02]],\n              \n                       [[ 2.4905e-02, -9.5489e-04, -4.0310e-05],\n                        [ 2.6839e-02,  1.0395e-02, -1.1824e-02],\n                        [ 1.3696e-02, -4.7753e-03,  4.4547e-03]],\n              \n                       ...,\n              \n                       [[-4.0551e-03, -2.0774e-02,  5.0831e-03],\n                        [ 8.9578e-03, -2.4251e-02, -2.7485e-02],\n                        [-1.1212e-02, -3.5667e-03, -2.9207e-02]],\n              \n                       [[-2.5817e-02,  2.8529e-02, -2.4398e-02],\n                        [ 2.0831e-02,  1.4292e-02, -1.8673e-02],\n                        [-8.5094e-04, -1.2406e-03,  3.7525e-04]],\n              \n                       [[ 2.1931e-03,  6.2044e-03, -9.8672e-03],\n                        [-6.0165e-03,  7.0416e-03, -3.2293e-03],\n                        [-1.1025e-02, -1.1666e-02, -1.8839e-02]]],\n              \n              \n                      [[[-1.9571e-02,  1.3345e-02, -3.1977e-03],\n                        [-2.4555e-02, -3.5323e-03, -2.8703e-02],\n                        [-1.5313e-02,  2.1116e-02, -1.0758e-03]],\n              \n                       [[-1.0014e-02,  1.1471e-02, -2.2742e-02],\n                        [ 2.5164e-02,  1.5579e-02, -2.2211e-02],\n                        [ 2.7174e-02,  1.9207e-02, -1.7626e-02]],\n              \n                       [[ 2.7689e-02, -5.7403e-03, -1.0863e-02],\n                        [ 5.0870e-03,  6.7373e-03, -2.0150e-02],\n                        [ 2.9319e-02, -9.6329e-03, -2.0385e-02]],\n              \n                       ...,\n              \n                       [[-2.4959e-02,  1.2766e-03,  2.4264e-03],\n                        [ 2.1160e-02, -2.1553e-02,  1.6825e-02],\n                        [ 2.6579e-02,  6.6060e-03,  2.5650e-02]],\n              \n                       [[ 4.5595e-03,  1.9319e-03, -2.5173e-02],\n                        [-2.3925e-02, -8.3372e-03, -9.0146e-03],\n                        [ 1.7461e-02, -2.5896e-02, -1.8144e-02]],\n              \n                       [[ 2.5831e-02, -2.1761e-02, -2.9396e-02],\n                        [ 2.7635e-02, -1.2928e-02,  5.8588e-03],\n                        [-2.0192e-02,  4.7528e-03,  2.8390e-02]]],\n              \n              \n                      [[[ 1.8739e-03, -1.3140e-02,  2.6128e-02],\n                        [ 1.1566e-02,  3.5446e-03, -5.1995e-03],\n                        [ 5.5016e-03, -4.5294e-03,  1.9544e-02]],\n              \n                       [[-9.9646e-03,  2.7664e-02,  1.1371e-02],\n                        [ 1.2055e-02,  1.6825e-02, -1.1272e-02],\n                        [ 1.3120e-02,  1.7465e-02,  1.1575e-02]],\n              \n                       [[-4.8596e-03,  9.3461e-03,  2.0105e-02],\n                        [ 1.2126e-02, -2.2240e-03,  1.3572e-02],\n                        [-2.8769e-02, -7.9955e-03, -1.2733e-02]],\n              \n                       ...,\n              \n                       [[ 2.5646e-02,  1.6559e-02, -2.2198e-02],\n                        [-3.0433e-03,  2.7646e-02,  2.8915e-02],\n                        [ 2.3706e-02, -2.5853e-02, -8.8919e-05]],\n              \n                       [[ 1.9385e-02,  9.4940e-03, -1.7507e-02],\n                        [-1.0995e-02, -1.9027e-02,  2.6517e-02],\n                        [ 6.5096e-03,  8.3432e-03,  4.3078e-03]],\n              \n                       [[-1.2435e-02, -1.2040e-02,  6.4921e-03],\n                        [-1.9559e-02,  2.2276e-02,  1.2324e-02],\n                        [ 7.4537e-03,  5.5965e-03, -2.4149e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.9395e-02,  2.0365e-02, -1.6215e-02],\n                        [ 1.8015e-02,  1.1132e-02, -5.3747e-03],\n                        [ 4.5775e-03,  1.9513e-02,  5.4436e-03]],\n              \n                       [[ 2.0589e-02,  4.0204e-03, -7.1212e-03],\n                        [-1.7708e-02, -2.7610e-02,  2.9521e-03],\n                        [ 1.4294e-02, -6.5115e-03, -1.4379e-03]],\n              \n                       [[ 2.8011e-02,  1.6216e-02,  2.5210e-02],\n                        [-1.6498e-02,  1.0523e-02,  2.6155e-02],\n                        [ 1.6074e-02, -8.3713e-03,  2.2026e-02]],\n              \n                       ...,\n              \n                       [[-1.3617e-02, -1.4065e-02, -2.3103e-02],\n                        [ 2.4879e-02, -8.9402e-03,  3.0990e-03],\n                        [ 1.3965e-03, -2.5021e-02, -2.0546e-02]],\n              \n                       [[ 2.0246e-03, -7.9078e-03, -2.6747e-02],\n                        [ 2.9376e-02, -6.2544e-03, -1.8549e-02],\n                        [ 1.5150e-02, -3.9595e-03,  2.3443e-03]],\n              \n                       [[-3.6495e-03, -1.0052e-02,  1.2397e-03],\n                        [ 3.8338e-03, -2.8786e-02, -5.1455e-03],\n                        [-1.5915e-02,  2.8991e-02,  6.3032e-03]]],\n              \n              \n                      [[[-2.0503e-02, -2.8574e-02,  1.7111e-02],\n                        [-1.5106e-02,  2.2639e-02,  3.2666e-03],\n                        [ 1.1444e-02, -9.7533e-03,  1.8418e-02]],\n              \n                       [[-2.8729e-02, -1.7639e-02,  1.5558e-02],\n                        [ 2.1907e-02,  2.6665e-02, -2.0398e-02],\n                        [ 4.7236e-03,  2.2406e-02, -1.1982e-03]],\n              \n                       [[-6.9613e-03,  1.6444e-02,  1.0986e-04],\n                        [-2.5102e-02,  2.7951e-02,  1.8224e-02],\n                        [-9.3261e-03, -2.2952e-02, -1.9339e-02]],\n              \n                       ...,\n              \n                       [[ 6.3333e-03, -8.1322e-03,  3.5560e-03],\n                        [-2.3900e-02, -2.8754e-02, -2.0715e-02],\n                        [ 1.3923e-02,  1.0834e-02, -1.1983e-02]],\n              \n                       [[-1.2872e-02,  6.1885e-03, -1.2684e-02],\n                        [ 8.5061e-03, -1.3273e-03, -1.6401e-03],\n                        [ 3.5566e-03,  1.4142e-02,  7.0110e-03]],\n              \n                       [[ 1.2880e-02,  6.1687e-03, -9.6315e-03],\n                        [ 1.5918e-02,  2.2629e-03, -2.7104e-03],\n                        [-8.4794e-04,  2.0819e-02, -2.2515e-02]]],\n              \n              \n                      [[[ 8.6197e-03,  2.3163e-02,  1.9551e-02],\n                        [ 2.2528e-02,  1.8106e-02,  1.0401e-02],\n                        [-1.7955e-03, -5.1270e-03,  9.9206e-03]],\n              \n                       [[ 2.3529e-02,  1.5074e-02, -1.5779e-02],\n                        [-2.8125e-02, -1.9706e-02, -2.7739e-02],\n                        [ 1.2969e-02, -6.8372e-03, -1.8700e-02]],\n              \n                       [[-1.6456e-02, -1.9319e-02,  2.9451e-02],\n                        [-4.3081e-03,  1.6394e-02,  2.0039e-02],\n                        [-2.6109e-02,  1.8154e-02, -4.1342e-03]],\n              \n                       ...,\n              \n                       [[ 1.4506e-02, -2.9666e-03,  3.6261e-03],\n                        [ 1.6303e-02, -4.9343e-03, -1.7006e-02],\n                        [ 2.6239e-02, -2.3413e-02,  1.2565e-02]],\n              \n                       [[-7.7776e-03,  2.6909e-02,  1.0444e-02],\n                        [-8.7274e-03, -8.3104e-03,  2.3266e-03],\n                        [-2.4073e-02, -1.0433e-02, -1.1619e-02]],\n              \n                       [[-1.0362e-02, -2.3291e-02, -1.0579e-02],\n                        [ 1.6419e-02,  2.0854e-02,  2.4889e-02],\n                        [ 1.3606e-03, -9.4291e-03, -1.6355e-03]]]])),\n             ('up3.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up3.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up3.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.0.weight',\n              tensor([[[[-2.4477e-02, -1.7234e-02,  2.2003e-03],\n                        [-7.8829e-03,  6.1736e-03,  1.4644e-02],\n                        [ 9.7539e-03,  5.7497e-04, -2.1407e-02]],\n              \n                       [[ 2.5615e-02,  6.0152e-03, -2.8486e-02],\n                        [ 2.1189e-02,  6.7674e-03, -1.4792e-03],\n                        [ 2.2734e-02,  1.7544e-03, -1.0535e-02]],\n              \n                       [[ 2.1016e-02,  3.9310e-03,  5.9241e-03],\n                        [-9.3318e-04,  1.3821e-02,  2.8222e-02],\n                        [ 7.3732e-03,  2.3611e-03,  2.2986e-02]],\n              \n                       ...,\n              \n                       [[-2.6076e-02,  9.7759e-03,  1.7446e-02],\n                        [-4.6081e-03, -7.8919e-03, -1.3171e-02],\n                        [ 3.6483e-03,  5.5107e-04, -2.6154e-02]],\n              \n                       [[ 2.4815e-02,  6.5554e-04, -2.6840e-02],\n                        [-5.4893e-03, -1.2978e-02, -7.7000e-03],\n                        [ 1.7822e-02, -2.0376e-02,  1.8151e-02]],\n              \n                       [[-1.3709e-02, -2.1298e-02,  1.4319e-02],\n                        [-1.1540e-02,  2.9451e-03,  4.6603e-03],\n                        [ 1.6498e-02, -2.2247e-02, -2.6400e-02]]],\n              \n              \n                      [[[-2.9053e-02,  6.6088e-03,  2.8600e-02],\n                        [-8.5117e-03,  3.7488e-03,  2.5909e-02],\n                        [-6.6344e-03, -1.8867e-02,  2.1232e-02]],\n              \n                       [[ 2.7659e-02, -1.5675e-02, -1.2514e-02],\n                        [ 6.8806e-03, -2.4540e-02, -2.0591e-02],\n                        [-6.2750e-03, -2.9055e-02,  2.7674e-02]],\n              \n                       [[ 6.6344e-03, -2.5097e-02, -2.7987e-02],\n                        [-1.9412e-02, -1.7099e-02,  2.4543e-02],\n                        [-6.0892e-03, -1.9663e-02, -2.1830e-02]],\n              \n                       ...,\n              \n                       [[-2.4330e-02, -5.3355e-04,  1.6593e-02],\n                        [-1.5296e-02, -1.2302e-02, -2.1773e-02],\n                        [-2.4805e-02, -2.7568e-02, -5.2265e-03]],\n              \n                       [[ 1.4438e-02, -1.1498e-02, -5.8588e-03],\n                        [ 2.3541e-02,  2.8545e-02, -2.1781e-02],\n                        [ 2.1298e-02, -1.4740e-02,  2.0063e-02]],\n              \n                       [[-1.4228e-02,  2.7397e-02,  1.9363e-03],\n                        [ 1.3088e-02,  1.8878e-02,  2.5326e-02],\n                        [-2.7118e-02,  1.8095e-02,  1.5554e-02]]],\n              \n              \n                      [[[-2.7807e-02,  2.8756e-02, -2.4947e-02],\n                        [ 2.8239e-03,  6.4158e-03,  1.7847e-02],\n                        [-2.1316e-02, -1.1236e-02, -7.1000e-03]],\n              \n                       [[-2.2642e-02, -2.9162e-02, -2.7960e-02],\n                        [ 2.2822e-02,  2.6365e-02, -2.2013e-02],\n                        [-4.3668e-03,  5.9663e-03, -2.2929e-02]],\n              \n                       [[ 2.6231e-02,  6.2513e-04, -1.5292e-02],\n                        [-2.3744e-02,  1.0287e-02, -1.7989e-02],\n                        [ 1.4567e-02, -5.4238e-04, -1.8888e-03]],\n              \n                       ...,\n              \n                       [[ 8.2702e-03, -3.9680e-03,  4.4591e-03],\n                        [ 1.2113e-02,  1.9210e-02, -2.1732e-02],\n                        [ 1.8309e-02, -2.5562e-02, -3.4519e-03]],\n              \n                       [[ 2.0920e-02,  5.1383e-03, -2.8351e-02],\n                        [ 2.4168e-02,  2.4032e-03,  4.4554e-03],\n                        [-9.5799e-03, -4.6795e-03,  2.1697e-02]],\n              \n                       [[ 5.9437e-03,  1.4123e-03, -8.3815e-03],\n                        [ 2.3132e-02, -2.6785e-02, -1.6763e-02],\n                        [-9.6515e-03, -2.1222e-02,  2.4000e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.3391e-02,  2.3395e-02, -2.1791e-02],\n                        [ 1.8008e-02,  5.3447e-03,  2.3465e-02],\n                        [ 1.7817e-02, -3.0541e-04,  1.8585e-02]],\n              \n                       [[-1.8773e-02,  9.5143e-03, -9.0805e-03],\n                        [-1.1845e-02, -2.0910e-02,  7.6076e-03],\n                        [-1.9462e-03,  2.5138e-02, -2.8411e-02]],\n              \n                       [[ 1.2022e-02, -1.4268e-02,  1.6846e-02],\n                        [-1.5587e-02, -2.2586e-02,  1.7113e-03],\n                        [-2.0474e-02,  2.1718e-02,  2.6473e-02]],\n              \n                       ...,\n              \n                       [[-9.5288e-04, -2.0567e-02, -5.8081e-03],\n                        [-9.2609e-03,  2.2689e-02,  7.9880e-03],\n                        [-2.3267e-02, -2.2080e-03, -3.7323e-04]],\n              \n                       [[ 7.0031e-03,  1.5936e-02, -1.7355e-02],\n                        [ 9.1528e-03,  6.0140e-04, -4.6582e-03],\n                        [-2.2403e-03,  1.1589e-02,  1.3004e-02]],\n              \n                       [[ 7.5902e-03, -2.7939e-02,  1.6827e-02],\n                        [-1.1944e-02, -2.1053e-02,  7.7404e-03],\n                        [-2.4648e-02,  1.0781e-02,  1.6477e-02]]],\n              \n              \n                      [[[ 2.8526e-02, -8.3310e-03, -3.3514e-03],\n                        [ 8.7738e-03,  3.3132e-03, -2.3501e-03],\n                        [-1.5227e-02, -6.8209e-03,  7.2189e-03]],\n              \n                       [[ 3.2429e-03,  2.9305e-02,  7.2086e-03],\n                        [-2.8544e-02, -2.1567e-02, -7.0302e-03],\n                        [-1.2484e-02,  4.2848e-03, -1.5662e-02]],\n              \n                       [[ 1.4185e-03,  6.2046e-03,  2.1498e-02],\n                        [ 1.4784e-02, -2.4929e-02, -2.7400e-02],\n                        [-2.6303e-05,  2.4616e-02, -1.2550e-02]],\n              \n                       ...,\n              \n                       [[-1.1245e-02, -6.3400e-03, -1.4372e-02],\n                        [-2.6327e-02, -9.7659e-03, -1.9709e-03],\n                        [-2.4333e-03,  5.2920e-03,  1.3149e-02]],\n              \n                       [[ 2.8700e-03,  7.3612e-03,  2.3691e-03],\n                        [-2.7523e-02,  1.5241e-02,  1.3450e-02],\n                        [ 2.5740e-03, -3.4698e-03, -1.3424e-02]],\n              \n                       [[-1.4515e-02, -2.1749e-02,  1.3343e-02],\n                        [ 2.5754e-02,  3.5074e-03,  1.9747e-02],\n                        [ 2.7382e-03,  1.4910e-02, -2.2954e-02]]],\n              \n              \n                      [[[-4.3458e-03, -1.3681e-02,  1.8517e-02],\n                        [-1.4100e-02,  2.4556e-02, -1.6581e-03],\n                        [-2.7384e-02,  1.7085e-02,  1.9694e-02]],\n              \n                       [[ 5.4223e-03, -1.7057e-02, -6.0624e-03],\n                        [ 2.8144e-02, -1.2404e-02, -9.2200e-05],\n                        [ 8.0187e-03, -2.4534e-02, -6.1641e-03]],\n              \n                       [[ 4.4628e-03, -2.3212e-02,  1.8625e-02],\n                        [ 2.0626e-03, -1.1065e-02,  2.2116e-02],\n                        [-2.3691e-02,  7.7271e-03,  2.3667e-02]],\n              \n                       ...,\n              \n                       [[ 1.6437e-02,  1.7844e-02,  4.2858e-03],\n                        [ 1.8507e-02, -1.4175e-02,  6.2452e-03],\n                        [-2.2591e-02, -1.6163e-02,  2.8446e-02]],\n              \n                       [[ 7.0578e-03,  8.5772e-03,  1.2336e-03],\n                        [-2.7270e-02, -4.7153e-03,  1.8364e-02],\n                        [-1.7723e-02, -6.1744e-03, -2.6519e-02]],\n              \n                       [[ 2.6981e-03,  2.3110e-02, -1.9544e-02],\n                        [ 2.8593e-02,  2.6731e-02,  2.1887e-02],\n                        [-9.6571e-04,  1.7459e-02,  3.4465e-03]]]])),\n             ('up4.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.1.num_batches_tracked', tensor(0)),\n             ('up4.conv.double_conv.3.weight',\n              tensor([[[[ 3.1426e-03, -3.7804e-02, -1.9636e-03],\n                        [-3.3168e-02,  2.4599e-03, -2.5361e-02],\n                        [ 2.0291e-02, -3.1659e-02, -2.2596e-02]],\n              \n                       [[-8.4917e-03, -3.0465e-04, -2.1817e-02],\n                        [ 2.9646e-03,  2.4069e-02, -2.6871e-02],\n                        [ 2.7976e-02, -2.9426e-02, -1.9063e-02]],\n              \n                       [[ 3.4714e-02,  2.5515e-02,  2.2645e-03],\n                        [ 1.1169e-02, -1.5637e-02, -3.2919e-02],\n                        [-1.3760e-02,  1.0523e-03,  3.2319e-02]],\n              \n                       ...,\n              \n                       [[-2.6632e-02,  1.5643e-02, -3.1304e-03],\n                        [-6.5018e-03,  1.7912e-02, -1.7220e-02],\n                        [ 3.1036e-02,  3.4784e-02, -1.4025e-02]],\n              \n                       [[ 3.3626e-02, -2.4100e-02,  3.6708e-02],\n                        [-2.1758e-02, -1.4161e-02, -2.8572e-02],\n                        [ 5.2657e-03,  2.2184e-02, -1.2249e-02]],\n              \n                       [[ 3.9889e-02, -9.9724e-03,  1.4062e-03],\n                        [ 1.6991e-02, -5.8726e-03, -1.2741e-02],\n                        [-2.3483e-02,  3.6793e-02,  1.0728e-03]]],\n              \n              \n                      [[[-1.1431e-02,  2.8004e-03, -2.1472e-02],\n                        [-4.7250e-03,  3.1195e-02, -3.4145e-02],\n                        [-3.9074e-02, -9.0451e-03,  3.6595e-02]],\n              \n                       [[-3.4954e-02, -2.8686e-02,  7.4445e-03],\n                        [-3.4594e-02, -1.5361e-02,  3.2916e-02],\n                        [ 7.3619e-03, -2.8733e-02, -2.8171e-02]],\n              \n                       [[-1.6132e-02,  9.1593e-03, -1.5983e-03],\n                        [ 1.9147e-02, -3.0231e-02,  3.5481e-02],\n                        [-2.8131e-02, -1.5797e-02,  1.4560e-02]],\n              \n                       ...,\n              \n                       [[-2.0996e-03, -2.3411e-02, -1.1860e-02],\n                        [ 3.8093e-02,  3.5264e-02,  3.0247e-02],\n                        [ 1.3708e-02, -2.7209e-02,  3.5293e-02]],\n              \n                       [[-1.4823e-02, -1.3127e-02, -1.8602e-02],\n                        [ 3.1382e-02, -2.8936e-02, -3.5547e-02],\n                        [ 2.8250e-02,  2.5477e-02, -1.1684e-02]],\n              \n                       [[-3.4762e-03, -2.8827e-02,  2.2720e-02],\n                        [ 1.9048e-02,  1.9151e-02,  4.8282e-03],\n                        [ 3.6979e-02,  1.1263e-02,  1.4983e-02]]],\n              \n              \n                      [[[ 4.0528e-02, -1.5267e-02,  4.1640e-02],\n                        [ 1.4580e-02,  2.1254e-03,  2.1454e-02],\n                        [ 2.3367e-02,  2.4535e-02, -2.9547e-02]],\n              \n                       [[ 1.2478e-02, -3.2175e-02,  3.1261e-02],\n                        [-2.5070e-02,  1.0443e-02, -1.7667e-02],\n                        [-3.9835e-03, -1.4524e-02,  2.9181e-02]],\n              \n                       [[ 8.7496e-03,  1.6791e-02, -3.3366e-02],\n                        [ 3.9007e-02,  1.0403e-02,  3.8254e-02],\n                        [-1.2029e-02,  1.1168e-02, -1.9442e-02]],\n              \n                       ...,\n              \n                       [[ 2.2030e-02,  1.0903e-02, -1.4863e-02],\n                        [-1.3346e-02, -3.5193e-02,  3.2643e-02],\n                        [-3.8632e-02, -8.3370e-03,  1.8904e-02]],\n              \n                       [[-3.9616e-02, -2.5855e-02,  3.3651e-02],\n                        [ 3.9193e-02,  2.7768e-02,  1.4065e-02],\n                        [-8.8412e-03, -2.1744e-02, -2.0466e-02]],\n              \n                       [[-9.5175e-03, -3.2115e-02,  2.8135e-02],\n                        [-3.5135e-02, -3.5658e-02, -1.6859e-02],\n                        [ 3.8371e-02,  4.0490e-03,  2.5179e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6391e-02,  5.2747e-03,  3.4211e-02],\n                        [-3.6951e-02, -2.0392e-02,  1.9124e-02],\n                        [-4.0592e-03, -2.1158e-02, -5.6858e-03]],\n              \n                       [[-1.2450e-02, -7.7264e-03, -2.7716e-02],\n                        [ 3.4721e-02,  2.8399e-02,  3.7686e-02],\n                        [ 3.6166e-02,  1.7743e-02, -3.3313e-02]],\n              \n                       [[-2.4009e-03,  2.7938e-02,  8.2821e-03],\n                        [-1.0567e-02, -1.0721e-02,  3.9096e-02],\n                        [-1.0329e-02,  3.5188e-04,  1.9992e-02]],\n              \n                       ...,\n              \n                       [[ 4.0091e-02,  2.7190e-02, -3.8786e-02],\n                        [ 3.7762e-02,  1.6390e-02, -4.1539e-02],\n                        [ 2.8608e-02, -3.4842e-02, -1.5290e-02]],\n              \n                       [[ 2.5458e-02,  3.8800e-02,  1.8157e-02],\n                        [-3.0404e-02, -2.8858e-02, -3.7904e-02],\n                        [-1.7384e-02,  1.3624e-02, -3.8238e-02]],\n              \n                       [[-3.4968e-02, -2.1631e-02,  1.8572e-02],\n                        [ 3.9958e-02,  3.1534e-02, -2.6919e-03],\n                        [ 2.9025e-02, -2.5323e-02,  1.8108e-02]]],\n              \n              \n                      [[[ 1.4118e-02,  1.3075e-02,  7.9425e-04],\n                        [-1.5709e-02,  2.2579e-02, -3.4406e-03],\n                        [ 3.9156e-02, -5.3889e-03, -4.1343e-02]],\n              \n                       [[-1.1825e-03, -7.4790e-03,  3.0482e-02],\n                        [-4.0314e-02, -1.9415e-02, -5.4573e-05],\n                        [-3.6205e-03, -4.0538e-02,  1.6526e-02]],\n              \n                       [[ 3.1517e-02,  1.2538e-02,  1.7676e-03],\n                        [ 2.2461e-02, -2.9065e-02,  3.1906e-02],\n                        [-3.9866e-02, -2.3473e-02,  4.0793e-02]],\n              \n                       ...,\n              \n                       [[-2.2015e-02, -1.4035e-03, -3.4191e-02],\n                        [ 3.4649e-02,  2.7996e-02,  2.5186e-02],\n                        [-2.6122e-02, -3.7787e-02, -3.5784e-02]],\n              \n                       [[-3.5926e-03, -1.5855e-02, -2.4558e-02],\n                        [-3.5714e-02,  4.0327e-02,  3.9204e-02],\n                        [ 1.6102e-03, -2.2671e-02,  3.9940e-02]],\n              \n                       [[-4.1120e-02,  6.4742e-03,  1.8772e-02],\n                        [ 3.4173e-02,  5.7441e-04, -1.9311e-02],\n                        [-1.4727e-02,  1.7990e-02, -1.8958e-02]]],\n              \n              \n                      [[[ 2.9624e-02, -8.9972e-03,  4.0076e-02],\n                        [ 1.4882e-02, -1.9439e-02,  8.6693e-03],\n                        [-4.0603e-02,  1.5571e-02, -2.9153e-02]],\n              \n                       [[-3.5557e-02,  1.8946e-04,  2.2721e-02],\n                        [ 2.9935e-03,  8.9930e-03, -2.0757e-02],\n                        [ 2.0412e-02,  5.7608e-03,  2.6245e-02]],\n              \n                       [[-6.2162e-03, -7.0439e-04,  1.3922e-02],\n                        [-9.8026e-03,  2.8211e-02, -3.7612e-03],\n                        [-3.1022e-02, -2.4241e-02,  2.0704e-03]],\n              \n                       ...,\n              \n                       [[ 1.8656e-05, -3.5449e-02, -1.9142e-02],\n                        [-3.7448e-02, -3.8316e-02,  3.6445e-02],\n                        [ 1.8268e-02, -3.2087e-02, -3.0568e-02]],\n              \n                       [[-2.6703e-02, -7.0255e-04,  1.3062e-02],\n                        [ 9.2566e-03,  3.0957e-02, -3.9456e-02],\n                        [ 2.6741e-02,  1.7924e-02,  2.6267e-02]],\n              \n                       [[-3.0110e-02, -1.6314e-03, -2.8098e-02],\n                        [ 2.0860e-02,  1.5562e-02,  2.9175e-02],\n                        [ 9.1814e-03,  2.6883e-02,  2.8830e-02]]]])),\n             ('up4.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('up4.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('up4.conv.double_conv.4.num_batches_tracked', tensor(0)),\n             ('outc.conv.weight',\n              tensor([[[[ 0.0984]],\n              \n                       [[-0.0668]],\n              \n                       [[-0.0782]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0089]],\n              \n                       [[-0.0501]],\n              \n                       [[-0.0261]],\n              \n                       [[ 0.0791]],\n              \n                       [[-0.1128]],\n              \n                       [[ 0.0102]],\n              \n                       [[ 0.0258]],\n              \n                       [[-0.0357]],\n              \n                       [[-0.0674]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.0549]],\n              \n                       [[-0.0972]],\n              \n                       [[-0.1207]],\n              \n                       [[ 0.1104]],\n              \n                       [[ 0.0293]],\n              \n                       [[-0.1182]],\n              \n                       [[ 0.1166]],\n              \n                       [[ 0.1038]],\n              \n                       [[-0.0085]],\n              \n                       [[-0.0039]],\n              \n                       [[ 0.0621]],\n              \n                       [[ 0.0331]],\n              \n                       [[ 0.0618]],\n              \n                       [[ 0.0310]],\n              \n                       [[ 0.1245]],\n              \n                       [[-0.1027]],\n              \n                       [[ 0.0523]],\n              \n                       [[ 0.0731]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0495]],\n              \n                       [[ 0.1218]],\n              \n                       [[ 0.1106]],\n              \n                       [[ 0.0079]],\n              \n                       [[-0.1117]],\n              \n                       [[ 0.1123]],\n              \n                       [[-0.0453]],\n              \n                       [[ 0.0750]],\n              \n                       [[ 0.0378]],\n              \n                       [[ 0.1220]],\n              \n                       [[-0.1052]],\n              \n                       [[-0.0909]],\n              \n                       [[-0.0841]],\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0207]],\n              \n                       [[-0.0161]],\n              \n                       [[-0.0815]],\n              \n                       [[ 0.0737]],\n              \n                       [[-0.0565]],\n              \n                       [[-0.0620]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.1087]],\n              \n                       [[ 0.0442]],\n              \n                       [[-0.0377]],\n              \n                       [[-0.0474]],\n              \n                       [[ 0.0807]],\n              \n                       [[ 0.0298]],\n              \n                       [[ 0.0700]],\n              \n                       [[ 0.0749]],\n              \n                       [[ 0.0847]],\n              \n                       [[-0.1145]]]])),\n             ('outc.conv.bias', tensor([-0.0712]))])</pre> In\u00a0[31]: Copied! <pre>## \u200b\u591a\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u3002\u200b\u6ce8\u610f\u200b\u6a21\u578b\u200b\u5c42\u200b\u540d\u79f0\u200b\u524d\u591a\u200b\u4e86\u200bmodule\n## \u200b\u4e0d\u200b\u5efa\u8bae\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200bGPU_id\u200b\u7b49\u200b\u4fe1\u606f\u200b\u548c\u200b\u8bfb\u53d6\u200b\u540e\u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b\u53ef\u80fd\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u8981\u200b\u628a\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea4\u7ed9\u200b\u53e6\u200b\u4e00\u200b\u7528\u6237\u200b\u4f7f\u7528\u200b\u7684\u200b\u60c5\u51b5\u200b\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\nunet_mul = copy.deepcopy(unet)\nunet_mul = nn.DataParallel(unet_mul).cuda()\nunet_mul\n</pre> ## \u200b\u591a\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u3002\u200b\u6ce8\u610f\u200b\u6a21\u578b\u200b\u5c42\u200b\u540d\u79f0\u200b\u524d\u591a\u200b\u4e86\u200bmodule ## \u200b\u4e0d\u200b\u5efa\u8bae\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200bGPU_id\u200b\u7b49\u200b\u4fe1\u606f\u200b\u548c\u200b\u8bfb\u53d6\u200b\u540e\u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b\u53ef\u80fd\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u8981\u200b\u628a\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea4\u7ed9\u200b\u53e6\u200b\u4e00\u200b\u7528\u6237\u200b\u4f7f\u7528\u200b\u7684\u200b\u60c5\u51b5\u200b os.environ['CUDA_VISIBLE_DEVICES'] = '2,3' unet_mul = copy.deepcopy(unet) unet_mul = nn.DataParallel(unet_mul).cuda() unet_mul Out[31]: <pre>DataParallel(\n  (module): UNet(\n    (inc): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (down1): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down2): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down3): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down4): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (up1): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up2): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up3): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up4): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (outc): OutConv(\n      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)</pre> In\u00a0[32]: Copied! <pre>torch.save(unet_mul, \"./unet_mul_example.pth\")\nloaded_unet_mul = torch.load(\"./unet_mul_example.pth\")\nloaded_unet_mul\n</pre> torch.save(unet_mul, \"./unet_mul_example.pth\") loaded_unet_mul = torch.load(\"./unet_mul_example.pth\") loaded_unet_mul Out[32]: <pre>DataParallel(\n  (module): UNet(\n    (inc): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (down1): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down2): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down3): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down4): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (up1): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up2): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up3): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up4): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (outc): OutConv(\n      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)</pre> In\u00a0[33]: Copied! <pre>## \u200b\u591a\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u3002\ntorch.save(unet_mul.state_dict(), \"./unet_weight_mul_example.pth\")\nloaded_unet_weights_mul = torch.load(\"./unet_weight_mul_example.pth\")\nunet_mul.load_state_dict(loaded_unet_weights_mul)\nunet_mul = nn.DataParallel(unet_mul).cuda()\nunet_mul.state_dict()\n</pre> ## \u200b\u591a\u5361\u200b\uff1a\u200b\u4fdd\u5b58\u200b&amp;\u200b\u8bfb\u53d6\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u3002 torch.save(unet_mul.state_dict(), \"./unet_weight_mul_example.pth\") loaded_unet_weights_mul = torch.load(\"./unet_weight_mul_example.pth\") unet_mul.load_state_dict(loaded_unet_weights_mul) unet_mul = nn.DataParallel(unet_mul).cuda() unet_mul.state_dict() Out[33]: <pre>OrderedDict([('module.module.inc.double_conv.0.weight',\n              tensor([[[[-0.1569, -0.0516,  0.1381],\n                        [-0.0167,  0.1114, -0.1482],\n                        [-0.1659, -0.0492, -0.1526]],\n              \n                       [[ 0.0871,  0.1102, -0.1270],\n                        [ 0.1058,  0.0541, -0.0767],\n                        [ 0.1247,  0.1813,  0.1895]],\n              \n                       [[ 0.0929, -0.1305,  0.0531],\n                        [-0.0972, -0.1668, -0.0183],\n                        [-0.1754, -0.0862,  0.0373]]],\n              \n              \n                      [[[-0.0014,  0.1440, -0.0519],\n                        [ 0.1643,  0.1829,  0.1713],\n                        [-0.0702, -0.0426,  0.0083]],\n              \n                       [[ 0.1057,  0.0303,  0.0280],\n                        [-0.0306, -0.0898,  0.1635],\n                        [-0.1388, -0.0430,  0.0839]],\n              \n                       [[ 0.0840,  0.1753,  0.0916],\n                        [ 0.0819,  0.1624,  0.1901],\n                        [ 0.1914,  0.0483, -0.0875]]],\n              \n              \n                      [[[ 0.1197, -0.1618, -0.1778],\n                        [ 0.0866, -0.0638, -0.1615],\n                        [ 0.1437, -0.1523, -0.1007]],\n              \n                       [[-0.1395, -0.0602, -0.0457],\n                        [ 0.0582, -0.1701,  0.0586],\n                        [-0.1828,  0.0463,  0.1460]],\n              \n                       [[ 0.0735,  0.0299, -0.0629],\n                        [-0.0345, -0.0038,  0.0794],\n                        [-0.0958, -0.1519, -0.0411]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1095,  0.0703, -0.0860],\n                        [-0.1243, -0.0596, -0.1636],\n                        [ 0.0819,  0.0457,  0.1248]],\n              \n                       [[-0.1077, -0.1394,  0.0295],\n                        [ 0.1442, -0.1271,  0.1462],\n                        [-0.1011,  0.1301, -0.1294]],\n              \n                       [[-0.1653, -0.1431, -0.1031],\n                        [ 0.0511,  0.1370,  0.0210],\n                        [-0.1709,  0.0438, -0.0352]]],\n              \n              \n                      [[[-0.0893,  0.1826, -0.0856],\n                        [-0.1679,  0.0620,  0.1056],\n                        [-0.0206, -0.1745, -0.0500]],\n              \n                       [[ 0.0784,  0.0502,  0.1084],\n                        [-0.0746, -0.1213,  0.0849],\n                        [-0.1682, -0.1131, -0.1769]],\n              \n                       [[ 0.1111, -0.0814,  0.1804],\n                        [-0.0183,  0.0950, -0.0082],\n                        [-0.0761, -0.0757, -0.1657]]],\n              \n              \n                      [[[ 0.0543, -0.0157, -0.1387],\n                        [ 0.1503,  0.1388,  0.0653],\n                        [ 0.1474, -0.0991, -0.1478]],\n              \n                       [[ 0.0953, -0.1215,  0.1848],\n                        [-0.0360,  0.0052, -0.1841],\n                        [-0.1859, -0.0946,  0.1727]],\n              \n                       [[-0.0668, -0.0142,  0.1517],\n                        [-0.1101,  0.0217, -0.1021],\n                        [-0.1509,  0.0912,  0.1346]]]], device='cuda:0')),\n             ('module.module.inc.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.inc.double_conv.3.weight',\n              tensor([[[[-4.1079e-02,  2.4625e-02, -5.8618e-03],\n                        [-3.6583e-02, -1.7239e-02,  2.4723e-02],\n                        [-2.0914e-03,  3.0168e-02, -2.0448e-02]],\n              \n                       [[ 4.1381e-03, -2.0328e-02, -2.9454e-02],\n                        [ 1.0681e-02, -3.6947e-02, -1.4246e-02],\n                        [-3.8679e-03,  2.3515e-02,  7.0796e-03]],\n              \n                       [[-3.3515e-02,  2.3345e-02, -5.7584e-04],\n                        [ 3.0752e-02, -3.5342e-02, -3.0192e-02],\n                        [ 3.0137e-02,  4.9735e-03,  3.0268e-02]],\n              \n                       ...,\n              \n                       [[ 2.6247e-02,  3.5036e-02, -2.7703e-02],\n                        [ 1.2037e-02, -1.1631e-02, -3.5691e-02],\n                        [ 1.8343e-02,  2.3172e-02, -2.3284e-02]],\n              \n                       [[ 3.9720e-02, -2.9578e-02, -3.8113e-02],\n                        [ 6.7576e-04, -4.0048e-02, -6.3216e-05],\n                        [ 1.9008e-02,  3.8545e-02,  3.0812e-02]],\n              \n                       [[-6.7981e-03, -1.5902e-03,  3.7965e-02],\n                        [ 8.6753e-03, -1.4569e-03, -1.9033e-02],\n                        [-2.0683e-02, -2.7206e-02,  2.5007e-02]]],\n              \n              \n                      [[[-1.3453e-02,  4.8410e-03,  6.3604e-03],\n                        [ 1.4860e-02, -1.9902e-04, -3.7245e-02],\n                        [ 1.2965e-02,  9.0473e-03,  2.3664e-02]],\n              \n                       [[-3.6142e-02, -2.9932e-02, -2.7691e-02],\n                        [ 2.6747e-02,  2.1051e-02, -6.9610e-03],\n                        [ 1.6672e-02,  2.4121e-02,  3.9934e-02]],\n              \n                       [[ 1.8793e-02,  3.8492e-02, -1.8463e-02],\n                        [ 2.4193e-02,  1.2931e-02, -2.9170e-02],\n                        [-2.2503e-02,  7.4183e-03, -9.9386e-03]],\n              \n                       ...,\n              \n                       [[-3.5583e-02,  1.0415e-02,  2.6884e-03],\n                        [-2.4120e-02, -1.6516e-02, -3.5117e-02],\n                        [-1.1389e-02, -3.2349e-02, -5.4190e-03]],\n              \n                       [[ 1.0794e-02, -1.4699e-02, -3.9218e-02],\n                        [ 7.2620e-03,  2.3942e-02, -9.0866e-03],\n                        [-3.9156e-02, -2.2665e-02,  3.0706e-02]],\n              \n                       [[ 2.5315e-02,  3.8635e-02, -1.4174e-03],\n                        [ 4.2061e-03, -3.3006e-02, -2.6736e-02],\n                        [-1.2201e-02,  2.4348e-02, -2.8096e-02]]],\n              \n              \n                      [[[-2.9801e-02,  1.3935e-02, -2.9342e-02],\n                        [-4.2913e-03,  9.5715e-03,  3.7494e-02],\n                        [ 2.2639e-02,  1.3474e-02,  2.3872e-02]],\n              \n                       [[ 1.6016e-03,  2.9424e-02,  2.3341e-02],\n                        [-1.2055e-02, -3.9560e-02, -1.5007e-02],\n                        [ 2.5384e-02, -4.1246e-02,  2.9730e-02]],\n              \n                       [[ 2.2965e-02, -2.7511e-02, -1.2306e-02],\n                        [-1.4792e-02,  2.7210e-03, -3.1689e-02],\n                        [ 3.1452e-02, -2.1154e-02,  3.2495e-02]],\n              \n                       ...,\n              \n                       [[ 6.1211e-03, -1.7085e-03,  1.0614e-02],\n                        [-1.3250e-03,  2.0869e-02,  7.6367e-03],\n                        [-3.3447e-02, -3.5193e-02, -3.4296e-02]],\n              \n                       [[ 2.6182e-02, -9.0026e-03,  4.3130e-03],\n                        [-1.9488e-02,  3.6438e-02, -2.9620e-02],\n                        [-4.0476e-02,  8.5702e-03,  2.2612e-02]],\n              \n                       [[ 1.9338e-03, -1.3990e-02,  8.3609e-03],\n                        [-1.3580e-02, -3.6543e-02,  2.8900e-02],\n                        [ 2.8948e-02, -2.2145e-03, -2.4276e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.0462e-03,  3.9649e-02,  1.0557e-02],\n                        [ 3.1926e-02,  3.8248e-02,  9.8494e-03],\n                        [ 1.2289e-03, -1.9980e-02, -3.3557e-02]],\n              \n                       [[-4.0275e-02,  1.1621e-02,  1.1366e-02],\n                        [-1.9881e-02,  6.3696e-03,  4.0948e-02],\n                        [-1.5219e-02, -1.6628e-02,  2.8343e-03]],\n              \n                       [[ 2.7490e-02,  3.5501e-02,  3.2039e-02],\n                        [ 3.5091e-03,  1.1285e-02,  1.5338e-02],\n                        [ 1.9410e-02, -5.1183e-03, -2.9545e-02]],\n              \n                       ...,\n              \n                       [[-2.0173e-02,  3.1788e-02,  8.5245e-03],\n                        [ 1.2969e-02,  1.4843e-02,  1.5726e-02],\n                        [ 3.1018e-02, -2.0554e-02,  1.6326e-02]],\n              \n                       [[-3.5004e-02,  3.6636e-02,  5.2004e-03],\n                        [ 2.9926e-02,  3.7449e-02,  6.1300e-04],\n                        [-5.1867e-04, -4.0083e-02, -3.0298e-02]],\n              \n                       [[-1.5009e-02,  4.1003e-02,  7.9811e-03],\n                        [ 6.5824e-03, -2.2011e-02,  8.9981e-03],\n                        [ 1.5385e-02, -3.9503e-02,  4.1086e-02]]],\n              \n              \n                      [[[-2.8993e-02, -3.7376e-02,  1.1231e-02],\n                        [ 1.7329e-02, -5.8507e-03,  1.9821e-02],\n                        [ 2.0648e-02, -3.9886e-02,  1.6316e-02]],\n              \n                       [[ 3.2519e-02,  1.6676e-02,  1.2690e-03],\n                        [ 1.6236e-03,  4.4074e-03, -2.0494e-02],\n                        [-3.6117e-02,  1.2012e-02, -2.8950e-02]],\n              \n                       [[-3.4818e-02, -1.8692e-02, -6.5148e-03],\n                        [-3.8199e-02, -2.1533e-03, -2.6669e-02],\n                        [ 2.0359e-03, -1.0877e-02,  3.2552e-02]],\n              \n                       ...,\n              \n                       [[ 2.6173e-03, -3.7495e-02,  8.6743e-03],\n                        [ 4.8354e-04,  4.1075e-02, -6.5880e-03],\n                        [ 3.3915e-02,  3.9410e-03, -1.2893e-02]],\n              \n                       [[ 2.6528e-02, -4.0759e-02,  1.9229e-02],\n                        [ 2.2432e-02, -3.9180e-03,  2.6232e-02],\n                        [ 1.2603e-02, -3.1149e-03, -1.4234e-02]],\n              \n                       [[-2.9655e-03,  1.3039e-03, -2.7197e-02],\n                        [ 3.9957e-02, -1.5892e-02,  2.0109e-02],\n                        [ 1.4106e-03,  6.4586e-04,  8.9162e-03]]],\n              \n              \n                      [[[ 3.1019e-02,  3.9165e-02, -2.7102e-02],\n                        [-3.8747e-02, -2.9976e-02, -8.2251e-04],\n                        [ 3.1431e-02, -9.7356e-03,  1.1533e-02]],\n              \n                       [[-8.6869e-03,  3.6680e-02,  1.8349e-02],\n                        [-3.1113e-02, -2.5772e-02, -1.2013e-02],\n                        [ 2.4810e-02,  2.1669e-02, -3.3620e-02]],\n              \n                       [[-3.0419e-02,  7.3520e-03, -1.9823e-02],\n                        [ 3.8660e-02,  2.6089e-02,  3.0254e-02],\n                        [ 1.4994e-02,  1.0452e-02,  3.4261e-02]],\n              \n                       ...,\n              \n                       [[-3.2601e-02, -3.6214e-02,  3.6512e-02],\n                        [-3.7527e-02, -2.9699e-02,  1.5305e-02],\n                        [-2.4764e-02,  2.2672e-02,  2.2486e-02]],\n              \n                       [[ 1.1033e-02,  3.0824e-02,  2.4714e-02],\n                        [-2.1154e-02,  2.5543e-02,  1.0087e-02],\n                        [ 2.3082e-02, -3.0461e-02,  3.4150e-02]],\n              \n                       [[-1.8519e-02, -7.6047e-03,  2.7975e-02],\n                        [-6.4077e-03, -2.6562e-02,  9.9592e-03],\n                        [-2.9076e-02, -2.5703e-02, -2.9623e-02]]]], device='cuda:0')),\n             ('module.module.inc.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[ 0.0357, -0.0264,  0.0201],\n                        [ 0.0235, -0.0205,  0.0169],\n                        [ 0.0325, -0.0087, -0.0301]],\n              \n                       [[-0.0252,  0.0130,  0.0105],\n                        [ 0.0278,  0.0094, -0.0272],\n                        [ 0.0324,  0.0047,  0.0045]],\n              \n                       [[-0.0352, -0.0399, -0.0170],\n                        [ 0.0144,  0.0158, -0.0144],\n                        [-0.0233,  0.0018, -0.0334]],\n              \n                       ...,\n              \n                       [[ 0.0116, -0.0235, -0.0296],\n                        [-0.0242,  0.0119,  0.0299],\n                        [ 0.0114,  0.0182,  0.0288]],\n              \n                       [[-0.0316, -0.0088, -0.0152],\n                        [-0.0325, -0.0183, -0.0030],\n                        [-0.0355, -0.0339,  0.0363]],\n              \n                       [[-0.0135,  0.0221,  0.0305],\n                        [-0.0268,  0.0040, -0.0396],\n                        [-0.0201,  0.0218, -0.0349]]],\n              \n              \n                      [[[ 0.0126,  0.0043, -0.0306],\n                        [-0.0146,  0.0352,  0.0244],\n                        [ 0.0250,  0.0273,  0.0250]],\n              \n                       [[-0.0412,  0.0087,  0.0332],\n                        [ 0.0187, -0.0076, -0.0089],\n                        [-0.0151, -0.0058, -0.0293]],\n              \n                       [[-0.0167, -0.0200,  0.0142],\n                        [-0.0356,  0.0294,  0.0118],\n                        [-0.0244, -0.0215,  0.0074]],\n              \n                       ...,\n              \n                       [[-0.0035,  0.0137, -0.0314],\n                        [ 0.0138, -0.0057,  0.0048],\n                        [ 0.0214, -0.0232, -0.0108]],\n              \n                       [[-0.0412, -0.0090, -0.0090],\n                        [-0.0287,  0.0126,  0.0135],\n                        [ 0.0138,  0.0354, -0.0151]],\n              \n                       [[ 0.0006, -0.0026,  0.0229],\n                        [ 0.0340,  0.0215,  0.0193],\n                        [-0.0062,  0.0044,  0.0232]]],\n              \n              \n                      [[[ 0.0393,  0.0131, -0.0272],\n                        [-0.0268, -0.0212,  0.0276],\n                        [-0.0300,  0.0367, -0.0406]],\n              \n                       [[ 0.0010, -0.0226, -0.0340],\n                        [ 0.0188,  0.0097, -0.0116],\n                        [ 0.0346, -0.0155,  0.0074]],\n              \n                       [[ 0.0277, -0.0405,  0.0331],\n                        [ 0.0064,  0.0333,  0.0368],\n                        [ 0.0375,  0.0212, -0.0242]],\n              \n                       ...,\n              \n                       [[-0.0069,  0.0186, -0.0329],\n                        [ 0.0099, -0.0293,  0.0133],\n                        [ 0.0385,  0.0099,  0.0152]],\n              \n                       [[ 0.0165,  0.0133,  0.0077],\n                        [-0.0347, -0.0064,  0.0321],\n                        [-0.0038, -0.0347,  0.0405]],\n              \n                       [[ 0.0055, -0.0044, -0.0135],\n                        [ 0.0195,  0.0027,  0.0329],\n                        [-0.0107,  0.0344, -0.0313]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0298, -0.0407, -0.0166],\n                        [-0.0002, -0.0221,  0.0067],\n                        [ 0.0178,  0.0013, -0.0193]],\n              \n                       [[-0.0238,  0.0293,  0.0269],\n                        [ 0.0277,  0.0384,  0.0140],\n                        [-0.0363, -0.0101,  0.0253]],\n              \n                       [[ 0.0334, -0.0225, -0.0067],\n                        [-0.0341,  0.0260, -0.0054],\n                        [ 0.0118,  0.0148,  0.0336]],\n              \n                       ...,\n              \n                       [[-0.0390,  0.0067, -0.0146],\n                        [-0.0058, -0.0076,  0.0248],\n                        [-0.0309, -0.0162, -0.0044]],\n              \n                       [[ 0.0156,  0.0133, -0.0077],\n                        [-0.0084, -0.0258,  0.0351],\n                        [ 0.0133, -0.0063,  0.0344]],\n              \n                       [[ 0.0333,  0.0093, -0.0372],\n                        [-0.0002,  0.0405, -0.0157],\n                        [-0.0018, -0.0008,  0.0080]]],\n              \n              \n                      [[[ 0.0330, -0.0097, -0.0083],\n                        [-0.0216,  0.0057, -0.0085],\n                        [ 0.0082,  0.0023,  0.0381]],\n              \n                       [[-0.0320,  0.0131, -0.0137],\n                        [-0.0037,  0.0201, -0.0339],\n                        [ 0.0327,  0.0375, -0.0072]],\n              \n                       [[-0.0085, -0.0173,  0.0102],\n                        [ 0.0381,  0.0038,  0.0299],\n                        [ 0.0261,  0.0366,  0.0206]],\n              \n                       ...,\n              \n                       [[-0.0330, -0.0098, -0.0026],\n                        [ 0.0038,  0.0086,  0.0258],\n                        [-0.0036,  0.0356, -0.0383]],\n              \n                       [[ 0.0014,  0.0289, -0.0069],\n                        [-0.0358, -0.0261, -0.0318],\n                        [-0.0223, -0.0333,  0.0221]],\n              \n                       [[ 0.0099, -0.0044,  0.0356],\n                        [-0.0416,  0.0245,  0.0219],\n                        [-0.0125, -0.0308, -0.0395]]],\n              \n              \n                      [[[-0.0059, -0.0348, -0.0104],\n                        [-0.0281, -0.0408,  0.0101],\n                        [-0.0012,  0.0124, -0.0115]],\n              \n                       [[-0.0382, -0.0336,  0.0156],\n                        [-0.0337,  0.0008,  0.0405],\n                        [-0.0058, -0.0384, -0.0303]],\n              \n                       [[-0.0357,  0.0154,  0.0037],\n                        [ 0.0079,  0.0382, -0.0023],\n                        [-0.0099,  0.0091, -0.0170]],\n              \n                       ...,\n              \n                       [[-0.0194,  0.0131, -0.0097],\n                        [-0.0112, -0.0016, -0.0009],\n                        [-0.0198, -0.0326, -0.0109]],\n              \n                       [[ 0.0248, -0.0348, -0.0202],\n                        [-0.0041, -0.0386, -0.0109],\n                        [-0.0228, -0.0399,  0.0372]],\n              \n                       [[-0.0010, -0.0073,  0.0204],\n                        [-0.0288,  0.0141,  0.0010],\n                        [-0.0160, -0.0138,  0.0360]]]], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.1305e-02, -1.2684e-03,  2.4892e-02],\n                        [-2.6919e-02, -1.1080e-02,  6.1028e-04],\n                        [-6.9626e-03,  2.4179e-02,  7.0370e-03]],\n              \n                       [[-8.0535e-03, -1.8495e-04, -2.7226e-02],\n                        [-1.6500e-02,  3.6307e-03,  2.3883e-02],\n                        [-7.6892e-03,  2.6147e-02,  1.8880e-02]],\n              \n                       [[-6.3356e-04, -7.4601e-03, -7.9877e-03],\n                        [ 1.3430e-02, -1.9490e-02,  3.8737e-03],\n                        [-1.6122e-02, -1.8464e-02,  2.0742e-02]],\n              \n                       ...,\n              \n                       [[ 1.8362e-03, -1.1564e-02, -2.8767e-02],\n                        [ 5.5608e-03,  6.5534e-03,  1.5489e-02],\n                        [-1.3676e-02, -2.4228e-02,  1.2859e-02]],\n              \n                       [[ 1.7046e-02,  3.1059e-03, -1.3043e-02],\n                        [-1.1144e-02,  8.5697e-03, -9.9781e-03],\n                        [ 6.2510e-03, -2.7031e-02, -8.6106e-03]],\n              \n                       [[ 2.8901e-02,  1.9356e-02, -2.5723e-02],\n                        [-2.0941e-02,  1.2509e-02,  2.8496e-02],\n                        [-1.6640e-02, -3.5848e-03, -1.0853e-02]]],\n              \n              \n                      [[[ 1.2726e-02, -1.6195e-02,  1.4709e-02],\n                        [-2.0562e-02, -2.8356e-02,  1.0373e-02],\n                        [ 1.6941e-02, -1.7723e-02,  2.5551e-02]],\n              \n                       [[-1.9462e-02,  2.7471e-02, -1.6930e-02],\n                        [-2.7676e-03, -1.4025e-03,  1.7487e-02],\n                        [ 1.6080e-02,  2.9447e-02, -1.8378e-02]],\n              \n                       [[ 2.8415e-03, -1.0617e-02, -1.0754e-03],\n                        [ 2.2315e-02, -1.2144e-02, -1.7454e-02],\n                        [-2.4725e-02, -1.4872e-02,  1.2383e-02]],\n              \n                       ...,\n              \n                       [[ 2.1383e-02, -2.6270e-02, -1.2159e-02],\n                        [-2.1438e-02, -2.4603e-02, -1.3974e-02],\n                        [-2.2166e-02,  2.9069e-02,  1.0996e-02]],\n              \n                       [[ 2.6262e-02, -3.3151e-03,  2.6866e-02],\n                        [-1.1902e-02,  2.3779e-03,  2.6081e-02],\n                        [ 5.4771e-03,  7.5126e-04, -8.3137e-03]],\n              \n                       [[ 2.5385e-02,  7.2457e-03, -1.6735e-02],\n                        [-4.7629e-03, -1.2607e-02, -4.5772e-03],\n                        [ 1.6854e-02,  1.9901e-02,  2.8703e-02]]],\n              \n              \n                      [[[-2.8001e-02, -4.4546e-04, -2.0191e-02],\n                        [ 2.4830e-02, -2.2498e-02, -2.0728e-02],\n                        [-1.0464e-02,  2.7569e-02,  2.9056e-02]],\n              \n                       [[-2.7124e-02, -7.6276e-03,  2.4910e-02],\n                        [-5.0865e-03, -1.3039e-02, -1.9636e-02],\n                        [-2.0727e-02, -2.3310e-02, -1.5865e-02]],\n              \n                       [[ 7.5711e-03,  7.3599e-03, -2.2980e-02],\n                        [-2.5551e-02,  2.2718e-02,  1.5489e-02],\n                        [-3.0655e-04,  1.2903e-02, -2.2033e-02]],\n              \n                       ...,\n              \n                       [[-1.5014e-02, -7.5347e-04,  1.6599e-03],\n                        [-5.4850e-03,  1.3427e-02,  2.9824e-03],\n                        [ 2.4041e-02,  1.7558e-03,  1.0491e-02]],\n              \n                       [[-1.7517e-02,  2.2218e-02,  2.1117e-02],\n                        [-8.5116e-05,  2.7633e-02,  1.1950e-03],\n                        [ 2.3484e-02, -2.0629e-02, -7.9562e-03]],\n              \n                       [[ 6.6841e-03, -2.7769e-02, -2.2987e-02],\n                        [-2.4637e-02,  2.2629e-02, -1.2457e-02],\n                        [-1.0986e-02, -1.6586e-02, -4.0791e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.6628e-03,  2.6667e-02,  6.7481e-03],\n                        [-1.4348e-02, -1.9016e-02,  2.1977e-02],\n                        [ 1.1526e-02,  2.0264e-03, -1.9429e-02]],\n              \n                       [[-1.5399e-02,  2.4140e-02,  1.7281e-02],\n                        [-5.1553e-05,  2.7146e-03, -2.2730e-02],\n                        [-2.2137e-02,  1.5756e-02,  9.6129e-03]],\n              \n                       [[-5.2356e-03,  1.8795e-02,  1.4753e-02],\n                        [-2.9235e-02, -2.4725e-02, -9.9595e-03],\n                        [-2.5816e-02, -1.2593e-02, -1.4906e-02]],\n              \n                       ...,\n              \n                       [[-5.1329e-04,  2.4464e-02,  1.0491e-02],\n                        [ 1.6588e-03, -1.9864e-02, -2.4729e-02],\n                        [-5.7917e-03,  1.2495e-02,  7.5220e-03]],\n              \n                       [[ 1.5368e-02, -2.5456e-02, -1.4819e-02],\n                        [-2.5614e-02, -2.3670e-03,  2.6447e-02],\n                        [-5.4125e-03, -4.6167e-03, -7.2054e-04]],\n              \n                       [[-1.7071e-02, -2.6587e-03,  2.1725e-02],\n                        [-2.8988e-02,  3.1809e-03,  1.3815e-03],\n                        [ 6.4158e-03, -2.6444e-04,  1.8910e-02]]],\n              \n              \n                      [[[ 2.5009e-02,  4.4661e-03, -2.5017e-02],\n                        [ 6.8237e-03,  1.3778e-02,  6.8838e-03],\n                        [-1.5440e-02, -1.2293e-03,  2.2054e-02]],\n              \n                       [[-1.6465e-02,  1.3906e-02,  2.9242e-02],\n                        [ 2.2392e-02, -6.8427e-03, -2.1006e-02],\n                        [ 2.3828e-02, -1.8528e-02,  4.6238e-03]],\n              \n                       [[ 2.6324e-02, -3.9792e-03, -2.8550e-02],\n                        [ 9.2739e-03,  8.2617e-03, -2.5574e-02],\n                        [ 1.6078e-02,  1.6129e-02,  6.8392e-03]],\n              \n                       ...,\n              \n                       [[ 2.7127e-02, -1.3369e-02,  8.5266e-03],\n                        [-1.0530e-02, -2.0817e-02, -8.6817e-03],\n                        [-2.9038e-02, -2.4825e-03,  1.3813e-02]],\n              \n                       [[ 1.2809e-02, -2.7485e-02, -2.8767e-02],\n                        [-5.6553e-03,  1.9724e-02,  1.1964e-02],\n                        [ 5.6818e-03,  1.9974e-02, -1.8658e-02]],\n              \n                       [[ 2.8031e-02, -2.4776e-02, -3.0622e-03],\n                        [ 1.4898e-02,  2.7475e-03, -2.2119e-02],\n                        [ 5.8204e-03,  6.9012e-03, -2.6735e-02]]],\n              \n              \n                      [[[ 9.7910e-03,  1.7056e-02, -4.8750e-03],\n                        [ 3.8653e-03,  9.2350e-03, -2.7748e-02],\n                        [ 2.4542e-02, -9.4870e-03,  2.7431e-02]],\n              \n                       [[ 1.5725e-03,  5.4433e-03,  6.2727e-03],\n                        [ 2.9122e-02,  1.9450e-02, -1.4450e-02],\n                        [ 7.3775e-03,  2.3615e-02, -1.2452e-02]],\n              \n                       [[-7.7901e-04,  5.2408e-03,  1.3440e-02],\n                        [ 1.1745e-02, -2.4794e-02,  5.6418e-03],\n                        [ 1.4150e-02, -1.9262e-02, -6.3717e-04]],\n              \n                       ...,\n              \n                       [[ 4.6180e-03,  2.1094e-03, -2.5070e-02],\n                        [-1.9577e-02,  2.3995e-02, -1.5351e-02],\n                        [-2.1875e-02, -2.0034e-03,  3.7910e-03]],\n              \n                       [[ 2.1114e-03,  2.1738e-02,  1.3168e-03],\n                        [-9.2969e-03,  1.9882e-02,  5.0677e-03],\n                        [ 6.9171e-03,  2.1555e-02, -1.1559e-02]],\n              \n                       [[-2.8176e-02, -2.6783e-02,  2.4445e-02],\n                        [ 1.4733e-02,  4.4278e-03,  7.2822e-03],\n                        [-2.4972e-02, -1.4935e-02,  2.7857e-02]]]], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-2.0874e-03,  2.8328e-02,  3.8197e-03],\n                        [ 2.0103e-02, -2.4530e-02,  3.5383e-03],\n                        [ 1.2657e-02,  2.5045e-02,  5.3281e-03]],\n              \n                       [[ 9.3871e-03,  2.5844e-02, -1.4631e-02],\n                        [ 2.7466e-02, -1.0389e-02,  1.5178e-02],\n                        [ 2.8453e-02,  1.3451e-02, -1.1607e-03]],\n              \n                       [[ 2.0450e-02,  1.3948e-02, -1.8822e-02],\n                        [-1.6178e-03,  2.4138e-02,  1.6494e-02],\n                        [-2.7684e-02, -1.6600e-02,  2.5942e-03]],\n              \n                       ...,\n              \n                       [[-2.5010e-03,  2.1981e-02,  1.0307e-02],\n                        [ 1.0725e-02,  2.8690e-02, -1.7391e-02],\n                        [ 3.5500e-03,  2.0341e-03,  5.9864e-03]],\n              \n                       [[-8.7539e-03,  1.3636e-02,  2.7444e-02],\n                        [-5.3241e-03,  1.4782e-02, -1.6061e-02],\n                        [ 2.8436e-02, -2.6700e-02, -5.3704e-03]],\n              \n                       [[-2.3932e-02,  6.0354e-03,  2.0279e-02],\n                        [-2.7523e-02, -2.8895e-02,  2.0104e-02],\n                        [-6.3520e-03,  8.0765e-03,  2.4935e-03]]],\n              \n              \n                      [[[-1.0771e-02, -3.8036e-03, -2.3648e-02],\n                        [-1.3159e-02,  2.4382e-02,  2.5068e-02],\n                        [-1.8793e-02, -2.5927e-02,  1.6405e-02]],\n              \n                       [[ 4.6219e-03,  2.3189e-02, -1.0743e-02],\n                        [ 2.8896e-02, -2.2556e-02,  5.3712e-03],\n                        [-8.8788e-03, -8.3982e-03, -9.5629e-03]],\n              \n                       [[-2.3292e-02,  1.9044e-02,  1.8797e-03],\n                        [-1.7992e-02, -2.8691e-02,  1.8576e-03],\n                        [-2.4593e-02,  8.3165e-03, -5.6803e-03]],\n              \n                       ...,\n              \n                       [[-2.7325e-02, -1.6579e-02, -2.7656e-02],\n                        [-1.4223e-02,  6.2641e-03, -2.7416e-02],\n                        [-1.8046e-02,  1.1367e-02, -1.2150e-02]],\n              \n                       [[-3.4729e-03,  5.4115e-04, -1.9539e-02],\n                        [ 1.6914e-02, -1.1351e-02,  2.0686e-02],\n                        [-1.0540e-02, -2.7865e-02,  3.4599e-03]],\n              \n                       [[-1.5403e-02, -5.0929e-03, -2.0951e-02],\n                        [ 1.8758e-02, -1.5846e-02, -2.6030e-02],\n                        [ 2.3687e-02, -2.6410e-02,  5.7963e-03]]],\n              \n              \n                      [[[-2.6278e-02, -1.2930e-02, -1.6344e-02],\n                        [ 8.9017e-03, -1.8674e-02, -1.6698e-02],\n                        [-1.0313e-02,  9.8180e-03,  1.0110e-02]],\n              \n                       [[-2.1049e-02,  1.4577e-02, -1.8113e-02],\n                        [-2.0648e-02, -1.4387e-02, -2.4280e-04],\n                        [-2.0775e-02, -4.0661e-03,  2.7782e-02]],\n              \n                       [[-2.7178e-02,  4.2496e-03, -2.3201e-02],\n                        [ 1.0937e-02, -6.5350e-03, -2.3540e-02],\n                        [-2.9455e-02,  2.3027e-02, -2.7718e-02]],\n              \n                       ...,\n              \n                       [[-2.1814e-02,  1.5335e-02, -2.3714e-02],\n                        [-2.8257e-02,  2.3738e-02, -1.3762e-02],\n                        [-3.1294e-03,  9.6518e-03,  6.7151e-03]],\n              \n                       [[-2.5689e-02,  4.9199e-03,  1.6813e-02],\n                        [ 2.7413e-02, -2.5757e-02, -2.6320e-02],\n                        [ 2.8428e-02, -1.9982e-02, -6.2184e-03]],\n              \n                       [[-4.9595e-03, -2.2561e-02,  2.1508e-02],\n                        [ 6.1043e-03, -1.9141e-02, -1.6917e-02],\n                        [-2.2802e-02, -7.2276e-03,  1.1010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8587e-04,  2.5234e-02,  1.2862e-02],\n                        [ 6.4087e-03,  2.9456e-03, -6.2891e-03],\n                        [ 1.3295e-02,  1.1122e-02, -3.8489e-03]],\n              \n                       [[ 2.4627e-02, -8.6374e-03,  9.6317e-03],\n                        [-4.4341e-03, -2.0696e-03,  5.3607e-05],\n                        [ 2.7382e-02, -1.1736e-03, -2.8442e-03]],\n              \n                       [[ 7.9895e-03, -6.4228e-03,  9.2783e-03],\n                        [ 1.0661e-03, -2.7210e-02,  2.9449e-02],\n                        [ 2.8375e-03, -2.2452e-02, -3.4423e-03]],\n              \n                       ...,\n              \n                       [[ 7.1594e-03, -2.7026e-02, -6.7921e-03],\n                        [-1.5202e-02, -7.0004e-04, -6.5862e-03],\n                        [ 2.7967e-02,  2.5300e-02,  5.7218e-03]],\n              \n                       [[ 1.9714e-02,  2.5212e-02,  2.6632e-02],\n                        [ 3.6115e-03, -2.2397e-02, -1.0878e-02],\n                        [-1.3762e-02,  4.6104e-04,  1.6057e-02]],\n              \n                       [[ 2.5034e-02, -2.9420e-02, -1.7739e-02],\n                        [ 1.0064e-02, -2.8722e-02, -1.6836e-02],\n                        [ 1.7448e-02,  2.8111e-02,  1.4150e-03]]],\n              \n              \n                      [[[-1.5742e-02, -1.3421e-02,  2.7663e-02],\n                        [-1.5744e-02,  2.0141e-03,  1.1419e-03],\n                        [ 2.5981e-02,  1.0222e-02, -1.5587e-02]],\n              \n                       [[ 1.3669e-02,  5.2103e-03, -7.6013e-03],\n                        [-1.6173e-02,  5.6269e-04,  2.4350e-03],\n                        [ 2.4261e-03,  2.5788e-02, -2.8097e-02]],\n              \n                       [[-1.4888e-02, -1.7731e-02, -6.4337e-03],\n                        [ 2.2471e-02,  2.3679e-04, -1.1437e-02],\n                        [-5.8912e-03,  1.0241e-02,  1.8909e-02]],\n              \n                       ...,\n              \n                       [[-1.4776e-02,  2.1398e-02,  8.8336e-04],\n                        [-3.3876e-03,  9.3768e-03, -5.3336e-03],\n                        [-4.4843e-03, -5.7139e-03, -6.8183e-03]],\n              \n                       [[-2.0888e-02, -2.4299e-02, -1.6261e-02],\n                        [-2.0847e-02,  1.3012e-02,  2.1894e-02],\n                        [-4.3075e-03,  2.1090e-02,  2.2750e-02]],\n              \n                       [[-1.7861e-02, -2.5487e-02, -9.7013e-03],\n                        [-2.8849e-03, -2.6374e-02, -2.2423e-02],\n                        [ 3.2294e-03,  1.0469e-02, -2.7943e-02]]],\n              \n              \n                      [[[ 4.1885e-03, -2.7628e-02, -2.5770e-02],\n                        [ 1.4383e-02, -3.2527e-03, -2.1710e-02],\n                        [-1.4146e-02,  7.5708e-03, -1.2968e-02]],\n              \n                       [[ 6.4110e-03,  1.5356e-02, -1.1846e-02],\n                        [ 2.1303e-02,  6.4434e-03, -2.6370e-02],\n                        [ 1.7484e-02,  1.9423e-02,  2.9357e-02]],\n              \n                       [[ 3.5598e-03,  2.6142e-02, -2.6987e-02],\n                        [ 9.4496e-03,  1.8193e-02,  1.0256e-02],\n                        [ 3.0655e-03,  2.6695e-03, -9.7217e-04]],\n              \n                       ...,\n              \n                       [[ 1.2180e-02,  2.1096e-02, -2.4789e-02],\n                        [ 6.3251e-03,  3.0475e-03, -6.8353e-03],\n                        [ 1.8787e-02, -9.2431e-03,  1.7185e-02]],\n              \n                       [[-1.1940e-02,  1.8412e-02,  1.7622e-02],\n                        [ 2.1504e-02,  2.3440e-02,  1.1492e-02],\n                        [-1.6089e-02, -1.5441e-02,  2.1249e-02]],\n              \n                       [[-2.3543e-02, -2.0001e-02, -2.0346e-02],\n                        [ 2.0520e-02,  2.9473e-03, -1.2873e-02],\n                        [ 1.3080e-02, -1.3335e-02,  2.4488e-02]]]], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-0.0199, -0.0207, -0.0025],\n                        [-0.0202,  0.0202, -0.0180],\n                        [-0.0126,  0.0164, -0.0123]],\n              \n                       [[ 0.0062, -0.0141,  0.0168],\n                        [ 0.0078,  0.0006, -0.0096],\n                        [ 0.0036, -0.0188,  0.0195]],\n              \n                       [[-0.0073, -0.0065, -0.0040],\n                        [ 0.0086,  0.0105,  0.0089],\n                        [-0.0055,  0.0144, -0.0161]],\n              \n                       ...,\n              \n                       [[ 0.0131, -0.0028, -0.0143],\n                        [-0.0057, -0.0096, -0.0171],\n                        [-0.0130, -0.0047, -0.0005]],\n              \n                       [[-0.0046, -0.0177,  0.0125],\n                        [-0.0102,  0.0154,  0.0072],\n                        [ 0.0206,  0.0169, -0.0156]],\n              \n                       [[ 0.0036,  0.0074,  0.0056],\n                        [ 0.0112, -0.0127, -0.0147],\n                        [ 0.0001,  0.0135,  0.0017]]],\n              \n              \n                      [[[-0.0075, -0.0151,  0.0206],\n                        [ 0.0001, -0.0105, -0.0072],\n                        [ 0.0066,  0.0189,  0.0178]],\n              \n                       [[ 0.0086, -0.0003,  0.0005],\n                        [ 0.0185, -0.0089, -0.0045],\n                        [ 0.0166, -0.0010,  0.0182]],\n              \n                       [[-0.0107, -0.0202,  0.0050],\n                        [-0.0029, -0.0139,  0.0134],\n                        [ 0.0037,  0.0136, -0.0140]],\n              \n                       ...,\n              \n                       [[ 0.0171,  0.0028,  0.0002],\n                        [ 0.0165,  0.0112,  0.0014],\n                        [-0.0089, -0.0016,  0.0104]],\n              \n                       [[-0.0161, -0.0097, -0.0042],\n                        [ 0.0174,  0.0107,  0.0100],\n                        [-0.0053, -0.0070,  0.0113]],\n              \n                       [[-0.0016, -0.0070,  0.0061],\n                        [ 0.0017,  0.0160,  0.0013],\n                        [ 0.0057,  0.0200, -0.0160]]],\n              \n              \n                      [[[-0.0060, -0.0105, -0.0198],\n                        [-0.0150, -0.0083,  0.0156],\n                        [-0.0090,  0.0120, -0.0199]],\n              \n                       [[ 0.0127,  0.0145, -0.0122],\n                        [ 0.0110, -0.0001, -0.0018],\n                        [ 0.0039,  0.0206, -0.0076]],\n              \n                       [[ 0.0101,  0.0061, -0.0136],\n                        [ 0.0194, -0.0136,  0.0016],\n                        [-0.0007,  0.0173,  0.0011]],\n              \n                       ...,\n              \n                       [[-0.0134, -0.0127, -0.0165],\n                        [ 0.0041, -0.0118,  0.0110],\n                        [ 0.0044,  0.0060,  0.0036]],\n              \n                       [[ 0.0056, -0.0185,  0.0055],\n                        [ 0.0114, -0.0050, -0.0185],\n                        [ 0.0116, -0.0140, -0.0148]],\n              \n                       [[ 0.0145,  0.0188, -0.0130],\n                        [ 0.0065, -0.0171,  0.0036],\n                        [-0.0037, -0.0078,  0.0077]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0090,  0.0069, -0.0124],\n                        [-0.0150, -0.0065,  0.0094],\n                        [-0.0195, -0.0163, -0.0144]],\n              \n                       [[-0.0142,  0.0055, -0.0013],\n                        [-0.0149, -0.0092,  0.0063],\n                        [ 0.0007,  0.0089,  0.0060]],\n              \n                       [[-0.0055, -0.0047, -0.0065],\n                        [-0.0140,  0.0113, -0.0194],\n                        [-0.0049,  0.0079,  0.0079]],\n              \n                       ...,\n              \n                       [[-0.0111, -0.0127,  0.0139],\n                        [ 0.0075, -0.0173, -0.0109],\n                        [ 0.0204, -0.0063, -0.0174]],\n              \n                       [[ 0.0198,  0.0142,  0.0200],\n                        [ 0.0188,  0.0201, -0.0102],\n                        [ 0.0027, -0.0103, -0.0160]],\n              \n                       [[ 0.0090,  0.0116,  0.0114],\n                        [-0.0037, -0.0078,  0.0121],\n                        [-0.0192, -0.0149, -0.0202]]],\n              \n              \n                      [[[ 0.0045, -0.0102,  0.0195],\n                        [-0.0163, -0.0012,  0.0005],\n                        [ 0.0079, -0.0045,  0.0198]],\n              \n                       [[ 0.0181,  0.0146, -0.0039],\n                        [ 0.0095,  0.0106, -0.0055],\n                        [ 0.0028,  0.0103,  0.0006]],\n              \n                       [[ 0.0039, -0.0051, -0.0071],\n                        [-0.0123, -0.0141,  0.0050],\n                        [-0.0146,  0.0068,  0.0163]],\n              \n                       ...,\n              \n                       [[-0.0144,  0.0072, -0.0097],\n                        [-0.0070,  0.0141,  0.0089],\n                        [-0.0034,  0.0030,  0.0124]],\n              \n                       [[ 0.0143, -0.0146, -0.0182],\n                        [-0.0080,  0.0061, -0.0181],\n                        [ 0.0166,  0.0175, -0.0116]],\n              \n                       [[-0.0095, -0.0014, -0.0191],\n                        [ 0.0184, -0.0074, -0.0144],\n                        [ 0.0201, -0.0136, -0.0001]]],\n              \n              \n                      [[[-0.0022, -0.0024,  0.0035],\n                        [-0.0075, -0.0206,  0.0173],\n                        [-0.0160,  0.0207,  0.0060]],\n              \n                       [[-0.0073,  0.0075, -0.0149],\n                        [-0.0112,  0.0081, -0.0034],\n                        [-0.0176, -0.0169,  0.0041]],\n              \n                       [[-0.0040,  0.0199, -0.0174],\n                        [ 0.0103,  0.0153, -0.0109],\n                        [-0.0044, -0.0160, -0.0072]],\n              \n                       ...,\n              \n                       [[ 0.0142, -0.0045,  0.0044],\n                        [-0.0134, -0.0153, -0.0110],\n                        [-0.0178,  0.0051, -0.0051]],\n              \n                       [[ 0.0090,  0.0175,  0.0111],\n                        [ 0.0201, -0.0061,  0.0081],\n                        [-0.0037,  0.0166,  0.0074]],\n              \n                       [[-0.0069,  0.0019, -0.0200],\n                        [-0.0047, -0.0145,  0.0192],\n                        [-0.0100,  0.0121, -0.0193]]]], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-4.6348e-03,  9.8509e-03,  1.6142e-02],\n                        [ 2.6825e-05, -8.4992e-03,  3.6535e-04],\n                        [-2.0749e-02, -2.7181e-03,  1.4475e-02]],\n              \n                       [[ 1.0194e-02,  6.9748e-03,  1.3849e-02],\n                        [ 1.4200e-03,  2.5024e-03,  1.5259e-02],\n                        [ 1.1671e-02,  4.0497e-03,  8.7697e-03]],\n              \n                       [[-4.4309e-03, -1.1845e-02, -1.6037e-02],\n                        [-7.8910e-03, -9.7038e-03,  5.6008e-03],\n                        [-1.6987e-02,  7.1697e-03,  1.7236e-02]],\n              \n                       ...,\n              \n                       [[-1.1635e-02,  1.8610e-02,  1.4086e-02],\n                        [-1.1576e-02, -1.9610e-03, -1.8455e-02],\n                        [-8.6874e-03, -1.1485e-02, -5.8817e-03]],\n              \n                       [[-1.3743e-02,  1.2879e-02,  2.2404e-03],\n                        [-6.8730e-03,  1.0492e-02,  8.4602e-03],\n                        [ 1.9366e-03, -1.0892e-02,  9.0133e-03]],\n              \n                       [[-6.9619e-03, -1.7941e-02, -1.1306e-02],\n                        [-6.8960e-03, -6.8894e-03, -6.9923e-04],\n                        [ 1.0807e-02,  1.8476e-02,  1.9441e-02]]],\n              \n              \n                      [[[ 6.4426e-03,  7.5100e-03,  6.7503e-03],\n                        [-1.8439e-02,  1.4277e-02, -1.0381e-02],\n                        [-1.7296e-02, -1.2204e-02,  5.2923e-03]],\n              \n                       [[-6.8046e-03,  6.3742e-03, -1.1632e-02],\n                        [ 4.2213e-03,  2.0774e-02, -3.7589e-03],\n                        [ 1.6312e-02,  7.4283e-04,  1.2614e-02]],\n              \n                       [[-6.7564e-03, -1.0808e-02, -1.6746e-02],\n                        [-6.2140e-03,  9.3120e-03, -9.2284e-03],\n                        [ 2.8789e-03,  1.2397e-03,  1.5193e-02]],\n              \n                       ...,\n              \n                       [[-1.4065e-02, -4.0645e-03, -1.4819e-02],\n                        [ 7.9262e-03, -1.4440e-02, -1.3676e-02],\n                        [ 8.2918e-04,  1.0951e-02,  6.6675e-03]],\n              \n                       [[ 1.8929e-02, -1.6932e-02,  7.8811e-03],\n                        [ 1.6661e-02, -1.4852e-02, -6.1440e-03],\n                        [-4.3739e-03,  1.0890e-02,  1.2552e-03]],\n              \n                       [[ 1.6674e-02,  8.4053e-03, -5.2151e-03],\n                        [-1.8711e-02, -6.0464e-04,  4.8782e-03],\n                        [-1.0599e-02, -8.5500e-03, -4.4493e-04]]],\n              \n              \n                      [[[ 7.4150e-03, -1.7817e-02, -9.8810e-03],\n                        [ 1.5139e-02, -5.4702e-03,  3.1069e-03],\n                        [ 1.6121e-02, -2.4298e-03, -3.4243e-03]],\n              \n                       [[ 5.2642e-03, -1.7880e-02, -1.8678e-02],\n                        [ 2.9048e-03,  1.0568e-02, -2.8701e-04],\n                        [-4.0345e-05, -2.8312e-03,  6.9242e-03]],\n              \n                       [[ 1.2557e-02,  1.3475e-02, -1.1946e-02],\n                        [ 1.0504e-02, -1.1848e-02,  1.4417e-02],\n                        [-1.8312e-02,  1.1722e-02, -6.9120e-03]],\n              \n                       ...,\n              \n                       [[ 1.9895e-02,  1.5509e-02,  1.9991e-02],\n                        [-1.5190e-02, -1.9972e-02, -1.3091e-02],\n                        [-1.1537e-02, -6.8988e-03,  1.1122e-02]],\n              \n                       [[ 1.0277e-02, -9.5677e-03,  1.4165e-02],\n                        [ 5.0890e-03,  1.1992e-02,  2.0542e-02],\n                        [-9.9942e-04,  1.1082e-02, -5.1328e-03]],\n              \n                       [[ 1.0213e-02, -4.6551e-03, -5.2989e-03],\n                        [ 1.5165e-02, -1.7655e-02,  5.5892e-03],\n                        [ 1.1311e-02, -1.2807e-02, -1.2253e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4459e-02,  4.5380e-04, -2.9677e-03],\n                        [ 1.8889e-02, -1.6052e-02, -1.5562e-02],\n                        [ 1.3935e-03, -1.6170e-02,  2.0204e-02]],\n              \n                       [[ 1.0080e-02, -3.7539e-03, -1.5059e-02],\n                        [ 6.8971e-03, -8.5807e-03,  1.5525e-02],\n                        [ 1.4992e-03, -7.8594e-03,  7.5005e-03]],\n              \n                       [[ 3.7703e-03,  9.6159e-03,  1.6808e-02],\n                        [-1.1511e-02, -1.9614e-02, -1.7621e-02],\n                        [ 6.5007e-03, -1.5883e-02, -1.3063e-02]],\n              \n                       ...,\n              \n                       [[ 1.1717e-02,  1.3965e-03, -5.3536e-03],\n                        [ 1.4582e-02, -1.8533e-03, -1.5276e-02],\n                        [-2.0322e-02, -1.0361e-02, -6.1722e-03]],\n              \n                       [[ 5.0393e-04,  3.0661e-03, -9.3391e-03],\n                        [-5.0653e-03,  1.3716e-02,  9.7900e-03],\n                        [-2.0547e-02,  1.3067e-02,  1.6991e-03]],\n              \n                       [[-8.7317e-03,  1.5140e-02, -9.8445e-03],\n                        [-2.9895e-03,  1.0854e-02, -7.8243e-03],\n                        [ 1.5019e-03,  1.9270e-02,  9.2994e-03]]],\n              \n              \n                      [[[-3.2868e-03, -1.6655e-03,  1.3082e-02],\n                        [ 7.1859e-03, -1.9157e-03, -3.5394e-03],\n                        [-1.9397e-02,  5.5216e-03, -1.8486e-02]],\n              \n                       [[ 9.8068e-03,  2.6197e-03,  4.8447e-04],\n                        [ 1.5565e-02,  1.1252e-02,  1.8660e-02],\n                        [ 3.1310e-03,  6.5078e-03, -1.4506e-02]],\n              \n                       [[-1.5900e-02, -3.8698e-03,  4.6403e-03],\n                        [ 1.0163e-02,  1.0891e-02,  1.9025e-02],\n                        [-7.0364e-03,  1.0454e-02,  7.3635e-03]],\n              \n                       ...,\n              \n                       [[ 1.5563e-02, -1.9394e-02,  1.5875e-03],\n                        [-4.1397e-03, -7.3719e-04, -8.6707e-03],\n                        [-1.5182e-02,  1.4803e-02, -1.7555e-02]],\n              \n                       [[-7.9233e-04,  1.1101e-03,  1.7634e-03],\n                        [ 1.5103e-02, -1.4403e-02,  1.4855e-02],\n                        [-7.4607e-03,  7.4488e-03, -1.7282e-02]],\n              \n                       [[ 1.4080e-02,  1.6888e-02,  1.6374e-02],\n                        [ 7.7976e-03, -6.2802e-03, -3.1626e-03],\n                        [ 2.0682e-02, -1.9079e-02,  1.3276e-02]]],\n              \n              \n                      [[[ 1.8058e-02, -9.1462e-03, -7.2015e-03],\n                        [-6.4691e-03, -2.9027e-03,  9.6589e-03],\n                        [-1.3747e-02,  1.9787e-02,  1.9956e-02]],\n              \n                       [[-1.1408e-02, -2.4681e-05,  7.7289e-03],\n                        [ 1.9633e-02, -8.2515e-03,  1.3016e-02],\n                        [-1.8417e-02,  1.8677e-02, -1.1818e-02]],\n              \n                       [[ 1.9430e-02,  1.0222e-02, -5.9156e-03],\n                        [ 1.5036e-02,  9.4860e-03,  2.0289e-03],\n                        [-6.1385e-03, -6.8786e-03, -1.0498e-02]],\n              \n                       ...,\n              \n                       [[ 1.8626e-02, -4.7810e-03,  1.8702e-02],\n                        [-7.9554e-03, -1.7242e-02, -1.2626e-03],\n                        [ 1.9328e-02, -5.6285e-03, -1.1736e-02]],\n              \n                       [[-4.1653e-04, -1.8020e-02, -1.2647e-02],\n                        [-4.7124e-03,  3.7225e-03,  3.3474e-03],\n                        [-2.6790e-03,  6.2666e-03,  3.8707e-03]],\n              \n                       [[ 1.9958e-03, -6.2181e-03, -1.5993e-02],\n                        [ 4.3567e-03,  2.8269e-03,  2.0313e-02],\n                        [-1.6953e-02, -1.2477e-02, -6.3685e-03]]]], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.3495e-02,  1.1336e-02,  3.2999e-03],\n                        [ 1.0248e-02,  4.9058e-03,  1.6721e-03],\n                        [ 1.4577e-02,  1.2254e-02, -1.0996e-02]],\n              \n                       [[ 2.8387e-03, -1.2857e-02, -6.3248e-04],\n                        [ 1.0179e-02, -7.9369e-03,  9.4359e-03],\n                        [ 2.8751e-03, -1.1316e-02, -2.7018e-03]],\n              \n                       [[ 1.3239e-02,  1.3039e-03, -1.3213e-02],\n                        [-8.4236e-03,  2.3438e-03, -1.4353e-02],\n                        [ 9.7540e-03,  7.3673e-03,  9.9629e-04]],\n              \n                       ...,\n              \n                       [[-1.2715e-02, -5.7416e-03,  8.1590e-04],\n                        [ 1.2467e-02,  5.0082e-03, -9.3793e-03],\n                        [-1.0866e-02,  6.1197e-03,  2.4678e-03]],\n              \n                       [[-1.3211e-02, -6.7648e-03,  1.4521e-02],\n                        [-5.5102e-03, -5.2198e-03,  1.0626e-02],\n                        [-1.1742e-02, -6.2968e-03, -3.1413e-03]],\n              \n                       [[ 5.9503e-04, -9.2838e-03,  2.2524e-03],\n                        [ 4.4587e-03, -6.3728e-04, -1.4285e-02],\n                        [-5.1423e-03, -5.7166e-03,  1.2934e-02]]],\n              \n              \n                      [[[ 1.8463e-03, -5.4794e-04, -1.8946e-03],\n                        [ 9.7586e-04,  3.5177e-03, -4.0504e-03],\n                        [-6.2299e-03,  5.2996e-03,  1.3720e-02]],\n              \n                       [[-5.9090e-03,  1.6445e-03,  2.7570e-03],\n                        [-9.9673e-04, -1.0245e-02,  5.6605e-03],\n                        [ 1.1391e-02, -1.1658e-02, -1.1734e-02]],\n              \n                       [[-1.1735e-02,  2.4595e-03,  5.7827e-03],\n                        [ 7.1670e-03, -1.6270e-03,  1.0687e-02],\n                        [ 6.0396e-03, -7.3033e-04, -8.5946e-03]],\n              \n                       ...,\n              \n                       [[ 1.1671e-02,  1.3118e-02, -1.3291e-02],\n                        [ 6.1538e-03, -6.0592e-04,  6.6185e-03],\n                        [ 1.2829e-03, -1.3731e-02,  1.4932e-03]],\n              \n                       [[-7.4605e-03,  6.8828e-04, -1.2302e-04],\n                        [-8.1735e-03,  1.2001e-02,  7.8193e-03],\n                        [ 2.0528e-03, -6.3210e-03,  1.3449e-02]],\n              \n                       [[ 2.9136e-03,  6.6908e-03, -3.7520e-03],\n                        [ 9.3340e-03, -4.1290e-03, -1.4161e-02],\n                        [-5.5939e-03,  5.1468e-03,  7.5768e-05]]],\n              \n              \n                      [[[ 7.9902e-03,  8.0955e-03,  1.0381e-02],\n                        [ 6.6680e-03,  2.9378e-03,  6.6944e-03],\n                        [-2.3877e-03, -4.8883e-03,  8.5533e-03]],\n              \n                       [[-1.2371e-02, -1.2348e-02,  4.0223e-03],\n                        [-6.9362e-03, -1.0553e-02,  5.3495e-03],\n                        [ 4.4429e-04,  5.7790e-03, -2.5581e-03]],\n              \n                       [[ 2.1132e-03, -1.0715e-02,  3.1263e-03],\n                        [ 1.4578e-02, -4.7421e-03, -4.1220e-03],\n                        [ 7.7216e-03, -7.0857e-03, -4.0999e-03]],\n              \n                       ...,\n              \n                       [[-1.2722e-02,  4.8952e-03,  3.1216e-03],\n                        [-3.6589e-03,  3.9157e-03,  7.6172e-05],\n                        [ 6.6556e-03,  1.3619e-02, -1.0715e-02]],\n              \n                       [[-8.3624e-03,  2.8966e-03,  7.7819e-03],\n                        [ 9.6693e-03, -1.3035e-02, -1.2682e-02],\n                        [-1.2393e-02,  1.4095e-02, -9.9444e-03]],\n              \n                       [[-2.6372e-03, -9.4880e-03, -4.2093e-03],\n                        [ 2.4768e-03,  5.2376e-03, -1.6081e-03],\n                        [ 1.4001e-03,  8.7849e-03, -6.4915e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.1331e-03, -1.0245e-02,  5.5679e-03],\n                        [-1.3925e-02, -5.4960e-03, -6.4326e-03],\n                        [ 1.0665e-03,  9.3625e-03, -1.0900e-02]],\n              \n                       [[-1.2820e-02, -1.4185e-02,  7.6603e-03],\n                        [ 5.5901e-03, -7.7663e-03, -1.3632e-02],\n                        [-7.8664e-03,  3.8328e-03, -6.1660e-03]],\n              \n                       [[ 2.2009e-03,  1.2656e-02, -5.1460e-03],\n                        [-7.3644e-03, -1.2076e-03,  1.9836e-03],\n                        [-1.4580e-03, -8.4020e-04,  1.0106e-02]],\n              \n                       ...,\n              \n                       [[ 7.8239e-03,  8.2156e-03,  5.3135e-03],\n                        [ 7.6519e-03,  2.5644e-03,  9.5596e-03],\n                        [ 1.2521e-02,  7.5805e-03, -1.3987e-02]],\n              \n                       [[ 1.0951e-02,  7.9635e-04, -6.1090e-03],\n                        [ 7.5488e-03,  1.2158e-02, -1.4382e-02],\n                        [-3.4198e-03, -3.9887e-03, -3.8113e-03]],\n              \n                       [[-1.1689e-02,  9.5688e-03, -5.1517e-03],\n                        [-1.1460e-02, -4.0730e-03, -5.6413e-03],\n                        [ 7.0657e-03,  2.6805e-03, -5.1478e-03]]],\n              \n              \n                      [[[-9.6095e-03, -1.3585e-03, -7.0119e-03],\n                        [ 9.6654e-03,  1.0712e-02,  1.0401e-02],\n                        [-3.5123e-03,  1.3850e-02,  1.0464e-02]],\n              \n                       [[-1.1702e-02, -7.7455e-03, -5.3939e-03],\n                        [-1.2093e-02, -8.4871e-03, -3.2977e-03],\n                        [-1.0420e-02,  8.9802e-03, -4.9594e-03]],\n              \n                       [[-1.2320e-02,  2.4707e-03, -2.3200e-03],\n                        [-3.9590e-03,  1.1381e-02, -3.2109e-03],\n                        [-1.9178e-03, -1.3853e-02, -4.3691e-03]],\n              \n                       ...,\n              \n                       [[ 1.0142e-02,  1.3061e-02,  1.1623e-02],\n                        [-5.8694e-03, -6.4008e-04,  1.3774e-02],\n                        [ 6.2873e-03,  3.2907e-03, -8.4393e-03]],\n              \n                       [[ 3.5045e-03,  4.6928e-03,  1.1195e-02],\n                        [ 5.2034e-03, -9.1595e-03,  1.1639e-02],\n                        [-7.8218e-03,  7.5058e-03, -1.4309e-02]],\n              \n                       [[-2.4525e-03, -3.6981e-03,  1.1964e-02],\n                        [-1.2757e-02, -5.8314e-03, -1.1045e-02],\n                        [ 6.1323e-03,  1.4707e-02, -9.2333e-03]]],\n              \n              \n                      [[[ 5.0627e-03,  1.4049e-02,  7.1501e-03],\n                        [-1.3210e-02,  1.1269e-02,  2.2428e-03],\n                        [-9.7158e-03,  5.5631e-03, -1.2279e-02]],\n              \n                       [[-9.5874e-03, -5.4147e-04,  1.4689e-02],\n                        [ 4.4917e-03, -1.3910e-02, -3.7383e-04],\n                        [-7.5597e-03,  9.3203e-03, -7.5512e-03]],\n              \n                       [[-1.4322e-02, -1.1102e-02,  1.1979e-02],\n                        [ 6.4091e-03, -1.3175e-02,  2.6744e-04],\n                        [ 1.1095e-03,  6.2741e-03,  5.1492e-04]],\n              \n                       ...,\n              \n                       [[ 1.3908e-02,  9.8417e-03,  9.4988e-03],\n                        [ 1.1376e-02,  1.9947e-04, -8.0265e-03],\n                        [-1.1771e-02, -1.0298e-02, -2.5397e-03]],\n              \n                       [[-2.3932e-03,  1.3351e-02,  1.0970e-02],\n                        [ 1.2986e-02,  3.9482e-03, -8.2351e-03],\n                        [-1.0508e-02, -3.3115e-03, -8.0658e-03]],\n              \n                       [[-2.9153e-03,  1.4376e-02, -3.0430e-03],\n                        [ 1.3600e-02, -2.1507e-03, -4.3007e-03],\n                        [-3.6526e-03,  8.3328e-03,  8.7380e-03]]]], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-1.3104e-02,  9.6535e-03,  7.0547e-03],\n                        [ 6.8489e-03,  5.6884e-03, -3.3797e-03],\n                        [-1.3077e-02,  1.1413e-02, -8.2186e-03]],\n              \n                       [[-6.4877e-03,  1.2398e-02,  1.4672e-02],\n                        [-2.8377e-03,  2.9911e-03,  8.6744e-03],\n                        [ 4.6708e-03, -1.9309e-03, -1.3963e-02]],\n              \n                       [[-8.8996e-04, -1.3098e-02, -1.2099e-02],\n                        [ 1.1789e-02, -6.3457e-03,  8.4533e-03],\n                        [ 6.9120e-04,  3.7103e-03, -3.9384e-03]],\n              \n                       ...,\n              \n                       [[-1.4631e-02,  7.6187e-03,  1.3055e-02],\n                        [ 8.7348e-03,  2.2455e-03,  1.4252e-02],\n                        [-7.8609e-03,  6.6497e-03,  1.2674e-02]],\n              \n                       [[ 1.0928e-02,  8.1940e-03,  1.4620e-03],\n                        [ 1.1112e-03, -7.0720e-03, -1.2397e-02],\n                        [ 1.3073e-02,  2.2528e-03,  6.1473e-03]],\n              \n                       [[-1.1589e-02, -9.5213e-03, -5.2496e-03],\n                        [-1.1412e-02, -1.3629e-02,  7.4268e-03],\n                        [-6.4922e-03,  1.1146e-02, -9.5554e-03]]],\n              \n              \n                      [[[ 2.3625e-05, -1.3995e-02, -7.6334e-03],\n                        [-9.4009e-03, -9.2042e-03,  5.7072e-03],\n                        [ 9.9287e-03, -5.7740e-03,  8.9586e-03]],\n              \n                       [[ 1.4008e-02, -1.0200e-02,  1.3237e-02],\n                        [ 1.4621e-02, -1.2051e-02,  6.9597e-03],\n                        [ 1.2422e-02, -8.4337e-03, -7.5494e-03]],\n              \n                       [[ 5.7422e-04, -8.9031e-03,  1.4246e-02],\n                        [-3.9909e-03, -1.2648e-05,  7.5228e-03],\n                        [ 4.5517e-03, -8.1091e-03, -2.5926e-03]],\n              \n                       ...,\n              \n                       [[ 1.7802e-03,  1.2118e-02, -8.6626e-04],\n                        [-6.0965e-04, -5.6477e-03, -4.7239e-03],\n                        [-1.4231e-03, -1.1298e-02,  4.0613e-03]],\n              \n                       [[ 2.4961e-05,  4.4265e-03,  1.4223e-02],\n                        [ 2.2458e-03,  1.3728e-02, -1.1796e-02],\n                        [-7.2479e-03,  1.2696e-02,  4.3921e-03]],\n              \n                       [[ 1.4457e-02, -1.0118e-02,  1.3083e-02],\n                        [-7.3051e-03,  1.3544e-02, -1.2357e-02],\n                        [ 3.5746e-03, -1.3268e-02, -9.3003e-03]]],\n              \n              \n                      [[[-3.1621e-03,  1.4471e-02,  1.0941e-02],\n                        [ 1.2192e-02,  5.9600e-03,  7.0732e-03],\n                        [ 1.6198e-03, -1.1914e-02, -1.1316e-02]],\n              \n                       [[-8.1733e-03, -4.6493e-03,  1.3078e-02],\n                        [-5.0052e-03, -1.0437e-02,  9.8975e-03],\n                        [-1.3412e-02, -8.9157e-03,  1.3293e-02]],\n              \n                       [[-5.0194e-03,  6.6695e-03,  3.4234e-04],\n                        [-1.3336e-02,  1.4430e-03,  7.5926e-03],\n                        [-1.0269e-03,  1.0630e-02, -8.4293e-03]],\n              \n                       ...,\n              \n                       [[ 1.0040e-02, -9.6519e-03,  1.1701e-02],\n                        [ 6.5308e-05,  3.5704e-03, -1.2048e-02],\n                        [-9.5033e-03, -1.2604e-02, -1.2307e-02]],\n              \n                       [[-6.6415e-03, -1.0024e-02,  1.3435e-02],\n                        [-6.3868e-03, -1.4265e-02, -2.8581e-03],\n                        [-1.3789e-02,  1.1855e-02,  7.1601e-03]],\n              \n                       [[-9.1238e-03,  4.7032e-05, -2.2387e-03],\n                        [ 4.9879e-04,  7.7738e-03,  5.1973e-03],\n                        [ 3.4793e-03,  9.1406e-03, -9.1121e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.2879e-03,  1.1191e-03, -6.0251e-03],\n                        [-3.2071e-03,  5.4502e-03,  1.2839e-04],\n                        [ 5.8309e-03, -1.3948e-02,  3.9841e-03]],\n              \n                       [[ 1.0795e-02,  5.7343e-03,  3.2873e-03],\n                        [ 5.4282e-03, -1.0134e-02,  3.3486e-03],\n                        [ 5.0658e-03, -1.4290e-02,  3.9768e-03]],\n              \n                       [[-1.4718e-02, -4.8749e-03,  8.8550e-03],\n                        [-1.2116e-02,  3.9706e-03, -1.5341e-04],\n                        [-5.6044e-03,  9.2914e-03,  2.6309e-03]],\n              \n                       ...,\n              \n                       [[ 1.1578e-02,  4.7662e-03,  1.0865e-02],\n                        [-9.9621e-03,  7.2204e-03,  6.7652e-03],\n                        [ 6.1930e-03,  5.5036e-03, -4.8385e-03]],\n              \n                       [[-1.1982e-02,  9.0713e-03, -6.7553e-03],\n                        [ 1.0392e-02, -6.3635e-03, -1.1598e-03],\n                        [ 1.0464e-02,  4.0243e-03,  1.4345e-03]],\n              \n                       [[ 3.2504e-03,  1.4237e-02, -7.7320e-03],\n                        [-1.0245e-02, -8.5657e-03, -1.2735e-02],\n                        [-3.5816e-03,  1.3560e-02, -1.2678e-02]]],\n              \n              \n                      [[[-1.4336e-02, -4.6926e-03,  1.3425e-02],\n                        [ 1.3409e-02, -6.8928e-03, -9.7946e-03],\n                        [-1.4182e-02, -8.6928e-03, -1.4202e-02]],\n              \n                       [[-5.0576e-03, -9.8077e-03,  5.6572e-03],\n                        [-1.4611e-02,  4.4676e-03, -1.3235e-02],\n                        [ 3.6478e-03,  4.1773e-04,  1.4504e-02]],\n              \n                       [[-8.5665e-03, -6.6888e-03, -5.9852e-03],\n                        [ 1.8548e-03,  1.2795e-02, -6.3900e-03],\n                        [-1.3038e-02,  7.2169e-03,  9.2560e-03]],\n              \n                       ...,\n              \n                       [[-5.8375e-03,  8.9250e-03,  1.2109e-02],\n                        [-1.3653e-02,  1.3453e-02, -6.7649e-03],\n                        [-1.2166e-02, -1.3578e-02, -1.2037e-03]],\n              \n                       [[-5.5372e-03, -3.9234e-03, -2.1640e-03],\n                        [-8.1456e-03, -8.1486e-03,  4.8608e-05],\n                        [-7.9746e-03,  3.5861e-03, -5.4110e-03]],\n              \n                       [[ 9.0684e-03, -4.6523e-03,  8.6029e-03],\n                        [-3.5470e-03, -2.6329e-03,  4.1187e-03],\n                        [-1.7698e-03,  3.1339e-03, -1.3087e-02]]],\n              \n              \n                      [[[ 1.3993e-02,  1.0210e-02, -9.8379e-03],\n                        [-3.6017e-03,  1.5505e-03, -7.5702e-03],\n                        [-1.3827e-03, -1.4429e-02, -1.3696e-02]],\n              \n                       [[ 1.2335e-02,  8.3124e-03, -4.6792e-03],\n                        [ 4.8468e-03,  1.3626e-04,  9.8758e-03],\n                        [-2.6817e-03,  3.2997e-03, -9.7415e-04]],\n              \n                       [[ 3.1673e-03, -7.1938e-03, -1.4500e-03],\n                        [-9.1013e-03,  8.4705e-03, -9.5864e-03],\n                        [ 1.6714e-03, -1.4101e-02,  1.1644e-02]],\n              \n                       ...,\n              \n                       [[ 1.4320e-02,  4.4366e-03, -5.8747e-03],\n                        [-8.1688e-03, -6.9629e-03,  3.0317e-04],\n                        [-1.2110e-02, -1.3646e-02, -6.0113e-03]],\n              \n                       [[-3.7647e-04,  7.6979e-03,  3.3129e-03],\n                        [ 7.6917e-03, -1.9005e-03,  6.3914e-03],\n                        [-2.9271e-03,  1.0327e-02, -9.8557e-03]],\n              \n                       [[ 1.1749e-02,  3.9048e-03, -7.2822e-03],\n                        [ 1.4049e-02,  1.3569e-02,  2.5594e-03],\n                        [ 1.2890e-02,  5.6545e-03,  6.2168e-03]]]], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-1.0162e-02, -7.9513e-03, -1.4126e-02],\n                        [-6.2557e-03, -9.7779e-03,  1.0858e-02],\n                        [ 9.1498e-03,  3.0958e-04,  9.0409e-03]],\n              \n                       [[-7.6646e-03, -9.0559e-03, -8.4516e-04],\n                        [-1.2277e-02,  2.7770e-03,  2.4928e-03],\n                        [ 2.1196e-03, -2.7451e-03, -1.3663e-02]],\n              \n                       [[-8.4018e-03,  3.2803e-03, -6.1505e-03],\n                        [ 1.3116e-02,  8.8065e-03,  4.6064e-03],\n                        [ 9.4382e-03, -7.7282e-03,  1.0306e-02]],\n              \n                       ...,\n              \n                       [[ 6.6357e-03, -2.2279e-03, -8.7835e-03],\n                        [-5.1093e-03,  3.9618e-03,  8.8206e-03],\n                        [ 1.4141e-02,  1.3784e-02,  1.1771e-02]],\n              \n                       [[-5.9949e-03, -1.3745e-04,  7.4454e-03],\n                        [-9.2404e-03,  1.3126e-02,  9.9188e-03],\n                        [-6.8859e-03, -1.4138e-02, -9.2198e-03]],\n              \n                       [[-1.4438e-02,  1.1573e-02,  1.1146e-02],\n                        [-8.7031e-03, -4.6383e-03,  7.3338e-03],\n                        [ 1.1381e-02, -9.0583e-03, -2.5293e-03]]],\n              \n              \n                      [[[-1.3852e-02, -6.8651e-03,  2.3293e-03],\n                        [ 1.2269e-02,  6.5710e-03,  3.9793e-03],\n                        [-7.3067e-03, -5.9318e-03, -6.7658e-03]],\n              \n                       [[ 9.5927e-03, -7.6682e-03, -1.3819e-02],\n                        [-9.0626e-03,  3.5546e-03, -8.5062e-03],\n                        [ 1.7261e-03, -2.6030e-03, -1.4632e-02]],\n              \n                       [[ 1.0916e-02,  1.0892e-02,  1.4228e-02],\n                        [ 1.1874e-02, -6.4073e-03, -5.1940e-03],\n                        [-7.4828e-03, -7.4947e-03,  2.5183e-03]],\n              \n                       ...,\n              \n                       [[ 9.7132e-03,  2.0456e-03, -4.0253e-03],\n                        [ 1.9973e-03,  1.2258e-02, -1.3174e-03],\n                        [-9.0220e-03, -8.2095e-03,  1.4117e-02]],\n              \n                       [[-1.0827e-02,  1.4226e-02, -6.4879e-03],\n                        [ 1.2198e-02, -1.2647e-02,  8.6206e-03],\n                        [-2.7980e-03, -2.0266e-03,  5.7236e-03]],\n              \n                       [[-1.2030e-02,  1.2822e-02, -8.4252e-03],\n                        [ 1.1277e-02, -7.0514e-03, -7.5673e-03],\n                        [ 8.1968e-03, -1.2170e-02, -7.3895e-03]]],\n              \n              \n                      [[[ 8.0684e-03,  1.3598e-02, -7.9777e-03],\n                        [-1.4268e-02,  4.8484e-03, -1.1704e-02],\n                        [ 4.8766e-03,  2.9658e-03,  2.0288e-03]],\n              \n                       [[-1.1000e-03, -2.6417e-03,  3.1051e-03],\n                        [ 1.2253e-02, -7.2229e-03, -1.1037e-03],\n                        [ 1.0293e-02,  3.9444e-03, -8.0077e-03]],\n              \n                       [[ 3.6599e-03,  1.3138e-02, -1.0403e-03],\n                        [-1.0804e-02, -2.9224e-03, -7.3381e-04],\n                        [-8.4483e-03, -3.5656e-03,  1.0923e-02]],\n              \n                       ...,\n              \n                       [[ 1.0183e-02, -1.0656e-02,  2.5374e-03],\n                        [-2.4001e-03,  9.3434e-03,  8.0887e-03],\n                        [-3.1470e-03, -3.6860e-03,  6.9349e-03]],\n              \n                       [[-1.4212e-02,  4.7419e-03,  2.2588e-03],\n                        [ 1.2572e-02,  2.5563e-03, -8.1275e-03],\n                        [-3.7703e-03,  2.5945e-03,  5.5602e-03]],\n              \n                       [[-1.2830e-02, -1.0370e-02,  9.9764e-03],\n                        [-1.0848e-02, -9.6209e-03,  8.2907e-03],\n                        [ 4.6423e-03, -4.9777e-03, -8.6183e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.9552e-03,  1.0103e-02, -4.7408e-03],\n                        [-1.3407e-02,  6.5927e-03, -7.2890e-03],\n                        [ 1.2902e-02, -7.3139e-03,  4.8173e-03]],\n              \n                       [[-8.6896e-03, -1.9172e-03,  5.9656e-03],\n                        [-7.3172e-05,  2.9933e-03, -1.1204e-02],\n                        [ 2.1456e-03,  2.6252e-03, -1.3978e-02]],\n              \n                       [[-8.2944e-03, -6.1581e-03,  1.3276e-02],\n                        [ 2.0285e-04, -6.9051e-03,  1.3585e-02],\n                        [-7.9958e-03,  5.1597e-03, -1.1482e-02]],\n              \n                       ...,\n              \n                       [[ 2.9236e-03,  8.6567e-03, -5.6918e-03],\n                        [ 1.2319e-02, -1.2173e-02, -1.1142e-02],\n                        [ 2.1955e-03,  2.1893e-03,  1.0226e-02]],\n              \n                       [[-1.3731e-02,  2.4001e-04,  1.0280e-02],\n                        [ 6.2036e-04,  9.4891e-03, -9.4363e-03],\n                        [ 7.7716e-03, -5.3223e-03, -1.1793e-02]],\n              \n                       [[ 9.0567e-03, -9.4963e-03,  1.2966e-02],\n                        [-3.5606e-03,  6.7127e-03,  9.2346e-03],\n                        [ 1.6610e-04,  9.7832e-04, -3.7458e-03]]],\n              \n              \n                      [[[ 1.8821e-03,  7.0609e-03, -9.9641e-03],\n                        [ 2.8442e-03, -3.4813e-04,  2.8147e-03],\n                        [-7.6718e-03,  1.4098e-03,  3.6991e-03]],\n              \n                       [[-7.4600e-03,  6.1319e-03, -6.6834e-03],\n                        [ 4.6137e-03, -9.7316e-03, -2.1926e-03],\n                        [-5.1150e-03,  8.5056e-03,  1.4168e-02]],\n              \n                       [[ 1.2746e-02,  8.4634e-03,  1.2394e-02],\n                        [ 6.5522e-03, -1.0927e-02, -1.4621e-02],\n                        [ 9.5033e-03,  3.9224e-03,  9.9719e-03]],\n              \n                       ...,\n              \n                       [[-4.0116e-03, -1.4190e-02, -2.6838e-03],\n                        [-1.9716e-04, -1.6087e-03, -2.2089e-03],\n                        [ 1.1347e-02,  5.0595e-04, -2.1228e-03]],\n              \n                       [[ 1.1465e-03,  6.0314e-03, -7.8767e-03],\n                        [-6.6732e-03, -5.0615e-03, -7.0481e-03],\n                        [-3.5145e-03, -1.4674e-02,  9.3690e-03]],\n              \n                       [[-2.1949e-03,  1.8604e-04, -3.8469e-04],\n                        [-6.0911e-03,  4.8625e-03,  9.1291e-04],\n                        [-4.2253e-03, -9.7373e-03,  3.0233e-03]]],\n              \n              \n                      [[[ 1.3092e-02, -9.1652e-03, -1.4018e-02],\n                        [-7.5290e-03, -1.1704e-02,  1.1918e-02],\n                        [-3.6753e-03,  8.3012e-03, -7.8185e-03]],\n              \n                       [[ 1.3660e-02, -1.0051e-04, -4.8537e-03],\n                        [ 4.5250e-03,  1.1501e-02, -1.2260e-02],\n                        [-1.2088e-02, -1.1217e-02, -8.9023e-03]],\n              \n                       [[ 3.9087e-03, -1.1512e-03, -1.3955e-02],\n                        [-2.1982e-03,  1.0120e-02, -5.0558e-03],\n                        [-1.3255e-02,  2.8492e-03, -4.1524e-03]],\n              \n                       ...,\n              \n                       [[-1.2921e-02, -1.8075e-03,  3.1186e-03],\n                        [ 4.0110e-03,  5.9678e-03, -1.5871e-03],\n                        [ 4.0160e-03,  4.9175e-04,  2.2130e-03]],\n              \n                       [[-3.4039e-03, -1.2438e-02,  6.7231e-03],\n                        [ 1.2851e-02, -5.3675e-03,  1.6797e-03],\n                        [-1.3136e-02, -2.5658e-03, -5.8660e-03]],\n              \n                       [[-2.0538e-03,  7.5002e-04,  6.9986e-03],\n                        [ 1.3422e-02, -9.2835e-04,  4.6620e-03],\n                        [-1.3815e-02,  5.7040e-03, -6.6107e-03]]]], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up1.conv.double_conv.0.weight',\n              tensor([[[[ 6.0052e-03, -6.1578e-03, -8.6970e-03],\n                        [ 1.6955e-03, -7.3866e-03,  5.3448e-03],\n                        [ 5.5082e-03,  9.1673e-03,  1.0191e-02]],\n              \n                       [[-3.7926e-03,  5.7925e-03,  1.0316e-02],\n                        [ 9.6915e-03,  8.8699e-03,  5.3047e-03],\n                        [ 5.0500e-03,  4.6066e-03,  1.0278e-02]],\n              \n                       [[-7.2442e-04, -7.9003e-03, -9.7175e-03],\n                        [ 4.6586e-04, -3.6655e-03, -9.5510e-03],\n                        [-9.1740e-03, -7.8502e-03, -5.3606e-03]],\n              \n                       ...,\n              \n                       [[ 2.1322e-03, -9.4887e-05, -4.9738e-03],\n                        [-6.1662e-03,  1.3903e-03, -7.2019e-03],\n                        [ 5.4206e-03,  8.7880e-03,  4.3695e-03]],\n              \n                       [[ 3.3114e-03, -4.8001e-03, -2.7326e-03],\n                        [-3.7524e-03,  7.7908e-03, -8.4219e-03],\n                        [ 2.0721e-03,  7.5771e-03,  6.9718e-03]],\n              \n                       [[-9.9150e-03, -2.1330e-03,  7.4038e-03],\n                        [-6.3372e-03, -8.1195e-03,  1.6034e-03],\n                        [ 5.8172e-03, -1.3327e-03, -7.0786e-03]]],\n              \n              \n                      [[[-4.7313e-03, -2.5325e-03, -6.1366e-03],\n                        [ 1.1530e-03, -5.3506e-03, -6.1344e-04],\n                        [ 2.7635e-03, -6.2766e-03,  4.6419e-03]],\n              \n                       [[ 4.3768e-03, -4.0070e-03,  8.7607e-03],\n                        [-8.9397e-03, -9.8516e-03, -2.8273e-03],\n                        [-3.7660e-03,  3.6542e-03,  1.0126e-02]],\n              \n                       [[-6.7512e-03,  6.0833e-03,  2.7166e-03],\n                        [ 9.3578e-04,  5.1147e-03,  6.3890e-03],\n                        [ 1.5687e-04,  7.4274e-03, -8.3365e-03]],\n              \n                       ...,\n              \n                       [[-4.8921e-03, -5.4093e-03,  5.6688e-03],\n                        [ 3.1983e-03,  3.9314e-03, -8.9410e-03],\n                        [ 6.5762e-03, -9.7403e-03, -4.1459e-03]],\n              \n                       [[ 8.1715e-03,  5.4453e-03, -7.9296e-03],\n                        [ 1.6348e-03, -1.7733e-04,  1.1809e-03],\n                        [-6.2941e-03,  6.1941e-03,  1.7227e-03]],\n              \n                       [[ 9.5111e-03, -8.0376e-03, -3.7345e-03],\n                        [ 5.4716e-03, -3.7542e-03,  2.9980e-03],\n                        [-7.5362e-03,  8.4094e-03,  8.9098e-03]]],\n              \n              \n                      [[[-9.6740e-03, -8.1277e-03,  3.9857e-03],\n                        [-3.5163e-03,  8.6464e-03,  4.2643e-03],\n                        [-5.0144e-03, -9.8802e-04,  4.8284e-04]],\n              \n                       [[-6.5739e-03,  9.1206e-03,  5.8876e-03],\n                        [-4.3970e-03,  3.9926e-04,  4.9571e-03],\n                        [-3.2965e-03,  4.1399e-04, -2.7867e-03]],\n              \n                       [[-4.9022e-03, -7.1855e-04,  5.2022e-04],\n                        [-3.8415e-03,  7.9072e-03,  1.0071e-02],\n                        [-6.5128e-03, -3.6828e-03, -8.3628e-03]],\n              \n                       ...,\n              \n                       [[ 8.5856e-03, -7.1988e-03,  9.1629e-03],\n                        [ 9.4906e-03, -6.0381e-03,  6.3775e-04],\n                        [ 3.2705e-03, -4.2573e-03,  7.2144e-03]],\n              \n                       [[-2.7434e-03, -5.6575e-03,  7.0926e-03],\n                        [ 6.5038e-03,  1.0222e-02,  7.6083e-03],\n                        [ 8.3256e-03,  7.9641e-03, -6.8926e-03]],\n              \n                       [[ 3.2581e-03, -3.4153e-03,  1.7781e-04],\n                        [-4.7329e-03, -2.7371e-03, -7.9243e-03],\n                        [-7.3951e-03, -3.6213e-03,  3.8721e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.3754e-03,  1.0256e-02, -9.6938e-03],\n                        [-5.2090e-03,  1.1899e-03,  6.6328e-03],\n                        [-6.4318e-03,  7.6097e-03,  3.2797e-03]],\n              \n                       [[-7.0052e-03,  4.5905e-03, -8.9286e-03],\n                        [-8.2543e-03, -5.1691e-03, -5.8590e-03],\n                        [ 8.7791e-03,  5.7680e-03, -8.9067e-03]],\n              \n                       [[-7.6416e-03, -9.3266e-03,  9.4770e-03],\n                        [ 1.4398e-03,  4.5831e-03, -3.4448e-03],\n                        [-4.5923e-03, -5.7610e-03, -4.3103e-03]],\n              \n                       ...,\n              \n                       [[-2.0614e-03, -8.5129e-03, -8.4951e-03],\n                        [ 2.6566e-03,  9.1776e-03,  2.6760e-03],\n                        [-1.7022e-04,  3.6392e-03,  5.0875e-03]],\n              \n                       [[-2.9073e-03, -7.8702e-03, -1.2811e-03],\n                        [-8.3429e-03, -8.4082e-03,  4.3443e-03],\n                        [-6.5337e-03,  3.0448e-03, -3.2978e-03]],\n              \n                       [[-6.3634e-03, -6.4584e-03, -9.4520e-03],\n                        [ 6.3613e-03,  1.3895e-03,  6.7184e-03],\n                        [ 1.9717e-04,  3.0919e-03, -9.3850e-03]]],\n              \n              \n                      [[[-7.3347e-03,  3.7111e-03, -1.4600e-03],\n                        [-8.9929e-03, -1.0001e-02, -9.7608e-03],\n                        [ 4.9672e-03, -5.1917e-03, -9.9102e-03]],\n              \n                       [[ 7.6933e-03, -4.9824e-03, -8.9469e-03],\n                        [ 4.8704e-03, -1.6437e-03,  8.8097e-03],\n                        [-3.0993e-03, -5.9778e-03, -3.1651e-03]],\n              \n                       [[ 8.6893e-03,  9.8990e-03,  7.1665e-03],\n                        [ 7.6924e-03, -1.0816e-03,  9.3137e-03],\n                        [-4.7224e-03, -3.9862e-03, -7.0841e-03]],\n              \n                       ...,\n              \n                       [[ 7.1673e-03,  5.2882e-03,  5.8690e-03],\n                        [ 4.2807e-04, -4.7009e-04,  9.8658e-03],\n                        [-3.6831e-03, -3.5520e-03,  4.0485e-03]],\n              \n                       [[-5.5522e-03,  9.4766e-03,  8.2692e-03],\n                        [-3.1187e-03, -8.5105e-03,  8.7861e-03],\n                        [-7.3462e-03,  5.8684e-03,  9.6273e-03]],\n              \n                       [[-3.7102e-03,  7.7810e-03, -1.4194e-03],\n                        [-4.0797e-03, -8.0059e-03,  8.5199e-03],\n                        [-9.1947e-03,  3.5915e-03, -4.6602e-03]]],\n              \n              \n                      [[[-1.3775e-03,  6.0666e-04, -6.9796e-04],\n                        [ 6.7400e-03,  6.6210e-03,  2.7429e-03],\n                        [-8.8243e-03, -9.8390e-03,  2.4116e-03]],\n              \n                       [[ 4.7119e-03,  3.2005e-03,  5.9726e-03],\n                        [ 9.5476e-03,  1.6969e-03,  9.7832e-03],\n                        [-2.6481e-03,  7.0522e-03, -7.9863e-03]],\n              \n                       [[ 4.9707e-03,  9.5256e-04, -1.3029e-03],\n                        [-6.9370e-03, -1.0068e-02,  1.0652e-03],\n                        [-2.0503e-03,  8.6360e-03, -1.5661e-03]],\n              \n                       ...,\n              \n                       [[-6.5328e-03, -9.1420e-04,  5.5855e-03],\n                        [ 8.4739e-03, -4.1916e-03,  1.0212e-02],\n                        [ 1.0342e-02, -8.0135e-03, -1.1019e-04]],\n              \n                       [[ 4.2931e-03,  4.7278e-03,  8.9549e-03],\n                        [ 7.2504e-03,  4.6937e-03, -6.7444e-03],\n                        [-1.0244e-02,  2.1343e-03, -3.2979e-03]],\n              \n                       [[ 9.3904e-03, -7.6412e-03,  2.0035e-03],\n                        [-6.8808e-03,  1.0404e-02,  9.5906e-03],\n                        [ 5.1486e-03,  1.8948e-03, -1.0138e-03]]]], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up1.conv.double_conv.3.weight',\n              tensor([[[[ 4.6532e-03, -7.6019e-03, -2.2726e-03],\n                        [ 4.6818e-03,  1.2958e-02,  7.4474e-03],\n                        [ 1.0656e-02,  7.3169e-03,  1.4385e-02]],\n              \n                       [[-7.1003e-03,  5.6198e-03,  1.1528e-02],\n                        [ 1.2165e-02,  2.7467e-03,  1.2221e-02],\n                        [ 1.0123e-02, -7.3388e-04, -1.3558e-02]],\n              \n                       [[ 6.1051e-04, -1.0071e-02,  1.0367e-02],\n                        [ 5.4181e-03,  3.2388e-03,  8.1533e-04],\n                        [ 9.9759e-03, -8.9243e-03, -1.0614e-02]],\n              \n                       ...,\n              \n                       [[-1.1593e-02,  4.4562e-03, -1.2794e-02],\n                        [-2.0847e-03,  8.4393e-03, -3.0718e-03],\n                        [ 1.2095e-02,  9.6634e-03, -6.1204e-03]],\n              \n                       [[-8.5692e-03, -5.3203e-03, -6.0301e-03],\n                        [-1.3060e-02, -4.9878e-03,  1.3536e-02],\n                        [-3.0446e-03, -3.7271e-03,  1.8943e-03]],\n              \n                       [[ 9.1236e-03,  6.2085e-03, -5.2066e-03],\n                        [ 7.0768e-03,  5.8855e-03, -1.3525e-02],\n                        [ 1.2969e-02, -3.1656e-03, -9.7805e-03]]],\n              \n              \n                      [[[-1.3448e-02, -1.4380e-02,  3.3876e-03],\n                        [-6.9893e-03, -8.7593e-03,  3.4935e-03],\n                        [ 6.0252e-03,  6.2473e-03, -7.2960e-04]],\n              \n                       [[ 1.2521e-03, -1.2604e-02, -1.4122e-02],\n                        [-7.8812e-03,  1.2843e-03,  3.4510e-03],\n                        [-8.0826e-03, -6.0928e-03,  1.4071e-02]],\n              \n                       [[ 1.2236e-02, -2.2066e-03,  7.5802e-03],\n                        [-3.4579e-03, -8.4028e-03,  1.2992e-02],\n                        [ 1.5273e-03,  9.6915e-03, -2.7779e-03]],\n              \n                       ...,\n              \n                       [[-9.7299e-03,  7.2240e-03,  3.2073e-04],\n                        [ 5.1952e-03,  1.3993e-02,  5.8187e-03],\n                        [-3.9472e-03,  9.5075e-03,  9.9508e-03]],\n              \n                       [[ 3.8860e-03, -7.5956e-03, -6.7716e-03],\n                        [-6.3491e-03,  1.1731e-02, -4.6717e-03],\n                        [ 5.6204e-04, -4.5982e-03, -1.3072e-03]],\n              \n                       [[-9.9374e-03, -1.4691e-03,  9.6274e-03],\n                        [-3.4154e-03, -9.9765e-03,  4.7587e-03],\n                        [ 1.1309e-02,  1.2087e-03,  1.1953e-02]]],\n              \n              \n                      [[[ 1.2883e-02, -7.2949e-03, -4.8458e-03],\n                        [ 9.7466e-03,  1.1054e-02,  1.2237e-02],\n                        [ 9.9405e-03,  1.4726e-02,  2.0744e-03]],\n              \n                       [[ 1.0789e-02,  1.3618e-02,  1.4625e-02],\n                        [-1.9228e-03,  5.1298e-03,  5.3312e-04],\n                        [ 1.4351e-02,  8.0309e-03, -1.3372e-02]],\n              \n                       [[-3.1131e-03, -6.5674e-04, -1.0796e-02],\n                        [-9.3562e-03,  6.5610e-03, -1.3210e-02],\n                        [ 7.9644e-03,  1.0064e-03,  6.2818e-04]],\n              \n                       ...,\n              \n                       [[-2.9593e-03, -3.4946e-03, -4.1973e-03],\n                        [ 1.2073e-02,  7.9237e-03,  9.7770e-05],\n                        [-4.5093e-03, -8.0024e-03, -3.3877e-03]],\n              \n                       [[ 4.1504e-04, -6.3685e-03,  2.9286e-04],\n                        [-1.4368e-02,  5.2549e-04, -1.2686e-02],\n                        [ 1.6020e-03,  4.4607e-03,  7.5159e-03]],\n              \n                       [[-6.6873e-03,  5.1561e-05,  8.2160e-03],\n                        [-7.2157e-03, -9.4008e-04, -9.3220e-03],\n                        [ 1.3272e-03,  1.3943e-03, -1.0126e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.3756e-03,  1.2603e-02,  1.0009e-02],\n                        [ 1.3332e-02,  2.2436e-03, -2.6538e-03],\n                        [ 1.2150e-02, -6.4561e-03, -1.2219e-02]],\n              \n                       [[-8.2563e-03,  1.4514e-02, -6.5334e-03],\n                        [ 1.0584e-02,  7.2743e-03, -7.7184e-03],\n                        [-1.3945e-02, -3.9507e-04, -1.3207e-02]],\n              \n                       [[-1.1936e-02,  1.2723e-02,  1.4794e-03],\n                        [-9.2238e-03,  1.2513e-02, -1.2755e-02],\n                        [-2.3135e-04, -1.2050e-02,  1.0637e-02]],\n              \n                       ...,\n              \n                       [[-1.7315e-03, -1.1583e-02, -6.2004e-03],\n                        [-3.6829e-03, -7.5475e-03, -1.1467e-02],\n                        [-1.2565e-04, -1.6956e-03,  7.3251e-03]],\n              \n                       [[ 4.5195e-03,  9.6949e-03, -1.1593e-02],\n                        [-1.0726e-02, -4.3706e-03, -1.0075e-02],\n                        [-1.1938e-02, -6.4125e-03,  5.7692e-04]],\n              \n                       [[-1.1380e-02, -9.5971e-03, -1.3420e-02],\n                        [ 1.0888e-02, -1.0871e-02,  4.6657e-05],\n                        [-2.8069e-03, -1.0725e-02,  2.2430e-03]]],\n              \n              \n                      [[[ 1.1839e-02,  1.3359e-02, -2.2681e-03],\n                        [ 1.8450e-03,  5.9289e-04, -1.2829e-02],\n                        [ 1.4203e-02,  2.5810e-03, -1.1913e-02]],\n              \n                       [[-1.3077e-02, -1.4014e-02, -4.2100e-03],\n                        [-9.9503e-03,  1.1108e-02, -3.2723e-03],\n                        [ 2.0312e-03,  4.5349e-03,  1.3859e-02]],\n              \n                       [[-1.4575e-02,  1.1122e-02, -7.5780e-03],\n                        [-3.8330e-03, -9.8024e-04,  5.9586e-03],\n                        [ 9.8220e-03, -6.8341e-03,  1.2393e-02]],\n              \n                       ...,\n              \n                       [[-3.4048e-03,  1.3819e-02, -2.6837e-03],\n                        [ 1.1734e-02,  1.4311e-03, -1.2245e-02],\n                        [-8.3261e-03,  1.3495e-02,  2.9223e-03]],\n              \n                       [[-1.2962e-02, -7.3929e-03, -7.3878e-03],\n                        [-1.7338e-03, -6.7076e-03, -7.7754e-03],\n                        [ 1.4972e-03, -6.4253e-03, -1.4126e-02]],\n              \n                       [[ 1.4451e-02, -4.8099e-03,  5.7255e-03],\n                        [-5.8516e-03,  4.0733e-03,  1.0094e-02],\n                        [ 8.1309e-04,  5.1471e-03,  5.1509e-03]]],\n              \n              \n                      [[[ 9.8223e-04,  1.1245e-02,  1.1552e-02],\n                        [-7.6653e-03,  6.1365e-04, -4.2670e-03],\n                        [ 5.1350e-03,  1.4145e-02, -8.8357e-04]],\n              \n                       [[ 1.2253e-02,  1.0491e-02, -1.4184e-02],\n                        [ 2.6855e-03,  7.4216e-03, -4.6636e-03],\n                        [-1.0291e-02, -1.2930e-02, -3.5078e-04]],\n              \n                       [[ 4.5516e-03, -9.4295e-03,  9.7718e-03],\n                        [-7.6455e-03,  1.0235e-02,  1.2030e-03],\n                        [-2.7815e-03,  6.6763e-03, -8.7617e-03]],\n              \n                       ...,\n              \n                       [[-9.8976e-03,  1.2484e-02, -2.8897e-03],\n                        [ 4.3479e-03,  8.9747e-03,  8.7985e-04],\n                        [ 1.2341e-02,  4.2616e-04,  4.2251e-03]],\n              \n                       [[ 1.2692e-02, -1.7026e-03,  7.1434e-03],\n                        [ 1.1852e-02, -1.1433e-02, -1.3874e-02],\n                        [ 1.2581e-02, -3.8352e-03, -7.5201e-04]],\n              \n                       [[-4.7592e-04, -3.9157e-03,  3.5884e-03],\n                        [-3.2631e-03, -1.6258e-03, -1.0496e-02],\n                        [ 1.3847e-03, -5.7536e-04, -1.0432e-02]]]], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up2.conv.double_conv.0.weight',\n              tensor([[[[-2.1518e-03,  1.0631e-02,  1.2601e-02],\n                        [ 9.9365e-03,  8.6478e-03, -1.2200e-02],\n                        [-8.7199e-03, -1.3551e-04,  2.7872e-03]],\n              \n                       [[ 1.0136e-02,  5.1465e-03, -7.2739e-03],\n                        [-1.0549e-02, -4.3726e-03, -1.0110e-02],\n                        [-1.2202e-02,  8.1444e-03,  1.2508e-02]],\n              \n                       [[-1.1105e-02, -3.2792e-03,  1.1186e-02],\n                        [-8.2915e-03,  8.8182e-03,  1.1263e-02],\n                        [-4.4057e-03,  8.6805e-03, -9.5922e-03]],\n              \n                       ...,\n              \n                       [[ 6.3221e-03, -1.2953e-02,  5.1380e-03],\n                        [ 2.9260e-04, -1.0260e-02,  6.4162e-03],\n                        [-5.8944e-03,  4.6316e-03,  1.4742e-03]],\n              \n                       [[-1.0956e-02, -3.5614e-03, -3.6777e-03],\n                        [ 1.2266e-02, -3.7897e-05, -1.1044e-02],\n                        [ 5.1852e-03,  8.2570e-03,  1.3097e-03]],\n              \n                       [[-2.4492e-03, -3.5821e-03, -1.4560e-02],\n                        [ 9.1054e-03, -4.1931e-03,  9.5132e-03],\n                        [ 5.1267e-03,  1.1881e-02,  5.6942e-04]]],\n              \n              \n                      [[[ 1.0638e-02, -5.4433e-03, -3.7759e-03],\n                        [ 1.1677e-02, -4.1737e-03, -1.0637e-02],\n                        [-1.6576e-03, -2.1487e-03, -1.1114e-02]],\n              \n                       [[ 1.8396e-03,  1.3266e-02,  6.8261e-03],\n                        [ 3.9165e-03, -8.8550e-03,  1.4806e-03],\n                        [ 7.0773e-04,  1.1756e-02, -1.0292e-02]],\n              \n                       [[ 1.3127e-02,  4.8850e-03,  2.1176e-03],\n                        [ 2.1249e-03, -5.7832e-03, -1.3140e-02],\n                        [ 8.5454e-03, -8.9114e-03, -1.3402e-02]],\n              \n                       ...,\n              \n                       [[ 1.1088e-02,  7.2383e-03,  1.2047e-02],\n                        [ 9.5457e-03,  1.3826e-02, -2.5452e-03],\n                        [ 9.1783e-03,  1.0598e-02, -8.6740e-04]],\n              \n                       [[ 4.5989e-03, -1.4716e-03, -1.2077e-02],\n                        [-9.6809e-04, -1.2336e-02,  9.3714e-04],\n                        [ 3.9654e-03, -7.3955e-03, -1.2232e-02]],\n              \n                       [[ 5.6303e-03, -8.0869e-03, -2.5287e-03],\n                        [ 1.8057e-03, -1.1487e-02, -2.8659e-03],\n                        [ 4.0015e-03, -1.2479e-02, -1.1998e-02]]],\n              \n              \n                      [[[ 9.4689e-03, -7.2081e-03,  1.4072e-03],\n                        [ 1.2932e-02, -3.2592e-03, -8.7485e-03],\n                        [ 9.2945e-03,  4.6018e-03,  4.0055e-03]],\n              \n                       [[-1.3764e-02, -4.2907e-03,  3.2547e-03],\n                        [ 3.3341e-03,  1.1304e-03, -1.2234e-02],\n                        [-1.3467e-02, -5.6734e-03,  7.4354e-03]],\n              \n                       [[-5.6023e-03, -2.8761e-03, -1.4718e-02],\n                        [ 1.0713e-02, -1.6779e-03, -1.1996e-02],\n                        [-1.2827e-02,  1.0703e-02, -9.7047e-03]],\n              \n                       ...,\n              \n                       [[ 3.2607e-03, -8.0475e-03,  6.1829e-03],\n                        [-2.9395e-03,  3.3496e-03,  5.1071e-03],\n                        [ 5.9723e-03,  4.7608e-03, -1.6388e-03]],\n              \n                       [[-4.3904e-03,  7.7792e-03, -1.2428e-02],\n                        [-3.2456e-03,  5.5866e-03, -1.4352e-02],\n                        [-1.1821e-02,  2.6534e-03,  7.5290e-03]],\n              \n                       [[ 4.6186e-03, -6.2310e-03,  1.1741e-02],\n                        [-1.4587e-02,  9.7592e-03,  1.2688e-02],\n                        [ 4.2982e-03,  5.2313e-03, -1.2822e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1165e-02,  7.8691e-04, -9.3187e-03],\n                        [-7.7603e-03, -3.0258e-03, -9.7707e-03],\n                        [ 7.5438e-03,  1.4036e-02,  1.0273e-02]],\n              \n                       [[-1.3591e-02,  7.4804e-03, -4.6866e-04],\n                        [-1.3815e-02,  1.2045e-02, -9.8406e-03],\n                        [ 1.0759e-02,  6.9177e-03, -1.3892e-02]],\n              \n                       [[ 1.2857e-02, -4.8749e-04,  9.5570e-03],\n                        [ 2.7064e-03, -8.0672e-03,  1.0471e-02],\n                        [ 5.2177e-03,  1.2281e-02, -6.2795e-03]],\n              \n                       ...,\n              \n                       [[ 1.0430e-03,  1.3958e-02, -1.1441e-02],\n                        [-1.0572e-02,  4.8599e-04, -8.1871e-03],\n                        [ 8.7779e-03,  8.1478e-03, -3.1877e-03]],\n              \n                       [[ 7.4461e-03,  2.9228e-03, -1.0984e-02],\n                        [ 9.8613e-03,  1.3081e-02,  1.2413e-02],\n                        [ 1.2035e-02, -3.1168e-03, -7.5135e-03]],\n              \n                       [[ 8.0283e-03, -4.2646e-03, -7.9841e-03],\n                        [-1.9161e-05, -6.6800e-03, -1.6066e-04],\n                        [ 9.5017e-03, -1.7248e-03,  7.0304e-03]]],\n              \n              \n                      [[[ 3.5356e-03, -7.6512e-03, -8.9665e-03],\n                        [-4.8910e-03,  2.0278e-03,  7.1160e-03],\n                        [-3.0881e-03, -4.1455e-03,  1.1920e-02]],\n              \n                       [[ 3.7466e-03, -3.9381e-03,  1.4420e-02],\n                        [-1.3107e-02, -5.7352e-03,  6.8331e-03],\n                        [-6.0296e-03,  1.2593e-02,  8.2828e-03]],\n              \n                       [[-9.1421e-03,  1.2051e-02,  9.1719e-03],\n                        [-2.3811e-03, -1.4370e-02, -1.1317e-02],\n                        [-5.8528e-03,  5.9658e-03, -7.2074e-03]],\n              \n                       ...,\n              \n                       [[ 1.4338e-02,  1.0304e-02, -6.8373e-03],\n                        [ 2.6406e-03, -2.9580e-03, -2.9774e-03],\n                        [-6.9043e-03,  1.4699e-02, -7.5011e-03]],\n              \n                       [[ 9.0359e-03, -7.4744e-03,  2.7057e-03],\n                        [-1.0241e-03, -9.2485e-03, -3.4580e-03],\n                        [ 3.8833e-03,  7.4134e-03, -1.1881e-02]],\n              \n                       [[-1.9624e-03,  2.7043e-03, -4.4755e-04],\n                        [-1.1581e-02, -1.3765e-02, -8.7221e-03],\n                        [ 1.3774e-02, -1.1876e-02, -1.0575e-02]]],\n              \n              \n                      [[[-1.7063e-04,  6.7622e-04,  8.8984e-03],\n                        [-5.9551e-03,  1.2280e-02, -1.2928e-02],\n                        [-1.2386e-02,  1.3566e-02,  3.3778e-03]],\n              \n                       [[-4.9461e-03, -1.1765e-03, -5.0370e-03],\n                        [-3.2352e-03,  8.2034e-03,  1.2355e-02],\n                        [ 3.5783e-03,  1.1220e-02, -1.3388e-02]],\n              \n                       [[-1.8399e-03,  5.9302e-03,  9.6810e-03],\n                        [ 5.0733e-03,  1.0453e-02, -4.8722e-03],\n                        [-1.3514e-02, -1.1929e-03,  1.7507e-03]],\n              \n                       ...,\n              \n                       [[-1.4605e-03,  2.2461e-03, -8.0156e-03],\n                        [ 1.0985e-02,  5.1273e-03, -1.1668e-02],\n                        [ 1.4627e-02,  2.7758e-03,  7.2483e-03]],\n              \n                       [[ 1.3621e-02, -4.5283e-03,  6.4443e-04],\n                        [ 1.0748e-02,  1.1094e-02,  1.4675e-02],\n                        [-9.0625e-03, -6.1689e-03, -2.2046e-03]],\n              \n                       [[-1.4035e-03, -1.3366e-02,  5.8688e-03],\n                        [ 2.4954e-04,  7.3011e-03,  8.3442e-03],\n                        [-2.7433e-04, -1.0389e-02,  3.1839e-03]]]], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up2.conv.double_conv.3.weight',\n              tensor([[[[ 7.9497e-03, -1.7790e-02, -1.7096e-02],\n                        [-1.6327e-02,  4.0280e-03, -1.9224e-02],\n                        [-4.1614e-03,  2.0345e-02, -1.3011e-02]],\n              \n                       [[-1.1634e-02,  5.5307e-03, -1.6266e-02],\n                        [-1.1103e-02,  8.3270e-03, -1.5757e-02],\n                        [ 1.5221e-02, -1.2837e-02,  9.6909e-04]],\n              \n                       [[-1.6213e-02,  6.1893e-03,  1.9967e-02],\n                        [-1.0630e-02,  2.0123e-02,  6.5128e-03],\n                        [-2.0276e-02,  2.0401e-02,  1.5855e-02]],\n              \n                       ...,\n              \n                       [[ 1.4602e-02, -9.3187e-03,  1.2791e-02],\n                        [ 3.5288e-03,  8.2964e-03,  1.7589e-02],\n                        [ 4.4983e-03, -4.8159e-04, -3.6260e-03]],\n              \n                       [[-8.9474e-05,  1.3904e-02,  1.9019e-02],\n                        [-1.9988e-02, -1.3111e-02,  6.4248e-04],\n                        [ 6.8580e-04,  1.7128e-03,  5.4387e-03]],\n              \n                       [[ 1.4890e-02, -9.2215e-03, -5.8313e-03],\n                        [ 1.1482e-02, -1.2943e-02,  1.7208e-02],\n                        [-2.3544e-03,  8.3377e-04, -1.4550e-02]]],\n              \n              \n                      [[[-2.5915e-03, -3.9138e-03, -1.6308e-02],\n                        [-1.9927e-02, -9.3398e-03, -1.9362e-02],\n                        [-1.4066e-02,  9.7209e-03,  1.6551e-02]],\n              \n                       [[-1.9409e-02, -1.3963e-02,  6.9585e-03],\n                        [-5.1612e-04, -1.9914e-02,  1.8270e-02],\n                        [-7.2831e-03,  1.2477e-02, -2.8120e-04]],\n              \n                       [[-1.5371e-02,  9.3540e-04,  9.9296e-03],\n                        [-1.0750e-02, -3.9004e-03,  1.7460e-02],\n                        [-1.9144e-02,  2.0190e-02, -1.1884e-02]],\n              \n                       ...,\n              \n                       [[ 7.7697e-03,  1.9071e-02, -3.6815e-03],\n                        [ 5.6426e-03, -8.5833e-03,  1.6836e-02],\n                        [ 1.8768e-03, -2.5059e-04,  8.1764e-03]],\n              \n                       [[ 5.9330e-03, -1.4364e-02, -3.9514e-03],\n                        [ 1.9684e-02, -1.4239e-02, -2.0091e-02],\n                        [ 2.0407e-02,  1.8737e-02, -5.8489e-03]],\n              \n                       [[ 5.4501e-03,  1.1028e-02, -1.9625e-02],\n                        [-1.3838e-02, -8.5165e-03,  2.6146e-03],\n                        [-6.4134e-03,  1.4367e-02,  1.4903e-02]]],\n              \n              \n                      [[[-1.1303e-03,  3.3091e-03, -6.1916e-03],\n                        [-1.5099e-02, -2.1207e-04,  4.5621e-03],\n                        [ 1.7857e-02, -2.7128e-03, -5.4803e-03]],\n              \n                       [[ 5.9743e-03,  2.0597e-02,  6.6697e-03],\n                        [ 9.8200e-03,  1.3099e-02,  1.7841e-03],\n                        [-1.6089e-02,  1.5824e-02,  8.0234e-04]],\n              \n                       [[-7.2984e-03,  1.2674e-02,  1.8605e-02],\n                        [ 3.9323e-03,  8.1922e-03, -9.3463e-04],\n                        [-1.9702e-02,  1.4019e-02,  1.6300e-02]],\n              \n                       ...,\n              \n                       [[ 1.6479e-02,  1.6218e-02, -1.5242e-02],\n                        [-3.6273e-03,  5.0512e-03,  1.1426e-02],\n                        [ 7.1217e-03,  7.2147e-03, -2.5175e-03]],\n              \n                       [[ 1.5327e-02,  1.4072e-02, -1.7085e-02],\n                        [ 4.0818e-04, -1.7114e-02, -3.8038e-03],\n                        [-1.5342e-02, -2.0213e-02, -1.3697e-02]],\n              \n                       [[-2.0410e-02, -1.5656e-02,  5.8427e-03],\n                        [-3.8405e-03,  1.0923e-02, -1.2858e-02],\n                        [ 1.8628e-02,  4.0466e-03, -2.0422e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.9150e-02,  1.2267e-02,  1.7782e-02],\n                        [ 1.3684e-02, -1.9804e-02, -9.2421e-03],\n                        [ 1.7435e-02,  1.7343e-02, -1.8515e-02]],\n              \n                       [[ 1.8531e-02, -6.2842e-03, -2.1436e-03],\n                        [-6.2577e-03,  1.8332e-02,  1.9857e-02],\n                        [-1.0869e-02, -5.4065e-03,  1.8648e-02]],\n              \n                       [[-9.8150e-03, -1.9312e-02, -5.3483e-04],\n                        [ 2.2209e-03,  2.0530e-02, -6.2797e-03],\n                        [ 3.1732e-03,  1.7359e-02,  1.0300e-02]],\n              \n                       ...,\n              \n                       [[ 5.3619e-03, -8.6172e-03,  1.9207e-02],\n                        [ 1.2767e-02, -3.0699e-03, -9.6391e-03],\n                        [-8.9599e-04,  6.0747e-03,  4.0384e-03]],\n              \n                       [[-5.2875e-03,  6.5115e-04,  5.4017e-03],\n                        [ 1.5804e-03,  8.6046e-03,  1.7447e-02],\n                        [ 7.5348e-03,  1.8965e-02,  1.9957e-02]],\n              \n                       [[-1.0331e-02, -1.1320e-02,  1.5131e-02],\n                        [ 2.9035e-03,  1.1799e-02, -1.5353e-03],\n                        [-8.3366e-03,  9.3031e-03, -1.7604e-02]]],\n              \n              \n                      [[[ 1.4307e-02,  1.1860e-02,  5.1069e-03],\n                        [-1.5284e-02,  8.2293e-03, -9.5887e-03],\n                        [ 5.3585e-03,  2.0224e-03,  1.5437e-02]],\n              \n                       [[ 1.2629e-03,  9.5884e-03,  1.5362e-02],\n                        [-4.8209e-03,  1.4933e-02, -1.2048e-02],\n                        [-3.0520e-05, -1.3378e-02, -2.1463e-03]],\n              \n                       [[-1.1527e-02,  7.7163e-03, -1.2359e-02],\n                        [-2.0476e-02, -1.7779e-02, -6.4546e-03],\n                        [ 3.1536e-03, -1.0851e-04, -1.9629e-02]],\n              \n                       ...,\n              \n                       [[-3.6267e-03, -1.7496e-02, -1.8531e-02],\n                        [ 3.0812e-03, -4.4989e-03, -5.3328e-03],\n                        [-3.5008e-03, -1.0352e-02,  2.0659e-02]],\n              \n                       [[-4.5241e-03,  6.3328e-03,  8.7361e-03],\n                        [-6.1625e-03, -1.3019e-02,  1.6934e-02],\n                        [-3.4158e-03,  8.9188e-03, -1.3646e-02]],\n              \n                       [[ 1.7996e-02,  1.7854e-02, -1.5007e-02],\n                        [ 2.2617e-04,  1.8391e-02,  2.0008e-02],\n                        [-1.4899e-03,  1.6801e-02,  2.3108e-03]]],\n              \n              \n                      [[[-1.5664e-02,  4.3163e-03,  1.2885e-02],\n                        [ 2.6682e-03,  1.6914e-02,  3.5899e-03],\n                        [ 1.9674e-02, -1.1662e-02, -1.2853e-02]],\n              \n                       [[-3.9540e-04, -1.7787e-02,  9.8214e-03],\n                        [ 1.3250e-02, -2.1693e-03, -4.9136e-03],\n                        [ 1.9610e-02,  1.1362e-03,  2.0132e-02]],\n              \n                       [[ 1.0343e-03,  8.4445e-03,  1.5850e-02],\n                        [ 1.1820e-02,  1.0775e-03, -1.8296e-02],\n                        [-1.1273e-02,  2.6236e-03,  1.3343e-02]],\n              \n                       ...,\n              \n                       [[ 1.6003e-02,  5.4038e-03, -3.7506e-03],\n                        [-2.4944e-03, -8.0193e-03, -6.6061e-03],\n                        [-1.2857e-02,  1.3497e-02,  8.1090e-03]],\n              \n                       [[-1.8006e-02, -8.5612e-03,  1.9954e-02],\n                        [-3.3323e-03, -7.7578e-04,  1.2751e-02],\n                        [ 8.0447e-03, -3.9115e-04,  2.0177e-02]],\n              \n                       [[-1.7435e-02, -8.4071e-03, -9.7204e-03],\n                        [ 1.8257e-02, -1.7279e-02, -1.8781e-02],\n                        [ 1.5807e-02, -1.8718e-02,  2.0478e-02]]]], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up3.conv.double_conv.0.weight',\n              tensor([[[[ 6.5360e-04, -1.1478e-02, -1.2108e-02],\n                        [-1.3628e-02, -9.4881e-03,  4.5922e-03],\n                        [-1.3436e-03, -9.4868e-03, -4.5939e-03]],\n              \n                       [[ 1.0784e-02, -1.2223e-03, -1.5292e-02],\n                        [-5.8855e-03, -1.8780e-02, -8.7660e-03],\n                        [ 1.8609e-03,  1.2953e-02, -1.4010e-02]],\n              \n                       [[-6.7148e-03, -1.5341e-02,  1.2591e-02],\n                        [ 7.5377e-03,  1.1052e-02, -1.1975e-02],\n                        [-1.9517e-02, -1.9137e-02, -7.4886e-04]],\n              \n                       ...,\n              \n                       [[ 2.0512e-02, -3.9202e-03,  1.4523e-02],\n                        [ 1.2714e-02,  1.3007e-02,  6.8676e-04],\n                        [-1.7327e-02, -8.6569e-03,  1.2416e-03]],\n              \n                       [[-2.0188e-02, -1.2779e-02, -7.3068e-03],\n                        [-9.3873e-03,  1.3301e-02,  1.6646e-02],\n                        [-1.7413e-02,  1.7294e-03, -1.5510e-02]],\n              \n                       [[-1.4983e-02,  1.7590e-02,  1.2623e-02],\n                        [-2.8354e-03, -2.8116e-03,  1.7879e-02],\n                        [-1.7114e-02,  1.2573e-02,  1.0661e-02]]],\n              \n              \n                      [[[ 1.1610e-02, -1.0957e-02,  1.8087e-02],\n                        [ 1.2981e-02, -1.2237e-02, -1.3717e-02],\n                        [-8.9545e-03,  1.0519e-02, -1.8804e-02]],\n              \n                       [[-5.7298e-03,  1.7915e-02, -3.1621e-03],\n                        [ 7.9957e-03,  3.4881e-03, -1.5158e-02],\n                        [ 1.8798e-03,  1.6252e-02, -1.5315e-03]],\n              \n                       [[-4.2252e-03,  8.9630e-03, -7.0830e-03],\n                        [-1.0045e-02, -2.2602e-03,  7.8443e-03],\n                        [-2.6957e-03,  1.3411e-02,  4.8645e-03]],\n              \n                       ...,\n              \n                       [[-5.3712e-03, -1.0452e-02, -1.6330e-02],\n                        [-1.0432e-02, -1.9882e-02, -1.6169e-02],\n                        [-7.2622e-03, -1.8196e-02, -6.7982e-03]],\n              \n                       [[-7.0105e-05, -1.2175e-02, -1.0749e-02],\n                        [ 1.1441e-02,  3.5827e-03,  1.7456e-02],\n                        [-4.9655e-03,  1.9057e-03, -1.7193e-02]],\n              \n                       [[ 1.7013e-02,  3.1988e-04,  5.7411e-03],\n                        [-3.7235e-04, -1.8450e-03,  3.6671e-03],\n                        [ 1.6459e-02,  1.1565e-02,  1.9842e-02]]],\n              \n              \n                      [[[ 1.6914e-02, -1.2111e-02,  1.4786e-02],\n                        [ 7.7207e-03,  2.5537e-03,  4.0743e-03],\n                        [ 1.0419e-04,  1.0066e-02, -8.1808e-03]],\n              \n                       [[ 5.5924e-03,  3.0751e-03, -1.4255e-02],\n                        [ 1.4609e-02, -6.0797e-03,  1.8090e-02],\n                        [-2.0465e-02, -1.9647e-02,  1.9963e-02]],\n              \n                       [[ 1.7703e-02,  9.7912e-04, -1.7088e-02],\n                        [-3.0930e-03,  1.0013e-02,  1.5110e-02],\n                        [-1.5153e-02, -6.5340e-03,  1.6374e-02]],\n              \n                       ...,\n              \n                       [[-1.0198e-02,  1.8628e-02, -7.3407e-03],\n                        [-2.0066e-02,  1.8155e-02,  8.2106e-03],\n                        [-5.0477e-04, -5.1193e-03, -1.9685e-02]],\n              \n                       [[ 7.3187e-03, -1.8577e-02, -1.9180e-02],\n                        [ 1.3858e-02, -1.6733e-02, -5.7723e-04],\n                        [ 1.2103e-02,  8.6336e-03, -2.0067e-02]],\n              \n                       [[-3.8180e-03,  1.9922e-03, -1.2753e-02],\n                        [ 1.9889e-02,  1.9218e-02,  1.2516e-02],\n                        [-1.6966e-02, -1.9937e-02,  6.3545e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4647e-02,  1.3599e-02, -1.1497e-02],\n                        [ 1.0819e-02,  6.2655e-03,  8.2514e-03],\n                        [ 9.7814e-03,  1.5446e-03,  5.0288e-03]],\n              \n                       [[-3.7955e-03,  1.2494e-02, -7.8703e-03],\n                        [ 4.0349e-03,  1.4197e-02, -1.1018e-02],\n                        [ 1.2082e-02, -1.9828e-03,  1.1344e-02]],\n              \n                       [[-1.6060e-02,  5.2254e-03,  1.3679e-02],\n                        [ 2.3551e-03, -5.8034e-03, -1.0188e-02],\n                        [-7.8099e-03, -7.3378e-03, -1.6845e-02]],\n              \n                       ...,\n              \n                       [[ 4.8750e-03, -1.5202e-02, -8.3033e-03],\n                        [-1.4143e-02,  9.6245e-03,  1.0595e-03],\n                        [-6.6992e-03,  1.8018e-02,  1.4028e-02]],\n              \n                       [[-2.4361e-03,  8.2809e-03, -6.7384e-03],\n                        [-2.4594e-03,  4.9077e-03,  1.8375e-02],\n                        [-4.1593e-03, -3.5705e-03, -1.3529e-02]],\n              \n                       [[-1.7012e-02,  1.9748e-02,  1.9104e-02],\n                        [-1.4910e-02, -1.9546e-02,  1.1406e-02],\n                        [-1.7544e-04,  1.5866e-02,  3.8805e-03]]],\n              \n              \n                      [[[-4.2661e-03,  2.0544e-02, -2.0223e-02],\n                        [-1.7558e-02,  1.2315e-02, -1.1358e-03],\n                        [-9.5695e-03,  1.7591e-02, -1.8437e-02]],\n              \n                       [[-7.6622e-03,  1.3523e-02, -1.2805e-02],\n                        [ 4.2950e-03, -7.9838e-03, -8.6255e-03],\n                        [ 1.5282e-03, -8.8083e-03,  5.8126e-03]],\n              \n                       [[ 1.2428e-02,  1.6649e-03, -1.8423e-02],\n                        [ 3.3804e-03, -9.0342e-03, -2.8731e-03],\n                        [ 2.8868e-03, -4.1382e-03,  1.6776e-02]],\n              \n                       ...,\n              \n                       [[ 1.6678e-02, -4.2476e-03, -9.8835e-03],\n                        [-9.7655e-03, -3.7623e-03,  5.0571e-03],\n                        [ 1.0131e-02, -7.6768e-03, -5.4080e-04]],\n              \n                       [[ 1.7999e-02,  5.0342e-03, -2.2092e-03],\n                        [ 1.2079e-02, -8.4492e-03, -1.6282e-02],\n                        [-2.0245e-02,  4.7685e-03, -9.7620e-03]],\n              \n                       [[-4.6216e-03, -1.1652e-02, -1.2818e-02],\n                        [ 1.2088e-02, -9.3832e-03, -4.1677e-03],\n                        [ 1.1476e-02, -4.4116e-03, -2.0018e-02]]],\n              \n              \n                      [[[ 3.7413e-03, -1.8938e-02, -1.2220e-02],\n                        [ 1.7449e-02,  9.5147e-03,  2.5178e-03],\n                        [-6.6552e-03,  2.6520e-03, -2.0583e-02]],\n              \n                       [[ 1.9046e-02,  1.7330e-03,  3.4585e-03],\n                        [ 1.6316e-02, -1.8740e-02,  1.6343e-02],\n                        [-8.1862e-03, -1.9654e-02,  6.7754e-04]],\n              \n                       [[-7.8348e-03, -1.0483e-02, -1.1580e-02],\n                        [ 2.0537e-02, -1.2595e-02,  4.6942e-03],\n                        [ 5.1139e-04, -8.2631e-04, -1.3213e-03]],\n              \n                       ...,\n              \n                       [[ 2.0120e-02, -1.8718e-02,  7.1457e-03],\n                        [ 8.7498e-03, -8.0881e-03, -8.0977e-03],\n                        [-1.8490e-02, -2.0089e-02,  2.6450e-04]],\n              \n                       [[ 3.0537e-03, -8.0446e-03, -9.7033e-03],\n                        [ 2.9420e-03,  1.5974e-02, -8.4568e-03],\n                        [-4.6306e-03,  7.5076e-03, -9.9498e-04]],\n              \n                       [[-1.7441e-02, -4.8928e-03,  2.0088e-02],\n                        [ 1.1744e-02, -1.9409e-02, -1.2495e-02],\n                        [ 1.6826e-02, -6.6388e-03, -1.3236e-03]]]], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up3.conv.double_conv.3.weight',\n              tensor([[[[-6.2617e-03,  5.1519e-03,  1.0535e-02],\n                        [ 2.2614e-02,  2.3770e-02,  7.1172e-03],\n                        [-9.0252e-04, -2.0448e-02, -2.0432e-02]],\n              \n                       [[-5.3073e-03,  2.0543e-03, -1.9999e-02],\n                        [ 1.7058e-02,  4.4323e-03,  2.0256e-02],\n                        [ 1.6059e-02,  7.8848e-03,  2.6898e-02]],\n              \n                       [[ 2.4905e-02, -9.5489e-04, -4.0310e-05],\n                        [ 2.6839e-02,  1.0395e-02, -1.1824e-02],\n                        [ 1.3696e-02, -4.7753e-03,  4.4547e-03]],\n              \n                       ...,\n              \n                       [[-4.0551e-03, -2.0774e-02,  5.0831e-03],\n                        [ 8.9578e-03, -2.4251e-02, -2.7485e-02],\n                        [-1.1212e-02, -3.5667e-03, -2.9207e-02]],\n              \n                       [[-2.5817e-02,  2.8529e-02, -2.4398e-02],\n                        [ 2.0831e-02,  1.4292e-02, -1.8673e-02],\n                        [-8.5094e-04, -1.2406e-03,  3.7525e-04]],\n              \n                       [[ 2.1931e-03,  6.2044e-03, -9.8672e-03],\n                        [-6.0165e-03,  7.0416e-03, -3.2293e-03],\n                        [-1.1025e-02, -1.1666e-02, -1.8839e-02]]],\n              \n              \n                      [[[-1.9571e-02,  1.3345e-02, -3.1977e-03],\n                        [-2.4555e-02, -3.5323e-03, -2.8703e-02],\n                        [-1.5313e-02,  2.1116e-02, -1.0758e-03]],\n              \n                       [[-1.0014e-02,  1.1471e-02, -2.2742e-02],\n                        [ 2.5164e-02,  1.5579e-02, -2.2211e-02],\n                        [ 2.7174e-02,  1.9207e-02, -1.7626e-02]],\n              \n                       [[ 2.7689e-02, -5.7403e-03, -1.0863e-02],\n                        [ 5.0870e-03,  6.7373e-03, -2.0150e-02],\n                        [ 2.9319e-02, -9.6329e-03, -2.0385e-02]],\n              \n                       ...,\n              \n                       [[-2.4959e-02,  1.2766e-03,  2.4264e-03],\n                        [ 2.1160e-02, -2.1553e-02,  1.6825e-02],\n                        [ 2.6579e-02,  6.6060e-03,  2.5650e-02]],\n              \n                       [[ 4.5595e-03,  1.9319e-03, -2.5173e-02],\n                        [-2.3925e-02, -8.3372e-03, -9.0146e-03],\n                        [ 1.7461e-02, -2.5896e-02, -1.8144e-02]],\n              \n                       [[ 2.5831e-02, -2.1761e-02, -2.9396e-02],\n                        [ 2.7635e-02, -1.2928e-02,  5.8588e-03],\n                        [-2.0192e-02,  4.7528e-03,  2.8390e-02]]],\n              \n              \n                      [[[ 1.8739e-03, -1.3140e-02,  2.6128e-02],\n                        [ 1.1566e-02,  3.5446e-03, -5.1995e-03],\n                        [ 5.5016e-03, -4.5294e-03,  1.9544e-02]],\n              \n                       [[-9.9646e-03,  2.7664e-02,  1.1371e-02],\n                        [ 1.2055e-02,  1.6825e-02, -1.1272e-02],\n                        [ 1.3120e-02,  1.7465e-02,  1.1575e-02]],\n              \n                       [[-4.8596e-03,  9.3461e-03,  2.0105e-02],\n                        [ 1.2126e-02, -2.2240e-03,  1.3572e-02],\n                        [-2.8769e-02, -7.9955e-03, -1.2733e-02]],\n              \n                       ...,\n              \n                       [[ 2.5646e-02,  1.6559e-02, -2.2198e-02],\n                        [-3.0433e-03,  2.7646e-02,  2.8915e-02],\n                        [ 2.3706e-02, -2.5853e-02, -8.8919e-05]],\n              \n                       [[ 1.9385e-02,  9.4940e-03, -1.7507e-02],\n                        [-1.0995e-02, -1.9027e-02,  2.6517e-02],\n                        [ 6.5096e-03,  8.3432e-03,  4.3078e-03]],\n              \n                       [[-1.2435e-02, -1.2040e-02,  6.4921e-03],\n                        [-1.9559e-02,  2.2276e-02,  1.2324e-02],\n                        [ 7.4537e-03,  5.5965e-03, -2.4149e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.9395e-02,  2.0365e-02, -1.6215e-02],\n                        [ 1.8015e-02,  1.1132e-02, -5.3747e-03],\n                        [ 4.5775e-03,  1.9513e-02,  5.4436e-03]],\n              \n                       [[ 2.0589e-02,  4.0204e-03, -7.1212e-03],\n                        [-1.7708e-02, -2.7610e-02,  2.9521e-03],\n                        [ 1.4294e-02, -6.5115e-03, -1.4379e-03]],\n              \n                       [[ 2.8011e-02,  1.6216e-02,  2.5210e-02],\n                        [-1.6498e-02,  1.0523e-02,  2.6155e-02],\n                        [ 1.6074e-02, -8.3713e-03,  2.2026e-02]],\n              \n                       ...,\n              \n                       [[-1.3617e-02, -1.4065e-02, -2.3103e-02],\n                        [ 2.4879e-02, -8.9402e-03,  3.0990e-03],\n                        [ 1.3965e-03, -2.5021e-02, -2.0546e-02]],\n              \n                       [[ 2.0246e-03, -7.9078e-03, -2.6747e-02],\n                        [ 2.9376e-02, -6.2544e-03, -1.8549e-02],\n                        [ 1.5150e-02, -3.9595e-03,  2.3443e-03]],\n              \n                       [[-3.6495e-03, -1.0052e-02,  1.2397e-03],\n                        [ 3.8338e-03, -2.8786e-02, -5.1455e-03],\n                        [-1.5915e-02,  2.8991e-02,  6.3032e-03]]],\n              \n              \n                      [[[-2.0503e-02, -2.8574e-02,  1.7111e-02],\n                        [-1.5106e-02,  2.2639e-02,  3.2666e-03],\n                        [ 1.1444e-02, -9.7533e-03,  1.8418e-02]],\n              \n                       [[-2.8729e-02, -1.7639e-02,  1.5558e-02],\n                        [ 2.1907e-02,  2.6665e-02, -2.0398e-02],\n                        [ 4.7236e-03,  2.2406e-02, -1.1982e-03]],\n              \n                       [[-6.9613e-03,  1.6444e-02,  1.0986e-04],\n                        [-2.5102e-02,  2.7951e-02,  1.8224e-02],\n                        [-9.3261e-03, -2.2952e-02, -1.9339e-02]],\n              \n                       ...,\n              \n                       [[ 6.3333e-03, -8.1322e-03,  3.5560e-03],\n                        [-2.3900e-02, -2.8754e-02, -2.0715e-02],\n                        [ 1.3923e-02,  1.0834e-02, -1.1983e-02]],\n              \n                       [[-1.2872e-02,  6.1885e-03, -1.2684e-02],\n                        [ 8.5061e-03, -1.3273e-03, -1.6401e-03],\n                        [ 3.5566e-03,  1.4142e-02,  7.0110e-03]],\n              \n                       [[ 1.2880e-02,  6.1687e-03, -9.6315e-03],\n                        [ 1.5918e-02,  2.2629e-03, -2.7104e-03],\n                        [-8.4794e-04,  2.0819e-02, -2.2515e-02]]],\n              \n              \n                      [[[ 8.6197e-03,  2.3163e-02,  1.9551e-02],\n                        [ 2.2528e-02,  1.8106e-02,  1.0401e-02],\n                        [-1.7955e-03, -5.1270e-03,  9.9206e-03]],\n              \n                       [[ 2.3529e-02,  1.5074e-02, -1.5779e-02],\n                        [-2.8125e-02, -1.9706e-02, -2.7739e-02],\n                        [ 1.2969e-02, -6.8372e-03, -1.8700e-02]],\n              \n                       [[-1.6456e-02, -1.9319e-02,  2.9451e-02],\n                        [-4.3081e-03,  1.6394e-02,  2.0039e-02],\n                        [-2.6109e-02,  1.8154e-02, -4.1342e-03]],\n              \n                       ...,\n              \n                       [[ 1.4506e-02, -2.9666e-03,  3.6261e-03],\n                        [ 1.6303e-02, -4.9343e-03, -1.7006e-02],\n                        [ 2.6239e-02, -2.3413e-02,  1.2565e-02]],\n              \n                       [[-7.7776e-03,  2.6909e-02,  1.0444e-02],\n                        [-8.7274e-03, -8.3104e-03,  2.3266e-03],\n                        [-2.4073e-02, -1.0433e-02, -1.1619e-02]],\n              \n                       [[-1.0362e-02, -2.3291e-02, -1.0579e-02],\n                        [ 1.6419e-02,  2.0854e-02,  2.4889e-02],\n                        [ 1.3606e-03, -9.4291e-03, -1.6355e-03]]]], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up4.conv.double_conv.0.weight',\n              tensor([[[[-2.4477e-02, -1.7234e-02,  2.2003e-03],\n                        [-7.8829e-03,  6.1736e-03,  1.4644e-02],\n                        [ 9.7539e-03,  5.7497e-04, -2.1407e-02]],\n              \n                       [[ 2.5615e-02,  6.0152e-03, -2.8486e-02],\n                        [ 2.1189e-02,  6.7674e-03, -1.4792e-03],\n                        [ 2.2734e-02,  1.7544e-03, -1.0535e-02]],\n              \n                       [[ 2.1016e-02,  3.9310e-03,  5.9241e-03],\n                        [-9.3318e-04,  1.3821e-02,  2.8222e-02],\n                        [ 7.3732e-03,  2.3611e-03,  2.2986e-02]],\n              \n                       ...,\n              \n                       [[-2.6076e-02,  9.7759e-03,  1.7446e-02],\n                        [-4.6081e-03, -7.8919e-03, -1.3171e-02],\n                        [ 3.6483e-03,  5.5107e-04, -2.6154e-02]],\n              \n                       [[ 2.4815e-02,  6.5554e-04, -2.6840e-02],\n                        [-5.4893e-03, -1.2978e-02, -7.7000e-03],\n                        [ 1.7822e-02, -2.0376e-02,  1.8151e-02]],\n              \n                       [[-1.3709e-02, -2.1298e-02,  1.4319e-02],\n                        [-1.1540e-02,  2.9451e-03,  4.6603e-03],\n                        [ 1.6498e-02, -2.2247e-02, -2.6400e-02]]],\n              \n              \n                      [[[-2.9053e-02,  6.6088e-03,  2.8600e-02],\n                        [-8.5117e-03,  3.7488e-03,  2.5909e-02],\n                        [-6.6344e-03, -1.8867e-02,  2.1232e-02]],\n              \n                       [[ 2.7659e-02, -1.5675e-02, -1.2514e-02],\n                        [ 6.8806e-03, -2.4540e-02, -2.0591e-02],\n                        [-6.2750e-03, -2.9055e-02,  2.7674e-02]],\n              \n                       [[ 6.6344e-03, -2.5097e-02, -2.7987e-02],\n                        [-1.9412e-02, -1.7099e-02,  2.4543e-02],\n                        [-6.0892e-03, -1.9663e-02, -2.1830e-02]],\n              \n                       ...,\n              \n                       [[-2.4330e-02, -5.3355e-04,  1.6593e-02],\n                        [-1.5296e-02, -1.2302e-02, -2.1773e-02],\n                        [-2.4805e-02, -2.7568e-02, -5.2265e-03]],\n              \n                       [[ 1.4438e-02, -1.1498e-02, -5.8588e-03],\n                        [ 2.3541e-02,  2.8545e-02, -2.1781e-02],\n                        [ 2.1298e-02, -1.4740e-02,  2.0063e-02]],\n              \n                       [[-1.4228e-02,  2.7397e-02,  1.9363e-03],\n                        [ 1.3088e-02,  1.8878e-02,  2.5326e-02],\n                        [-2.7118e-02,  1.8095e-02,  1.5554e-02]]],\n              \n              \n                      [[[-2.7807e-02,  2.8756e-02, -2.4947e-02],\n                        [ 2.8239e-03,  6.4158e-03,  1.7847e-02],\n                        [-2.1316e-02, -1.1236e-02, -7.1000e-03]],\n              \n                       [[-2.2642e-02, -2.9162e-02, -2.7960e-02],\n                        [ 2.2822e-02,  2.6365e-02, -2.2013e-02],\n                        [-4.3668e-03,  5.9663e-03, -2.2929e-02]],\n              \n                       [[ 2.6231e-02,  6.2513e-04, -1.5292e-02],\n                        [-2.3744e-02,  1.0287e-02, -1.7989e-02],\n                        [ 1.4567e-02, -5.4238e-04, -1.8888e-03]],\n              \n                       ...,\n              \n                       [[ 8.2702e-03, -3.9680e-03,  4.4591e-03],\n                        [ 1.2113e-02,  1.9210e-02, -2.1732e-02],\n                        [ 1.8309e-02, -2.5562e-02, -3.4519e-03]],\n              \n                       [[ 2.0920e-02,  5.1383e-03, -2.8351e-02],\n                        [ 2.4168e-02,  2.4032e-03,  4.4554e-03],\n                        [-9.5799e-03, -4.6795e-03,  2.1697e-02]],\n              \n                       [[ 5.9437e-03,  1.4123e-03, -8.3815e-03],\n                        [ 2.3132e-02, -2.6785e-02, -1.6763e-02],\n                        [-9.6515e-03, -2.1222e-02,  2.4000e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.3391e-02,  2.3395e-02, -2.1791e-02],\n                        [ 1.8008e-02,  5.3447e-03,  2.3465e-02],\n                        [ 1.7817e-02, -3.0541e-04,  1.8585e-02]],\n              \n                       [[-1.8773e-02,  9.5143e-03, -9.0805e-03],\n                        [-1.1845e-02, -2.0910e-02,  7.6076e-03],\n                        [-1.9462e-03,  2.5138e-02, -2.8411e-02]],\n              \n                       [[ 1.2022e-02, -1.4268e-02,  1.6846e-02],\n                        [-1.5587e-02, -2.2586e-02,  1.7113e-03],\n                        [-2.0474e-02,  2.1718e-02,  2.6473e-02]],\n              \n                       ...,\n              \n                       [[-9.5288e-04, -2.0567e-02, -5.8081e-03],\n                        [-9.2609e-03,  2.2689e-02,  7.9880e-03],\n                        [-2.3267e-02, -2.2080e-03, -3.7323e-04]],\n              \n                       [[ 7.0031e-03,  1.5936e-02, -1.7355e-02],\n                        [ 9.1528e-03,  6.0140e-04, -4.6582e-03],\n                        [-2.2403e-03,  1.1589e-02,  1.3004e-02]],\n              \n                       [[ 7.5902e-03, -2.7939e-02,  1.6827e-02],\n                        [-1.1944e-02, -2.1053e-02,  7.7404e-03],\n                        [-2.4648e-02,  1.0781e-02,  1.6477e-02]]],\n              \n              \n                      [[[ 2.8526e-02, -8.3310e-03, -3.3514e-03],\n                        [ 8.7738e-03,  3.3132e-03, -2.3501e-03],\n                        [-1.5227e-02, -6.8209e-03,  7.2189e-03]],\n              \n                       [[ 3.2429e-03,  2.9305e-02,  7.2086e-03],\n                        [-2.8544e-02, -2.1567e-02, -7.0302e-03],\n                        [-1.2484e-02,  4.2848e-03, -1.5662e-02]],\n              \n                       [[ 1.4185e-03,  6.2046e-03,  2.1498e-02],\n                        [ 1.4784e-02, -2.4929e-02, -2.7400e-02],\n                        [-2.6303e-05,  2.4616e-02, -1.2550e-02]],\n              \n                       ...,\n              \n                       [[-1.1245e-02, -6.3400e-03, -1.4372e-02],\n                        [-2.6327e-02, -9.7659e-03, -1.9709e-03],\n                        [-2.4333e-03,  5.2920e-03,  1.3149e-02]],\n              \n                       [[ 2.8700e-03,  7.3612e-03,  2.3691e-03],\n                        [-2.7523e-02,  1.5241e-02,  1.3450e-02],\n                        [ 2.5740e-03, -3.4698e-03, -1.3424e-02]],\n              \n                       [[-1.4515e-02, -2.1749e-02,  1.3343e-02],\n                        [ 2.5754e-02,  3.5074e-03,  1.9747e-02],\n                        [ 2.7382e-03,  1.4910e-02, -2.2954e-02]]],\n              \n              \n                      [[[-4.3458e-03, -1.3681e-02,  1.8517e-02],\n                        [-1.4100e-02,  2.4556e-02, -1.6581e-03],\n                        [-2.7384e-02,  1.7085e-02,  1.9694e-02]],\n              \n                       [[ 5.4223e-03, -1.7057e-02, -6.0624e-03],\n                        [ 2.8144e-02, -1.2404e-02, -9.2200e-05],\n                        [ 8.0187e-03, -2.4534e-02, -6.1641e-03]],\n              \n                       [[ 4.4628e-03, -2.3212e-02,  1.8625e-02],\n                        [ 2.0626e-03, -1.1065e-02,  2.2116e-02],\n                        [-2.3691e-02,  7.7271e-03,  2.3667e-02]],\n              \n                       ...,\n              \n                       [[ 1.6437e-02,  1.7844e-02,  4.2858e-03],\n                        [ 1.8507e-02, -1.4175e-02,  6.2452e-03],\n                        [-2.2591e-02, -1.6163e-02,  2.8446e-02]],\n              \n                       [[ 7.0578e-03,  8.5772e-03,  1.2336e-03],\n                        [-2.7270e-02, -4.7153e-03,  1.8364e-02],\n                        [-1.7723e-02, -6.1744e-03, -2.6519e-02]],\n              \n                       [[ 2.6981e-03,  2.3110e-02, -1.9544e-02],\n                        [ 2.8593e-02,  2.6731e-02,  2.1887e-02],\n                        [-9.6571e-04,  1.7459e-02,  3.4465e-03]]]], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up4.conv.double_conv.3.weight',\n              tensor([[[[ 3.1426e-03, -3.7804e-02, -1.9636e-03],\n                        [-3.3168e-02,  2.4599e-03, -2.5361e-02],\n                        [ 2.0291e-02, -3.1659e-02, -2.2596e-02]],\n              \n                       [[-8.4917e-03, -3.0465e-04, -2.1817e-02],\n                        [ 2.9646e-03,  2.4069e-02, -2.6871e-02],\n                        [ 2.7976e-02, -2.9426e-02, -1.9063e-02]],\n              \n                       [[ 3.4714e-02,  2.5515e-02,  2.2645e-03],\n                        [ 1.1169e-02, -1.5637e-02, -3.2919e-02],\n                        [-1.3760e-02,  1.0523e-03,  3.2319e-02]],\n              \n                       ...,\n              \n                       [[-2.6632e-02,  1.5643e-02, -3.1304e-03],\n                        [-6.5018e-03,  1.7912e-02, -1.7220e-02],\n                        [ 3.1036e-02,  3.4784e-02, -1.4025e-02]],\n              \n                       [[ 3.3626e-02, -2.4100e-02,  3.6708e-02],\n                        [-2.1758e-02, -1.4161e-02, -2.8572e-02],\n                        [ 5.2657e-03,  2.2184e-02, -1.2249e-02]],\n              \n                       [[ 3.9889e-02, -9.9724e-03,  1.4062e-03],\n                        [ 1.6991e-02, -5.8726e-03, -1.2741e-02],\n                        [-2.3483e-02,  3.6793e-02,  1.0728e-03]]],\n              \n              \n                      [[[-1.1431e-02,  2.8004e-03, -2.1472e-02],\n                        [-4.7250e-03,  3.1195e-02, -3.4145e-02],\n                        [-3.9074e-02, -9.0451e-03,  3.6595e-02]],\n              \n                       [[-3.4954e-02, -2.8686e-02,  7.4445e-03],\n                        [-3.4594e-02, -1.5361e-02,  3.2916e-02],\n                        [ 7.3619e-03, -2.8733e-02, -2.8171e-02]],\n              \n                       [[-1.6132e-02,  9.1593e-03, -1.5983e-03],\n                        [ 1.9147e-02, -3.0231e-02,  3.5481e-02],\n                        [-2.8131e-02, -1.5797e-02,  1.4560e-02]],\n              \n                       ...,\n              \n                       [[-2.0996e-03, -2.3411e-02, -1.1860e-02],\n                        [ 3.8093e-02,  3.5264e-02,  3.0247e-02],\n                        [ 1.3708e-02, -2.7209e-02,  3.5293e-02]],\n              \n                       [[-1.4823e-02, -1.3127e-02, -1.8602e-02],\n                        [ 3.1382e-02, -2.8936e-02, -3.5547e-02],\n                        [ 2.8250e-02,  2.5477e-02, -1.1684e-02]],\n              \n                       [[-3.4762e-03, -2.8827e-02,  2.2720e-02],\n                        [ 1.9048e-02,  1.9151e-02,  4.8282e-03],\n                        [ 3.6979e-02,  1.1263e-02,  1.4983e-02]]],\n              \n              \n                      [[[ 4.0528e-02, -1.5267e-02,  4.1640e-02],\n                        [ 1.4580e-02,  2.1254e-03,  2.1454e-02],\n                        [ 2.3367e-02,  2.4535e-02, -2.9547e-02]],\n              \n                       [[ 1.2478e-02, -3.2175e-02,  3.1261e-02],\n                        [-2.5070e-02,  1.0443e-02, -1.7667e-02],\n                        [-3.9835e-03, -1.4524e-02,  2.9181e-02]],\n              \n                       [[ 8.7496e-03,  1.6791e-02, -3.3366e-02],\n                        [ 3.9007e-02,  1.0403e-02,  3.8254e-02],\n                        [-1.2029e-02,  1.1168e-02, -1.9442e-02]],\n              \n                       ...,\n              \n                       [[ 2.2030e-02,  1.0903e-02, -1.4863e-02],\n                        [-1.3346e-02, -3.5193e-02,  3.2643e-02],\n                        [-3.8632e-02, -8.3370e-03,  1.8904e-02]],\n              \n                       [[-3.9616e-02, -2.5855e-02,  3.3651e-02],\n                        [ 3.9193e-02,  2.7768e-02,  1.4065e-02],\n                        [-8.8412e-03, -2.1744e-02, -2.0466e-02]],\n              \n                       [[-9.5175e-03, -3.2115e-02,  2.8135e-02],\n                        [-3.5135e-02, -3.5658e-02, -1.6859e-02],\n                        [ 3.8371e-02,  4.0490e-03,  2.5179e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6391e-02,  5.2747e-03,  3.4211e-02],\n                        [-3.6951e-02, -2.0392e-02,  1.9124e-02],\n                        [-4.0592e-03, -2.1158e-02, -5.6858e-03]],\n              \n                       [[-1.2450e-02, -7.7264e-03, -2.7716e-02],\n                        [ 3.4721e-02,  2.8399e-02,  3.7686e-02],\n                        [ 3.6166e-02,  1.7743e-02, -3.3313e-02]],\n              \n                       [[-2.4009e-03,  2.7938e-02,  8.2821e-03],\n                        [-1.0567e-02, -1.0721e-02,  3.9096e-02],\n                        [-1.0329e-02,  3.5188e-04,  1.9992e-02]],\n              \n                       ...,\n              \n                       [[ 4.0091e-02,  2.7190e-02, -3.8786e-02],\n                        [ 3.7762e-02,  1.6390e-02, -4.1539e-02],\n                        [ 2.8608e-02, -3.4842e-02, -1.5290e-02]],\n              \n                       [[ 2.5458e-02,  3.8800e-02,  1.8157e-02],\n                        [-3.0404e-02, -2.8858e-02, -3.7904e-02],\n                        [-1.7384e-02,  1.3624e-02, -3.8238e-02]],\n              \n                       [[-3.4968e-02, -2.1631e-02,  1.8572e-02],\n                        [ 3.9958e-02,  3.1534e-02, -2.6919e-03],\n                        [ 2.9025e-02, -2.5323e-02,  1.8108e-02]]],\n              \n              \n                      [[[ 1.4118e-02,  1.3075e-02,  7.9425e-04],\n                        [-1.5709e-02,  2.2579e-02, -3.4406e-03],\n                        [ 3.9156e-02, -5.3889e-03, -4.1343e-02]],\n              \n                       [[-1.1825e-03, -7.4790e-03,  3.0482e-02],\n                        [-4.0314e-02, -1.9415e-02, -5.4573e-05],\n                        [-3.6205e-03, -4.0538e-02,  1.6526e-02]],\n              \n                       [[ 3.1517e-02,  1.2538e-02,  1.7676e-03],\n                        [ 2.2461e-02, -2.9065e-02,  3.1906e-02],\n                        [-3.9866e-02, -2.3473e-02,  4.0793e-02]],\n              \n                       ...,\n              \n                       [[-2.2015e-02, -1.4035e-03, -3.4191e-02],\n                        [ 3.4649e-02,  2.7996e-02,  2.5186e-02],\n                        [-2.6122e-02, -3.7787e-02, -3.5784e-02]],\n              \n                       [[-3.5926e-03, -1.5855e-02, -2.4558e-02],\n                        [-3.5714e-02,  4.0327e-02,  3.9204e-02],\n                        [ 1.6102e-03, -2.2671e-02,  3.9940e-02]],\n              \n                       [[-4.1120e-02,  6.4742e-03,  1.8772e-02],\n                        [ 3.4173e-02,  5.7441e-04, -1.9311e-02],\n                        [-1.4727e-02,  1.7990e-02, -1.8958e-02]]],\n              \n              \n                      [[[ 2.9624e-02, -8.9972e-03,  4.0076e-02],\n                        [ 1.4882e-02, -1.9439e-02,  8.6693e-03],\n                        [-4.0603e-02,  1.5571e-02, -2.9153e-02]],\n              \n                       [[-3.5557e-02,  1.8946e-04,  2.2721e-02],\n                        [ 2.9935e-03,  8.9930e-03, -2.0757e-02],\n                        [ 2.0412e-02,  5.7608e-03,  2.6245e-02]],\n              \n                       [[-6.2162e-03, -7.0439e-04,  1.3922e-02],\n                        [-9.8026e-03,  2.8211e-02, -3.7612e-03],\n                        [-3.1022e-02, -2.4241e-02,  2.0704e-03]],\n              \n                       ...,\n              \n                       [[ 1.8656e-05, -3.5449e-02, -1.9142e-02],\n                        [-3.7448e-02, -3.8316e-02,  3.6445e-02],\n                        [ 1.8268e-02, -3.2087e-02, -3.0568e-02]],\n              \n                       [[-2.6703e-02, -7.0255e-04,  1.3062e-02],\n                        [ 9.2566e-03,  3.0957e-02, -3.9456e-02],\n                        [ 2.6741e-02,  1.7924e-02,  2.6267e-02]],\n              \n                       [[-3.0110e-02, -1.6314e-03, -2.8098e-02],\n                        [ 2.0860e-02,  1.5562e-02,  2.9175e-02],\n                        [ 9.1814e-03,  2.6883e-02,  2.8830e-02]]]], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.outc.conv.weight',\n              tensor([[[[ 0.0984]],\n              \n                       [[-0.0668]],\n              \n                       [[-0.0782]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0089]],\n              \n                       [[-0.0501]],\n              \n                       [[-0.0261]],\n              \n                       [[ 0.0791]],\n              \n                       [[-0.1128]],\n              \n                       [[ 0.0102]],\n              \n                       [[ 0.0258]],\n              \n                       [[-0.0357]],\n              \n                       [[-0.0674]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.0549]],\n              \n                       [[-0.0972]],\n              \n                       [[-0.1207]],\n              \n                       [[ 0.1104]],\n              \n                       [[ 0.0293]],\n              \n                       [[-0.1182]],\n              \n                       [[ 0.1166]],\n              \n                       [[ 0.1038]],\n              \n                       [[-0.0085]],\n              \n                       [[-0.0039]],\n              \n                       [[ 0.0621]],\n              \n                       [[ 0.0331]],\n              \n                       [[ 0.0618]],\n              \n                       [[ 0.0310]],\n              \n                       [[ 0.1245]],\n              \n                       [[-0.1027]],\n              \n                       [[ 0.0523]],\n              \n                       [[ 0.0731]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0495]],\n              \n                       [[ 0.1218]],\n              \n                       [[ 0.1106]],\n              \n                       [[ 0.0079]],\n              \n                       [[-0.1117]],\n              \n                       [[ 0.1123]],\n              \n                       [[-0.0453]],\n              \n                       [[ 0.0750]],\n              \n                       [[ 0.0378]],\n              \n                       [[ 0.1220]],\n              \n                       [[-0.1052]],\n              \n                       [[-0.0909]],\n              \n                       [[-0.0841]],\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0207]],\n              \n                       [[-0.0161]],\n              \n                       [[-0.0815]],\n              \n                       [[ 0.0737]],\n              \n                       [[-0.0565]],\n              \n                       [[-0.0620]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.1087]],\n              \n                       [[ 0.0442]],\n              \n                       [[-0.0377]],\n              \n                       [[-0.0474]],\n              \n                       [[ 0.0807]],\n              \n                       [[ 0.0298]],\n              \n                       [[ 0.0700]],\n              \n                       [[ 0.0749]],\n              \n                       [[ 0.0847]],\n              \n                       [[-0.1145]]]], device='cuda:0')),\n             ('module.module.outc.conv.bias',\n              tensor([-0.0712], device='cuda:0'))])</pre> In\u00a0[34]: Copied! <pre># \u200b\u53e6\u5916\u200b\uff0c\u200b\u5982\u679c\u200b\u4fdd\u5b58\u200b\u7684\u200b\u662f\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u5efa\u8bae\u200b\u91c7\u7528\u200b\u63d0\u53d6\u200b\u6743\u91cd\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6784\u5efa\u200b\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a\nunet_mul.state_dict = loaded_unet_mul.state_dict\nunet_mul = nn.DataParallel(unet_mul).cuda()\nunet_mul.state_dict()\n</pre> # \u200b\u53e6\u5916\u200b\uff0c\u200b\u5982\u679c\u200b\u4fdd\u5b58\u200b\u7684\u200b\u662f\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u5efa\u8bae\u200b\u91c7\u7528\u200b\u63d0\u53d6\u200b\u6743\u91cd\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6784\u5efa\u200b\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a unet_mul.state_dict = loaded_unet_mul.state_dict unet_mul = nn.DataParallel(unet_mul).cuda() unet_mul.state_dict() Out[34]: <pre>OrderedDict([('module.module.inc.double_conv.0.weight',\n              tensor([[[[-0.1569, -0.0516,  0.1381],\n                        [-0.0167,  0.1114, -0.1482],\n                        [-0.1659, -0.0492, -0.1526]],\n              \n                       [[ 0.0871,  0.1102, -0.1270],\n                        [ 0.1058,  0.0541, -0.0767],\n                        [ 0.1247,  0.1813,  0.1895]],\n              \n                       [[ 0.0929, -0.1305,  0.0531],\n                        [-0.0972, -0.1668, -0.0183],\n                        [-0.1754, -0.0862,  0.0373]]],\n              \n              \n                      [[[-0.0014,  0.1440, -0.0519],\n                        [ 0.1643,  0.1829,  0.1713],\n                        [-0.0702, -0.0426,  0.0083]],\n              \n                       [[ 0.1057,  0.0303,  0.0280],\n                        [-0.0306, -0.0898,  0.1635],\n                        [-0.1388, -0.0430,  0.0839]],\n              \n                       [[ 0.0840,  0.1753,  0.0916],\n                        [ 0.0819,  0.1624,  0.1901],\n                        [ 0.1914,  0.0483, -0.0875]]],\n              \n              \n                      [[[ 0.1197, -0.1618, -0.1778],\n                        [ 0.0866, -0.0638, -0.1615],\n                        [ 0.1437, -0.1523, -0.1007]],\n              \n                       [[-0.1395, -0.0602, -0.0457],\n                        [ 0.0582, -0.1701,  0.0586],\n                        [-0.1828,  0.0463,  0.1460]],\n              \n                       [[ 0.0735,  0.0299, -0.0629],\n                        [-0.0345, -0.0038,  0.0794],\n                        [-0.0958, -0.1519, -0.0411]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1095,  0.0703, -0.0860],\n                        [-0.1243, -0.0596, -0.1636],\n                        [ 0.0819,  0.0457,  0.1248]],\n              \n                       [[-0.1077, -0.1394,  0.0295],\n                        [ 0.1442, -0.1271,  0.1462],\n                        [-0.1011,  0.1301, -0.1294]],\n              \n                       [[-0.1653, -0.1431, -0.1031],\n                        [ 0.0511,  0.1370,  0.0210],\n                        [-0.1709,  0.0438, -0.0352]]],\n              \n              \n                      [[[-0.0893,  0.1826, -0.0856],\n                        [-0.1679,  0.0620,  0.1056],\n                        [-0.0206, -0.1745, -0.0500]],\n              \n                       [[ 0.0784,  0.0502,  0.1084],\n                        [-0.0746, -0.1213,  0.0849],\n                        [-0.1682, -0.1131, -0.1769]],\n              \n                       [[ 0.1111, -0.0814,  0.1804],\n                        [-0.0183,  0.0950, -0.0082],\n                        [-0.0761, -0.0757, -0.1657]]],\n              \n              \n                      [[[ 0.0543, -0.0157, -0.1387],\n                        [ 0.1503,  0.1388,  0.0653],\n                        [ 0.1474, -0.0991, -0.1478]],\n              \n                       [[ 0.0953, -0.1215,  0.1848],\n                        [-0.0360,  0.0052, -0.1841],\n                        [-0.1859, -0.0946,  0.1727]],\n              \n                       [[-0.0668, -0.0142,  0.1517],\n                        [-0.1101,  0.0217, -0.1021],\n                        [-0.1509,  0.0912,  0.1346]]]], device='cuda:0')),\n             ('module.module.inc.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.inc.double_conv.3.weight',\n              tensor([[[[-4.1079e-02,  2.4625e-02, -5.8618e-03],\n                        [-3.6583e-02, -1.7239e-02,  2.4723e-02],\n                        [-2.0914e-03,  3.0168e-02, -2.0448e-02]],\n              \n                       [[ 4.1381e-03, -2.0328e-02, -2.9454e-02],\n                        [ 1.0681e-02, -3.6947e-02, -1.4246e-02],\n                        [-3.8679e-03,  2.3515e-02,  7.0796e-03]],\n              \n                       [[-3.3515e-02,  2.3345e-02, -5.7584e-04],\n                        [ 3.0752e-02, -3.5342e-02, -3.0192e-02],\n                        [ 3.0137e-02,  4.9735e-03,  3.0268e-02]],\n              \n                       ...,\n              \n                       [[ 2.6247e-02,  3.5036e-02, -2.7703e-02],\n                        [ 1.2037e-02, -1.1631e-02, -3.5691e-02],\n                        [ 1.8343e-02,  2.3172e-02, -2.3284e-02]],\n              \n                       [[ 3.9720e-02, -2.9578e-02, -3.8113e-02],\n                        [ 6.7576e-04, -4.0048e-02, -6.3216e-05],\n                        [ 1.9008e-02,  3.8545e-02,  3.0812e-02]],\n              \n                       [[-6.7981e-03, -1.5902e-03,  3.7965e-02],\n                        [ 8.6753e-03, -1.4569e-03, -1.9033e-02],\n                        [-2.0683e-02, -2.7206e-02,  2.5007e-02]]],\n              \n              \n                      [[[-1.3453e-02,  4.8410e-03,  6.3604e-03],\n                        [ 1.4860e-02, -1.9902e-04, -3.7245e-02],\n                        [ 1.2965e-02,  9.0473e-03,  2.3664e-02]],\n              \n                       [[-3.6142e-02, -2.9932e-02, -2.7691e-02],\n                        [ 2.6747e-02,  2.1051e-02, -6.9610e-03],\n                        [ 1.6672e-02,  2.4121e-02,  3.9934e-02]],\n              \n                       [[ 1.8793e-02,  3.8492e-02, -1.8463e-02],\n                        [ 2.4193e-02,  1.2931e-02, -2.9170e-02],\n                        [-2.2503e-02,  7.4183e-03, -9.9386e-03]],\n              \n                       ...,\n              \n                       [[-3.5583e-02,  1.0415e-02,  2.6884e-03],\n                        [-2.4120e-02, -1.6516e-02, -3.5117e-02],\n                        [-1.1389e-02, -3.2349e-02, -5.4190e-03]],\n              \n                       [[ 1.0794e-02, -1.4699e-02, -3.9218e-02],\n                        [ 7.2620e-03,  2.3942e-02, -9.0866e-03],\n                        [-3.9156e-02, -2.2665e-02,  3.0706e-02]],\n              \n                       [[ 2.5315e-02,  3.8635e-02, -1.4174e-03],\n                        [ 4.2061e-03, -3.3006e-02, -2.6736e-02],\n                        [-1.2201e-02,  2.4348e-02, -2.8096e-02]]],\n              \n              \n                      [[[-2.9801e-02,  1.3935e-02, -2.9342e-02],\n                        [-4.2913e-03,  9.5715e-03,  3.7494e-02],\n                        [ 2.2639e-02,  1.3474e-02,  2.3872e-02]],\n              \n                       [[ 1.6016e-03,  2.9424e-02,  2.3341e-02],\n                        [-1.2055e-02, -3.9560e-02, -1.5007e-02],\n                        [ 2.5384e-02, -4.1246e-02,  2.9730e-02]],\n              \n                       [[ 2.2965e-02, -2.7511e-02, -1.2306e-02],\n                        [-1.4792e-02,  2.7210e-03, -3.1689e-02],\n                        [ 3.1452e-02, -2.1154e-02,  3.2495e-02]],\n              \n                       ...,\n              \n                       [[ 6.1211e-03, -1.7085e-03,  1.0614e-02],\n                        [-1.3250e-03,  2.0869e-02,  7.6367e-03],\n                        [-3.3447e-02, -3.5193e-02, -3.4296e-02]],\n              \n                       [[ 2.6182e-02, -9.0026e-03,  4.3130e-03],\n                        [-1.9488e-02,  3.6438e-02, -2.9620e-02],\n                        [-4.0476e-02,  8.5702e-03,  2.2612e-02]],\n              \n                       [[ 1.9338e-03, -1.3990e-02,  8.3609e-03],\n                        [-1.3580e-02, -3.6543e-02,  2.8900e-02],\n                        [ 2.8948e-02, -2.2145e-03, -2.4276e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.0462e-03,  3.9649e-02,  1.0557e-02],\n                        [ 3.1926e-02,  3.8248e-02,  9.8494e-03],\n                        [ 1.2289e-03, -1.9980e-02, -3.3557e-02]],\n              \n                       [[-4.0275e-02,  1.1621e-02,  1.1366e-02],\n                        [-1.9881e-02,  6.3696e-03,  4.0948e-02],\n                        [-1.5219e-02, -1.6628e-02,  2.8343e-03]],\n              \n                       [[ 2.7490e-02,  3.5501e-02,  3.2039e-02],\n                        [ 3.5091e-03,  1.1285e-02,  1.5338e-02],\n                        [ 1.9410e-02, -5.1183e-03, -2.9545e-02]],\n              \n                       ...,\n              \n                       [[-2.0173e-02,  3.1788e-02,  8.5245e-03],\n                        [ 1.2969e-02,  1.4843e-02,  1.5726e-02],\n                        [ 3.1018e-02, -2.0554e-02,  1.6326e-02]],\n              \n                       [[-3.5004e-02,  3.6636e-02,  5.2004e-03],\n                        [ 2.9926e-02,  3.7449e-02,  6.1300e-04],\n                        [-5.1867e-04, -4.0083e-02, -3.0298e-02]],\n              \n                       [[-1.5009e-02,  4.1003e-02,  7.9811e-03],\n                        [ 6.5824e-03, -2.2011e-02,  8.9981e-03],\n                        [ 1.5385e-02, -3.9503e-02,  4.1086e-02]]],\n              \n              \n                      [[[-2.8993e-02, -3.7376e-02,  1.1231e-02],\n                        [ 1.7329e-02, -5.8507e-03,  1.9821e-02],\n                        [ 2.0648e-02, -3.9886e-02,  1.6316e-02]],\n              \n                       [[ 3.2519e-02,  1.6676e-02,  1.2690e-03],\n                        [ 1.6236e-03,  4.4074e-03, -2.0494e-02],\n                        [-3.6117e-02,  1.2012e-02, -2.8950e-02]],\n              \n                       [[-3.4818e-02, -1.8692e-02, -6.5148e-03],\n                        [-3.8199e-02, -2.1533e-03, -2.6669e-02],\n                        [ 2.0359e-03, -1.0877e-02,  3.2552e-02]],\n              \n                       ...,\n              \n                       [[ 2.6173e-03, -3.7495e-02,  8.6743e-03],\n                        [ 4.8354e-04,  4.1075e-02, -6.5880e-03],\n                        [ 3.3915e-02,  3.9410e-03, -1.2893e-02]],\n              \n                       [[ 2.6528e-02, -4.0759e-02,  1.9229e-02],\n                        [ 2.2432e-02, -3.9180e-03,  2.6232e-02],\n                        [ 1.2603e-02, -3.1149e-03, -1.4234e-02]],\n              \n                       [[-2.9655e-03,  1.3039e-03, -2.7197e-02],\n                        [ 3.9957e-02, -1.5892e-02,  2.0109e-02],\n                        [ 1.4106e-03,  6.4586e-04,  8.9162e-03]]],\n              \n              \n                      [[[ 3.1019e-02,  3.9165e-02, -2.7102e-02],\n                        [-3.8747e-02, -2.9976e-02, -8.2251e-04],\n                        [ 3.1431e-02, -9.7356e-03,  1.1533e-02]],\n              \n                       [[-8.6869e-03,  3.6680e-02,  1.8349e-02],\n                        [-3.1113e-02, -2.5772e-02, -1.2013e-02],\n                        [ 2.4810e-02,  2.1669e-02, -3.3620e-02]],\n              \n                       [[-3.0419e-02,  7.3520e-03, -1.9823e-02],\n                        [ 3.8660e-02,  2.6089e-02,  3.0254e-02],\n                        [ 1.4994e-02,  1.0452e-02,  3.4261e-02]],\n              \n                       ...,\n              \n                       [[-3.2601e-02, -3.6214e-02,  3.6512e-02],\n                        [-3.7527e-02, -2.9699e-02,  1.5305e-02],\n                        [-2.4764e-02,  2.2672e-02,  2.2486e-02]],\n              \n                       [[ 1.1033e-02,  3.0824e-02,  2.4714e-02],\n                        [-2.1154e-02,  2.5543e-02,  1.0087e-02],\n                        [ 2.3082e-02, -3.0461e-02,  3.4150e-02]],\n              \n                       [[-1.8519e-02, -7.6047e-03,  2.7975e-02],\n                        [-6.4077e-03, -2.6562e-02,  9.9592e-03],\n                        [-2.9076e-02, -2.5703e-02, -2.9623e-02]]]], device='cuda:0')),\n             ('module.module.inc.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.inc.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.inc.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[ 0.0357, -0.0264,  0.0201],\n                        [ 0.0235, -0.0205,  0.0169],\n                        [ 0.0325, -0.0087, -0.0301]],\n              \n                       [[-0.0252,  0.0130,  0.0105],\n                        [ 0.0278,  0.0094, -0.0272],\n                        [ 0.0324,  0.0047,  0.0045]],\n              \n                       [[-0.0352, -0.0399, -0.0170],\n                        [ 0.0144,  0.0158, -0.0144],\n                        [-0.0233,  0.0018, -0.0334]],\n              \n                       ...,\n              \n                       [[ 0.0116, -0.0235, -0.0296],\n                        [-0.0242,  0.0119,  0.0299],\n                        [ 0.0114,  0.0182,  0.0288]],\n              \n                       [[-0.0316, -0.0088, -0.0152],\n                        [-0.0325, -0.0183, -0.0030],\n                        [-0.0355, -0.0339,  0.0363]],\n              \n                       [[-0.0135,  0.0221,  0.0305],\n                        [-0.0268,  0.0040, -0.0396],\n                        [-0.0201,  0.0218, -0.0349]]],\n              \n              \n                      [[[ 0.0126,  0.0043, -0.0306],\n                        [-0.0146,  0.0352,  0.0244],\n                        [ 0.0250,  0.0273,  0.0250]],\n              \n                       [[-0.0412,  0.0087,  0.0332],\n                        [ 0.0187, -0.0076, -0.0089],\n                        [-0.0151, -0.0058, -0.0293]],\n              \n                       [[-0.0167, -0.0200,  0.0142],\n                        [-0.0356,  0.0294,  0.0118],\n                        [-0.0244, -0.0215,  0.0074]],\n              \n                       ...,\n              \n                       [[-0.0035,  0.0137, -0.0314],\n                        [ 0.0138, -0.0057,  0.0048],\n                        [ 0.0214, -0.0232, -0.0108]],\n              \n                       [[-0.0412, -0.0090, -0.0090],\n                        [-0.0287,  0.0126,  0.0135],\n                        [ 0.0138,  0.0354, -0.0151]],\n              \n                       [[ 0.0006, -0.0026,  0.0229],\n                        [ 0.0340,  0.0215,  0.0193],\n                        [-0.0062,  0.0044,  0.0232]]],\n              \n              \n                      [[[ 0.0393,  0.0131, -0.0272],\n                        [-0.0268, -0.0212,  0.0276],\n                        [-0.0300,  0.0367, -0.0406]],\n              \n                       [[ 0.0010, -0.0226, -0.0340],\n                        [ 0.0188,  0.0097, -0.0116],\n                        [ 0.0346, -0.0155,  0.0074]],\n              \n                       [[ 0.0277, -0.0405,  0.0331],\n                        [ 0.0064,  0.0333,  0.0368],\n                        [ 0.0375,  0.0212, -0.0242]],\n              \n                       ...,\n              \n                       [[-0.0069,  0.0186, -0.0329],\n                        [ 0.0099, -0.0293,  0.0133],\n                        [ 0.0385,  0.0099,  0.0152]],\n              \n                       [[ 0.0165,  0.0133,  0.0077],\n                        [-0.0347, -0.0064,  0.0321],\n                        [-0.0038, -0.0347,  0.0405]],\n              \n                       [[ 0.0055, -0.0044, -0.0135],\n                        [ 0.0195,  0.0027,  0.0329],\n                        [-0.0107,  0.0344, -0.0313]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0298, -0.0407, -0.0166],\n                        [-0.0002, -0.0221,  0.0067],\n                        [ 0.0178,  0.0013, -0.0193]],\n              \n                       [[-0.0238,  0.0293,  0.0269],\n                        [ 0.0277,  0.0384,  0.0140],\n                        [-0.0363, -0.0101,  0.0253]],\n              \n                       [[ 0.0334, -0.0225, -0.0067],\n                        [-0.0341,  0.0260, -0.0054],\n                        [ 0.0118,  0.0148,  0.0336]],\n              \n                       ...,\n              \n                       [[-0.0390,  0.0067, -0.0146],\n                        [-0.0058, -0.0076,  0.0248],\n                        [-0.0309, -0.0162, -0.0044]],\n              \n                       [[ 0.0156,  0.0133, -0.0077],\n                        [-0.0084, -0.0258,  0.0351],\n                        [ 0.0133, -0.0063,  0.0344]],\n              \n                       [[ 0.0333,  0.0093, -0.0372],\n                        [-0.0002,  0.0405, -0.0157],\n                        [-0.0018, -0.0008,  0.0080]]],\n              \n              \n                      [[[ 0.0330, -0.0097, -0.0083],\n                        [-0.0216,  0.0057, -0.0085],\n                        [ 0.0082,  0.0023,  0.0381]],\n              \n                       [[-0.0320,  0.0131, -0.0137],\n                        [-0.0037,  0.0201, -0.0339],\n                        [ 0.0327,  0.0375, -0.0072]],\n              \n                       [[-0.0085, -0.0173,  0.0102],\n                        [ 0.0381,  0.0038,  0.0299],\n                        [ 0.0261,  0.0366,  0.0206]],\n              \n                       ...,\n              \n                       [[-0.0330, -0.0098, -0.0026],\n                        [ 0.0038,  0.0086,  0.0258],\n                        [-0.0036,  0.0356, -0.0383]],\n              \n                       [[ 0.0014,  0.0289, -0.0069],\n                        [-0.0358, -0.0261, -0.0318],\n                        [-0.0223, -0.0333,  0.0221]],\n              \n                       [[ 0.0099, -0.0044,  0.0356],\n                        [-0.0416,  0.0245,  0.0219],\n                        [-0.0125, -0.0308, -0.0395]]],\n              \n              \n                      [[[-0.0059, -0.0348, -0.0104],\n                        [-0.0281, -0.0408,  0.0101],\n                        [-0.0012,  0.0124, -0.0115]],\n              \n                       [[-0.0382, -0.0336,  0.0156],\n                        [-0.0337,  0.0008,  0.0405],\n                        [-0.0058, -0.0384, -0.0303]],\n              \n                       [[-0.0357,  0.0154,  0.0037],\n                        [ 0.0079,  0.0382, -0.0023],\n                        [-0.0099,  0.0091, -0.0170]],\n              \n                       ...,\n              \n                       [[-0.0194,  0.0131, -0.0097],\n                        [-0.0112, -0.0016, -0.0009],\n                        [-0.0198, -0.0326, -0.0109]],\n              \n                       [[ 0.0248, -0.0348, -0.0202],\n                        [-0.0041, -0.0386, -0.0109],\n                        [-0.0228, -0.0399,  0.0372]],\n              \n                       [[-0.0010, -0.0073,  0.0204],\n                        [-0.0288,  0.0141,  0.0010],\n                        [-0.0160, -0.0138,  0.0360]]]], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.1305e-02, -1.2684e-03,  2.4892e-02],\n                        [-2.6919e-02, -1.1080e-02,  6.1028e-04],\n                        [-6.9626e-03,  2.4179e-02,  7.0370e-03]],\n              \n                       [[-8.0535e-03, -1.8495e-04, -2.7226e-02],\n                        [-1.6500e-02,  3.6307e-03,  2.3883e-02],\n                        [-7.6892e-03,  2.6147e-02,  1.8880e-02]],\n              \n                       [[-6.3356e-04, -7.4601e-03, -7.9877e-03],\n                        [ 1.3430e-02, -1.9490e-02,  3.8737e-03],\n                        [-1.6122e-02, -1.8464e-02,  2.0742e-02]],\n              \n                       ...,\n              \n                       [[ 1.8362e-03, -1.1564e-02, -2.8767e-02],\n                        [ 5.5608e-03,  6.5534e-03,  1.5489e-02],\n                        [-1.3676e-02, -2.4228e-02,  1.2859e-02]],\n              \n                       [[ 1.7046e-02,  3.1059e-03, -1.3043e-02],\n                        [-1.1144e-02,  8.5697e-03, -9.9781e-03],\n                        [ 6.2510e-03, -2.7031e-02, -8.6106e-03]],\n              \n                       [[ 2.8901e-02,  1.9356e-02, -2.5723e-02],\n                        [-2.0941e-02,  1.2509e-02,  2.8496e-02],\n                        [-1.6640e-02, -3.5848e-03, -1.0853e-02]]],\n              \n              \n                      [[[ 1.2726e-02, -1.6195e-02,  1.4709e-02],\n                        [-2.0562e-02, -2.8356e-02,  1.0373e-02],\n                        [ 1.6941e-02, -1.7723e-02,  2.5551e-02]],\n              \n                       [[-1.9462e-02,  2.7471e-02, -1.6930e-02],\n                        [-2.7676e-03, -1.4025e-03,  1.7487e-02],\n                        [ 1.6080e-02,  2.9447e-02, -1.8378e-02]],\n              \n                       [[ 2.8415e-03, -1.0617e-02, -1.0754e-03],\n                        [ 2.2315e-02, -1.2144e-02, -1.7454e-02],\n                        [-2.4725e-02, -1.4872e-02,  1.2383e-02]],\n              \n                       ...,\n              \n                       [[ 2.1383e-02, -2.6270e-02, -1.2159e-02],\n                        [-2.1438e-02, -2.4603e-02, -1.3974e-02],\n                        [-2.2166e-02,  2.9069e-02,  1.0996e-02]],\n              \n                       [[ 2.6262e-02, -3.3151e-03,  2.6866e-02],\n                        [-1.1902e-02,  2.3779e-03,  2.6081e-02],\n                        [ 5.4771e-03,  7.5126e-04, -8.3137e-03]],\n              \n                       [[ 2.5385e-02,  7.2457e-03, -1.6735e-02],\n                        [-4.7629e-03, -1.2607e-02, -4.5772e-03],\n                        [ 1.6854e-02,  1.9901e-02,  2.8703e-02]]],\n              \n              \n                      [[[-2.8001e-02, -4.4546e-04, -2.0191e-02],\n                        [ 2.4830e-02, -2.2498e-02, -2.0728e-02],\n                        [-1.0464e-02,  2.7569e-02,  2.9056e-02]],\n              \n                       [[-2.7124e-02, -7.6276e-03,  2.4910e-02],\n                        [-5.0865e-03, -1.3039e-02, -1.9636e-02],\n                        [-2.0727e-02, -2.3310e-02, -1.5865e-02]],\n              \n                       [[ 7.5711e-03,  7.3599e-03, -2.2980e-02],\n                        [-2.5551e-02,  2.2718e-02,  1.5489e-02],\n                        [-3.0655e-04,  1.2903e-02, -2.2033e-02]],\n              \n                       ...,\n              \n                       [[-1.5014e-02, -7.5347e-04,  1.6599e-03],\n                        [-5.4850e-03,  1.3427e-02,  2.9824e-03],\n                        [ 2.4041e-02,  1.7558e-03,  1.0491e-02]],\n              \n                       [[-1.7517e-02,  2.2218e-02,  2.1117e-02],\n                        [-8.5116e-05,  2.7633e-02,  1.1950e-03],\n                        [ 2.3484e-02, -2.0629e-02, -7.9562e-03]],\n              \n                       [[ 6.6841e-03, -2.7769e-02, -2.2987e-02],\n                        [-2.4637e-02,  2.2629e-02, -1.2457e-02],\n                        [-1.0986e-02, -1.6586e-02, -4.0791e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.6628e-03,  2.6667e-02,  6.7481e-03],\n                        [-1.4348e-02, -1.9016e-02,  2.1977e-02],\n                        [ 1.1526e-02,  2.0264e-03, -1.9429e-02]],\n              \n                       [[-1.5399e-02,  2.4140e-02,  1.7281e-02],\n                        [-5.1553e-05,  2.7146e-03, -2.2730e-02],\n                        [-2.2137e-02,  1.5756e-02,  9.6129e-03]],\n              \n                       [[-5.2356e-03,  1.8795e-02,  1.4753e-02],\n                        [-2.9235e-02, -2.4725e-02, -9.9595e-03],\n                        [-2.5816e-02, -1.2593e-02, -1.4906e-02]],\n              \n                       ...,\n              \n                       [[-5.1329e-04,  2.4464e-02,  1.0491e-02],\n                        [ 1.6588e-03, -1.9864e-02, -2.4729e-02],\n                        [-5.7917e-03,  1.2495e-02,  7.5220e-03]],\n              \n                       [[ 1.5368e-02, -2.5456e-02, -1.4819e-02],\n                        [-2.5614e-02, -2.3670e-03,  2.6447e-02],\n                        [-5.4125e-03, -4.6167e-03, -7.2054e-04]],\n              \n                       [[-1.7071e-02, -2.6587e-03,  2.1725e-02],\n                        [-2.8988e-02,  3.1809e-03,  1.3815e-03],\n                        [ 6.4158e-03, -2.6444e-04,  1.8910e-02]]],\n              \n              \n                      [[[ 2.5009e-02,  4.4661e-03, -2.5017e-02],\n                        [ 6.8237e-03,  1.3778e-02,  6.8838e-03],\n                        [-1.5440e-02, -1.2293e-03,  2.2054e-02]],\n              \n                       [[-1.6465e-02,  1.3906e-02,  2.9242e-02],\n                        [ 2.2392e-02, -6.8427e-03, -2.1006e-02],\n                        [ 2.3828e-02, -1.8528e-02,  4.6238e-03]],\n              \n                       [[ 2.6324e-02, -3.9792e-03, -2.8550e-02],\n                        [ 9.2739e-03,  8.2617e-03, -2.5574e-02],\n                        [ 1.6078e-02,  1.6129e-02,  6.8392e-03]],\n              \n                       ...,\n              \n                       [[ 2.7127e-02, -1.3369e-02,  8.5266e-03],\n                        [-1.0530e-02, -2.0817e-02, -8.6817e-03],\n                        [-2.9038e-02, -2.4825e-03,  1.3813e-02]],\n              \n                       [[ 1.2809e-02, -2.7485e-02, -2.8767e-02],\n                        [-5.6553e-03,  1.9724e-02,  1.1964e-02],\n                        [ 5.6818e-03,  1.9974e-02, -1.8658e-02]],\n              \n                       [[ 2.8031e-02, -2.4776e-02, -3.0622e-03],\n                        [ 1.4898e-02,  2.7475e-03, -2.2119e-02],\n                        [ 5.8204e-03,  6.9012e-03, -2.6735e-02]]],\n              \n              \n                      [[[ 9.7910e-03,  1.7056e-02, -4.8750e-03],\n                        [ 3.8653e-03,  9.2350e-03, -2.7748e-02],\n                        [ 2.4542e-02, -9.4870e-03,  2.7431e-02]],\n              \n                       [[ 1.5725e-03,  5.4433e-03,  6.2727e-03],\n                        [ 2.9122e-02,  1.9450e-02, -1.4450e-02],\n                        [ 7.3775e-03,  2.3615e-02, -1.2452e-02]],\n              \n                       [[-7.7901e-04,  5.2408e-03,  1.3440e-02],\n                        [ 1.1745e-02, -2.4794e-02,  5.6418e-03],\n                        [ 1.4150e-02, -1.9262e-02, -6.3717e-04]],\n              \n                       ...,\n              \n                       [[ 4.6180e-03,  2.1094e-03, -2.5070e-02],\n                        [-1.9577e-02,  2.3995e-02, -1.5351e-02],\n                        [-2.1875e-02, -2.0034e-03,  3.7910e-03]],\n              \n                       [[ 2.1114e-03,  2.1738e-02,  1.3168e-03],\n                        [-9.2969e-03,  1.9882e-02,  5.0677e-03],\n                        [ 6.9171e-03,  2.1555e-02, -1.1559e-02]],\n              \n                       [[-2.8176e-02, -2.6783e-02,  2.4445e-02],\n                        [ 1.4733e-02,  4.4278e-03,  7.2822e-03],\n                        [-2.4972e-02, -1.4935e-02,  2.7857e-02]]]], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.down1.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-2.0874e-03,  2.8328e-02,  3.8197e-03],\n                        [ 2.0103e-02, -2.4530e-02,  3.5383e-03],\n                        [ 1.2657e-02,  2.5045e-02,  5.3281e-03]],\n              \n                       [[ 9.3871e-03,  2.5844e-02, -1.4631e-02],\n                        [ 2.7466e-02, -1.0389e-02,  1.5178e-02],\n                        [ 2.8453e-02,  1.3451e-02, -1.1607e-03]],\n              \n                       [[ 2.0450e-02,  1.3948e-02, -1.8822e-02],\n                        [-1.6178e-03,  2.4138e-02,  1.6494e-02],\n                        [-2.7684e-02, -1.6600e-02,  2.5942e-03]],\n              \n                       ...,\n              \n                       [[-2.5010e-03,  2.1981e-02,  1.0307e-02],\n                        [ 1.0725e-02,  2.8690e-02, -1.7391e-02],\n                        [ 3.5500e-03,  2.0341e-03,  5.9864e-03]],\n              \n                       [[-8.7539e-03,  1.3636e-02,  2.7444e-02],\n                        [-5.3241e-03,  1.4782e-02, -1.6061e-02],\n                        [ 2.8436e-02, -2.6700e-02, -5.3704e-03]],\n              \n                       [[-2.3932e-02,  6.0354e-03,  2.0279e-02],\n                        [-2.7523e-02, -2.8895e-02,  2.0104e-02],\n                        [-6.3520e-03,  8.0765e-03,  2.4935e-03]]],\n              \n              \n                      [[[-1.0771e-02, -3.8036e-03, -2.3648e-02],\n                        [-1.3159e-02,  2.4382e-02,  2.5068e-02],\n                        [-1.8793e-02, -2.5927e-02,  1.6405e-02]],\n              \n                       [[ 4.6219e-03,  2.3189e-02, -1.0743e-02],\n                        [ 2.8896e-02, -2.2556e-02,  5.3712e-03],\n                        [-8.8788e-03, -8.3982e-03, -9.5629e-03]],\n              \n                       [[-2.3292e-02,  1.9044e-02,  1.8797e-03],\n                        [-1.7992e-02, -2.8691e-02,  1.8576e-03],\n                        [-2.4593e-02,  8.3165e-03, -5.6803e-03]],\n              \n                       ...,\n              \n                       [[-2.7325e-02, -1.6579e-02, -2.7656e-02],\n                        [-1.4223e-02,  6.2641e-03, -2.7416e-02],\n                        [-1.8046e-02,  1.1367e-02, -1.2150e-02]],\n              \n                       [[-3.4729e-03,  5.4115e-04, -1.9539e-02],\n                        [ 1.6914e-02, -1.1351e-02,  2.0686e-02],\n                        [-1.0540e-02, -2.7865e-02,  3.4599e-03]],\n              \n                       [[-1.5403e-02, -5.0929e-03, -2.0951e-02],\n                        [ 1.8758e-02, -1.5846e-02, -2.6030e-02],\n                        [ 2.3687e-02, -2.6410e-02,  5.7963e-03]]],\n              \n              \n                      [[[-2.6278e-02, -1.2930e-02, -1.6344e-02],\n                        [ 8.9017e-03, -1.8674e-02, -1.6698e-02],\n                        [-1.0313e-02,  9.8180e-03,  1.0110e-02]],\n              \n                       [[-2.1049e-02,  1.4577e-02, -1.8113e-02],\n                        [-2.0648e-02, -1.4387e-02, -2.4280e-04],\n                        [-2.0775e-02, -4.0661e-03,  2.7782e-02]],\n              \n                       [[-2.7178e-02,  4.2496e-03, -2.3201e-02],\n                        [ 1.0937e-02, -6.5350e-03, -2.3540e-02],\n                        [-2.9455e-02,  2.3027e-02, -2.7718e-02]],\n              \n                       ...,\n              \n                       [[-2.1814e-02,  1.5335e-02, -2.3714e-02],\n                        [-2.8257e-02,  2.3738e-02, -1.3762e-02],\n                        [-3.1294e-03,  9.6518e-03,  6.7151e-03]],\n              \n                       [[-2.5689e-02,  4.9199e-03,  1.6813e-02],\n                        [ 2.7413e-02, -2.5757e-02, -2.6320e-02],\n                        [ 2.8428e-02, -1.9982e-02, -6.2184e-03]],\n              \n                       [[-4.9595e-03, -2.2561e-02,  2.1508e-02],\n                        [ 6.1043e-03, -1.9141e-02, -1.6917e-02],\n                        [-2.2802e-02, -7.2276e-03,  1.1010e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8587e-04,  2.5234e-02,  1.2862e-02],\n                        [ 6.4087e-03,  2.9456e-03, -6.2891e-03],\n                        [ 1.3295e-02,  1.1122e-02, -3.8489e-03]],\n              \n                       [[ 2.4627e-02, -8.6374e-03,  9.6317e-03],\n                        [-4.4341e-03, -2.0696e-03,  5.3607e-05],\n                        [ 2.7382e-02, -1.1736e-03, -2.8442e-03]],\n              \n                       [[ 7.9895e-03, -6.4228e-03,  9.2783e-03],\n                        [ 1.0661e-03, -2.7210e-02,  2.9449e-02],\n                        [ 2.8375e-03, -2.2452e-02, -3.4423e-03]],\n              \n                       ...,\n              \n                       [[ 7.1594e-03, -2.7026e-02, -6.7921e-03],\n                        [-1.5202e-02, -7.0004e-04, -6.5862e-03],\n                        [ 2.7967e-02,  2.5300e-02,  5.7218e-03]],\n              \n                       [[ 1.9714e-02,  2.5212e-02,  2.6632e-02],\n                        [ 3.6115e-03, -2.2397e-02, -1.0878e-02],\n                        [-1.3762e-02,  4.6104e-04,  1.6057e-02]],\n              \n                       [[ 2.5034e-02, -2.9420e-02, -1.7739e-02],\n                        [ 1.0064e-02, -2.8722e-02, -1.6836e-02],\n                        [ 1.7448e-02,  2.8111e-02,  1.4150e-03]]],\n              \n              \n                      [[[-1.5742e-02, -1.3421e-02,  2.7663e-02],\n                        [-1.5744e-02,  2.0141e-03,  1.1419e-03],\n                        [ 2.5981e-02,  1.0222e-02, -1.5587e-02]],\n              \n                       [[ 1.3669e-02,  5.2103e-03, -7.6013e-03],\n                        [-1.6173e-02,  5.6269e-04,  2.4350e-03],\n                        [ 2.4261e-03,  2.5788e-02, -2.8097e-02]],\n              \n                       [[-1.4888e-02, -1.7731e-02, -6.4337e-03],\n                        [ 2.2471e-02,  2.3679e-04, -1.1437e-02],\n                        [-5.8912e-03,  1.0241e-02,  1.8909e-02]],\n              \n                       ...,\n              \n                       [[-1.4776e-02,  2.1398e-02,  8.8336e-04],\n                        [-3.3876e-03,  9.3768e-03, -5.3336e-03],\n                        [-4.4843e-03, -5.7139e-03, -6.8183e-03]],\n              \n                       [[-2.0888e-02, -2.4299e-02, -1.6261e-02],\n                        [-2.0847e-02,  1.3012e-02,  2.1894e-02],\n                        [-4.3075e-03,  2.1090e-02,  2.2750e-02]],\n              \n                       [[-1.7861e-02, -2.5487e-02, -9.7013e-03],\n                        [-2.8849e-03, -2.6374e-02, -2.2423e-02],\n                        [ 3.2294e-03,  1.0469e-02, -2.7943e-02]]],\n              \n              \n                      [[[ 4.1885e-03, -2.7628e-02, -2.5770e-02],\n                        [ 1.4383e-02, -3.2527e-03, -2.1710e-02],\n                        [-1.4146e-02,  7.5708e-03, -1.2968e-02]],\n              \n                       [[ 6.4110e-03,  1.5356e-02, -1.1846e-02],\n                        [ 2.1303e-02,  6.4434e-03, -2.6370e-02],\n                        [ 1.7484e-02,  1.9423e-02,  2.9357e-02]],\n              \n                       [[ 3.5598e-03,  2.6142e-02, -2.6987e-02],\n                        [ 9.4496e-03,  1.8193e-02,  1.0256e-02],\n                        [ 3.0655e-03,  2.6695e-03, -9.7217e-04]],\n              \n                       ...,\n              \n                       [[ 1.2180e-02,  2.1096e-02, -2.4789e-02],\n                        [ 6.3251e-03,  3.0475e-03, -6.8353e-03],\n                        [ 1.8787e-02, -9.2431e-03,  1.7185e-02]],\n              \n                       [[-1.1940e-02,  1.8412e-02,  1.7622e-02],\n                        [ 2.1504e-02,  2.3440e-02,  1.1492e-02],\n                        [-1.6089e-02, -1.5441e-02,  2.1249e-02]],\n              \n                       [[-2.3543e-02, -2.0001e-02, -2.0346e-02],\n                        [ 2.0520e-02,  2.9473e-03, -1.2873e-02],\n                        [ 1.3080e-02, -1.3335e-02,  2.4488e-02]]]], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-0.0199, -0.0207, -0.0025],\n                        [-0.0202,  0.0202, -0.0180],\n                        [-0.0126,  0.0164, -0.0123]],\n              \n                       [[ 0.0062, -0.0141,  0.0168],\n                        [ 0.0078,  0.0006, -0.0096],\n                        [ 0.0036, -0.0188,  0.0195]],\n              \n                       [[-0.0073, -0.0065, -0.0040],\n                        [ 0.0086,  0.0105,  0.0089],\n                        [-0.0055,  0.0144, -0.0161]],\n              \n                       ...,\n              \n                       [[ 0.0131, -0.0028, -0.0143],\n                        [-0.0057, -0.0096, -0.0171],\n                        [-0.0130, -0.0047, -0.0005]],\n              \n                       [[-0.0046, -0.0177,  0.0125],\n                        [-0.0102,  0.0154,  0.0072],\n                        [ 0.0206,  0.0169, -0.0156]],\n              \n                       [[ 0.0036,  0.0074,  0.0056],\n                        [ 0.0112, -0.0127, -0.0147],\n                        [ 0.0001,  0.0135,  0.0017]]],\n              \n              \n                      [[[-0.0075, -0.0151,  0.0206],\n                        [ 0.0001, -0.0105, -0.0072],\n                        [ 0.0066,  0.0189,  0.0178]],\n              \n                       [[ 0.0086, -0.0003,  0.0005],\n                        [ 0.0185, -0.0089, -0.0045],\n                        [ 0.0166, -0.0010,  0.0182]],\n              \n                       [[-0.0107, -0.0202,  0.0050],\n                        [-0.0029, -0.0139,  0.0134],\n                        [ 0.0037,  0.0136, -0.0140]],\n              \n                       ...,\n              \n                       [[ 0.0171,  0.0028,  0.0002],\n                        [ 0.0165,  0.0112,  0.0014],\n                        [-0.0089, -0.0016,  0.0104]],\n              \n                       [[-0.0161, -0.0097, -0.0042],\n                        [ 0.0174,  0.0107,  0.0100],\n                        [-0.0053, -0.0070,  0.0113]],\n              \n                       [[-0.0016, -0.0070,  0.0061],\n                        [ 0.0017,  0.0160,  0.0013],\n                        [ 0.0057,  0.0200, -0.0160]]],\n              \n              \n                      [[[-0.0060, -0.0105, -0.0198],\n                        [-0.0150, -0.0083,  0.0156],\n                        [-0.0090,  0.0120, -0.0199]],\n              \n                       [[ 0.0127,  0.0145, -0.0122],\n                        [ 0.0110, -0.0001, -0.0018],\n                        [ 0.0039,  0.0206, -0.0076]],\n              \n                       [[ 0.0101,  0.0061, -0.0136],\n                        [ 0.0194, -0.0136,  0.0016],\n                        [-0.0007,  0.0173,  0.0011]],\n              \n                       ...,\n              \n                       [[-0.0134, -0.0127, -0.0165],\n                        [ 0.0041, -0.0118,  0.0110],\n                        [ 0.0044,  0.0060,  0.0036]],\n              \n                       [[ 0.0056, -0.0185,  0.0055],\n                        [ 0.0114, -0.0050, -0.0185],\n                        [ 0.0116, -0.0140, -0.0148]],\n              \n                       [[ 0.0145,  0.0188, -0.0130],\n                        [ 0.0065, -0.0171,  0.0036],\n                        [-0.0037, -0.0078,  0.0077]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0090,  0.0069, -0.0124],\n                        [-0.0150, -0.0065,  0.0094],\n                        [-0.0195, -0.0163, -0.0144]],\n              \n                       [[-0.0142,  0.0055, -0.0013],\n                        [-0.0149, -0.0092,  0.0063],\n                        [ 0.0007,  0.0089,  0.0060]],\n              \n                       [[-0.0055, -0.0047, -0.0065],\n                        [-0.0140,  0.0113, -0.0194],\n                        [-0.0049,  0.0079,  0.0079]],\n              \n                       ...,\n              \n                       [[-0.0111, -0.0127,  0.0139],\n                        [ 0.0075, -0.0173, -0.0109],\n                        [ 0.0204, -0.0063, -0.0174]],\n              \n                       [[ 0.0198,  0.0142,  0.0200],\n                        [ 0.0188,  0.0201, -0.0102],\n                        [ 0.0027, -0.0103, -0.0160]],\n              \n                       [[ 0.0090,  0.0116,  0.0114],\n                        [-0.0037, -0.0078,  0.0121],\n                        [-0.0192, -0.0149, -0.0202]]],\n              \n              \n                      [[[ 0.0045, -0.0102,  0.0195],\n                        [-0.0163, -0.0012,  0.0005],\n                        [ 0.0079, -0.0045,  0.0198]],\n              \n                       [[ 0.0181,  0.0146, -0.0039],\n                        [ 0.0095,  0.0106, -0.0055],\n                        [ 0.0028,  0.0103,  0.0006]],\n              \n                       [[ 0.0039, -0.0051, -0.0071],\n                        [-0.0123, -0.0141,  0.0050],\n                        [-0.0146,  0.0068,  0.0163]],\n              \n                       ...,\n              \n                       [[-0.0144,  0.0072, -0.0097],\n                        [-0.0070,  0.0141,  0.0089],\n                        [-0.0034,  0.0030,  0.0124]],\n              \n                       [[ 0.0143, -0.0146, -0.0182],\n                        [-0.0080,  0.0061, -0.0181],\n                        [ 0.0166,  0.0175, -0.0116]],\n              \n                       [[-0.0095, -0.0014, -0.0191],\n                        [ 0.0184, -0.0074, -0.0144],\n                        [ 0.0201, -0.0136, -0.0001]]],\n              \n              \n                      [[[-0.0022, -0.0024,  0.0035],\n                        [-0.0075, -0.0206,  0.0173],\n                        [-0.0160,  0.0207,  0.0060]],\n              \n                       [[-0.0073,  0.0075, -0.0149],\n                        [-0.0112,  0.0081, -0.0034],\n                        [-0.0176, -0.0169,  0.0041]],\n              \n                       [[-0.0040,  0.0199, -0.0174],\n                        [ 0.0103,  0.0153, -0.0109],\n                        [-0.0044, -0.0160, -0.0072]],\n              \n                       ...,\n              \n                       [[ 0.0142, -0.0045,  0.0044],\n                        [-0.0134, -0.0153, -0.0110],\n                        [-0.0178,  0.0051, -0.0051]],\n              \n                       [[ 0.0090,  0.0175,  0.0111],\n                        [ 0.0201, -0.0061,  0.0081],\n                        [-0.0037,  0.0166,  0.0074]],\n              \n                       [[-0.0069,  0.0019, -0.0200],\n                        [-0.0047, -0.0145,  0.0192],\n                        [-0.0100,  0.0121, -0.0193]]]], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down2.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-4.6348e-03,  9.8509e-03,  1.6142e-02],\n                        [ 2.6825e-05, -8.4992e-03,  3.6535e-04],\n                        [-2.0749e-02, -2.7181e-03,  1.4475e-02]],\n              \n                       [[ 1.0194e-02,  6.9748e-03,  1.3849e-02],\n                        [ 1.4200e-03,  2.5024e-03,  1.5259e-02],\n                        [ 1.1671e-02,  4.0497e-03,  8.7697e-03]],\n              \n                       [[-4.4309e-03, -1.1845e-02, -1.6037e-02],\n                        [-7.8910e-03, -9.7038e-03,  5.6008e-03],\n                        [-1.6987e-02,  7.1697e-03,  1.7236e-02]],\n              \n                       ...,\n              \n                       [[-1.1635e-02,  1.8610e-02,  1.4086e-02],\n                        [-1.1576e-02, -1.9610e-03, -1.8455e-02],\n                        [-8.6874e-03, -1.1485e-02, -5.8817e-03]],\n              \n                       [[-1.3743e-02,  1.2879e-02,  2.2404e-03],\n                        [-6.8730e-03,  1.0492e-02,  8.4602e-03],\n                        [ 1.9366e-03, -1.0892e-02,  9.0133e-03]],\n              \n                       [[-6.9619e-03, -1.7941e-02, -1.1306e-02],\n                        [-6.8960e-03, -6.8894e-03, -6.9923e-04],\n                        [ 1.0807e-02,  1.8476e-02,  1.9441e-02]]],\n              \n              \n                      [[[ 6.4426e-03,  7.5100e-03,  6.7503e-03],\n                        [-1.8439e-02,  1.4277e-02, -1.0381e-02],\n                        [-1.7296e-02, -1.2204e-02,  5.2923e-03]],\n              \n                       [[-6.8046e-03,  6.3742e-03, -1.1632e-02],\n                        [ 4.2213e-03,  2.0774e-02, -3.7589e-03],\n                        [ 1.6312e-02,  7.4283e-04,  1.2614e-02]],\n              \n                       [[-6.7564e-03, -1.0808e-02, -1.6746e-02],\n                        [-6.2140e-03,  9.3120e-03, -9.2284e-03],\n                        [ 2.8789e-03,  1.2397e-03,  1.5193e-02]],\n              \n                       ...,\n              \n                       [[-1.4065e-02, -4.0645e-03, -1.4819e-02],\n                        [ 7.9262e-03, -1.4440e-02, -1.3676e-02],\n                        [ 8.2918e-04,  1.0951e-02,  6.6675e-03]],\n              \n                       [[ 1.8929e-02, -1.6932e-02,  7.8811e-03],\n                        [ 1.6661e-02, -1.4852e-02, -6.1440e-03],\n                        [-4.3739e-03,  1.0890e-02,  1.2552e-03]],\n              \n                       [[ 1.6674e-02,  8.4053e-03, -5.2151e-03],\n                        [-1.8711e-02, -6.0464e-04,  4.8782e-03],\n                        [-1.0599e-02, -8.5500e-03, -4.4493e-04]]],\n              \n              \n                      [[[ 7.4150e-03, -1.7817e-02, -9.8810e-03],\n                        [ 1.5139e-02, -5.4702e-03,  3.1069e-03],\n                        [ 1.6121e-02, -2.4298e-03, -3.4243e-03]],\n              \n                       [[ 5.2642e-03, -1.7880e-02, -1.8678e-02],\n                        [ 2.9048e-03,  1.0568e-02, -2.8701e-04],\n                        [-4.0345e-05, -2.8312e-03,  6.9242e-03]],\n              \n                       [[ 1.2557e-02,  1.3475e-02, -1.1946e-02],\n                        [ 1.0504e-02, -1.1848e-02,  1.4417e-02],\n                        [-1.8312e-02,  1.1722e-02, -6.9120e-03]],\n              \n                       ...,\n              \n                       [[ 1.9895e-02,  1.5509e-02,  1.9991e-02],\n                        [-1.5190e-02, -1.9972e-02, -1.3091e-02],\n                        [-1.1537e-02, -6.8988e-03,  1.1122e-02]],\n              \n                       [[ 1.0277e-02, -9.5677e-03,  1.4165e-02],\n                        [ 5.0890e-03,  1.1992e-02,  2.0542e-02],\n                        [-9.9942e-04,  1.1082e-02, -5.1328e-03]],\n              \n                       [[ 1.0213e-02, -4.6551e-03, -5.2989e-03],\n                        [ 1.5165e-02, -1.7655e-02,  5.5892e-03],\n                        [ 1.1311e-02, -1.2807e-02, -1.2253e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4459e-02,  4.5380e-04, -2.9677e-03],\n                        [ 1.8889e-02, -1.6052e-02, -1.5562e-02],\n                        [ 1.3935e-03, -1.6170e-02,  2.0204e-02]],\n              \n                       [[ 1.0080e-02, -3.7539e-03, -1.5059e-02],\n                        [ 6.8971e-03, -8.5807e-03,  1.5525e-02],\n                        [ 1.4992e-03, -7.8594e-03,  7.5005e-03]],\n              \n                       [[ 3.7703e-03,  9.6159e-03,  1.6808e-02],\n                        [-1.1511e-02, -1.9614e-02, -1.7621e-02],\n                        [ 6.5007e-03, -1.5883e-02, -1.3063e-02]],\n              \n                       ...,\n              \n                       [[ 1.1717e-02,  1.3965e-03, -5.3536e-03],\n                        [ 1.4582e-02, -1.8533e-03, -1.5276e-02],\n                        [-2.0322e-02, -1.0361e-02, -6.1722e-03]],\n              \n                       [[ 5.0393e-04,  3.0661e-03, -9.3391e-03],\n                        [-5.0653e-03,  1.3716e-02,  9.7900e-03],\n                        [-2.0547e-02,  1.3067e-02,  1.6991e-03]],\n              \n                       [[-8.7317e-03,  1.5140e-02, -9.8445e-03],\n                        [-2.9895e-03,  1.0854e-02, -7.8243e-03],\n                        [ 1.5019e-03,  1.9270e-02,  9.2994e-03]]],\n              \n              \n                      [[[-3.2868e-03, -1.6655e-03,  1.3082e-02],\n                        [ 7.1859e-03, -1.9157e-03, -3.5394e-03],\n                        [-1.9397e-02,  5.5216e-03, -1.8486e-02]],\n              \n                       [[ 9.8068e-03,  2.6197e-03,  4.8447e-04],\n                        [ 1.5565e-02,  1.1252e-02,  1.8660e-02],\n                        [ 3.1310e-03,  6.5078e-03, -1.4506e-02]],\n              \n                       [[-1.5900e-02, -3.8698e-03,  4.6403e-03],\n                        [ 1.0163e-02,  1.0891e-02,  1.9025e-02],\n                        [-7.0364e-03,  1.0454e-02,  7.3635e-03]],\n              \n                       ...,\n              \n                       [[ 1.5563e-02, -1.9394e-02,  1.5875e-03],\n                        [-4.1397e-03, -7.3719e-04, -8.6707e-03],\n                        [-1.5182e-02,  1.4803e-02, -1.7555e-02]],\n              \n                       [[-7.9233e-04,  1.1101e-03,  1.7634e-03],\n                        [ 1.5103e-02, -1.4403e-02,  1.4855e-02],\n                        [-7.4607e-03,  7.4488e-03, -1.7282e-02]],\n              \n                       [[ 1.4080e-02,  1.6888e-02,  1.6374e-02],\n                        [ 7.7976e-03, -6.2802e-03, -3.1626e-03],\n                        [ 2.0682e-02, -1.9079e-02,  1.3276e-02]]],\n              \n              \n                      [[[ 1.8058e-02, -9.1462e-03, -7.2015e-03],\n                        [-6.4691e-03, -2.9027e-03,  9.6589e-03],\n                        [-1.3747e-02,  1.9787e-02,  1.9956e-02]],\n              \n                       [[-1.1408e-02, -2.4681e-05,  7.7289e-03],\n                        [ 1.9633e-02, -8.2515e-03,  1.3016e-02],\n                        [-1.8417e-02,  1.8677e-02, -1.1818e-02]],\n              \n                       [[ 1.9430e-02,  1.0222e-02, -5.9156e-03],\n                        [ 1.5036e-02,  9.4860e-03,  2.0289e-03],\n                        [-6.1385e-03, -6.8786e-03, -1.0498e-02]],\n              \n                       ...,\n              \n                       [[ 1.8626e-02, -4.7810e-03,  1.8702e-02],\n                        [-7.9554e-03, -1.7242e-02, -1.2626e-03],\n                        [ 1.9328e-02, -5.6285e-03, -1.1736e-02]],\n              \n                       [[-4.1653e-04, -1.8020e-02, -1.2647e-02],\n                        [-4.7124e-03,  3.7225e-03,  3.3474e-03],\n                        [-2.6790e-03,  6.2666e-03,  3.8707e-03]],\n              \n                       [[ 1.9958e-03, -6.2181e-03, -1.5993e-02],\n                        [ 4.3567e-03,  2.8269e-03,  2.0313e-02],\n                        [-1.6953e-02, -1.2477e-02, -6.3685e-03]]]], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[ 1.3495e-02,  1.1336e-02,  3.2999e-03],\n                        [ 1.0248e-02,  4.9058e-03,  1.6721e-03],\n                        [ 1.4577e-02,  1.2254e-02, -1.0996e-02]],\n              \n                       [[ 2.8387e-03, -1.2857e-02, -6.3248e-04],\n                        [ 1.0179e-02, -7.9369e-03,  9.4359e-03],\n                        [ 2.8751e-03, -1.1316e-02, -2.7018e-03]],\n              \n                       [[ 1.3239e-02,  1.3039e-03, -1.3213e-02],\n                        [-8.4236e-03,  2.3438e-03, -1.4353e-02],\n                        [ 9.7540e-03,  7.3673e-03,  9.9629e-04]],\n              \n                       ...,\n              \n                       [[-1.2715e-02, -5.7416e-03,  8.1590e-04],\n                        [ 1.2467e-02,  5.0082e-03, -9.3793e-03],\n                        [-1.0866e-02,  6.1197e-03,  2.4678e-03]],\n              \n                       [[-1.3211e-02, -6.7648e-03,  1.4521e-02],\n                        [-5.5102e-03, -5.2198e-03,  1.0626e-02],\n                        [-1.1742e-02, -6.2968e-03, -3.1413e-03]],\n              \n                       [[ 5.9503e-04, -9.2838e-03,  2.2524e-03],\n                        [ 4.4587e-03, -6.3728e-04, -1.4285e-02],\n                        [-5.1423e-03, -5.7166e-03,  1.2934e-02]]],\n              \n              \n                      [[[ 1.8463e-03, -5.4794e-04, -1.8946e-03],\n                        [ 9.7586e-04,  3.5177e-03, -4.0504e-03],\n                        [-6.2299e-03,  5.2996e-03,  1.3720e-02]],\n              \n                       [[-5.9090e-03,  1.6445e-03,  2.7570e-03],\n                        [-9.9673e-04, -1.0245e-02,  5.6605e-03],\n                        [ 1.1391e-02, -1.1658e-02, -1.1734e-02]],\n              \n                       [[-1.1735e-02,  2.4595e-03,  5.7827e-03],\n                        [ 7.1670e-03, -1.6270e-03,  1.0687e-02],\n                        [ 6.0396e-03, -7.3033e-04, -8.5946e-03]],\n              \n                       ...,\n              \n                       [[ 1.1671e-02,  1.3118e-02, -1.3291e-02],\n                        [ 6.1538e-03, -6.0592e-04,  6.6185e-03],\n                        [ 1.2829e-03, -1.3731e-02,  1.4932e-03]],\n              \n                       [[-7.4605e-03,  6.8828e-04, -1.2302e-04],\n                        [-8.1735e-03,  1.2001e-02,  7.8193e-03],\n                        [ 2.0528e-03, -6.3210e-03,  1.3449e-02]],\n              \n                       [[ 2.9136e-03,  6.6908e-03, -3.7520e-03],\n                        [ 9.3340e-03, -4.1290e-03, -1.4161e-02],\n                        [-5.5939e-03,  5.1468e-03,  7.5768e-05]]],\n              \n              \n                      [[[ 7.9902e-03,  8.0955e-03,  1.0381e-02],\n                        [ 6.6680e-03,  2.9378e-03,  6.6944e-03],\n                        [-2.3877e-03, -4.8883e-03,  8.5533e-03]],\n              \n                       [[-1.2371e-02, -1.2348e-02,  4.0223e-03],\n                        [-6.9362e-03, -1.0553e-02,  5.3495e-03],\n                        [ 4.4429e-04,  5.7790e-03, -2.5581e-03]],\n              \n                       [[ 2.1132e-03, -1.0715e-02,  3.1263e-03],\n                        [ 1.4578e-02, -4.7421e-03, -4.1220e-03],\n                        [ 7.7216e-03, -7.0857e-03, -4.0999e-03]],\n              \n                       ...,\n              \n                       [[-1.2722e-02,  4.8952e-03,  3.1216e-03],\n                        [-3.6589e-03,  3.9157e-03,  7.6172e-05],\n                        [ 6.6556e-03,  1.3619e-02, -1.0715e-02]],\n              \n                       [[-8.3624e-03,  2.8966e-03,  7.7819e-03],\n                        [ 9.6693e-03, -1.3035e-02, -1.2682e-02],\n                        [-1.2393e-02,  1.4095e-02, -9.9444e-03]],\n              \n                       [[-2.6372e-03, -9.4880e-03, -4.2093e-03],\n                        [ 2.4768e-03,  5.2376e-03, -1.6081e-03],\n                        [ 1.4001e-03,  8.7849e-03, -6.4915e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-6.1331e-03, -1.0245e-02,  5.5679e-03],\n                        [-1.3925e-02, -5.4960e-03, -6.4326e-03],\n                        [ 1.0665e-03,  9.3625e-03, -1.0900e-02]],\n              \n                       [[-1.2820e-02, -1.4185e-02,  7.6603e-03],\n                        [ 5.5901e-03, -7.7663e-03, -1.3632e-02],\n                        [-7.8664e-03,  3.8328e-03, -6.1660e-03]],\n              \n                       [[ 2.2009e-03,  1.2656e-02, -5.1460e-03],\n                        [-7.3644e-03, -1.2076e-03,  1.9836e-03],\n                        [-1.4580e-03, -8.4020e-04,  1.0106e-02]],\n              \n                       ...,\n              \n                       [[ 7.8239e-03,  8.2156e-03,  5.3135e-03],\n                        [ 7.6519e-03,  2.5644e-03,  9.5596e-03],\n                        [ 1.2521e-02,  7.5805e-03, -1.3987e-02]],\n              \n                       [[ 1.0951e-02,  7.9635e-04, -6.1090e-03],\n                        [ 7.5488e-03,  1.2158e-02, -1.4382e-02],\n                        [-3.4198e-03, -3.9887e-03, -3.8113e-03]],\n              \n                       [[-1.1689e-02,  9.5688e-03, -5.1517e-03],\n                        [-1.1460e-02, -4.0730e-03, -5.6413e-03],\n                        [ 7.0657e-03,  2.6805e-03, -5.1478e-03]]],\n              \n              \n                      [[[-9.6095e-03, -1.3585e-03, -7.0119e-03],\n                        [ 9.6654e-03,  1.0712e-02,  1.0401e-02],\n                        [-3.5123e-03,  1.3850e-02,  1.0464e-02]],\n              \n                       [[-1.1702e-02, -7.7455e-03, -5.3939e-03],\n                        [-1.2093e-02, -8.4871e-03, -3.2977e-03],\n                        [-1.0420e-02,  8.9802e-03, -4.9594e-03]],\n              \n                       [[-1.2320e-02,  2.4707e-03, -2.3200e-03],\n                        [-3.9590e-03,  1.1381e-02, -3.2109e-03],\n                        [-1.9178e-03, -1.3853e-02, -4.3691e-03]],\n              \n                       ...,\n              \n                       [[ 1.0142e-02,  1.3061e-02,  1.1623e-02],\n                        [-5.8694e-03, -6.4008e-04,  1.3774e-02],\n                        [ 6.2873e-03,  3.2907e-03, -8.4393e-03]],\n              \n                       [[ 3.5045e-03,  4.6928e-03,  1.1195e-02],\n                        [ 5.2034e-03, -9.1595e-03,  1.1639e-02],\n                        [-7.8218e-03,  7.5058e-03, -1.4309e-02]],\n              \n                       [[-2.4525e-03, -3.6981e-03,  1.1964e-02],\n                        [-1.2757e-02, -5.8314e-03, -1.1045e-02],\n                        [ 6.1323e-03,  1.4707e-02, -9.2333e-03]]],\n              \n              \n                      [[[ 5.0627e-03,  1.4049e-02,  7.1501e-03],\n                        [-1.3210e-02,  1.1269e-02,  2.2428e-03],\n                        [-9.7158e-03,  5.5631e-03, -1.2279e-02]],\n              \n                       [[-9.5874e-03, -5.4147e-04,  1.4689e-02],\n                        [ 4.4917e-03, -1.3910e-02, -3.7383e-04],\n                        [-7.5597e-03,  9.3203e-03, -7.5512e-03]],\n              \n                       [[-1.4322e-02, -1.1102e-02,  1.1979e-02],\n                        [ 6.4091e-03, -1.3175e-02,  2.6744e-04],\n                        [ 1.1095e-03,  6.2741e-03,  5.1492e-04]],\n              \n                       ...,\n              \n                       [[ 1.3908e-02,  9.8417e-03,  9.4988e-03],\n                        [ 1.1376e-02,  1.9947e-04, -8.0265e-03],\n                        [-1.1771e-02, -1.0298e-02, -2.5397e-03]],\n              \n                       [[-2.3932e-03,  1.3351e-02,  1.0970e-02],\n                        [ 1.2986e-02,  3.9482e-03, -8.2351e-03],\n                        [-1.0508e-02, -3.3115e-03, -8.0658e-03]],\n              \n                       [[-2.9153e-03,  1.4376e-02, -3.0430e-03],\n                        [ 1.3600e-02, -2.1507e-03, -4.3007e-03],\n                        [-3.6526e-03,  8.3328e-03,  8.7380e-03]]]], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down3.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.0.weight',\n              tensor([[[[-1.3104e-02,  9.6535e-03,  7.0547e-03],\n                        [ 6.8489e-03,  5.6884e-03, -3.3797e-03],\n                        [-1.3077e-02,  1.1413e-02, -8.2186e-03]],\n              \n                       [[-6.4877e-03,  1.2398e-02,  1.4672e-02],\n                        [-2.8377e-03,  2.9911e-03,  8.6744e-03],\n                        [ 4.6708e-03, -1.9309e-03, -1.3963e-02]],\n              \n                       [[-8.8996e-04, -1.3098e-02, -1.2099e-02],\n                        [ 1.1789e-02, -6.3457e-03,  8.4533e-03],\n                        [ 6.9120e-04,  3.7103e-03, -3.9384e-03]],\n              \n                       ...,\n              \n                       [[-1.4631e-02,  7.6187e-03,  1.3055e-02],\n                        [ 8.7348e-03,  2.2455e-03,  1.4252e-02],\n                        [-7.8609e-03,  6.6497e-03,  1.2674e-02]],\n              \n                       [[ 1.0928e-02,  8.1940e-03,  1.4620e-03],\n                        [ 1.1112e-03, -7.0720e-03, -1.2397e-02],\n                        [ 1.3073e-02,  2.2528e-03,  6.1473e-03]],\n              \n                       [[-1.1589e-02, -9.5213e-03, -5.2496e-03],\n                        [-1.1412e-02, -1.3629e-02,  7.4268e-03],\n                        [-6.4922e-03,  1.1146e-02, -9.5554e-03]]],\n              \n              \n                      [[[ 2.3625e-05, -1.3995e-02, -7.6334e-03],\n                        [-9.4009e-03, -9.2042e-03,  5.7072e-03],\n                        [ 9.9287e-03, -5.7740e-03,  8.9586e-03]],\n              \n                       [[ 1.4008e-02, -1.0200e-02,  1.3237e-02],\n                        [ 1.4621e-02, -1.2051e-02,  6.9597e-03],\n                        [ 1.2422e-02, -8.4337e-03, -7.5494e-03]],\n              \n                       [[ 5.7422e-04, -8.9031e-03,  1.4246e-02],\n                        [-3.9909e-03, -1.2648e-05,  7.5228e-03],\n                        [ 4.5517e-03, -8.1091e-03, -2.5926e-03]],\n              \n                       ...,\n              \n                       [[ 1.7802e-03,  1.2118e-02, -8.6626e-04],\n                        [-6.0965e-04, -5.6477e-03, -4.7239e-03],\n                        [-1.4231e-03, -1.1298e-02,  4.0613e-03]],\n              \n                       [[ 2.4961e-05,  4.4265e-03,  1.4223e-02],\n                        [ 2.2458e-03,  1.3728e-02, -1.1796e-02],\n                        [-7.2479e-03,  1.2696e-02,  4.3921e-03]],\n              \n                       [[ 1.4457e-02, -1.0118e-02,  1.3083e-02],\n                        [-7.3051e-03,  1.3544e-02, -1.2357e-02],\n                        [ 3.5746e-03, -1.3268e-02, -9.3003e-03]]],\n              \n              \n                      [[[-3.1621e-03,  1.4471e-02,  1.0941e-02],\n                        [ 1.2192e-02,  5.9600e-03,  7.0732e-03],\n                        [ 1.6198e-03, -1.1914e-02, -1.1316e-02]],\n              \n                       [[-8.1733e-03, -4.6493e-03,  1.3078e-02],\n                        [-5.0052e-03, -1.0437e-02,  9.8975e-03],\n                        [-1.3412e-02, -8.9157e-03,  1.3293e-02]],\n              \n                       [[-5.0194e-03,  6.6695e-03,  3.4234e-04],\n                        [-1.3336e-02,  1.4430e-03,  7.5926e-03],\n                        [-1.0269e-03,  1.0630e-02, -8.4293e-03]],\n              \n                       ...,\n              \n                       [[ 1.0040e-02, -9.6519e-03,  1.1701e-02],\n                        [ 6.5308e-05,  3.5704e-03, -1.2048e-02],\n                        [-9.5033e-03, -1.2604e-02, -1.2307e-02]],\n              \n                       [[-6.6415e-03, -1.0024e-02,  1.3435e-02],\n                        [-6.3868e-03, -1.4265e-02, -2.8581e-03],\n                        [-1.3789e-02,  1.1855e-02,  7.1601e-03]],\n              \n                       [[-9.1238e-03,  4.7032e-05, -2.2387e-03],\n                        [ 4.9879e-04,  7.7738e-03,  5.1973e-03],\n                        [ 3.4793e-03,  9.1406e-03, -9.1121e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.2879e-03,  1.1191e-03, -6.0251e-03],\n                        [-3.2071e-03,  5.4502e-03,  1.2839e-04],\n                        [ 5.8309e-03, -1.3948e-02,  3.9841e-03]],\n              \n                       [[ 1.0795e-02,  5.7343e-03,  3.2873e-03],\n                        [ 5.4282e-03, -1.0134e-02,  3.3486e-03],\n                        [ 5.0658e-03, -1.4290e-02,  3.9768e-03]],\n              \n                       [[-1.4718e-02, -4.8749e-03,  8.8550e-03],\n                        [-1.2116e-02,  3.9706e-03, -1.5341e-04],\n                        [-5.6044e-03,  9.2914e-03,  2.6309e-03]],\n              \n                       ...,\n              \n                       [[ 1.1578e-02,  4.7662e-03,  1.0865e-02],\n                        [-9.9621e-03,  7.2204e-03,  6.7652e-03],\n                        [ 6.1930e-03,  5.5036e-03, -4.8385e-03]],\n              \n                       [[-1.1982e-02,  9.0713e-03, -6.7553e-03],\n                        [ 1.0392e-02, -6.3635e-03, -1.1598e-03],\n                        [ 1.0464e-02,  4.0243e-03,  1.4345e-03]],\n              \n                       [[ 3.2504e-03,  1.4237e-02, -7.7320e-03],\n                        [-1.0245e-02, -8.5657e-03, -1.2735e-02],\n                        [-3.5816e-03,  1.3560e-02, -1.2678e-02]]],\n              \n              \n                      [[[-1.4336e-02, -4.6926e-03,  1.3425e-02],\n                        [ 1.3409e-02, -6.8928e-03, -9.7946e-03],\n                        [-1.4182e-02, -8.6928e-03, -1.4202e-02]],\n              \n                       [[-5.0576e-03, -9.8077e-03,  5.6572e-03],\n                        [-1.4611e-02,  4.4676e-03, -1.3235e-02],\n                        [ 3.6478e-03,  4.1773e-04,  1.4504e-02]],\n              \n                       [[-8.5665e-03, -6.6888e-03, -5.9852e-03],\n                        [ 1.8548e-03,  1.2795e-02, -6.3900e-03],\n                        [-1.3038e-02,  7.2169e-03,  9.2560e-03]],\n              \n                       ...,\n              \n                       [[-5.8375e-03,  8.9250e-03,  1.2109e-02],\n                        [-1.3653e-02,  1.3453e-02, -6.7649e-03],\n                        [-1.2166e-02, -1.3578e-02, -1.2037e-03]],\n              \n                       [[-5.5372e-03, -3.9234e-03, -2.1640e-03],\n                        [-8.1456e-03, -8.1486e-03,  4.8608e-05],\n                        [-7.9746e-03,  3.5861e-03, -5.4110e-03]],\n              \n                       [[ 9.0684e-03, -4.6523e-03,  8.6029e-03],\n                        [-3.5470e-03, -2.6329e-03,  4.1187e-03],\n                        [-1.7698e-03,  3.1339e-03, -1.3087e-02]]],\n              \n              \n                      [[[ 1.3993e-02,  1.0210e-02, -9.8379e-03],\n                        [-3.6017e-03,  1.5505e-03, -7.5702e-03],\n                        [-1.3827e-03, -1.4429e-02, -1.3696e-02]],\n              \n                       [[ 1.2335e-02,  8.3124e-03, -4.6792e-03],\n                        [ 4.8468e-03,  1.3626e-04,  9.8758e-03],\n                        [-2.6817e-03,  3.2997e-03, -9.7415e-04]],\n              \n                       [[ 3.1673e-03, -7.1938e-03, -1.4500e-03],\n                        [-9.1013e-03,  8.4705e-03, -9.5864e-03],\n                        [ 1.6714e-03, -1.4101e-02,  1.1644e-02]],\n              \n                       ...,\n              \n                       [[ 1.4320e-02,  4.4366e-03, -5.8747e-03],\n                        [-8.1688e-03, -6.9629e-03,  3.0317e-04],\n                        [-1.2110e-02, -1.3646e-02, -6.0113e-03]],\n              \n                       [[-3.7647e-04,  7.6979e-03,  3.3129e-03],\n                        [ 7.6917e-03, -1.9005e-03,  6.3914e-03],\n                        [-2.9271e-03,  1.0327e-02, -9.8557e-03]],\n              \n                       [[ 1.1749e-02,  3.9048e-03, -7.2822e-03],\n                        [ 1.4049e-02,  1.3569e-02,  2.5594e-03],\n                        [ 1.2890e-02,  5.6545e-03,  6.2168e-03]]]], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.3.weight',\n              tensor([[[[-1.0162e-02, -7.9513e-03, -1.4126e-02],\n                        [-6.2557e-03, -9.7779e-03,  1.0858e-02],\n                        [ 9.1498e-03,  3.0958e-04,  9.0409e-03]],\n              \n                       [[-7.6646e-03, -9.0559e-03, -8.4516e-04],\n                        [-1.2277e-02,  2.7770e-03,  2.4928e-03],\n                        [ 2.1196e-03, -2.7451e-03, -1.3663e-02]],\n              \n                       [[-8.4018e-03,  3.2803e-03, -6.1505e-03],\n                        [ 1.3116e-02,  8.8065e-03,  4.6064e-03],\n                        [ 9.4382e-03, -7.7282e-03,  1.0306e-02]],\n              \n                       ...,\n              \n                       [[ 6.6357e-03, -2.2279e-03, -8.7835e-03],\n                        [-5.1093e-03,  3.9618e-03,  8.8206e-03],\n                        [ 1.4141e-02,  1.3784e-02,  1.1771e-02]],\n              \n                       [[-5.9949e-03, -1.3745e-04,  7.4454e-03],\n                        [-9.2404e-03,  1.3126e-02,  9.9188e-03],\n                        [-6.8859e-03, -1.4138e-02, -9.2198e-03]],\n              \n                       [[-1.4438e-02,  1.1573e-02,  1.1146e-02],\n                        [-8.7031e-03, -4.6383e-03,  7.3338e-03],\n                        [ 1.1381e-02, -9.0583e-03, -2.5293e-03]]],\n              \n              \n                      [[[-1.3852e-02, -6.8651e-03,  2.3293e-03],\n                        [ 1.2269e-02,  6.5710e-03,  3.9793e-03],\n                        [-7.3067e-03, -5.9318e-03, -6.7658e-03]],\n              \n                       [[ 9.5927e-03, -7.6682e-03, -1.3819e-02],\n                        [-9.0626e-03,  3.5546e-03, -8.5062e-03],\n                        [ 1.7261e-03, -2.6030e-03, -1.4632e-02]],\n              \n                       [[ 1.0916e-02,  1.0892e-02,  1.4228e-02],\n                        [ 1.1874e-02, -6.4073e-03, -5.1940e-03],\n                        [-7.4828e-03, -7.4947e-03,  2.5183e-03]],\n              \n                       ...,\n              \n                       [[ 9.7132e-03,  2.0456e-03, -4.0253e-03],\n                        [ 1.9973e-03,  1.2258e-02, -1.3174e-03],\n                        [-9.0220e-03, -8.2095e-03,  1.4117e-02]],\n              \n                       [[-1.0827e-02,  1.4226e-02, -6.4879e-03],\n                        [ 1.2198e-02, -1.2647e-02,  8.6206e-03],\n                        [-2.7980e-03, -2.0266e-03,  5.7236e-03]],\n              \n                       [[-1.2030e-02,  1.2822e-02, -8.4252e-03],\n                        [ 1.1277e-02, -7.0514e-03, -7.5673e-03],\n                        [ 8.1968e-03, -1.2170e-02, -7.3895e-03]]],\n              \n              \n                      [[[ 8.0684e-03,  1.3598e-02, -7.9777e-03],\n                        [-1.4268e-02,  4.8484e-03, -1.1704e-02],\n                        [ 4.8766e-03,  2.9658e-03,  2.0288e-03]],\n              \n                       [[-1.1000e-03, -2.6417e-03,  3.1051e-03],\n                        [ 1.2253e-02, -7.2229e-03, -1.1037e-03],\n                        [ 1.0293e-02,  3.9444e-03, -8.0077e-03]],\n              \n                       [[ 3.6599e-03,  1.3138e-02, -1.0403e-03],\n                        [-1.0804e-02, -2.9224e-03, -7.3381e-04],\n                        [-8.4483e-03, -3.5656e-03,  1.0923e-02]],\n              \n                       ...,\n              \n                       [[ 1.0183e-02, -1.0656e-02,  2.5374e-03],\n                        [-2.4001e-03,  9.3434e-03,  8.0887e-03],\n                        [-3.1470e-03, -3.6860e-03,  6.9349e-03]],\n              \n                       [[-1.4212e-02,  4.7419e-03,  2.2588e-03],\n                        [ 1.2572e-02,  2.5563e-03, -8.1275e-03],\n                        [-3.7703e-03,  2.5945e-03,  5.5602e-03]],\n              \n                       [[-1.2830e-02, -1.0370e-02,  9.9764e-03],\n                        [-1.0848e-02, -9.6209e-03,  8.2907e-03],\n                        [ 4.6423e-03, -4.9777e-03, -8.6183e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.9552e-03,  1.0103e-02, -4.7408e-03],\n                        [-1.3407e-02,  6.5927e-03, -7.2890e-03],\n                        [ 1.2902e-02, -7.3139e-03,  4.8173e-03]],\n              \n                       [[-8.6896e-03, -1.9172e-03,  5.9656e-03],\n                        [-7.3172e-05,  2.9933e-03, -1.1204e-02],\n                        [ 2.1456e-03,  2.6252e-03, -1.3978e-02]],\n              \n                       [[-8.2944e-03, -6.1581e-03,  1.3276e-02],\n                        [ 2.0285e-04, -6.9051e-03,  1.3585e-02],\n                        [-7.9958e-03,  5.1597e-03, -1.1482e-02]],\n              \n                       ...,\n              \n                       [[ 2.9236e-03,  8.6567e-03, -5.6918e-03],\n                        [ 1.2319e-02, -1.2173e-02, -1.1142e-02],\n                        [ 2.1955e-03,  2.1893e-03,  1.0226e-02]],\n              \n                       [[-1.3731e-02,  2.4001e-04,  1.0280e-02],\n                        [ 6.2036e-04,  9.4891e-03, -9.4363e-03],\n                        [ 7.7716e-03, -5.3223e-03, -1.1793e-02]],\n              \n                       [[ 9.0567e-03, -9.4963e-03,  1.2966e-02],\n                        [-3.5606e-03,  6.7127e-03,  9.2346e-03],\n                        [ 1.6610e-04,  9.7832e-04, -3.7458e-03]]],\n              \n              \n                      [[[ 1.8821e-03,  7.0609e-03, -9.9641e-03],\n                        [ 2.8442e-03, -3.4813e-04,  2.8147e-03],\n                        [-7.6718e-03,  1.4098e-03,  3.6991e-03]],\n              \n                       [[-7.4600e-03,  6.1319e-03, -6.6834e-03],\n                        [ 4.6137e-03, -9.7316e-03, -2.1926e-03],\n                        [-5.1150e-03,  8.5056e-03,  1.4168e-02]],\n              \n                       [[ 1.2746e-02,  8.4634e-03,  1.2394e-02],\n                        [ 6.5522e-03, -1.0927e-02, -1.4621e-02],\n                        [ 9.5033e-03,  3.9224e-03,  9.9719e-03]],\n              \n                       ...,\n              \n                       [[-4.0116e-03, -1.4190e-02, -2.6838e-03],\n                        [-1.9716e-04, -1.6087e-03, -2.2089e-03],\n                        [ 1.1347e-02,  5.0595e-04, -2.1228e-03]],\n              \n                       [[ 1.1465e-03,  6.0314e-03, -7.8767e-03],\n                        [-6.6732e-03, -5.0615e-03, -7.0481e-03],\n                        [-3.5145e-03, -1.4674e-02,  9.3690e-03]],\n              \n                       [[-2.1949e-03,  1.8604e-04, -3.8469e-04],\n                        [-6.0911e-03,  4.8625e-03,  9.1291e-04],\n                        [-4.2253e-03, -9.7373e-03,  3.0233e-03]]],\n              \n              \n                      [[[ 1.3092e-02, -9.1652e-03, -1.4018e-02],\n                        [-7.5290e-03, -1.1704e-02,  1.1918e-02],\n                        [-3.6753e-03,  8.3012e-03, -7.8185e-03]],\n              \n                       [[ 1.3660e-02, -1.0051e-04, -4.8537e-03],\n                        [ 4.5250e-03,  1.1501e-02, -1.2260e-02],\n                        [-1.2088e-02, -1.1217e-02, -8.9023e-03]],\n              \n                       [[ 3.9087e-03, -1.1512e-03, -1.3955e-02],\n                        [-2.1982e-03,  1.0120e-02, -5.0558e-03],\n                        [-1.3255e-02,  2.8492e-03, -4.1524e-03]],\n              \n                       ...,\n              \n                       [[-1.2921e-02, -1.8075e-03,  3.1186e-03],\n                        [ 4.0110e-03,  5.9678e-03, -1.5871e-03],\n                        [ 4.0160e-03,  4.9175e-04,  2.2130e-03]],\n              \n                       [[-3.4039e-03, -1.2438e-02,  6.7231e-03],\n                        [ 1.2851e-02, -5.3675e-03,  1.6797e-03],\n                        [-1.3136e-02, -2.5658e-03, -5.8660e-03]],\n              \n                       [[-2.0538e-03,  7.5002e-04,  6.9986e-03],\n                        [ 1.3422e-02, -9.2835e-04,  4.6620e-03],\n                        [-1.3815e-02,  5.7040e-03, -6.6107e-03]]]], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.down4.maxpool_conv.1.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up1.conv.double_conv.0.weight',\n              tensor([[[[ 6.0052e-03, -6.1578e-03, -8.6970e-03],\n                        [ 1.6955e-03, -7.3866e-03,  5.3448e-03],\n                        [ 5.5082e-03,  9.1673e-03,  1.0191e-02]],\n              \n                       [[-3.7926e-03,  5.7925e-03,  1.0316e-02],\n                        [ 9.6915e-03,  8.8699e-03,  5.3047e-03],\n                        [ 5.0500e-03,  4.6066e-03,  1.0278e-02]],\n              \n                       [[-7.2442e-04, -7.9003e-03, -9.7175e-03],\n                        [ 4.6586e-04, -3.6655e-03, -9.5510e-03],\n                        [-9.1740e-03, -7.8502e-03, -5.3606e-03]],\n              \n                       ...,\n              \n                       [[ 2.1322e-03, -9.4887e-05, -4.9738e-03],\n                        [-6.1662e-03,  1.3903e-03, -7.2019e-03],\n                        [ 5.4206e-03,  8.7880e-03,  4.3695e-03]],\n              \n                       [[ 3.3114e-03, -4.8001e-03, -2.7326e-03],\n                        [-3.7524e-03,  7.7908e-03, -8.4219e-03],\n                        [ 2.0721e-03,  7.5771e-03,  6.9718e-03]],\n              \n                       [[-9.9150e-03, -2.1330e-03,  7.4038e-03],\n                        [-6.3372e-03, -8.1195e-03,  1.6034e-03],\n                        [ 5.8172e-03, -1.3327e-03, -7.0786e-03]]],\n              \n              \n                      [[[-4.7313e-03, -2.5325e-03, -6.1366e-03],\n                        [ 1.1530e-03, -5.3506e-03, -6.1344e-04],\n                        [ 2.7635e-03, -6.2766e-03,  4.6419e-03]],\n              \n                       [[ 4.3768e-03, -4.0070e-03,  8.7607e-03],\n                        [-8.9397e-03, -9.8516e-03, -2.8273e-03],\n                        [-3.7660e-03,  3.6542e-03,  1.0126e-02]],\n              \n                       [[-6.7512e-03,  6.0833e-03,  2.7166e-03],\n                        [ 9.3578e-04,  5.1147e-03,  6.3890e-03],\n                        [ 1.5687e-04,  7.4274e-03, -8.3365e-03]],\n              \n                       ...,\n              \n                       [[-4.8921e-03, -5.4093e-03,  5.6688e-03],\n                        [ 3.1983e-03,  3.9314e-03, -8.9410e-03],\n                        [ 6.5762e-03, -9.7403e-03, -4.1459e-03]],\n              \n                       [[ 8.1715e-03,  5.4453e-03, -7.9296e-03],\n                        [ 1.6348e-03, -1.7733e-04,  1.1809e-03],\n                        [-6.2941e-03,  6.1941e-03,  1.7227e-03]],\n              \n                       [[ 9.5111e-03, -8.0376e-03, -3.7345e-03],\n                        [ 5.4716e-03, -3.7542e-03,  2.9980e-03],\n                        [-7.5362e-03,  8.4094e-03,  8.9098e-03]]],\n              \n              \n                      [[[-9.6740e-03, -8.1277e-03,  3.9857e-03],\n                        [-3.5163e-03,  8.6464e-03,  4.2643e-03],\n                        [-5.0144e-03, -9.8802e-04,  4.8284e-04]],\n              \n                       [[-6.5739e-03,  9.1206e-03,  5.8876e-03],\n                        [-4.3970e-03,  3.9926e-04,  4.9571e-03],\n                        [-3.2965e-03,  4.1399e-04, -2.7867e-03]],\n              \n                       [[-4.9022e-03, -7.1855e-04,  5.2022e-04],\n                        [-3.8415e-03,  7.9072e-03,  1.0071e-02],\n                        [-6.5128e-03, -3.6828e-03, -8.3628e-03]],\n              \n                       ...,\n              \n                       [[ 8.5856e-03, -7.1988e-03,  9.1629e-03],\n                        [ 9.4906e-03, -6.0381e-03,  6.3775e-04],\n                        [ 3.2705e-03, -4.2573e-03,  7.2144e-03]],\n              \n                       [[-2.7434e-03, -5.6575e-03,  7.0926e-03],\n                        [ 6.5038e-03,  1.0222e-02,  7.6083e-03],\n                        [ 8.3256e-03,  7.9641e-03, -6.8926e-03]],\n              \n                       [[ 3.2581e-03, -3.4153e-03,  1.7781e-04],\n                        [-4.7329e-03, -2.7371e-03, -7.9243e-03],\n                        [-7.3951e-03, -3.6213e-03,  3.8721e-04]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.3754e-03,  1.0256e-02, -9.6938e-03],\n                        [-5.2090e-03,  1.1899e-03,  6.6328e-03],\n                        [-6.4318e-03,  7.6097e-03,  3.2797e-03]],\n              \n                       [[-7.0052e-03,  4.5905e-03, -8.9286e-03],\n                        [-8.2543e-03, -5.1691e-03, -5.8590e-03],\n                        [ 8.7791e-03,  5.7680e-03, -8.9067e-03]],\n              \n                       [[-7.6416e-03, -9.3266e-03,  9.4770e-03],\n                        [ 1.4398e-03,  4.5831e-03, -3.4448e-03],\n                        [-4.5923e-03, -5.7610e-03, -4.3103e-03]],\n              \n                       ...,\n              \n                       [[-2.0614e-03, -8.5129e-03, -8.4951e-03],\n                        [ 2.6566e-03,  9.1776e-03,  2.6760e-03],\n                        [-1.7022e-04,  3.6392e-03,  5.0875e-03]],\n              \n                       [[-2.9073e-03, -7.8702e-03, -1.2811e-03],\n                        [-8.3429e-03, -8.4082e-03,  4.3443e-03],\n                        [-6.5337e-03,  3.0448e-03, -3.2978e-03]],\n              \n                       [[-6.3634e-03, -6.4584e-03, -9.4520e-03],\n                        [ 6.3613e-03,  1.3895e-03,  6.7184e-03],\n                        [ 1.9717e-04,  3.0919e-03, -9.3850e-03]]],\n              \n              \n                      [[[-7.3347e-03,  3.7111e-03, -1.4600e-03],\n                        [-8.9929e-03, -1.0001e-02, -9.7608e-03],\n                        [ 4.9672e-03, -5.1917e-03, -9.9102e-03]],\n              \n                       [[ 7.6933e-03, -4.9824e-03, -8.9469e-03],\n                        [ 4.8704e-03, -1.6437e-03,  8.8097e-03],\n                        [-3.0993e-03, -5.9778e-03, -3.1651e-03]],\n              \n                       [[ 8.6893e-03,  9.8990e-03,  7.1665e-03],\n                        [ 7.6924e-03, -1.0816e-03,  9.3137e-03],\n                        [-4.7224e-03, -3.9862e-03, -7.0841e-03]],\n              \n                       ...,\n              \n                       [[ 7.1673e-03,  5.2882e-03,  5.8690e-03],\n                        [ 4.2807e-04, -4.7009e-04,  9.8658e-03],\n                        [-3.6831e-03, -3.5520e-03,  4.0485e-03]],\n              \n                       [[-5.5522e-03,  9.4766e-03,  8.2692e-03],\n                        [-3.1187e-03, -8.5105e-03,  8.7861e-03],\n                        [-7.3462e-03,  5.8684e-03,  9.6273e-03]],\n              \n                       [[-3.7102e-03,  7.7810e-03, -1.4194e-03],\n                        [-4.0797e-03, -8.0059e-03,  8.5199e-03],\n                        [-9.1947e-03,  3.5915e-03, -4.6602e-03]]],\n              \n              \n                      [[[-1.3775e-03,  6.0666e-04, -6.9796e-04],\n                        [ 6.7400e-03,  6.6210e-03,  2.7429e-03],\n                        [-8.8243e-03, -9.8390e-03,  2.4116e-03]],\n              \n                       [[ 4.7119e-03,  3.2005e-03,  5.9726e-03],\n                        [ 9.5476e-03,  1.6969e-03,  9.7832e-03],\n                        [-2.6481e-03,  7.0522e-03, -7.9863e-03]],\n              \n                       [[ 4.9707e-03,  9.5256e-04, -1.3029e-03],\n                        [-6.9370e-03, -1.0068e-02,  1.0652e-03],\n                        [-2.0503e-03,  8.6360e-03, -1.5661e-03]],\n              \n                       ...,\n              \n                       [[-6.5328e-03, -9.1420e-04,  5.5855e-03],\n                        [ 8.4739e-03, -4.1916e-03,  1.0212e-02],\n                        [ 1.0342e-02, -8.0135e-03, -1.1019e-04]],\n              \n                       [[ 4.2931e-03,  4.7278e-03,  8.9549e-03],\n                        [ 7.2504e-03,  4.6937e-03, -6.7444e-03],\n                        [-1.0244e-02,  2.1343e-03, -3.2979e-03]],\n              \n                       [[ 9.3904e-03, -7.6412e-03,  2.0035e-03],\n                        [-6.8808e-03,  1.0404e-02,  9.5906e-03],\n                        [ 5.1486e-03,  1.8948e-03, -1.0138e-03]]]], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up1.conv.double_conv.3.weight',\n              tensor([[[[ 4.6532e-03, -7.6019e-03, -2.2726e-03],\n                        [ 4.6818e-03,  1.2958e-02,  7.4474e-03],\n                        [ 1.0656e-02,  7.3169e-03,  1.4385e-02]],\n              \n                       [[-7.1003e-03,  5.6198e-03,  1.1528e-02],\n                        [ 1.2165e-02,  2.7467e-03,  1.2221e-02],\n                        [ 1.0123e-02, -7.3388e-04, -1.3558e-02]],\n              \n                       [[ 6.1051e-04, -1.0071e-02,  1.0367e-02],\n                        [ 5.4181e-03,  3.2388e-03,  8.1533e-04],\n                        [ 9.9759e-03, -8.9243e-03, -1.0614e-02]],\n              \n                       ...,\n              \n                       [[-1.1593e-02,  4.4562e-03, -1.2794e-02],\n                        [-2.0847e-03,  8.4393e-03, -3.0718e-03],\n                        [ 1.2095e-02,  9.6634e-03, -6.1204e-03]],\n              \n                       [[-8.5692e-03, -5.3203e-03, -6.0301e-03],\n                        [-1.3060e-02, -4.9878e-03,  1.3536e-02],\n                        [-3.0446e-03, -3.7271e-03,  1.8943e-03]],\n              \n                       [[ 9.1236e-03,  6.2085e-03, -5.2066e-03],\n                        [ 7.0768e-03,  5.8855e-03, -1.3525e-02],\n                        [ 1.2969e-02, -3.1656e-03, -9.7805e-03]]],\n              \n              \n                      [[[-1.3448e-02, -1.4380e-02,  3.3876e-03],\n                        [-6.9893e-03, -8.7593e-03,  3.4935e-03],\n                        [ 6.0252e-03,  6.2473e-03, -7.2960e-04]],\n              \n                       [[ 1.2521e-03, -1.2604e-02, -1.4122e-02],\n                        [-7.8812e-03,  1.2843e-03,  3.4510e-03],\n                        [-8.0826e-03, -6.0928e-03,  1.4071e-02]],\n              \n                       [[ 1.2236e-02, -2.2066e-03,  7.5802e-03],\n                        [-3.4579e-03, -8.4028e-03,  1.2992e-02],\n                        [ 1.5273e-03,  9.6915e-03, -2.7779e-03]],\n              \n                       ...,\n              \n                       [[-9.7299e-03,  7.2240e-03,  3.2073e-04],\n                        [ 5.1952e-03,  1.3993e-02,  5.8187e-03],\n                        [-3.9472e-03,  9.5075e-03,  9.9508e-03]],\n              \n                       [[ 3.8860e-03, -7.5956e-03, -6.7716e-03],\n                        [-6.3491e-03,  1.1731e-02, -4.6717e-03],\n                        [ 5.6204e-04, -4.5982e-03, -1.3072e-03]],\n              \n                       [[-9.9374e-03, -1.4691e-03,  9.6274e-03],\n                        [-3.4154e-03, -9.9765e-03,  4.7587e-03],\n                        [ 1.1309e-02,  1.2087e-03,  1.1953e-02]]],\n              \n              \n                      [[[ 1.2883e-02, -7.2949e-03, -4.8458e-03],\n                        [ 9.7466e-03,  1.1054e-02,  1.2237e-02],\n                        [ 9.9405e-03,  1.4726e-02,  2.0744e-03]],\n              \n                       [[ 1.0789e-02,  1.3618e-02,  1.4625e-02],\n                        [-1.9228e-03,  5.1298e-03,  5.3312e-04],\n                        [ 1.4351e-02,  8.0309e-03, -1.3372e-02]],\n              \n                       [[-3.1131e-03, -6.5674e-04, -1.0796e-02],\n                        [-9.3562e-03,  6.5610e-03, -1.3210e-02],\n                        [ 7.9644e-03,  1.0064e-03,  6.2818e-04]],\n              \n                       ...,\n              \n                       [[-2.9593e-03, -3.4946e-03, -4.1973e-03],\n                        [ 1.2073e-02,  7.9237e-03,  9.7770e-05],\n                        [-4.5093e-03, -8.0024e-03, -3.3877e-03]],\n              \n                       [[ 4.1504e-04, -6.3685e-03,  2.9286e-04],\n                        [-1.4368e-02,  5.2549e-04, -1.2686e-02],\n                        [ 1.6020e-03,  4.4607e-03,  7.5159e-03]],\n              \n                       [[-6.6873e-03,  5.1561e-05,  8.2160e-03],\n                        [-7.2157e-03, -9.4008e-04, -9.3220e-03],\n                        [ 1.3272e-03,  1.3943e-03, -1.0126e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.3756e-03,  1.2603e-02,  1.0009e-02],\n                        [ 1.3332e-02,  2.2436e-03, -2.6538e-03],\n                        [ 1.2150e-02, -6.4561e-03, -1.2219e-02]],\n              \n                       [[-8.2563e-03,  1.4514e-02, -6.5334e-03],\n                        [ 1.0584e-02,  7.2743e-03, -7.7184e-03],\n                        [-1.3945e-02, -3.9507e-04, -1.3207e-02]],\n              \n                       [[-1.1936e-02,  1.2723e-02,  1.4794e-03],\n                        [-9.2238e-03,  1.2513e-02, -1.2755e-02],\n                        [-2.3135e-04, -1.2050e-02,  1.0637e-02]],\n              \n                       ...,\n              \n                       [[-1.7315e-03, -1.1583e-02, -6.2004e-03],\n                        [-3.6829e-03, -7.5475e-03, -1.1467e-02],\n                        [-1.2565e-04, -1.6956e-03,  7.3251e-03]],\n              \n                       [[ 4.5195e-03,  9.6949e-03, -1.1593e-02],\n                        [-1.0726e-02, -4.3706e-03, -1.0075e-02],\n                        [-1.1938e-02, -6.4125e-03,  5.7692e-04]],\n              \n                       [[-1.1380e-02, -9.5971e-03, -1.3420e-02],\n                        [ 1.0888e-02, -1.0871e-02,  4.6657e-05],\n                        [-2.8069e-03, -1.0725e-02,  2.2430e-03]]],\n              \n              \n                      [[[ 1.1839e-02,  1.3359e-02, -2.2681e-03],\n                        [ 1.8450e-03,  5.9289e-04, -1.2829e-02],\n                        [ 1.4203e-02,  2.5810e-03, -1.1913e-02]],\n              \n                       [[-1.3077e-02, -1.4014e-02, -4.2100e-03],\n                        [-9.9503e-03,  1.1108e-02, -3.2723e-03],\n                        [ 2.0312e-03,  4.5349e-03,  1.3859e-02]],\n              \n                       [[-1.4575e-02,  1.1122e-02, -7.5780e-03],\n                        [-3.8330e-03, -9.8024e-04,  5.9586e-03],\n                        [ 9.8220e-03, -6.8341e-03,  1.2393e-02]],\n              \n                       ...,\n              \n                       [[-3.4048e-03,  1.3819e-02, -2.6837e-03],\n                        [ 1.1734e-02,  1.4311e-03, -1.2245e-02],\n                        [-8.3261e-03,  1.3495e-02,  2.9223e-03]],\n              \n                       [[-1.2962e-02, -7.3929e-03, -7.3878e-03],\n                        [-1.7338e-03, -6.7076e-03, -7.7754e-03],\n                        [ 1.4972e-03, -6.4253e-03, -1.4126e-02]],\n              \n                       [[ 1.4451e-02, -4.8099e-03,  5.7255e-03],\n                        [-5.8516e-03,  4.0733e-03,  1.0094e-02],\n                        [ 8.1309e-04,  5.1471e-03,  5.1509e-03]]],\n              \n              \n                      [[[ 9.8223e-04,  1.1245e-02,  1.1552e-02],\n                        [-7.6653e-03,  6.1365e-04, -4.2670e-03],\n                        [ 5.1350e-03,  1.4145e-02, -8.8357e-04]],\n              \n                       [[ 1.2253e-02,  1.0491e-02, -1.4184e-02],\n                        [ 2.6855e-03,  7.4216e-03, -4.6636e-03],\n                        [-1.0291e-02, -1.2930e-02, -3.5078e-04]],\n              \n                       [[ 4.5516e-03, -9.4295e-03,  9.7718e-03],\n                        [-7.6455e-03,  1.0235e-02,  1.2030e-03],\n                        [-2.7815e-03,  6.6763e-03, -8.7617e-03]],\n              \n                       ...,\n              \n                       [[-9.8976e-03,  1.2484e-02, -2.8897e-03],\n                        [ 4.3479e-03,  8.9747e-03,  8.7985e-04],\n                        [ 1.2341e-02,  4.2616e-04,  4.2251e-03]],\n              \n                       [[ 1.2692e-02, -1.7026e-03,  7.1434e-03],\n                        [ 1.1852e-02, -1.1433e-02, -1.3874e-02],\n                        [ 1.2581e-02, -3.8352e-03, -7.5201e-04]],\n              \n                       [[-4.7592e-04, -3.9157e-03,  3.5884e-03],\n                        [-3.2631e-03, -1.6258e-03, -1.0496e-02],\n                        [ 1.3847e-03, -5.7536e-04, -1.0432e-02]]]], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up1.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up2.conv.double_conv.0.weight',\n              tensor([[[[-2.1518e-03,  1.0631e-02,  1.2601e-02],\n                        [ 9.9365e-03,  8.6478e-03, -1.2200e-02],\n                        [-8.7199e-03, -1.3551e-04,  2.7872e-03]],\n              \n                       [[ 1.0136e-02,  5.1465e-03, -7.2739e-03],\n                        [-1.0549e-02, -4.3726e-03, -1.0110e-02],\n                        [-1.2202e-02,  8.1444e-03,  1.2508e-02]],\n              \n                       [[-1.1105e-02, -3.2792e-03,  1.1186e-02],\n                        [-8.2915e-03,  8.8182e-03,  1.1263e-02],\n                        [-4.4057e-03,  8.6805e-03, -9.5922e-03]],\n              \n                       ...,\n              \n                       [[ 6.3221e-03, -1.2953e-02,  5.1380e-03],\n                        [ 2.9260e-04, -1.0260e-02,  6.4162e-03],\n                        [-5.8944e-03,  4.6316e-03,  1.4742e-03]],\n              \n                       [[-1.0956e-02, -3.5614e-03, -3.6777e-03],\n                        [ 1.2266e-02, -3.7897e-05, -1.1044e-02],\n                        [ 5.1852e-03,  8.2570e-03,  1.3097e-03]],\n              \n                       [[-2.4492e-03, -3.5821e-03, -1.4560e-02],\n                        [ 9.1054e-03, -4.1931e-03,  9.5132e-03],\n                        [ 5.1267e-03,  1.1881e-02,  5.6942e-04]]],\n              \n              \n                      [[[ 1.0638e-02, -5.4433e-03, -3.7759e-03],\n                        [ 1.1677e-02, -4.1737e-03, -1.0637e-02],\n                        [-1.6576e-03, -2.1487e-03, -1.1114e-02]],\n              \n                       [[ 1.8396e-03,  1.3266e-02,  6.8261e-03],\n                        [ 3.9165e-03, -8.8550e-03,  1.4806e-03],\n                        [ 7.0773e-04,  1.1756e-02, -1.0292e-02]],\n              \n                       [[ 1.3127e-02,  4.8850e-03,  2.1176e-03],\n                        [ 2.1249e-03, -5.7832e-03, -1.3140e-02],\n                        [ 8.5454e-03, -8.9114e-03, -1.3402e-02]],\n              \n                       ...,\n              \n                       [[ 1.1088e-02,  7.2383e-03,  1.2047e-02],\n                        [ 9.5457e-03,  1.3826e-02, -2.5452e-03],\n                        [ 9.1783e-03,  1.0598e-02, -8.6740e-04]],\n              \n                       [[ 4.5989e-03, -1.4716e-03, -1.2077e-02],\n                        [-9.6809e-04, -1.2336e-02,  9.3714e-04],\n                        [ 3.9654e-03, -7.3955e-03, -1.2232e-02]],\n              \n                       [[ 5.6303e-03, -8.0869e-03, -2.5287e-03],\n                        [ 1.8057e-03, -1.1487e-02, -2.8659e-03],\n                        [ 4.0015e-03, -1.2479e-02, -1.1998e-02]]],\n              \n              \n                      [[[ 9.4689e-03, -7.2081e-03,  1.4072e-03],\n                        [ 1.2932e-02, -3.2592e-03, -8.7485e-03],\n                        [ 9.2945e-03,  4.6018e-03,  4.0055e-03]],\n              \n                       [[-1.3764e-02, -4.2907e-03,  3.2547e-03],\n                        [ 3.3341e-03,  1.1304e-03, -1.2234e-02],\n                        [-1.3467e-02, -5.6734e-03,  7.4354e-03]],\n              \n                       [[-5.6023e-03, -2.8761e-03, -1.4718e-02],\n                        [ 1.0713e-02, -1.6779e-03, -1.1996e-02],\n                        [-1.2827e-02,  1.0703e-02, -9.7047e-03]],\n              \n                       ...,\n              \n                       [[ 3.2607e-03, -8.0475e-03,  6.1829e-03],\n                        [-2.9395e-03,  3.3496e-03,  5.1071e-03],\n                        [ 5.9723e-03,  4.7608e-03, -1.6388e-03]],\n              \n                       [[-4.3904e-03,  7.7792e-03, -1.2428e-02],\n                        [-3.2456e-03,  5.5866e-03, -1.4352e-02],\n                        [-1.1821e-02,  2.6534e-03,  7.5290e-03]],\n              \n                       [[ 4.6186e-03, -6.2310e-03,  1.1741e-02],\n                        [-1.4587e-02,  9.7592e-03,  1.2688e-02],\n                        [ 4.2982e-03,  5.2313e-03, -1.2822e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1165e-02,  7.8691e-04, -9.3187e-03],\n                        [-7.7603e-03, -3.0258e-03, -9.7707e-03],\n                        [ 7.5438e-03,  1.4036e-02,  1.0273e-02]],\n              \n                       [[-1.3591e-02,  7.4804e-03, -4.6866e-04],\n                        [-1.3815e-02,  1.2045e-02, -9.8406e-03],\n                        [ 1.0759e-02,  6.9177e-03, -1.3892e-02]],\n              \n                       [[ 1.2857e-02, -4.8749e-04,  9.5570e-03],\n                        [ 2.7064e-03, -8.0672e-03,  1.0471e-02],\n                        [ 5.2177e-03,  1.2281e-02, -6.2795e-03]],\n              \n                       ...,\n              \n                       [[ 1.0430e-03,  1.3958e-02, -1.1441e-02],\n                        [-1.0572e-02,  4.8599e-04, -8.1871e-03],\n                        [ 8.7779e-03,  8.1478e-03, -3.1877e-03]],\n              \n                       [[ 7.4461e-03,  2.9228e-03, -1.0984e-02],\n                        [ 9.8613e-03,  1.3081e-02,  1.2413e-02],\n                        [ 1.2035e-02, -3.1168e-03, -7.5135e-03]],\n              \n                       [[ 8.0283e-03, -4.2646e-03, -7.9841e-03],\n                        [-1.9161e-05, -6.6800e-03, -1.6066e-04],\n                        [ 9.5017e-03, -1.7248e-03,  7.0304e-03]]],\n              \n              \n                      [[[ 3.5356e-03, -7.6512e-03, -8.9665e-03],\n                        [-4.8910e-03,  2.0278e-03,  7.1160e-03],\n                        [-3.0881e-03, -4.1455e-03,  1.1920e-02]],\n              \n                       [[ 3.7466e-03, -3.9381e-03,  1.4420e-02],\n                        [-1.3107e-02, -5.7352e-03,  6.8331e-03],\n                        [-6.0296e-03,  1.2593e-02,  8.2828e-03]],\n              \n                       [[-9.1421e-03,  1.2051e-02,  9.1719e-03],\n                        [-2.3811e-03, -1.4370e-02, -1.1317e-02],\n                        [-5.8528e-03,  5.9658e-03, -7.2074e-03]],\n              \n                       ...,\n              \n                       [[ 1.4338e-02,  1.0304e-02, -6.8373e-03],\n                        [ 2.6406e-03, -2.9580e-03, -2.9774e-03],\n                        [-6.9043e-03,  1.4699e-02, -7.5011e-03]],\n              \n                       [[ 9.0359e-03, -7.4744e-03,  2.7057e-03],\n                        [-1.0241e-03, -9.2485e-03, -3.4580e-03],\n                        [ 3.8833e-03,  7.4134e-03, -1.1881e-02]],\n              \n                       [[-1.9624e-03,  2.7043e-03, -4.4755e-04],\n                        [-1.1581e-02, -1.3765e-02, -8.7221e-03],\n                        [ 1.3774e-02, -1.1876e-02, -1.0575e-02]]],\n              \n              \n                      [[[-1.7063e-04,  6.7622e-04,  8.8984e-03],\n                        [-5.9551e-03,  1.2280e-02, -1.2928e-02],\n                        [-1.2386e-02,  1.3566e-02,  3.3778e-03]],\n              \n                       [[-4.9461e-03, -1.1765e-03, -5.0370e-03],\n                        [-3.2352e-03,  8.2034e-03,  1.2355e-02],\n                        [ 3.5783e-03,  1.1220e-02, -1.3388e-02]],\n              \n                       [[-1.8399e-03,  5.9302e-03,  9.6810e-03],\n                        [ 5.0733e-03,  1.0453e-02, -4.8722e-03],\n                        [-1.3514e-02, -1.1929e-03,  1.7507e-03]],\n              \n                       ...,\n              \n                       [[-1.4605e-03,  2.2461e-03, -8.0156e-03],\n                        [ 1.0985e-02,  5.1273e-03, -1.1668e-02],\n                        [ 1.4627e-02,  2.7758e-03,  7.2483e-03]],\n              \n                       [[ 1.3621e-02, -4.5283e-03,  6.4443e-04],\n                        [ 1.0748e-02,  1.1094e-02,  1.4675e-02],\n                        [-9.0625e-03, -6.1689e-03, -2.2046e-03]],\n              \n                       [[-1.4035e-03, -1.3366e-02,  5.8688e-03],\n                        [ 2.4954e-04,  7.3011e-03,  8.3442e-03],\n                        [-2.7433e-04, -1.0389e-02,  3.1839e-03]]]], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up2.conv.double_conv.3.weight',\n              tensor([[[[ 7.9497e-03, -1.7790e-02, -1.7096e-02],\n                        [-1.6327e-02,  4.0280e-03, -1.9224e-02],\n                        [-4.1614e-03,  2.0345e-02, -1.3011e-02]],\n              \n                       [[-1.1634e-02,  5.5307e-03, -1.6266e-02],\n                        [-1.1103e-02,  8.3270e-03, -1.5757e-02],\n                        [ 1.5221e-02, -1.2837e-02,  9.6909e-04]],\n              \n                       [[-1.6213e-02,  6.1893e-03,  1.9967e-02],\n                        [-1.0630e-02,  2.0123e-02,  6.5128e-03],\n                        [-2.0276e-02,  2.0401e-02,  1.5855e-02]],\n              \n                       ...,\n              \n                       [[ 1.4602e-02, -9.3187e-03,  1.2791e-02],\n                        [ 3.5288e-03,  8.2964e-03,  1.7589e-02],\n                        [ 4.4983e-03, -4.8159e-04, -3.6260e-03]],\n              \n                       [[-8.9474e-05,  1.3904e-02,  1.9019e-02],\n                        [-1.9988e-02, -1.3111e-02,  6.4248e-04],\n                        [ 6.8580e-04,  1.7128e-03,  5.4387e-03]],\n              \n                       [[ 1.4890e-02, -9.2215e-03, -5.8313e-03],\n                        [ 1.1482e-02, -1.2943e-02,  1.7208e-02],\n                        [-2.3544e-03,  8.3377e-04, -1.4550e-02]]],\n              \n              \n                      [[[-2.5915e-03, -3.9138e-03, -1.6308e-02],\n                        [-1.9927e-02, -9.3398e-03, -1.9362e-02],\n                        [-1.4066e-02,  9.7209e-03,  1.6551e-02]],\n              \n                       [[-1.9409e-02, -1.3963e-02,  6.9585e-03],\n                        [-5.1612e-04, -1.9914e-02,  1.8270e-02],\n                        [-7.2831e-03,  1.2477e-02, -2.8120e-04]],\n              \n                       [[-1.5371e-02,  9.3540e-04,  9.9296e-03],\n                        [-1.0750e-02, -3.9004e-03,  1.7460e-02],\n                        [-1.9144e-02,  2.0190e-02, -1.1884e-02]],\n              \n                       ...,\n              \n                       [[ 7.7697e-03,  1.9071e-02, -3.6815e-03],\n                        [ 5.6426e-03, -8.5833e-03,  1.6836e-02],\n                        [ 1.8768e-03, -2.5059e-04,  8.1764e-03]],\n              \n                       [[ 5.9330e-03, -1.4364e-02, -3.9514e-03],\n                        [ 1.9684e-02, -1.4239e-02, -2.0091e-02],\n                        [ 2.0407e-02,  1.8737e-02, -5.8489e-03]],\n              \n                       [[ 5.4501e-03,  1.1028e-02, -1.9625e-02],\n                        [-1.3838e-02, -8.5165e-03,  2.6146e-03],\n                        [-6.4134e-03,  1.4367e-02,  1.4903e-02]]],\n              \n              \n                      [[[-1.1303e-03,  3.3091e-03, -6.1916e-03],\n                        [-1.5099e-02, -2.1207e-04,  4.5621e-03],\n                        [ 1.7857e-02, -2.7128e-03, -5.4803e-03]],\n              \n                       [[ 5.9743e-03,  2.0597e-02,  6.6697e-03],\n                        [ 9.8200e-03,  1.3099e-02,  1.7841e-03],\n                        [-1.6089e-02,  1.5824e-02,  8.0234e-04]],\n              \n                       [[-7.2984e-03,  1.2674e-02,  1.8605e-02],\n                        [ 3.9323e-03,  8.1922e-03, -9.3463e-04],\n                        [-1.9702e-02,  1.4019e-02,  1.6300e-02]],\n              \n                       ...,\n              \n                       [[ 1.6479e-02,  1.6218e-02, -1.5242e-02],\n                        [-3.6273e-03,  5.0512e-03,  1.1426e-02],\n                        [ 7.1217e-03,  7.2147e-03, -2.5175e-03]],\n              \n                       [[ 1.5327e-02,  1.4072e-02, -1.7085e-02],\n                        [ 4.0818e-04, -1.7114e-02, -3.8038e-03],\n                        [-1.5342e-02, -2.0213e-02, -1.3697e-02]],\n              \n                       [[-2.0410e-02, -1.5656e-02,  5.8427e-03],\n                        [-3.8405e-03,  1.0923e-02, -1.2858e-02],\n                        [ 1.8628e-02,  4.0466e-03, -2.0422e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.9150e-02,  1.2267e-02,  1.7782e-02],\n                        [ 1.3684e-02, -1.9804e-02, -9.2421e-03],\n                        [ 1.7435e-02,  1.7343e-02, -1.8515e-02]],\n              \n                       [[ 1.8531e-02, -6.2842e-03, -2.1436e-03],\n                        [-6.2577e-03,  1.8332e-02,  1.9857e-02],\n                        [-1.0869e-02, -5.4065e-03,  1.8648e-02]],\n              \n                       [[-9.8150e-03, -1.9312e-02, -5.3483e-04],\n                        [ 2.2209e-03,  2.0530e-02, -6.2797e-03],\n                        [ 3.1732e-03,  1.7359e-02,  1.0300e-02]],\n              \n                       ...,\n              \n                       [[ 5.3619e-03, -8.6172e-03,  1.9207e-02],\n                        [ 1.2767e-02, -3.0699e-03, -9.6391e-03],\n                        [-8.9599e-04,  6.0747e-03,  4.0384e-03]],\n              \n                       [[-5.2875e-03,  6.5115e-04,  5.4017e-03],\n                        [ 1.5804e-03,  8.6046e-03,  1.7447e-02],\n                        [ 7.5348e-03,  1.8965e-02,  1.9957e-02]],\n              \n                       [[-1.0331e-02, -1.1320e-02,  1.5131e-02],\n                        [ 2.9035e-03,  1.1799e-02, -1.5353e-03],\n                        [-8.3366e-03,  9.3031e-03, -1.7604e-02]]],\n              \n              \n                      [[[ 1.4307e-02,  1.1860e-02,  5.1069e-03],\n                        [-1.5284e-02,  8.2293e-03, -9.5887e-03],\n                        [ 5.3585e-03,  2.0224e-03,  1.5437e-02]],\n              \n                       [[ 1.2629e-03,  9.5884e-03,  1.5362e-02],\n                        [-4.8209e-03,  1.4933e-02, -1.2048e-02],\n                        [-3.0520e-05, -1.3378e-02, -2.1463e-03]],\n              \n                       [[-1.1527e-02,  7.7163e-03, -1.2359e-02],\n                        [-2.0476e-02, -1.7779e-02, -6.4546e-03],\n                        [ 3.1536e-03, -1.0851e-04, -1.9629e-02]],\n              \n                       ...,\n              \n                       [[-3.6267e-03, -1.7496e-02, -1.8531e-02],\n                        [ 3.0812e-03, -4.4989e-03, -5.3328e-03],\n                        [-3.5008e-03, -1.0352e-02,  2.0659e-02]],\n              \n                       [[-4.5241e-03,  6.3328e-03,  8.7361e-03],\n                        [-6.1625e-03, -1.3019e-02,  1.6934e-02],\n                        [-3.4158e-03,  8.9188e-03, -1.3646e-02]],\n              \n                       [[ 1.7996e-02,  1.7854e-02, -1.5007e-02],\n                        [ 2.2617e-04,  1.8391e-02,  2.0008e-02],\n                        [-1.4899e-03,  1.6801e-02,  2.3108e-03]]],\n              \n              \n                      [[[-1.5664e-02,  4.3163e-03,  1.2885e-02],\n                        [ 2.6682e-03,  1.6914e-02,  3.5899e-03],\n                        [ 1.9674e-02, -1.1662e-02, -1.2853e-02]],\n              \n                       [[-3.9540e-04, -1.7787e-02,  9.8214e-03],\n                        [ 1.3250e-02, -2.1693e-03, -4.9136e-03],\n                        [ 1.9610e-02,  1.1362e-03,  2.0132e-02]],\n              \n                       [[ 1.0343e-03,  8.4445e-03,  1.5850e-02],\n                        [ 1.1820e-02,  1.0775e-03, -1.8296e-02],\n                        [-1.1273e-02,  2.6236e-03,  1.3343e-02]],\n              \n                       ...,\n              \n                       [[ 1.6003e-02,  5.4038e-03, -3.7506e-03],\n                        [-2.4944e-03, -8.0193e-03, -6.6061e-03],\n                        [-1.2857e-02,  1.3497e-02,  8.1090e-03]],\n              \n                       [[-1.8006e-02, -8.5612e-03,  1.9954e-02],\n                        [-3.3323e-03, -7.7578e-04,  1.2751e-02],\n                        [ 8.0447e-03, -3.9115e-04,  2.0177e-02]],\n              \n                       [[-1.7435e-02, -8.4071e-03, -9.7204e-03],\n                        [ 1.8257e-02, -1.7279e-02, -1.8781e-02],\n                        [ 1.5807e-02, -1.8718e-02,  2.0478e-02]]]], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up2.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up3.conv.double_conv.0.weight',\n              tensor([[[[ 6.5360e-04, -1.1478e-02, -1.2108e-02],\n                        [-1.3628e-02, -9.4881e-03,  4.5922e-03],\n                        [-1.3436e-03, -9.4868e-03, -4.5939e-03]],\n              \n                       [[ 1.0784e-02, -1.2223e-03, -1.5292e-02],\n                        [-5.8855e-03, -1.8780e-02, -8.7660e-03],\n                        [ 1.8609e-03,  1.2953e-02, -1.4010e-02]],\n              \n                       [[-6.7148e-03, -1.5341e-02,  1.2591e-02],\n                        [ 7.5377e-03,  1.1052e-02, -1.1975e-02],\n                        [-1.9517e-02, -1.9137e-02, -7.4886e-04]],\n              \n                       ...,\n              \n                       [[ 2.0512e-02, -3.9202e-03,  1.4523e-02],\n                        [ 1.2714e-02,  1.3007e-02,  6.8676e-04],\n                        [-1.7327e-02, -8.6569e-03,  1.2416e-03]],\n              \n                       [[-2.0188e-02, -1.2779e-02, -7.3068e-03],\n                        [-9.3873e-03,  1.3301e-02,  1.6646e-02],\n                        [-1.7413e-02,  1.7294e-03, -1.5510e-02]],\n              \n                       [[-1.4983e-02,  1.7590e-02,  1.2623e-02],\n                        [-2.8354e-03, -2.8116e-03,  1.7879e-02],\n                        [-1.7114e-02,  1.2573e-02,  1.0661e-02]]],\n              \n              \n                      [[[ 1.1610e-02, -1.0957e-02,  1.8087e-02],\n                        [ 1.2981e-02, -1.2237e-02, -1.3717e-02],\n                        [-8.9545e-03,  1.0519e-02, -1.8804e-02]],\n              \n                       [[-5.7298e-03,  1.7915e-02, -3.1621e-03],\n                        [ 7.9957e-03,  3.4881e-03, -1.5158e-02],\n                        [ 1.8798e-03,  1.6252e-02, -1.5315e-03]],\n              \n                       [[-4.2252e-03,  8.9630e-03, -7.0830e-03],\n                        [-1.0045e-02, -2.2602e-03,  7.8443e-03],\n                        [-2.6957e-03,  1.3411e-02,  4.8645e-03]],\n              \n                       ...,\n              \n                       [[-5.3712e-03, -1.0452e-02, -1.6330e-02],\n                        [-1.0432e-02, -1.9882e-02, -1.6169e-02],\n                        [-7.2622e-03, -1.8196e-02, -6.7982e-03]],\n              \n                       [[-7.0105e-05, -1.2175e-02, -1.0749e-02],\n                        [ 1.1441e-02,  3.5827e-03,  1.7456e-02],\n                        [-4.9655e-03,  1.9057e-03, -1.7193e-02]],\n              \n                       [[ 1.7013e-02,  3.1988e-04,  5.7411e-03],\n                        [-3.7235e-04, -1.8450e-03,  3.6671e-03],\n                        [ 1.6459e-02,  1.1565e-02,  1.9842e-02]]],\n              \n              \n                      [[[ 1.6914e-02, -1.2111e-02,  1.4786e-02],\n                        [ 7.7207e-03,  2.5537e-03,  4.0743e-03],\n                        [ 1.0419e-04,  1.0066e-02, -8.1808e-03]],\n              \n                       [[ 5.5924e-03,  3.0751e-03, -1.4255e-02],\n                        [ 1.4609e-02, -6.0797e-03,  1.8090e-02],\n                        [-2.0465e-02, -1.9647e-02,  1.9963e-02]],\n              \n                       [[ 1.7703e-02,  9.7912e-04, -1.7088e-02],\n                        [-3.0930e-03,  1.0013e-02,  1.5110e-02],\n                        [-1.5153e-02, -6.5340e-03,  1.6374e-02]],\n              \n                       ...,\n              \n                       [[-1.0198e-02,  1.8628e-02, -7.3407e-03],\n                        [-2.0066e-02,  1.8155e-02,  8.2106e-03],\n                        [-5.0477e-04, -5.1193e-03, -1.9685e-02]],\n              \n                       [[ 7.3187e-03, -1.8577e-02, -1.9180e-02],\n                        [ 1.3858e-02, -1.6733e-02, -5.7723e-04],\n                        [ 1.2103e-02,  8.6336e-03, -2.0067e-02]],\n              \n                       [[-3.8180e-03,  1.9922e-03, -1.2753e-02],\n                        [ 1.9889e-02,  1.9218e-02,  1.2516e-02],\n                        [-1.6966e-02, -1.9937e-02,  6.3545e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.4647e-02,  1.3599e-02, -1.1497e-02],\n                        [ 1.0819e-02,  6.2655e-03,  8.2514e-03],\n                        [ 9.7814e-03,  1.5446e-03,  5.0288e-03]],\n              \n                       [[-3.7955e-03,  1.2494e-02, -7.8703e-03],\n                        [ 4.0349e-03,  1.4197e-02, -1.1018e-02],\n                        [ 1.2082e-02, -1.9828e-03,  1.1344e-02]],\n              \n                       [[-1.6060e-02,  5.2254e-03,  1.3679e-02],\n                        [ 2.3551e-03, -5.8034e-03, -1.0188e-02],\n                        [-7.8099e-03, -7.3378e-03, -1.6845e-02]],\n              \n                       ...,\n              \n                       [[ 4.8750e-03, -1.5202e-02, -8.3033e-03],\n                        [-1.4143e-02,  9.6245e-03,  1.0595e-03],\n                        [-6.6992e-03,  1.8018e-02,  1.4028e-02]],\n              \n                       [[-2.4361e-03,  8.2809e-03, -6.7384e-03],\n                        [-2.4594e-03,  4.9077e-03,  1.8375e-02],\n                        [-4.1593e-03, -3.5705e-03, -1.3529e-02]],\n              \n                       [[-1.7012e-02,  1.9748e-02,  1.9104e-02],\n                        [-1.4910e-02, -1.9546e-02,  1.1406e-02],\n                        [-1.7544e-04,  1.5866e-02,  3.8805e-03]]],\n              \n              \n                      [[[-4.2661e-03,  2.0544e-02, -2.0223e-02],\n                        [-1.7558e-02,  1.2315e-02, -1.1358e-03],\n                        [-9.5695e-03,  1.7591e-02, -1.8437e-02]],\n              \n                       [[-7.6622e-03,  1.3523e-02, -1.2805e-02],\n                        [ 4.2950e-03, -7.9838e-03, -8.6255e-03],\n                        [ 1.5282e-03, -8.8083e-03,  5.8126e-03]],\n              \n                       [[ 1.2428e-02,  1.6649e-03, -1.8423e-02],\n                        [ 3.3804e-03, -9.0342e-03, -2.8731e-03],\n                        [ 2.8868e-03, -4.1382e-03,  1.6776e-02]],\n              \n                       ...,\n              \n                       [[ 1.6678e-02, -4.2476e-03, -9.8835e-03],\n                        [-9.7655e-03, -3.7623e-03,  5.0571e-03],\n                        [ 1.0131e-02, -7.6768e-03, -5.4080e-04]],\n              \n                       [[ 1.7999e-02,  5.0342e-03, -2.2092e-03],\n                        [ 1.2079e-02, -8.4492e-03, -1.6282e-02],\n                        [-2.0245e-02,  4.7685e-03, -9.7620e-03]],\n              \n                       [[-4.6216e-03, -1.1652e-02, -1.2818e-02],\n                        [ 1.2088e-02, -9.3832e-03, -4.1677e-03],\n                        [ 1.1476e-02, -4.4116e-03, -2.0018e-02]]],\n              \n              \n                      [[[ 3.7413e-03, -1.8938e-02, -1.2220e-02],\n                        [ 1.7449e-02,  9.5147e-03,  2.5178e-03],\n                        [-6.6552e-03,  2.6520e-03, -2.0583e-02]],\n              \n                       [[ 1.9046e-02,  1.7330e-03,  3.4585e-03],\n                        [ 1.6316e-02, -1.8740e-02,  1.6343e-02],\n                        [-8.1862e-03, -1.9654e-02,  6.7754e-04]],\n              \n                       [[-7.8348e-03, -1.0483e-02, -1.1580e-02],\n                        [ 2.0537e-02, -1.2595e-02,  4.6942e-03],\n                        [ 5.1139e-04, -8.2631e-04, -1.3213e-03]],\n              \n                       ...,\n              \n                       [[ 2.0120e-02, -1.8718e-02,  7.1457e-03],\n                        [ 8.7498e-03, -8.0881e-03, -8.0977e-03],\n                        [-1.8490e-02, -2.0089e-02,  2.6450e-04]],\n              \n                       [[ 3.0537e-03, -8.0446e-03, -9.7033e-03],\n                        [ 2.9420e-03,  1.5974e-02, -8.4568e-03],\n                        [-4.6306e-03,  7.5076e-03, -9.9498e-04]],\n              \n                       [[-1.7441e-02, -4.8928e-03,  2.0088e-02],\n                        [ 1.1744e-02, -1.9409e-02, -1.2495e-02],\n                        [ 1.6826e-02, -6.6388e-03, -1.3236e-03]]]], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up3.conv.double_conv.3.weight',\n              tensor([[[[-6.2617e-03,  5.1519e-03,  1.0535e-02],\n                        [ 2.2614e-02,  2.3770e-02,  7.1172e-03],\n                        [-9.0252e-04, -2.0448e-02, -2.0432e-02]],\n              \n                       [[-5.3073e-03,  2.0543e-03, -1.9999e-02],\n                        [ 1.7058e-02,  4.4323e-03,  2.0256e-02],\n                        [ 1.6059e-02,  7.8848e-03,  2.6898e-02]],\n              \n                       [[ 2.4905e-02, -9.5489e-04, -4.0310e-05],\n                        [ 2.6839e-02,  1.0395e-02, -1.1824e-02],\n                        [ 1.3696e-02, -4.7753e-03,  4.4547e-03]],\n              \n                       ...,\n              \n                       [[-4.0551e-03, -2.0774e-02,  5.0831e-03],\n                        [ 8.9578e-03, -2.4251e-02, -2.7485e-02],\n                        [-1.1212e-02, -3.5667e-03, -2.9207e-02]],\n              \n                       [[-2.5817e-02,  2.8529e-02, -2.4398e-02],\n                        [ 2.0831e-02,  1.4292e-02, -1.8673e-02],\n                        [-8.5094e-04, -1.2406e-03,  3.7525e-04]],\n              \n                       [[ 2.1931e-03,  6.2044e-03, -9.8672e-03],\n                        [-6.0165e-03,  7.0416e-03, -3.2293e-03],\n                        [-1.1025e-02, -1.1666e-02, -1.8839e-02]]],\n              \n              \n                      [[[-1.9571e-02,  1.3345e-02, -3.1977e-03],\n                        [-2.4555e-02, -3.5323e-03, -2.8703e-02],\n                        [-1.5313e-02,  2.1116e-02, -1.0758e-03]],\n              \n                       [[-1.0014e-02,  1.1471e-02, -2.2742e-02],\n                        [ 2.5164e-02,  1.5579e-02, -2.2211e-02],\n                        [ 2.7174e-02,  1.9207e-02, -1.7626e-02]],\n              \n                       [[ 2.7689e-02, -5.7403e-03, -1.0863e-02],\n                        [ 5.0870e-03,  6.7373e-03, -2.0150e-02],\n                        [ 2.9319e-02, -9.6329e-03, -2.0385e-02]],\n              \n                       ...,\n              \n                       [[-2.4959e-02,  1.2766e-03,  2.4264e-03],\n                        [ 2.1160e-02, -2.1553e-02,  1.6825e-02],\n                        [ 2.6579e-02,  6.6060e-03,  2.5650e-02]],\n              \n                       [[ 4.5595e-03,  1.9319e-03, -2.5173e-02],\n                        [-2.3925e-02, -8.3372e-03, -9.0146e-03],\n                        [ 1.7461e-02, -2.5896e-02, -1.8144e-02]],\n              \n                       [[ 2.5831e-02, -2.1761e-02, -2.9396e-02],\n                        [ 2.7635e-02, -1.2928e-02,  5.8588e-03],\n                        [-2.0192e-02,  4.7528e-03,  2.8390e-02]]],\n              \n              \n                      [[[ 1.8739e-03, -1.3140e-02,  2.6128e-02],\n                        [ 1.1566e-02,  3.5446e-03, -5.1995e-03],\n                        [ 5.5016e-03, -4.5294e-03,  1.9544e-02]],\n              \n                       [[-9.9646e-03,  2.7664e-02,  1.1371e-02],\n                        [ 1.2055e-02,  1.6825e-02, -1.1272e-02],\n                        [ 1.3120e-02,  1.7465e-02,  1.1575e-02]],\n              \n                       [[-4.8596e-03,  9.3461e-03,  2.0105e-02],\n                        [ 1.2126e-02, -2.2240e-03,  1.3572e-02],\n                        [-2.8769e-02, -7.9955e-03, -1.2733e-02]],\n              \n                       ...,\n              \n                       [[ 2.5646e-02,  1.6559e-02, -2.2198e-02],\n                        [-3.0433e-03,  2.7646e-02,  2.8915e-02],\n                        [ 2.3706e-02, -2.5853e-02, -8.8919e-05]],\n              \n                       [[ 1.9385e-02,  9.4940e-03, -1.7507e-02],\n                        [-1.0995e-02, -1.9027e-02,  2.6517e-02],\n                        [ 6.5096e-03,  8.3432e-03,  4.3078e-03]],\n              \n                       [[-1.2435e-02, -1.2040e-02,  6.4921e-03],\n                        [-1.9559e-02,  2.2276e-02,  1.2324e-02],\n                        [ 7.4537e-03,  5.5965e-03, -2.4149e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.9395e-02,  2.0365e-02, -1.6215e-02],\n                        [ 1.8015e-02,  1.1132e-02, -5.3747e-03],\n                        [ 4.5775e-03,  1.9513e-02,  5.4436e-03]],\n              \n                       [[ 2.0589e-02,  4.0204e-03, -7.1212e-03],\n                        [-1.7708e-02, -2.7610e-02,  2.9521e-03],\n                        [ 1.4294e-02, -6.5115e-03, -1.4379e-03]],\n              \n                       [[ 2.8011e-02,  1.6216e-02,  2.5210e-02],\n                        [-1.6498e-02,  1.0523e-02,  2.6155e-02],\n                        [ 1.6074e-02, -8.3713e-03,  2.2026e-02]],\n              \n                       ...,\n              \n                       [[-1.3617e-02, -1.4065e-02, -2.3103e-02],\n                        [ 2.4879e-02, -8.9402e-03,  3.0990e-03],\n                        [ 1.3965e-03, -2.5021e-02, -2.0546e-02]],\n              \n                       [[ 2.0246e-03, -7.9078e-03, -2.6747e-02],\n                        [ 2.9376e-02, -6.2544e-03, -1.8549e-02],\n                        [ 1.5150e-02, -3.9595e-03,  2.3443e-03]],\n              \n                       [[-3.6495e-03, -1.0052e-02,  1.2397e-03],\n                        [ 3.8338e-03, -2.8786e-02, -5.1455e-03],\n                        [-1.5915e-02,  2.8991e-02,  6.3032e-03]]],\n              \n              \n                      [[[-2.0503e-02, -2.8574e-02,  1.7111e-02],\n                        [-1.5106e-02,  2.2639e-02,  3.2666e-03],\n                        [ 1.1444e-02, -9.7533e-03,  1.8418e-02]],\n              \n                       [[-2.8729e-02, -1.7639e-02,  1.5558e-02],\n                        [ 2.1907e-02,  2.6665e-02, -2.0398e-02],\n                        [ 4.7236e-03,  2.2406e-02, -1.1982e-03]],\n              \n                       [[-6.9613e-03,  1.6444e-02,  1.0986e-04],\n                        [-2.5102e-02,  2.7951e-02,  1.8224e-02],\n                        [-9.3261e-03, -2.2952e-02, -1.9339e-02]],\n              \n                       ...,\n              \n                       [[ 6.3333e-03, -8.1322e-03,  3.5560e-03],\n                        [-2.3900e-02, -2.8754e-02, -2.0715e-02],\n                        [ 1.3923e-02,  1.0834e-02, -1.1983e-02]],\n              \n                       [[-1.2872e-02,  6.1885e-03, -1.2684e-02],\n                        [ 8.5061e-03, -1.3273e-03, -1.6401e-03],\n                        [ 3.5566e-03,  1.4142e-02,  7.0110e-03]],\n              \n                       [[ 1.2880e-02,  6.1687e-03, -9.6315e-03],\n                        [ 1.5918e-02,  2.2629e-03, -2.7104e-03],\n                        [-8.4794e-04,  2.0819e-02, -2.2515e-02]]],\n              \n              \n                      [[[ 8.6197e-03,  2.3163e-02,  1.9551e-02],\n                        [ 2.2528e-02,  1.8106e-02,  1.0401e-02],\n                        [-1.7955e-03, -5.1270e-03,  9.9206e-03]],\n              \n                       [[ 2.3529e-02,  1.5074e-02, -1.5779e-02],\n                        [-2.8125e-02, -1.9706e-02, -2.7739e-02],\n                        [ 1.2969e-02, -6.8372e-03, -1.8700e-02]],\n              \n                       [[-1.6456e-02, -1.9319e-02,  2.9451e-02],\n                        [-4.3081e-03,  1.6394e-02,  2.0039e-02],\n                        [-2.6109e-02,  1.8154e-02, -4.1342e-03]],\n              \n                       ...,\n              \n                       [[ 1.4506e-02, -2.9666e-03,  3.6261e-03],\n                        [ 1.6303e-02, -4.9343e-03, -1.7006e-02],\n                        [ 2.6239e-02, -2.3413e-02,  1.2565e-02]],\n              \n                       [[-7.7776e-03,  2.6909e-02,  1.0444e-02],\n                        [-8.7274e-03, -8.3104e-03,  2.3266e-03],\n                        [-2.4073e-02, -1.0433e-02, -1.1619e-02]],\n              \n                       [[-1.0362e-02, -2.3291e-02, -1.0579e-02],\n                        [ 1.6419e-02,  2.0854e-02,  2.4889e-02],\n                        [ 1.3606e-03, -9.4291e-03, -1.6355e-03]]]], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up3.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up4.conv.double_conv.0.weight',\n              tensor([[[[-2.4477e-02, -1.7234e-02,  2.2003e-03],\n                        [-7.8829e-03,  6.1736e-03,  1.4644e-02],\n                        [ 9.7539e-03,  5.7497e-04, -2.1407e-02]],\n              \n                       [[ 2.5615e-02,  6.0152e-03, -2.8486e-02],\n                        [ 2.1189e-02,  6.7674e-03, -1.4792e-03],\n                        [ 2.2734e-02,  1.7544e-03, -1.0535e-02]],\n              \n                       [[ 2.1016e-02,  3.9310e-03,  5.9241e-03],\n                        [-9.3318e-04,  1.3821e-02,  2.8222e-02],\n                        [ 7.3732e-03,  2.3611e-03,  2.2986e-02]],\n              \n                       ...,\n              \n                       [[-2.6076e-02,  9.7759e-03,  1.7446e-02],\n                        [-4.6081e-03, -7.8919e-03, -1.3171e-02],\n                        [ 3.6483e-03,  5.5107e-04, -2.6154e-02]],\n              \n                       [[ 2.4815e-02,  6.5554e-04, -2.6840e-02],\n                        [-5.4893e-03, -1.2978e-02, -7.7000e-03],\n                        [ 1.7822e-02, -2.0376e-02,  1.8151e-02]],\n              \n                       [[-1.3709e-02, -2.1298e-02,  1.4319e-02],\n                        [-1.1540e-02,  2.9451e-03,  4.6603e-03],\n                        [ 1.6498e-02, -2.2247e-02, -2.6400e-02]]],\n              \n              \n                      [[[-2.9053e-02,  6.6088e-03,  2.8600e-02],\n                        [-8.5117e-03,  3.7488e-03,  2.5909e-02],\n                        [-6.6344e-03, -1.8867e-02,  2.1232e-02]],\n              \n                       [[ 2.7659e-02, -1.5675e-02, -1.2514e-02],\n                        [ 6.8806e-03, -2.4540e-02, -2.0591e-02],\n                        [-6.2750e-03, -2.9055e-02,  2.7674e-02]],\n              \n                       [[ 6.6344e-03, -2.5097e-02, -2.7987e-02],\n                        [-1.9412e-02, -1.7099e-02,  2.4543e-02],\n                        [-6.0892e-03, -1.9663e-02, -2.1830e-02]],\n              \n                       ...,\n              \n                       [[-2.4330e-02, -5.3355e-04,  1.6593e-02],\n                        [-1.5296e-02, -1.2302e-02, -2.1773e-02],\n                        [-2.4805e-02, -2.7568e-02, -5.2265e-03]],\n              \n                       [[ 1.4438e-02, -1.1498e-02, -5.8588e-03],\n                        [ 2.3541e-02,  2.8545e-02, -2.1781e-02],\n                        [ 2.1298e-02, -1.4740e-02,  2.0063e-02]],\n              \n                       [[-1.4228e-02,  2.7397e-02,  1.9363e-03],\n                        [ 1.3088e-02,  1.8878e-02,  2.5326e-02],\n                        [-2.7118e-02,  1.8095e-02,  1.5554e-02]]],\n              \n              \n                      [[[-2.7807e-02,  2.8756e-02, -2.4947e-02],\n                        [ 2.8239e-03,  6.4158e-03,  1.7847e-02],\n                        [-2.1316e-02, -1.1236e-02, -7.1000e-03]],\n              \n                       [[-2.2642e-02, -2.9162e-02, -2.7960e-02],\n                        [ 2.2822e-02,  2.6365e-02, -2.2013e-02],\n                        [-4.3668e-03,  5.9663e-03, -2.2929e-02]],\n              \n                       [[ 2.6231e-02,  6.2513e-04, -1.5292e-02],\n                        [-2.3744e-02,  1.0287e-02, -1.7989e-02],\n                        [ 1.4567e-02, -5.4238e-04, -1.8888e-03]],\n              \n                       ...,\n              \n                       [[ 8.2702e-03, -3.9680e-03,  4.4591e-03],\n                        [ 1.2113e-02,  1.9210e-02, -2.1732e-02],\n                        [ 1.8309e-02, -2.5562e-02, -3.4519e-03]],\n              \n                       [[ 2.0920e-02,  5.1383e-03, -2.8351e-02],\n                        [ 2.4168e-02,  2.4032e-03,  4.4554e-03],\n                        [-9.5799e-03, -4.6795e-03,  2.1697e-02]],\n              \n                       [[ 5.9437e-03,  1.4123e-03, -8.3815e-03],\n                        [ 2.3132e-02, -2.6785e-02, -1.6763e-02],\n                        [-9.6515e-03, -2.1222e-02,  2.4000e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.3391e-02,  2.3395e-02, -2.1791e-02],\n                        [ 1.8008e-02,  5.3447e-03,  2.3465e-02],\n                        [ 1.7817e-02, -3.0541e-04,  1.8585e-02]],\n              \n                       [[-1.8773e-02,  9.5143e-03, -9.0805e-03],\n                        [-1.1845e-02, -2.0910e-02,  7.6076e-03],\n                        [-1.9462e-03,  2.5138e-02, -2.8411e-02]],\n              \n                       [[ 1.2022e-02, -1.4268e-02,  1.6846e-02],\n                        [-1.5587e-02, -2.2586e-02,  1.7113e-03],\n                        [-2.0474e-02,  2.1718e-02,  2.6473e-02]],\n              \n                       ...,\n              \n                       [[-9.5288e-04, -2.0567e-02, -5.8081e-03],\n                        [-9.2609e-03,  2.2689e-02,  7.9880e-03],\n                        [-2.3267e-02, -2.2080e-03, -3.7323e-04]],\n              \n                       [[ 7.0031e-03,  1.5936e-02, -1.7355e-02],\n                        [ 9.1528e-03,  6.0140e-04, -4.6582e-03],\n                        [-2.2403e-03,  1.1589e-02,  1.3004e-02]],\n              \n                       [[ 7.5902e-03, -2.7939e-02,  1.6827e-02],\n                        [-1.1944e-02, -2.1053e-02,  7.7404e-03],\n                        [-2.4648e-02,  1.0781e-02,  1.6477e-02]]],\n              \n              \n                      [[[ 2.8526e-02, -8.3310e-03, -3.3514e-03],\n                        [ 8.7738e-03,  3.3132e-03, -2.3501e-03],\n                        [-1.5227e-02, -6.8209e-03,  7.2189e-03]],\n              \n                       [[ 3.2429e-03,  2.9305e-02,  7.2086e-03],\n                        [-2.8544e-02, -2.1567e-02, -7.0302e-03],\n                        [-1.2484e-02,  4.2848e-03, -1.5662e-02]],\n              \n                       [[ 1.4185e-03,  6.2046e-03,  2.1498e-02],\n                        [ 1.4784e-02, -2.4929e-02, -2.7400e-02],\n                        [-2.6303e-05,  2.4616e-02, -1.2550e-02]],\n              \n                       ...,\n              \n                       [[-1.1245e-02, -6.3400e-03, -1.4372e-02],\n                        [-2.6327e-02, -9.7659e-03, -1.9709e-03],\n                        [-2.4333e-03,  5.2920e-03,  1.3149e-02]],\n              \n                       [[ 2.8700e-03,  7.3612e-03,  2.3691e-03],\n                        [-2.7523e-02,  1.5241e-02,  1.3450e-02],\n                        [ 2.5740e-03, -3.4698e-03, -1.3424e-02]],\n              \n                       [[-1.4515e-02, -2.1749e-02,  1.3343e-02],\n                        [ 2.5754e-02,  3.5074e-03,  1.9747e-02],\n                        [ 2.7382e-03,  1.4910e-02, -2.2954e-02]]],\n              \n              \n                      [[[-4.3458e-03, -1.3681e-02,  1.8517e-02],\n                        [-1.4100e-02,  2.4556e-02, -1.6581e-03],\n                        [-2.7384e-02,  1.7085e-02,  1.9694e-02]],\n              \n                       [[ 5.4223e-03, -1.7057e-02, -6.0624e-03],\n                        [ 2.8144e-02, -1.2404e-02, -9.2200e-05],\n                        [ 8.0187e-03, -2.4534e-02, -6.1641e-03]],\n              \n                       [[ 4.4628e-03, -2.3212e-02,  1.8625e-02],\n                        [ 2.0626e-03, -1.1065e-02,  2.2116e-02],\n                        [-2.3691e-02,  7.7271e-03,  2.3667e-02]],\n              \n                       ...,\n              \n                       [[ 1.6437e-02,  1.7844e-02,  4.2858e-03],\n                        [ 1.8507e-02, -1.4175e-02,  6.2452e-03],\n                        [-2.2591e-02, -1.6163e-02,  2.8446e-02]],\n              \n                       [[ 7.0578e-03,  8.5772e-03,  1.2336e-03],\n                        [-2.7270e-02, -4.7153e-03,  1.8364e-02],\n                        [-1.7723e-02, -6.1744e-03, -2.6519e-02]],\n              \n                       [[ 2.6981e-03,  2.3110e-02, -1.9544e-02],\n                        [ 2.8593e-02,  2.6731e-02,  2.1887e-02],\n                        [-9.6571e-04,  1.7459e-02,  3.4465e-03]]]], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.1.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.up4.conv.double_conv.3.weight',\n              tensor([[[[ 3.1426e-03, -3.7804e-02, -1.9636e-03],\n                        [-3.3168e-02,  2.4599e-03, -2.5361e-02],\n                        [ 2.0291e-02, -3.1659e-02, -2.2596e-02]],\n              \n                       [[-8.4917e-03, -3.0465e-04, -2.1817e-02],\n                        [ 2.9646e-03,  2.4069e-02, -2.6871e-02],\n                        [ 2.7976e-02, -2.9426e-02, -1.9063e-02]],\n              \n                       [[ 3.4714e-02,  2.5515e-02,  2.2645e-03],\n                        [ 1.1169e-02, -1.5637e-02, -3.2919e-02],\n                        [-1.3760e-02,  1.0523e-03,  3.2319e-02]],\n              \n                       ...,\n              \n                       [[-2.6632e-02,  1.5643e-02, -3.1304e-03],\n                        [-6.5018e-03,  1.7912e-02, -1.7220e-02],\n                        [ 3.1036e-02,  3.4784e-02, -1.4025e-02]],\n              \n                       [[ 3.3626e-02, -2.4100e-02,  3.6708e-02],\n                        [-2.1758e-02, -1.4161e-02, -2.8572e-02],\n                        [ 5.2657e-03,  2.2184e-02, -1.2249e-02]],\n              \n                       [[ 3.9889e-02, -9.9724e-03,  1.4062e-03],\n                        [ 1.6991e-02, -5.8726e-03, -1.2741e-02],\n                        [-2.3483e-02,  3.6793e-02,  1.0728e-03]]],\n              \n              \n                      [[[-1.1431e-02,  2.8004e-03, -2.1472e-02],\n                        [-4.7250e-03,  3.1195e-02, -3.4145e-02],\n                        [-3.9074e-02, -9.0451e-03,  3.6595e-02]],\n              \n                       [[-3.4954e-02, -2.8686e-02,  7.4445e-03],\n                        [-3.4594e-02, -1.5361e-02,  3.2916e-02],\n                        [ 7.3619e-03, -2.8733e-02, -2.8171e-02]],\n              \n                       [[-1.6132e-02,  9.1593e-03, -1.5983e-03],\n                        [ 1.9147e-02, -3.0231e-02,  3.5481e-02],\n                        [-2.8131e-02, -1.5797e-02,  1.4560e-02]],\n              \n                       ...,\n              \n                       [[-2.0996e-03, -2.3411e-02, -1.1860e-02],\n                        [ 3.8093e-02,  3.5264e-02,  3.0247e-02],\n                        [ 1.3708e-02, -2.7209e-02,  3.5293e-02]],\n              \n                       [[-1.4823e-02, -1.3127e-02, -1.8602e-02],\n                        [ 3.1382e-02, -2.8936e-02, -3.5547e-02],\n                        [ 2.8250e-02,  2.5477e-02, -1.1684e-02]],\n              \n                       [[-3.4762e-03, -2.8827e-02,  2.2720e-02],\n                        [ 1.9048e-02,  1.9151e-02,  4.8282e-03],\n                        [ 3.6979e-02,  1.1263e-02,  1.4983e-02]]],\n              \n              \n                      [[[ 4.0528e-02, -1.5267e-02,  4.1640e-02],\n                        [ 1.4580e-02,  2.1254e-03,  2.1454e-02],\n                        [ 2.3367e-02,  2.4535e-02, -2.9547e-02]],\n              \n                       [[ 1.2478e-02, -3.2175e-02,  3.1261e-02],\n                        [-2.5070e-02,  1.0443e-02, -1.7667e-02],\n                        [-3.9835e-03, -1.4524e-02,  2.9181e-02]],\n              \n                       [[ 8.7496e-03,  1.6791e-02, -3.3366e-02],\n                        [ 3.9007e-02,  1.0403e-02,  3.8254e-02],\n                        [-1.2029e-02,  1.1168e-02, -1.9442e-02]],\n              \n                       ...,\n              \n                       [[ 2.2030e-02,  1.0903e-02, -1.4863e-02],\n                        [-1.3346e-02, -3.5193e-02,  3.2643e-02],\n                        [-3.8632e-02, -8.3370e-03,  1.8904e-02]],\n              \n                       [[-3.9616e-02, -2.5855e-02,  3.3651e-02],\n                        [ 3.9193e-02,  2.7768e-02,  1.4065e-02],\n                        [-8.8412e-03, -2.1744e-02, -2.0466e-02]],\n              \n                       [[-9.5175e-03, -3.2115e-02,  2.8135e-02],\n                        [-3.5135e-02, -3.5658e-02, -1.6859e-02],\n                        [ 3.8371e-02,  4.0490e-03,  2.5179e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6391e-02,  5.2747e-03,  3.4211e-02],\n                        [-3.6951e-02, -2.0392e-02,  1.9124e-02],\n                        [-4.0592e-03, -2.1158e-02, -5.6858e-03]],\n              \n                       [[-1.2450e-02, -7.7264e-03, -2.7716e-02],\n                        [ 3.4721e-02,  2.8399e-02,  3.7686e-02],\n                        [ 3.6166e-02,  1.7743e-02, -3.3313e-02]],\n              \n                       [[-2.4009e-03,  2.7938e-02,  8.2821e-03],\n                        [-1.0567e-02, -1.0721e-02,  3.9096e-02],\n                        [-1.0329e-02,  3.5188e-04,  1.9992e-02]],\n              \n                       ...,\n              \n                       [[ 4.0091e-02,  2.7190e-02, -3.8786e-02],\n                        [ 3.7762e-02,  1.6390e-02, -4.1539e-02],\n                        [ 2.8608e-02, -3.4842e-02, -1.5290e-02]],\n              \n                       [[ 2.5458e-02,  3.8800e-02,  1.8157e-02],\n                        [-3.0404e-02, -2.8858e-02, -3.7904e-02],\n                        [-1.7384e-02,  1.3624e-02, -3.8238e-02]],\n              \n                       [[-3.4968e-02, -2.1631e-02,  1.8572e-02],\n                        [ 3.9958e-02,  3.1534e-02, -2.6919e-03],\n                        [ 2.9025e-02, -2.5323e-02,  1.8108e-02]]],\n              \n              \n                      [[[ 1.4118e-02,  1.3075e-02,  7.9425e-04],\n                        [-1.5709e-02,  2.2579e-02, -3.4406e-03],\n                        [ 3.9156e-02, -5.3889e-03, -4.1343e-02]],\n              \n                       [[-1.1825e-03, -7.4790e-03,  3.0482e-02],\n                        [-4.0314e-02, -1.9415e-02, -5.4573e-05],\n                        [-3.6205e-03, -4.0538e-02,  1.6526e-02]],\n              \n                       [[ 3.1517e-02,  1.2538e-02,  1.7676e-03],\n                        [ 2.2461e-02, -2.9065e-02,  3.1906e-02],\n                        [-3.9866e-02, -2.3473e-02,  4.0793e-02]],\n              \n                       ...,\n              \n                       [[-2.2015e-02, -1.4035e-03, -3.4191e-02],\n                        [ 3.4649e-02,  2.7996e-02,  2.5186e-02],\n                        [-2.6122e-02, -3.7787e-02, -3.5784e-02]],\n              \n                       [[-3.5926e-03, -1.5855e-02, -2.4558e-02],\n                        [-3.5714e-02,  4.0327e-02,  3.9204e-02],\n                        [ 1.6102e-03, -2.2671e-02,  3.9940e-02]],\n              \n                       [[-4.1120e-02,  6.4742e-03,  1.8772e-02],\n                        [ 3.4173e-02,  5.7441e-04, -1.9311e-02],\n                        [-1.4727e-02,  1.7990e-02, -1.8958e-02]]],\n              \n              \n                      [[[ 2.9624e-02, -8.9972e-03,  4.0076e-02],\n                        [ 1.4882e-02, -1.9439e-02,  8.6693e-03],\n                        [-4.0603e-02,  1.5571e-02, -2.9153e-02]],\n              \n                       [[-3.5557e-02,  1.8946e-04,  2.2721e-02],\n                        [ 2.9935e-03,  8.9930e-03, -2.0757e-02],\n                        [ 2.0412e-02,  5.7608e-03,  2.6245e-02]],\n              \n                       [[-6.2162e-03, -7.0439e-04,  1.3922e-02],\n                        [-9.8026e-03,  2.8211e-02, -3.7612e-03],\n                        [-3.1022e-02, -2.4241e-02,  2.0704e-03]],\n              \n                       ...,\n              \n                       [[ 1.8656e-05, -3.5449e-02, -1.9142e-02],\n                        [-3.7448e-02, -3.8316e-02,  3.6445e-02],\n                        [ 1.8268e-02, -3.2087e-02, -3.0568e-02]],\n              \n                       [[-2.6703e-02, -7.0255e-04,  1.3062e-02],\n                        [ 9.2566e-03,  3.0957e-02, -3.9456e-02],\n                        [ 2.6741e-02,  1.7924e-02,  2.6267e-02]],\n              \n                       [[-3.0110e-02, -1.6314e-03, -2.8098e-02],\n                        [ 2.0860e-02,  1.5562e-02,  2.9175e-02],\n                        [ 9.1814e-03,  2.6883e-02,  2.8830e-02]]]], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.module.up4.conv.double_conv.4.num_batches_tracked',\n              tensor(0, device='cuda:0')),\n             ('module.module.outc.conv.weight',\n              tensor([[[[ 0.0984]],\n              \n                       [[-0.0668]],\n              \n                       [[-0.0782]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0089]],\n              \n                       [[-0.0501]],\n              \n                       [[-0.0261]],\n              \n                       [[ 0.0791]],\n              \n                       [[-0.1128]],\n              \n                       [[ 0.0102]],\n              \n                       [[ 0.0258]],\n              \n                       [[-0.0357]],\n              \n                       [[-0.0674]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.0549]],\n              \n                       [[-0.0972]],\n              \n                       [[-0.1207]],\n              \n                       [[ 0.1104]],\n              \n                       [[ 0.0293]],\n              \n                       [[-0.1182]],\n              \n                       [[ 0.1166]],\n              \n                       [[ 0.1038]],\n              \n                       [[-0.0085]],\n              \n                       [[-0.0039]],\n              \n                       [[ 0.0621]],\n              \n                       [[ 0.0331]],\n              \n                       [[ 0.0618]],\n              \n                       [[ 0.0310]],\n              \n                       [[ 0.1245]],\n              \n                       [[-0.1027]],\n              \n                       [[ 0.0523]],\n              \n                       [[ 0.0731]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0495]],\n              \n                       [[ 0.1218]],\n              \n                       [[ 0.1106]],\n              \n                       [[ 0.0079]],\n              \n                       [[-0.1117]],\n              \n                       [[ 0.1123]],\n              \n                       [[-0.0453]],\n              \n                       [[ 0.0750]],\n              \n                       [[ 0.0378]],\n              \n                       [[ 0.1220]],\n              \n                       [[-0.1052]],\n              \n                       [[-0.0909]],\n              \n                       [[-0.0841]],\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0207]],\n              \n                       [[-0.0161]],\n              \n                       [[-0.0815]],\n              \n                       [[ 0.0737]],\n              \n                       [[-0.0565]],\n              \n                       [[-0.0620]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.1087]],\n              \n                       [[ 0.0442]],\n              \n                       [[-0.0377]],\n              \n                       [[-0.0474]],\n              \n                       [[ 0.0807]],\n              \n                       [[ 0.0298]],\n              \n                       [[ 0.0700]],\n              \n                       [[ 0.0749]],\n              \n                       [[ 0.0847]],\n              \n                       [[-0.1145]]]], device='cuda:0')),\n             ('module.module.outc.conv.bias',\n              tensor([-0.0712], device='cuda:0'))])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>\u200b\u63a5\u4e0b\u6765\u200b\u8fdb\u5165\u200b\u7b2c\u516d\u7ae0\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ee5\u200b\u524d\u9762\u200b\u5df2\u7ecf\u200b\u642d\u5efa\u200b\u597d\u200b\u7684\u200bU-Net\u200b\u6a21\u578b\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u63a2\u7d22\u200b\u5982\u4f55\u200b\u66f4\u200b\u4f18\u96c5\u200b\u5730\u200b\u8bad\u7ec3\u200bPyTorch\u200b\u6a21\u578b\u200b\u3002 \u200b\u9996\u5148\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bCarvana\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4e00\u4e2a\u200b\u57fa\u672c\u200b\u7684\u200bU-Net\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b</p> In\u00a0[8]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport PIL\nfrom sklearn.model_selection import train_test_split\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n</pre> from torch.utils.data import Dataset, DataLoader from torchvision import transforms import torch.optim as optim import matplotlib.pyplot as plt import PIL from sklearn.model_selection import train_test_split  os.environ['CUDA_VISIBLE_DEVICES'] = '2,3' In\u00a0[9]: Copied! <pre>class CarvanaDataset(Dataset):\n    def __init__(self, base_dir, idx_list, mode=\"train\", transform=None):\n        self.base_dir = base_dir\n        self.idx_list = idx_list\n        self.images = os.listdir(base_dir+\"train\")\n        self.masks = os.listdir(base_dir+\"train_masks\")\n        self.mode = mode\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.idx_list)\n\n    def __getitem__(self, index):\n        image_file = self.images[self.idx_list[index]]\n        mask_file = image_file[:-4]+\"_mask.gif\"\n        image = PIL.Image.open(os.path.join(base_dir, \"train\", image_file))\n        if self.mode==\"train\":\n            mask = PIL.Image.open(os.path.join(base_dir, \"train_masks\", mask_file))\n            if self.transform is not None:\n                image = self.transform(image)\n                mask = self.transform(mask)\n                mask[mask!=0] = 1.0\n            return image, mask.float()\n        else:\n            if self.transform is not None:\n                image = self.transform(image)\n            return image\n\nbase_dir = \"./\"\ntransform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])\ntrain_idxs, val_idxs = train_test_split(range(len(os.listdir(base_dir+\"train_masks\"))), test_size=0.3)\ntrain_data = CarvanaDataset(base_dir, train_idxs, transform=transform)\nval_data = CarvanaDataset(base_dir, val_idxs, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=True)\nval_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=False)\n</pre> class CarvanaDataset(Dataset):     def __init__(self, base_dir, idx_list, mode=\"train\", transform=None):         self.base_dir = base_dir         self.idx_list = idx_list         self.images = os.listdir(base_dir+\"train\")         self.masks = os.listdir(base_dir+\"train_masks\")         self.mode = mode         self.transform = transform          def __len__(self):         return len(self.idx_list)      def __getitem__(self, index):         image_file = self.images[self.idx_list[index]]         mask_file = image_file[:-4]+\"_mask.gif\"         image = PIL.Image.open(os.path.join(base_dir, \"train\", image_file))         if self.mode==\"train\":             mask = PIL.Image.open(os.path.join(base_dir, \"train_masks\", mask_file))             if self.transform is not None:                 image = self.transform(image)                 mask = self.transform(mask)                 mask[mask!=0] = 1.0             return image, mask.float()         else:             if self.transform is not None:                 image = self.transform(image)             return image  base_dir = \"./\" transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]) train_idxs, val_idxs = train_test_split(range(len(os.listdir(base_dir+\"train_masks\"))), test_size=0.3) train_data = CarvanaDataset(base_dir, train_idxs, transform=transform) val_data = CarvanaDataset(base_dir, val_idxs, transform=transform) train_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=True) val_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=False)      In\u00a0[10]: Copied! <pre>image, mask = next(iter(train_loader))\nplt.subplot(121)\nplt.imshow(image[0,0])\nplt.subplot(122)\nplt.imshow(mask[0,0], cmap=\"gray\")\n</pre> image, mask = next(iter(train_loader)) plt.subplot(121) plt.imshow(image[0,0]) plt.subplot(122) plt.imshow(mask[0,0], cmap=\"gray\") Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4b61982d00&gt;</pre> In\u00a0[11]: Copied! <pre># \u200b\u4f7f\u7528\u200bBinary Cross Entropy Loss\uff0c\u200b\u4e4b\u540e\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5c1d\u8bd5\u200b\u66ff\u6362\u200b\u4e3a\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200bloss\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(unet.parameters(), lr=1e-3, weight_decay=1e-8)\n\nunet = nn.DataParallel(unet).cuda()\n</pre> # \u200b\u4f7f\u7528\u200bBinary Cross Entropy Loss\uff0c\u200b\u4e4b\u540e\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5c1d\u8bd5\u200b\u66ff\u6362\u200b\u4e3a\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200bloss criterion = nn.BCEWithLogitsLoss() optimizer = optim.Adam(unet.parameters(), lr=1e-3, weight_decay=1e-8)  unet = nn.DataParallel(unet).cuda() In\u00a0[12]: Copied! <pre>def dice_coeff(pred, target):\n    eps = 0.0001\n    num = pred.size(0)\n    m1 = pred.view(num, -1)  # Flatten\n    m2 = target.view(num, -1)  # Flatten\n    intersection = (m1 * m2).sum()\n    return (2. * intersection + eps) / (m1.sum() + m2.sum() + eps)\n\ndef train(epoch):\n    unet.train()\n    train_loss = 0\n    for data, mask in train_loader:\n        data, mask = data.cuda(), mask.cuda()\n        optimizer.zero_grad()\n        output = unet(data)\n        loss = criterion(output,mask)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n    train_loss = train_loss/len(train_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n\ndef val(epoch):  \n    print(\"current learning rate: \", optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n    unet.eval()\n    val_loss = 0\n    dice_score = 0\n    with torch.no_grad():\n        for data, mask in val_loader:\n            data, mask = data.cuda(), mask.cuda()\n            output = unet(data)\n            loss = criterion(output, mask)\n            val_loss += loss.item()*data.size(0)\n            dice_score += dice_coeff(torch.sigmoid(output).cpu(), mask.cpu())*data.size(0)\n    val_loss = val_loss/len(val_loader.dataset)\n    dice_score = dice_score/len(val_loader.dataset)\n    print('Epoch: {} \\tValidation Loss: {:.6f}, Dice score: {:.6f}'.format(epoch, val_loss, dice_score))\n</pre> def dice_coeff(pred, target):     eps = 0.0001     num = pred.size(0)     m1 = pred.view(num, -1)  # Flatten     m2 = target.view(num, -1)  # Flatten     intersection = (m1 * m2).sum()     return (2. * intersection + eps) / (m1.sum() + m2.sum() + eps)  def train(epoch):     unet.train()     train_loss = 0     for data, mask in train_loader:         data, mask = data.cuda(), mask.cuda()         optimizer.zero_grad()         output = unet(data)         loss = criterion(output,mask)         loss.backward()         optimizer.step()         train_loss += loss.item()*data.size(0)     train_loss = train_loss/len(train_loader.dataset)     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))  def val(epoch):       print(\"current learning rate: \", optimizer.state_dict()[\"param_groups\"][0][\"lr\"])     unet.eval()     val_loss = 0     dice_score = 0     with torch.no_grad():         for data, mask in val_loader:             data, mask = data.cuda(), mask.cuda()             output = unet(data)             loss = criterion(output, mask)             val_loss += loss.item()*data.size(0)             dice_score += dice_coeff(torch.sigmoid(output).cpu(), mask.cpu())*data.size(0)     val_loss = val_loss/len(val_loader.dataset)     dice_score = dice_score/len(val_loader.dataset)     print('Epoch: {} \\tValidation Loss: {:.6f}, Dice score: {:.6f}'.format(epoch, val_loss, dice_score)) In\u00a0[40]: Copied! <pre>epochs = 100\nfor epoch in range(1, epochs+1):\n    train(epoch)\n    val(epoch)\n</pre> epochs = 100 for epoch in range(1, epochs+1):     train(epoch)     val(epoch) <pre>Epoch: 1 \tTraining Loss: 0.179544\ncurrent learning rate:  0.001\nEpoch: 1 \tValidation Loss: 0.142780, Dice score: 0.774944\nEpoch: 2 \tTraining Loss: 0.060674\ncurrent learning rate:  0.001\nEpoch: 2 \tValidation Loss: 0.054721, Dice score: 0.909441\nEpoch: 3 \tTraining Loss: 0.033283\ncurrent learning rate:  0.001\nEpoch: 3 \tValidation Loss: 0.034448, Dice score: 0.945890\nEpoch: 4 \tTraining Loss: 0.023393\ncurrent learning rate:  0.001\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-40-ad1cfccde69f&gt; in &lt;module&gt;\n      2 for epoch in range(1, epochs+1):\n      3     train(epoch)\n----&gt; 4     val(epoch)\n\n&lt;ipython-input-39-93e1f04769e1&gt; in val(epoch)\n     27     dice_score = 0\n     28     with torch.no_grad():\n---&gt; 29         for data, mask in val_loader:\n     30             data, mask = data.cuda(), mask.cuda()\n     31             output = unet(data)\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    519             if self._sampler_iter is None:\n    520                 self._reset()\n--&gt; 521             data = self._next_data()\n    522             self._num_yielded += 1\n    523             if self._dataset_kind == _DatasetKind.Iterable and \\\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n   1173                 # no valid `self._rcvd_idx` is found (i.e., didn't break)\n   1174                 if not self._persistent_workers:\n-&gt; 1175                     self._shutdown_workers()\n   1176                 raise StopIteration\n   1177 \n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _shutdown_workers(self)\n   1299                     # wrong, we set a timeout and if the workers fail to join,\n   1300                     # they are killed in the `finally` block.\n-&gt; 1301                     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n   1302                 for q in self._index_queues:\n   1303                     q.cancel_join_thread()\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/process.py in join(self, timeout)\n    147         assert self._parent_pid == os.getpid(), 'can only join a child process'\n    148         assert self._popen is not None, 'can only join a started process'\n--&gt; 149         res = self._popen.wait(timeout)\n    150         if res is not None:\n    151             _children.discard(self)\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/popen_fork.py in wait(self, timeout)\n     42             if timeout is not None:\n     43                 from multiprocessing.connection import wait\n---&gt; 44                 if not wait([self.sentinel], timeout):\n     45                     return None\n     46             # This shouldn't block if wait() returned successfully.\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/connection.py in wait(object_list, timeout)\n    929 \n    930             while True:\n--&gt; 931                 ready = selector.select(timeout)\n    932                 if ready:\n    933                     return [key.fileobj for (key, events) in ready]\n\n/data1/ljq/anaconda3/lib/python3.8/selectors.py in select(self, timeout)\n    413         ready = []\n    414         try:\n--&gt; 415             fd_event_list = self._selector.poll(timeout)\n    416         except InterruptedError:\n    417             return ready\n\nKeyboardInterrupt: </pre> In\u00a0[41]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sun Mar 27 20:57:34 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 208...  Off  | 00000000:18:00.0 Off |                  N/A |\n| 27%   30C    P8     6W / 250W |   1126MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n| 27%   28C    P8    16W / 250W |      3MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |\n| 58%   53C    P2    61W / 250W |  10154MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0 Off |                  N/A |\n| 62%   55C    P2    62W / 250W |   9854MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A   1140407      C   .../envs/r411py37/bin/python     1123MiB |\n|    2   N/A  N/A   2211464      C   .../ljq/anaconda3/bin/python    10151MiB |\n|    3   N/A  N/A   2211464      C   .../ljq/anaconda3/bin/python     9851MiB |\n+-----------------------------------------------------------------------------+\n</pre> <p>Point 5\uff1a\u200b\u81ea\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4e0d\u60f3\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u51fd\u6570\u200b\uff0c\u200b\u800c\u662f\u200b\u60f3\u200b\u9488\u5bf9\u200b\u5206\u5272\u200b\u6a21\u578b\u200b\u5e38\u7528\u200b\u7684\u200bDice\u200b\u7cfb\u6570\u200b\u8bbe\u8ba1\u200b\u4e13\u95e8\u200b\u7684\u200bloss\uff0c\u200b\u5373\u200bDiceLoss\uff0c\u200b\u8fd9\u65f6\u200b\u5c31\u200b\u9700\u8981\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200bPyTorch\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b</p> In\u00a0[42]: Copied! <pre>class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n        \n    def forward(self,inputs,targets,smooth=1):\n        inputs = torch.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (inputs * targets).sum()                   \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        return 1 - dice\n</pre> class DiceLoss(nn.Module):     def __init__(self, weight=None, size_average=True):         super(DiceLoss, self).__init__()              def forward(self,inputs,targets,smooth=1):         inputs = torch.sigmoid(inputs)                inputs = inputs.view(-1)         targets = targets.view(-1)         intersection = (inputs * targets).sum()                            dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)           return 1 - dice  In\u00a0[43]: Copied! <pre>newcriterion = DiceLoss()\n\nunet.eval()\nimage, mask = next(iter(val_loader))\nout_unet = unet(image.cuda())\nloss = newcriterion(out_unet, mask.cuda())\nprint(loss)\n</pre> newcriterion = DiceLoss()  unet.eval() image, mask = next(iter(val_loader)) out_unet = unet(image.cuda()) loss = newcriterion(out_unet, mask.cuda()) print(loss) <pre>tensor(0.1071, device='cuda:0', grad_fn=&lt;RsubBackward1&gt;)\n</pre> <p>Point 6\uff1a\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b \u200b\u968f\u7740\u200b\u4f18\u5316\u200b\u7684\u200b\u8fdb\u884c\u200b\uff0c\u200b\u56fa\u5b9a\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u53ef\u80fd\u200b\u65e0\u6cd5\u200b\u6ee1\u8db3\u200b\u4f18\u5316\u200b\u7684\u200b\u9700\u6c42\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u964d\u4f4e\u200b\u4f18\u5316\u200b\u7684\u200b\u901f\u5ea6\u200b \u200b\u8fd9\u91cc\u200b\u6f14\u793a\u200b\u4f7f\u7528\u200bPyTorch\u200b\u81ea\u5e26\u200b\u7684\u200bStepLR scheduler\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u6587\u5b57\u7248\u200b\u6559\u7a0b\u200b\u4e2d\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u81ea\u5b9a\u4e49\u200bscheduler\u200b\u7684\u200b\u65b9\u5f0f\u200b</p> In\u00a0[13]: Copied! <pre>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n</pre> scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8) In\u00a0[14]: Copied! <pre>epochs = 100\nfor epoch in range(1, epochs+1):\n    train(epoch)\n    val(epoch)\n    scheduler.step()\n</pre> epochs = 100 for epoch in range(1, epochs+1):     train(epoch)     val(epoch)     scheduler.step() <pre>Epoch: 1 \tTraining Loss: 0.177876\ncurrent learning rate:  0.001\nEpoch: 1 \tValidation Loss: 1.773043, Dice score: 0.432005\nEpoch: 2 \tTraining Loss: 0.056940\ncurrent learning rate:  0.0008\nEpoch: 2 \tValidation Loss: 0.061336, Dice score: 0.906578\nEpoch: 3 \tTraining Loss: 0.042535\ncurrent learning rate:  0.00064\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-14-ddbcf64e618a&gt; in &lt;module&gt;\n      2 for epoch in range(1, epochs+1):\n      3     train(epoch)\n----&gt; 4     val(epoch)\n      5     scheduler.step()\n\n&lt;ipython-input-12-93e1f04769e1&gt; in val(epoch)\n     27     dice_score = 0\n     28     with torch.no_grad():\n---&gt; 29         for data, mask in val_loader:\n     30             data, mask = data.cuda(), mask.cuda()\n     31             output = unet(data)\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    519             if self._sampler_iter is None:\n    520                 self._reset()\n--&gt; 521             data = self._next_data()\n    522             self._num_yielded += 1\n    523             if self._dataset_kind == _DatasetKind.Iterable and \\\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n   1184 \n   1185             assert not self._shutdown and self._tasks_outstanding &gt; 0\n-&gt; 1186             idx, data = self._get_data()\n   1187             self._tasks_outstanding -= 1\n   1188             if self._dataset_kind == _DatasetKind.Iterable:\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _get_data(self)\n   1150         else:\n   1151             while True:\n-&gt; 1152                 success, data = self._try_get_data()\n   1153                 if success:\n   1154                     return data\n\n/data1/ljq/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _try_get_data(self, timeout)\n    988         #   (bool: whether successfully get data, any: data if successful else None)\n    989         try:\n--&gt; 990             data = self._data_queue.get(timeout=timeout)\n    991             return (True, data)\n    992         except Exception as e:\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/queues.py in get(self, block, timeout)\n    105                 if block:\n    106                     timeout = deadline - time.monotonic()\n--&gt; 107                     if not self._poll(timeout):\n    108                         raise Empty\n    109                 elif not self._poll():\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/connection.py in poll(self, timeout)\n    255         self._check_closed()\n    256         self._check_readable()\n--&gt; 257         return self._poll(timeout)\n    258 \n    259     def __enter__(self):\n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/connection.py in _poll(self, timeout)\n    422 \n    423     def _poll(self, timeout):\n--&gt; 424         r = wait([self], timeout)\n    425         return bool(r)\n    426 \n\n/data1/ljq/anaconda3/lib/python3.8/multiprocessing/connection.py in wait(object_list, timeout)\n    929 \n    930             while True:\n--&gt; 931                 ready = selector.select(timeout)\n    932                 if ready:\n    933                     return [key.fileobj for (key, events) in ready]\n\n/data1/ljq/anaconda3/lib/python3.8/selectors.py in select(self, timeout)\n    413         ready = []\n    414         try:\n--&gt; 415             fd_event_list = self._selector.poll(timeout)\n    416         except InterruptedError:\n    417             return ready\n\nKeyboardInterrupt: </pre> In\u00a0[15]: Copied! <pre>?optim.lr_scheduler.StepLR\n</pre> ?optim.lr_scheduler.StepLR <p>Point 7\uff1a\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b</p> In\u00a0[16]: Copied! <pre>unet\n</pre> unet Out[16]: <pre>DataParallel(\n  (module): UNet(\n    (inc): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (down1): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down2): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down3): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (down4): Down(\n      (maxpool_conv): Sequential(\n        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (1): DoubleConv(\n          (double_conv): Sequential(\n            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n    )\n    (up1): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up2): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up3): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up4): Up(\n      (up): Upsample(scale_factor=2.0, mode=bilinear)\n      (conv): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (outc): OutConv(\n      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)</pre> In\u00a0[22]: Copied! <pre>unet.module.outc.conv.weight.requires_grad = False\nunet.module.outc.conv.bias.requires_grad = False\n\nfor layer, param in unet.named_parameters():\n    print(layer, '\\t', param.requires_grad)\n</pre> unet.module.outc.conv.weight.requires_grad = False unet.module.outc.conv.bias.requires_grad = False  for layer, param in unet.named_parameters():     print(layer, '\\t', param.requires_grad) <pre>module.inc.double_conv.0.weight \t True\nmodule.inc.double_conv.1.weight \t True\nmodule.inc.double_conv.1.bias \t True\nmodule.inc.double_conv.3.weight \t True\nmodule.inc.double_conv.4.weight \t True\nmodule.inc.double_conv.4.bias \t True\nmodule.down1.maxpool_conv.1.double_conv.0.weight \t True\nmodule.down1.maxpool_conv.1.double_conv.1.weight \t True\nmodule.down1.maxpool_conv.1.double_conv.1.bias \t True\nmodule.down1.maxpool_conv.1.double_conv.3.weight \t True\nmodule.down1.maxpool_conv.1.double_conv.4.weight \t True\nmodule.down1.maxpool_conv.1.double_conv.4.bias \t True\nmodule.down2.maxpool_conv.1.double_conv.0.weight \t True\nmodule.down2.maxpool_conv.1.double_conv.1.weight \t True\nmodule.down2.maxpool_conv.1.double_conv.1.bias \t True\nmodule.down2.maxpool_conv.1.double_conv.3.weight \t True\nmodule.down2.maxpool_conv.1.double_conv.4.weight \t True\nmodule.down2.maxpool_conv.1.double_conv.4.bias \t True\nmodule.down3.maxpool_conv.1.double_conv.0.weight \t True\nmodule.down3.maxpool_conv.1.double_conv.1.weight \t True\nmodule.down3.maxpool_conv.1.double_conv.1.bias \t True\nmodule.down3.maxpool_conv.1.double_conv.3.weight \t True\nmodule.down3.maxpool_conv.1.double_conv.4.weight \t True\nmodule.down3.maxpool_conv.1.double_conv.4.bias \t True\nmodule.down4.maxpool_conv.1.double_conv.0.weight \t True\nmodule.down4.maxpool_conv.1.double_conv.1.weight \t True\nmodule.down4.maxpool_conv.1.double_conv.1.bias \t True\nmodule.down4.maxpool_conv.1.double_conv.3.weight \t True\nmodule.down4.maxpool_conv.1.double_conv.4.weight \t True\nmodule.down4.maxpool_conv.1.double_conv.4.bias \t True\nmodule.up1.conv.double_conv.0.weight \t True\nmodule.up1.conv.double_conv.1.weight \t True\nmodule.up1.conv.double_conv.1.bias \t True\nmodule.up1.conv.double_conv.3.weight \t True\nmodule.up1.conv.double_conv.4.weight \t True\nmodule.up1.conv.double_conv.4.bias \t True\nmodule.up2.conv.double_conv.0.weight \t True\nmodule.up2.conv.double_conv.1.weight \t True\nmodule.up2.conv.double_conv.1.bias \t True\nmodule.up2.conv.double_conv.3.weight \t True\nmodule.up2.conv.double_conv.4.weight \t True\nmodule.up2.conv.double_conv.4.bias \t True\nmodule.up3.conv.double_conv.0.weight \t True\nmodule.up3.conv.double_conv.1.weight \t True\nmodule.up3.conv.double_conv.1.bias \t True\nmodule.up3.conv.double_conv.3.weight \t True\nmodule.up3.conv.double_conv.4.weight \t True\nmodule.up3.conv.double_conv.4.bias \t True\nmodule.up4.conv.double_conv.0.weight \t True\nmodule.up4.conv.double_conv.1.weight \t True\nmodule.up4.conv.double_conv.1.bias \t True\nmodule.up4.conv.double_conv.3.weight \t True\nmodule.up4.conv.double_conv.4.weight \t True\nmodule.up4.conv.double_conv.4.bias \t True\nmodule.outc.conv.weight \t False\nmodule.outc.conv.bias \t False\n</pre> In\u00a0[23]: Copied! <pre>param\n</pre> param Out[23]: <pre>Parameter containing:\ntensor([-0.1994], device='cuda:0')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Point 8\uff1a\u200b\u534a\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b</p> In\u00a0[9]: Copied! <pre>## \u200b\u6f14\u793a\u200b\u65f6\u200b\u9700\u8981\u200brestart kernel\uff0c\u200b\u5e76\u200b\u8fd0\u884c\u200bUnet\u200b\u6a21\u5757\u200b\n\nfrom torch.cuda.amp import autocast\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n</pre> ## \u200b\u6f14\u793a\u200b\u65f6\u200b\u9700\u8981\u200brestart kernel\uff0c\u200b\u5e76\u200b\u8fd0\u884c\u200bUnet\u200b\u6a21\u5757\u200b  from torch.cuda.amp import autocast os.environ['CUDA_VISIBLE_DEVICES'] = '2,3' In\u00a0[10]: Copied! <pre>class CarvanaDataset(Dataset):\n    def __init__(self, base_dir, idx_list, mode=\"train\", transform=None):\n        self.base_dir = base_dir\n        self.idx_list = idx_list\n        self.images = os.listdir(base_dir+\"train\")\n        self.masks = os.listdir(base_dir+\"train_masks\")\n        self.mode = mode\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.idx_list)\n\n    def __getitem__(self, index):\n        image_file = self.images[self.idx_list[index]]\n        mask_file = image_file[:-4]+\"_mask.gif\"\n        image = PIL.Image.open(os.path.join(base_dir, \"train\", image_file))\n        if self.mode==\"train\":\n            mask = PIL.Image.open(os.path.join(base_dir, \"train_masks\", mask_file))\n            if self.transform is not None:\n                image = self.transform(image)\n                mask = self.transform(mask)\n                mask[mask!=0] = 1.0\n            return image, mask.float()\n        else:\n            if self.transform is not None:\n                image = self.transform(image)\n            return image\n\nbase_dir = \"./\"\ntransform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])\ntrain_idxs, val_idxs = train_test_split(range(len(os.listdir(base_dir+\"train_masks\"))), test_size=0.3)\ntrain_data = CarvanaDataset(base_dir, train_idxs, transform=transform)\nval_data = CarvanaDataset(base_dir, val_idxs, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=True)\nval_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=False)\n</pre> class CarvanaDataset(Dataset):     def __init__(self, base_dir, idx_list, mode=\"train\", transform=None):         self.base_dir = base_dir         self.idx_list = idx_list         self.images = os.listdir(base_dir+\"train\")         self.masks = os.listdir(base_dir+\"train_masks\")         self.mode = mode         self.transform = transform          def __len__(self):         return len(self.idx_list)      def __getitem__(self, index):         image_file = self.images[self.idx_list[index]]         mask_file = image_file[:-4]+\"_mask.gif\"         image = PIL.Image.open(os.path.join(base_dir, \"train\", image_file))         if self.mode==\"train\":             mask = PIL.Image.open(os.path.join(base_dir, \"train_masks\", mask_file))             if self.transform is not None:                 image = self.transform(image)                 mask = self.transform(mask)                 mask[mask!=0] = 1.0             return image, mask.float()         else:             if self.transform is not None:                 image = self.transform(image)             return image  base_dir = \"./\" transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]) train_idxs, val_idxs = train_test_split(range(len(os.listdir(base_dir+\"train_masks\"))), test_size=0.3) train_data = CarvanaDataset(base_dir, train_idxs, transform=transform) val_data = CarvanaDataset(base_dir, val_idxs, transform=transform) train_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=True) val_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=False)      In\u00a0[11]: Copied! <pre>class UNet_half(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet_half, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n    \n    @autocast()\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\nunet_half = UNet_half(3,1)\nunet_half = nn.DataParallel(unet_half).cuda()\n</pre> class UNet_half(nn.Module):     def __init__(self, n_channels, n_classes, bilinear=True):         super(UNet_half, self).__init__()         self.n_channels = n_channels         self.n_classes = n_classes         self.bilinear = bilinear          self.inc = DoubleConv(n_channels, 64)         self.down1 = Down(64, 128)         self.down2 = Down(128, 256)         self.down3 = Down(256, 512)         factor = 2 if bilinear else 1         self.down4 = Down(512, 1024 // factor)         self.up1 = Up(1024, 512 // factor, bilinear)         self.up2 = Up(512, 256 // factor, bilinear)         self.up3 = Up(256, 128 // factor, bilinear)         self.up4 = Up(128, 64, bilinear)         self.outc = OutConv(64, n_classes)          @autocast()     def forward(self, x):         x1 = self.inc(x)         x2 = self.down1(x1)         x3 = self.down2(x2)         x4 = self.down3(x3)         x5 = self.down4(x4)         x = self.up1(x5, x4)         x = self.up2(x, x3)         x = self.up3(x, x2)         x = self.up4(x, x1)         logits = self.outc(x)         return logits  unet_half = UNet_half(3,1) unet_half = nn.DataParallel(unet_half).cuda() In\u00a0[12]: Copied! <pre>criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(unet_half.parameters(), lr=1e-3, weight_decay=1e-8)\n</pre> criterion = nn.BCEWithLogitsLoss() optimizer = optim.Adam(unet_half.parameters(), lr=1e-3, weight_decay=1e-8) In\u00a0[13]: Copied! <pre>def dice_coeff(pred, target):\n    eps = 0.0001\n    num = pred.size(0)\n    m1 = pred.view(num, -1)  # Flatten\n    m2 = target.view(num, -1)  # Flatten\n    intersection = (m1 * m2).sum()\n    return (2. * intersection + eps) / (m1.sum() + m2.sum() + eps)\n\ndef train_half(epoch):\n    unet_half.train()\n    train_loss = 0\n    for data, mask in train_loader:\n        data, mask = data.cuda(), mask.cuda()\n        with autocast():\n            optimizer.zero_grad()\n            output = unet_half(data)\n            loss = criterion(output,mask)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()*data.size(0)\n    train_loss = train_loss/len(train_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n\ndef val_half(epoch):  \n    print(\"current learning rate: \", optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n    unet_half.eval()\n    val_loss = 0\n    dice_score = 0\n    with torch.no_grad():\n        for data, mask in val_loader:\n            data, mask = data.cuda(), mask.cuda()\n            with autocast():\n                output = unet_half(data)\n                loss = criterion(output, mask)\n                val_loss += loss.item()*data.size(0)\n                dice_score += dice_coeff(torch.sigmoid(output).cpu(), mask.cpu())*data.size(0)\n    val_loss = val_loss/len(val_loader.dataset)\n    dice_score = dice_score/len(val_loader.dataset)\n    print('Epoch: {} \\tValidation Loss: {:.6f}, Dice score: {:.6f}'.format(epoch, val_loss, dice_score))\n</pre> def dice_coeff(pred, target):     eps = 0.0001     num = pred.size(0)     m1 = pred.view(num, -1)  # Flatten     m2 = target.view(num, -1)  # Flatten     intersection = (m1 * m2).sum()     return (2. * intersection + eps) / (m1.sum() + m2.sum() + eps)  def train_half(epoch):     unet_half.train()     train_loss = 0     for data, mask in train_loader:         data, mask = data.cuda(), mask.cuda()         with autocast():             optimizer.zero_grad()             output = unet_half(data)             loss = criterion(output,mask)             loss.backward()             optimizer.step()             train_loss += loss.item()*data.size(0)     train_loss = train_loss/len(train_loader.dataset)     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))  def val_half(epoch):       print(\"current learning rate: \", optimizer.state_dict()[\"param_groups\"][0][\"lr\"])     unet_half.eval()     val_loss = 0     dice_score = 0     with torch.no_grad():         for data, mask in val_loader:             data, mask = data.cuda(), mask.cuda()             with autocast():                 output = unet_half(data)                 loss = criterion(output, mask)                 val_loss += loss.item()*data.size(0)                 dice_score += dice_coeff(torch.sigmoid(output).cpu(), mask.cpu())*data.size(0)     val_loss = val_loss/len(val_loader.dataset)     dice_score = dice_score/len(val_loader.dataset)     print('Epoch: {} \\tValidation Loss: {:.6f}, Dice score: {:.6f}'.format(epoch, val_loss, dice_score)) In\u00a0[15]: Copied! <pre>epochs = 100\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\nfor epoch in range(1, epochs+1):\n    train_half(epoch)\n    val_half(epoch)\n    scheduler.step()\n</pre> epochs = 100 scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8) for epoch in range(1, epochs+1):     train_half(epoch)     val_half(epoch)     scheduler.step() <pre>Epoch: 1 \tTraining Loss: 0.431690\ncurrent learning rate:  0.001\nEpoch: 1 \tValidation Loss: 0.399930, Dice score: 0.000200\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-15-82cf28da3fc0&gt; in &lt;module&gt;\n      2 scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n      3 for epoch in range(1, epochs+1):\n----&gt; 4     train_half(epoch)\n      5     val_half(epoch)\n      6     scheduler.step()\n\n&lt;ipython-input-13-c7b3b098e802&gt; in train_half(epoch)\n     18             loss.backward()\n     19             optimizer.step()\n---&gt; 20             train_loss += loss.item()*data.size(0)\n     21     train_loss = train_loss/len(train_loader.dataset)\n     22     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n\nKeyboardInterrupt: </pre> In\u00a0[16]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sun Mar 27 21:17:28 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 208...  Off  | 00000000:18:00.0 Off |                  N/A |\n| 49%   49C    P2    52W / 250W |   8989MiB / 11019MiB |      1%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n| 59%   53C    P2    64W / 250W |   7866MiB / 11019MiB |      1%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |\n| 52%   54C    P2    61W / 250W |   5862MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0 Off |                  N/A |\n| 52%   54C    P2    61W / 250W |   5814MiB / 11019MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A   1140407      C   .../envs/r411py37/bin/python     1123MiB |\n|    0   N/A  N/A   2231210      C   python                           7863MiB |\n|    1   N/A  N/A   2233380      C   python                           7863MiB |\n|    2   N/A  N/A   2233881      C   .../ljq/anaconda3/bin/python     5859MiB |\n|    3   N/A  N/A   2233881      C   .../ljq/anaconda3/bin/python     5811MiB |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/PyTorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E4%B8%8E%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/","title":"\u8bf4\u660e\u200b\u00b6","text":"<p>notebook\u200b\u914d\u5957\u200b\u6559\u7a0b\u200b\u7b2c\u4e94\u7ae0\u200b\u548c\u200b\u7b2c\u516d\u7ae0\u200b\uff0c\u200b\u5c06\u200b\u7ed3\u5408\u200bU-Net\u200b\u6a21\u578b\u200b\u6765\u200b\u63a2\u7d22\u200bPyTorch\u200b\u7684\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u65b9\u5f0f\u200b\u548c\u200b\u8fdb\u9636\u200b\u8bad\u7ec3\u200b\u6280\u5de7\u200b\u3002 \u200b\u4e0b\u65b9\u200b\u6bcf\u4e2a\u200b\u201cPoint\u201d\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u6559\u7a0b\u200b\u4e2d\u200b\u6bcf\u200b\u4e00\u8282\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/","title":"1 \u200b\u6982\u8ff0","text":"<p>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u626e\u6f14\u7740\u200b\u91cd\u8981\u200b\u7684\u200b\u89d2\u8272\u200b\u3002\u200b\u8c37\u6b4c\u200b\u4e3a\u200bTensorflow\u200b\u6253\u9020\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200bTensorboard\uff0c\u200b\u800c\u200bFacebook\u200b\u4e3a\u200bPyTorch\u200b\u5f00\u53d1\u200b\u4e86\u200b\u4e00\u6b3e\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200b\uff0c\u200b\u540d\u4e3a\u200bVisdom\u3002Visdom\u200b\u5341\u5206\u200b\u8f7b\u91cf\u7ea7\u200b\uff0c\u200b\u5374\u200b\u652f\u6301\u200b\u975e\u5e38\u200b\u4e30\u5bcc\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u80fd\u200b\u80dc\u4efb\u200b\u7edd\u5927\u591a\u6570\u200b\u7684\u200b\u79d1\u5b66\u200b\u8fd0\u7b97\u200b\u53ef\u89c6\u5316\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5176\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\u3002</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200bVisdom\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5c55\u793a\u200b\u6570\u636e\u200b\u7684\u200b\u5206\u5e03\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3001\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3001\u200b\u53c2\u6570\u200b\u5206\u5e03\u200b\u7b49\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200bdebug\u200b\u4e2d\u200b\u67e5\u627e\u200b\u95ee\u9898\u200b\u6765\u6e90\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u4ecb\u7ecd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4e0b\u65b9\u200b\u7684\u200b\u4e24\u4e2a\u200b\u94fe\u63a5\u200b\uff1a</p> <ul> <li>Visdom Github\u200b\u94fe\u63a5\u200b\uff1aGithub</li> <li>Visdom \u200b\u5b98\u65b9\u7f51\u7ad9\u200b\uff1a\u200b\u5b98\u7f51\u200b</li> </ul> <p>\u200b\u7ecf\u8fc7\u200b\u672c\u8282\u200b\u8bfe\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u6536\u83b7\u200b\uff1a</p> <ul> <li>\u200b\u5982\u4f55\u200b\u5b89\u88c5\u200b\u548c\u200b\u4f7f\u7528\u200bVisdom</li> <li>\u200b\u4e86\u89e3\u200bVisdom\u200b\u57fa\u672c\u77e5\u8bc6\u200b</li> <li>\u200b\u4f7f\u7528\u200bVisdom\u200b\u8fdb\u884c\u200b\u7ed8\u56fe\u200b\u64cd\u4f5c\u200b</li> <li>\u200b\u5229\u7528\u200bVisdom\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b</li> </ul> <p>\u200b\u51c6\u5907\u200b\u597d\u4e86\u5417\u200b\uff1f\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5f00\u59cb\u200b\u5427\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8be5\u200bpip\u200b\u547d\u4ee4\u200b\u6765\u200b\u5b89\u88c5\u200bvisdom <code>pip install visdom</code></p> <p>\u200b\u5b89\u88c5\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u542f\u52a8\u200bVisdom\u200b\u5462\u200b\uff1f</p> <p>\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e0b\u65b9\u200b\u547d\u4ee4\u200b\u6765\u200b\u542f\u52a8\u200b\uff0c\u200b\u7b2c\u4e00\u6b21\u200b\u542f\u52a8\u200b\u65f6\u4f1a\u200b\u4e0b\u8f7d\u200b\u4e00\u4e9b\u200b\u76f8\u5173\u200b\u6587\u4ef6\u200b\u3002\u200b\u521d\u6b21\u200b\u542f\u52a8\u200b\u540e\u200b\u6700\u597d\u200b\u5728\u200b\u7ec8\u7aef\u200b\u91cd\u65b0\u200b\u8f93\u5165\u200b\u4e00\u6b21\u200b\u6307\u4ee4\u200b\u770b\u770b\u200b\u80fd\u5426\u200b\u6b63\u5e38\u200b\u542f\u52a8\u200b\u3002</p> <pre>python -m visdom.server  # \u200b\u6216\u200b\u76f4\u63a5\u200b\u8f93\u5165\u200b visdom\nnohup python -m visdom.server &amp;  # \u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8be5\u200b\u547d\u4ee4\u200b\u5c06\u200b\u670d\u52a1\u200b\u653e\u5230\u200b\u540e\u53f0\u200b\u8fd0\u884c\u200b\n</pre> <p>\u200b\u5982\u679c\u200b\u80fd\u200b\u6b63\u5e38\u200b\u542f\u52a8\u200b\uff0c\u200b\u7ec8\u7aef\u200b\u5c06\u4f1a\u200b\u663e\u793a\u200b\u5982\u4e0b\u200b\u4fe1\u606f\u200b\uff1a</p> <p>\u200b\u590d\u5236\u200b<code>http://localhost:8097</code>\u200b\u5230\u200b\u6d4f\u89c8\u5668\u200b\u540e\u200b\uff0c\u200b\u53d1\u73b0\u200b\u6b64\u65f6\u200b\u7684\u200b\u754c\u9762\u200b\u5e76\u200b\u6ca1\u6709\u200b\u663e\u793a\u200b\u4efb\u4f55\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\u3002</p> <p>\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u5728\u200b\u968f\u540e\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b66\u4e60\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u591a\u4e2a\u200bPanes\uff08\u200b\u7a97\u683c\u200b\uff09\u200b\u6765\u200b\u586b\u5145\u200b\u5b83\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fd9\u4e9b\u200b\u7a97\u683c\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u7f29\u653e\u200b\u3001\u200b\u79fb\u52a8\u200b\u3001\u200b\u5220\u9664\u200b\u7b49\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u9876\u90e8\u200b\u7684\u200b\u6309\u94ae\u200b\u542b\u4e49\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <ul> <li>\u200b\u6ce8\u610f\u200bclear\u200b\u64cd\u4f5c\u200b\u9700\u200b\u53cc\u51fb\u200b\u3002</li> <li>\u200b\u5728\u200b\u72b6\u6001\u200b\u4e3a\u200b\u201coffline\u201d\u200b\u65f6\u200b\uff0c\u200b\u65e0\u6cd5\u200b\u4fdd\u5b58\u200b/\u200b\u5220\u9664\u200b/\u200b\u6e05\u7a7a\u200b\u73af\u5883\u200b\u3002\u200b\u53ea\u80fd\u200b\u8fdb\u884c\u200b\u8fc7\u6ee4\u200b\u7b5b\u9009\u200b\u64cd\u4f5c\u200b\u3002</li> <li>\u200b\u70b9\u51fb\u200b\u7ba1\u7406\u200b\uff08\u200b\u5916\u89c2\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\u7684\u200bicon\uff09\u200b\u6309\u94ae\u200b\u540e\u200b\uff0c\u200b\u5f39\u200b\u51fa\u200b\u4ee5\u4e0b\u200b\u9875\u9762\u200b\uff0c\u200b\u53ef\u200b\u4fdd\u5b58\u200b\u6216\u200b\u5220\u9664\u200b\u5f53\u524d\u200b\u73af\u5883\u200b\u7684\u200b\u89c6\u56fe\u200b\u5185\u5bb9\u200b\u3002</li> </ul> <p>Visdom\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\uff0c\u200b\u5171\u4eab\u200b\u591a\u79cd\u200b\u6570\u636e\u200b\u5f62\u5f0f\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u5305\u62ec\u200b\u6570\u503c\u200b\uff0c\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6587\u672c\u200b\u548c\u200b\u89c6\u9891\u200b\uff0c\u200b\u652f\u6301\u200bPyTorch\uff0cNumpy\u200b\u7b49\u200b\u63a5\u53e3\u200b\u3002Visdom\u200b\u4e2d\u200b\u4e3b\u8981\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u51e0\u4e2a\u200b\u91cd\u8981\u200b\u6982\u5ff5\u200b\u3002</p> <p>Environment \u200b\u5bf9\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u533a\u57df\u200b\u8fdb\u884c\u200b\u5206\u533a\u200b\uff0c\u200b\u8fd9\u6837\u200b\u4f7f\u5f97\u200b\u4e0d\u540c\u200b\u73af\u5883\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u76f8\u4e92\u200b\u9694\u79bb\u200b\uff0c\u200b\u4e92\u4e0d\u200b\u5f71\u54cd\u200b\uff0c\u200b\u5728\u200b\u4f7f\u7528\u200b\u65f6\u200b\u5982\u679c\u200b\u4e0d\u200b\u6307\u5b9a\u200b\u7279\u5b9a\u200b\u7684\u200benv\uff0c\u200b\u9ed8\u8ba4\u200b\u5c06\u4f1a\u200b\u4f7f\u7528\u200bmain\u200b\u9ed8\u8ba4\u200b\u73af\u5883\u200b.</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7f16\u7a0b\u200b\u6216\u200bUI\u200b\u521b\u5efa\u200b\u65b0\u200b\u7684\u200benv\u3002\u200b\u4e0d\u540c\u200b\u7528\u6237\u200b\u3001\u200b\u4e0d\u540c\u200b\u7a0b\u5e8f\u200b\u4e00\u822c\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200benv\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u505a\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5206\u4eab\u200burl: http://localhost.com:8097/env/env_name \u200b\u8ba9\u200b\u5176\u4ed6\u4eba\u200b\u8bbf\u95ee\u200b\u7279\u5b9a\u200b\u7684\u200benv\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b envs \u200b\u9ed8\u8ba4\u200b\u901a\u8fc7\u200b<code>$HOME/.visdom/ </code>\u200b\u52a0\u8f7d\u200b\u3002\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u8def\u5f84\u200b\u5f53\u4f5c\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u4f20\u5165\u200b\u3002</p> <p>\u200b\u4e0d\u8981\u200b\u8f7b\u6613\u200b\u79fb\u9664\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u7684\u200b<code>env_name.json</code>\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u5bfc\u81f4\u200b\u76f8\u5e94\u200b\u7684\u200b\u73af\u5883\u200b\u4e5f\u200b\u4f1a\u200b\u88ab\u200b\u5220\u9664\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u5efa\u7acb\u200b\u65b0\u200b\u7684\u200benvironment\u3002</p> In\u00a0[\u00a0]: Copied! <pre>import visdom \nvis = visdom.Visdom(env='pytorchenv')\n# vis = visdom.Visdom(env=env_name)\u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u5ba2\u6237\u7aef\u200b\n# env_name \u200b\u662f\u200b\u6307\u5b9a\u200b\u7684\u200b\u73af\u5883\u200b\u7684\u200b\u540d\u79f0\u200b\uff08\u200b\u5b57\u7b26\u4e32\u200b\uff09\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200bhost\uff0cport\u200b\u7b49\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\n</pre> import visdom  vis = visdom.Visdom(env='pytorchenv') # vis = visdom.Visdom(env=env_name)\u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u5ba2\u6237\u7aef\u200b # env_name \u200b\u662f\u200b\u6307\u5b9a\u200b\u7684\u200b\u73af\u5883\u200b\u7684\u200b\u540d\u79f0\u200b\uff08\u200b\u5b57\u7b26\u4e32\u200b\uff09\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6307\u5b9a\u200bhost\uff0cport\u200b\u7b49\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b <p>Pane\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\u7528\u4e8e\u200b\u53ef\u89c6\u5316\u200b\u56fe\u8868\u200b\u3001\u200b\u56fe\u7247\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u89c6\u9891\u200b\u7684\u200b\u5bb9\u5668\u200b\u3002\u200b\u4e00\u4e2a\u200b\u73af\u5883\u200b\u91cc\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u7a97\u683c\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u6216\u200b\u8bb0\u5f55\u200b\u67d0\u4e00\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200bpane\u200b\u8fdb\u884c\u200b\u62d6\u653e\u200b\uff0c\u200b\u5220\u9664\u200b\uff0c\u200b\u8c03\u6574\u200b\u5927\u5c0f\u200b\u548c\u200b\u9500\u6bc1\u200b\u7b49\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u7a0b\u5e8f\u200b\u65e2\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u540c\u4e00\u4e2a\u200benv\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200bpane\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6307\u5b9a\u200b<code>win</code>\u200b\u6765\u200b\u4f7f\u7528\u200b\u540c\u4e00\u4e2a\u200benv\u200b\u4e2d\u200b\u7684\u200bpane\u3002</p> In\u00a0[\u00a0]: Copied! <pre>import visdom\nimport numpy as np\nvis = visdom.Visdom(env = 'pytorchenv')  # \u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200b\u73af\u5883\u200b\uff0c\u200b\u82e5\u200b\u4e0d\u200b\u6307\u5b9a\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200bmain\nvis.text('Hello, world!')  # \u200b\u8f93\u51fa\u200b\u6587\u672c\u200b\u5b57\u7b26\u200b\nvis.image(np.zeros((3, 224, 224)))  # \u200b\u8f93\u51fa\u200b\u5927\u5c0f\u200b\u4e3a\u200b3*224*224\uff08CxHxW\uff09\u200b\u5927\u5c0f\u200b\u7684\u200b\u9ed1\u8272\u200b\u56fe\u7247\u200b\n</pre> import visdom import numpy as np vis = visdom.Visdom(env = 'pytorchenv')  # \u200b\u6307\u5b9a\u200b\u4f7f\u7528\u200b\u7684\u200b\u73af\u5883\u200b\uff0c\u200b\u82e5\u200b\u4e0d\u200b\u6307\u5b9a\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200bmain vis.text('Hello, world!')  # \u200b\u8f93\u51fa\u200b\u6587\u672c\u200b\u5b57\u7b26\u200b vis.image(np.zeros((3, 224, 224)))  # \u200b\u8f93\u51fa\u200b\u5927\u5c0f\u200b\u4e3a\u200b3*224*224\uff08CxHxW\uff09\u200b\u5927\u5c0f\u200b\u7684\u200b\u9ed1\u8272\u200b\u56fe\u7247\u200b <p>\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u539f\u6765\u200b\u7684\u200b\u754c\u9762\u200b\u51fa\u73b0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6587\u672c\u200b\u548c\u200b\u4e00\u5e45\u200b\u56fe\u50cf\u200b</p> <p>\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5de6\u4e0a\u89d2\u200b\u7684\u200b\u56fe\u6807\u200b\u4ece\u5de6\u5230\u53f3\u200b\u5206\u522b\u200b\u4ee3\u8868\u200b\u201c\u200b\u5173\u95ed\u200b\u201d\u3001\u201c\u200b\u4e0b\u8f7d\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u5237\u65b0\u200b\u201d\u3002\u200b\u70b9\u51fb\u200b\u53f3\u4e0b\u89d2\u200b\u6309\u94ae\u200b\u53ef\u200b\u8fdb\u884c\u200b\u62d6\u62fd\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u9664\u6b64\u4ee5\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u4f1a\u200b\u4f20\u5165\u200bwin\u200b\u548c\u200bopt\u200b\u6765\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\u3002</p> <ul> <li><code>win</code>\uff1a\u200b\u7528\u4e8e\u200b\u6307\u5b9a\u200bpane\u200b\u7684\u200b\u540d\u5b57\u200b\uff0c\u200b\u82e5\u200b\u4e0d\u200b\u6307\u5b9a\u200b\uff0cvisdom\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u5206\u914d\u200b\u7ed9\u200b\u6211\u4eec\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200bpane\u3002\u200b\u4f46\u662f\u200b\u6211\u4eec\u200b\u4e00\u822c\u200b\u9700\u8981\u200b\u5728\u200b\u539f\u59cb\u200b\u56fe\u7247\u200b\u4e0a\u200b\u4fee\u6539\u200b\u3002\u200b\u56e0\u6b64\u200b\u5efa\u8bae\u200b\u6bcf\u6b21\u200b\u64cd\u4f5c\u200b\u90fd\u200b\u6307\u5b9a\u200bwin\u3002</li> <li><code>opts</code>\uff1a\u200b\u7528\u6765\u200b\u53ef\u89c6\u5316\u200b\u914d\u7f6e\u200b\uff0c\u200b\u63a5\u6536\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200boption\u200b\u5305\u62ec\u200btitle\uff0cxlabel\uff0cylabel\uff0cwidth\u200b\u7b49\u200b\uff0c\u200b\u7528\u6765\u200b\u8bbe\u7f6e\u200bpane\u200b\u7684\u200b\u663e\u793a\u200b\u683c\u5f0f\u200b\u3002</li> <li><code>append</code>\uff1a\u200b\u5728\u200bvisdom\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u64cd\u4f5c\u200b\u90fd\u200b\u4f1a\u200b\u8986\u76d6\u200b\u524d\u9762\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u53ef\u89c6\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u65f6\u200b\u5f80\u5f80\u200b\u9700\u8981\u200b\u4e0d\u65ad\u66f4\u65b0\u200b\u6570\u503c\u200b\u4e14\u200b\u4e0d\u200b\u8986\u76d6\u200b\u524d\u9762\u200b\u7684\u200b\u6570\u503c\u200b\uff0c\u200b\u8fd9\u65f6\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u4f20\u5165\u200b<code>update = 'append'</code>\u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u6765\u200b\u907f\u514d\u200b\u8986\u76d6\u200b\u4e4b\u524d\u200b\u7684\u200b\u6570\u503c\u200b\u5373\u53ef\u200b\u3002</li> </ul> <p>\u200b\u518d\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u5176\u4ed6\u200b\u6848\u4f8b\u200b\u5427\u200b~\uff01</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport visdom as vis\nvis = vis.Visdom(env='pytorchenv')\nx = torch.arange(1,30,0.01)\ny = torch.sin(x)\nvis.line(X=x,Y=y,win='sinx',opts={'title':'y.sin(x)'})\n</pre> import torch import visdom as vis vis = vis.Visdom(env='pytorchenv') x = torch.arange(1,30,0.01) y = torch.sin(x) vis.line(X=x,Y=y,win='sinx',opts={'title':'y.sin(x)'}) <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> In\u00a0[\u00a0]: Copied! <pre>import visdom as vis\nvis = vis.Visdom(env='pytorchenv')\nvis.image(torch.randn(64,64),win='rand1')  #\u200b\u53ef\u89c6\u5316\u200b\u4e00\u5f20\u200b\u968f\u673a\u200b\u7684\u200b\u9ed1\u767d\u56fe\u7247\u200b\nvis.image(torch.randn(3,64,64),win='rand2')  #\u200b\u53ef\u89c6\u5316\u200b\u4e00\u5f20\u200b\u968f\u673a\u200b\u7684\u200b\u5f69\u8272\u56fe\u7247\u200b\nvis.images(torch.randn(36,3,64,64).numpy(), nrow=6, win='rand3', opts={'title':'demo'})  #\u200b\u53ef\u89c6\u5316\u200b36\u200b\u5f20\u200b\u968f\u673a\u200b\u5f69\u8272\u56fe\u7247\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5f20\u200b6\u200b\u884c\u200b\n</pre> import visdom as vis vis = vis.Visdom(env='pytorchenv') vis.image(torch.randn(64,64),win='rand1')  #\u200b\u53ef\u89c6\u5316\u200b\u4e00\u5f20\u200b\u968f\u673a\u200b\u7684\u200b\u9ed1\u767d\u56fe\u7247\u200b vis.image(torch.randn(3,64,64),win='rand2')  #\u200b\u53ef\u89c6\u5316\u200b\u4e00\u5f20\u200b\u968f\u673a\u200b\u7684\u200b\u5f69\u8272\u56fe\u7247\u200b vis.images(torch.randn(36,3,64,64).numpy(), nrow=6, win='rand3', opts={'title':'demo'})  #\u200b\u53ef\u89c6\u5316\u200b36\u200b\u5f20\u200b\u968f\u673a\u200b\u5f69\u8272\u56fe\u7247\u200b\uff0c\u200b\u6bcf\u200b\u4e00\u5f20\u200b6\u200b\u884c\u200b <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p>\u200b\u9664\u4e86\u200btext\u3001line\u3001image\u3001images\u200b\u4ee5\u5916\u200b\uff0cVisdom\u200b\u8fd8\u200b\u652f\u6301\u200b\u4ee5\u4e0b\u200b\u57fa\u672c\u200b\u53ef\u89c6\u5316\u200b\u51fd\u6570\u200b\uff1a</p> <ul> <li>vis.image : \u200b\u56fe\u7247\u200b</li> <li>vis.line: \u200b\u66f2\u7ebf\u200b</li> <li>vis.images : \u200b\u56fe\u7247\u200b\u5217\u8868\u200b</li> <li>vis.text : \u200b\u62bd\u8c61\u200bHTML \u200b\u8f93\u51fa\u200b\u6587\u5b57\u200b</li> <li>vis.properties : \u200b\u5c5e\u6027\u200b\u7f51\u683c\u200b</li> <li>vis.audio : \u200b\u97f3\u9891\u200b</li> <li>vis.video : \u200b\u89c6\u9891\u200b</li> <li>vis.svg : SVG\u200b\u5bf9\u8c61\u200b</li> <li>vis.matplot : matplotlib\u200b\u56fe\u200b</li> <li>vis.save : \u200b\u5e8f\u5217\u5316\u200b\u72b6\u6001\u200b\u670d\u52a1\u7aef\u200b</li> </ul> <p>\u200b\u4e0a\u8ff0\u200b\u51fd\u6570\u200b\u53ef\u200b\u4f20\u5165\u200b\u7684\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li>opts.title : \u200b\u56fe\u200b\u6807\u9898\u200b</li> <li>win : \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b</li> <li>opts.width : \u200b\u56fe\u5bbd\u200b</li> <li>opts.height : \u200b\u56fe\u9ad8\u200b</li> <li>opts.showlegend : \u200b\u663e\u793a\u200b\u56fe\u4f8b\u200b (true or false)</li> <li>opts.xtype : x\u200b\u8f74\u200b\u7684\u200b\u7c7b\u578b\u200b ('linear' or 'log')</li> <li>opts.xlabel : x\u200b\u8f74\u200b\u7684\u200b\u6807\u7b7e\u200b</li> <li>opts.xtick : \u200b\u663e\u793a\u200bx\u200b\u8f74\u4e0a\u200b\u7684\u200b\u523b\u5ea6\u200b (boolean)</li> <li>opts.xtickmin : \u200b\u6307\u5b9a\u200bx\u200b\u8f74\u4e0a\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u523b\u5ea6\u200b (number)</li> <li>opts.xtickmax : \u200b\u6307\u5b9a\u200bx\u200b\u8f74\u4e0a\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u523b\u5ea6\u200b (number)</li> <li>opts.xtickvals : x\u200b\u8f74\u4e0a\u200b\u523b\u5ea6\u200b\u7684\u200b\u4f4d\u7f6e\u200b(table of numbers)</li> <li>opts.xticklabels : \u200b\u5728\u200bx\u200b\u8f74\u4e0a\u200b\u6807\u8bb0\u200b\u6807\u7b7e\u200b (table of strings)</li> <li>opts.xtickstep : x\u200b\u8f74\u4e0a\u200b\u523b\u5ea6\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ddd\u79bb\u200b (number)</li> <li>opts.xtickfont :x\u200b\u8f74\u200b\u6807\u7b7e\u200b\u7684\u200b\u5b57\u4f53\u200b (dict of font information)</li> <li>\u200b\u6709\u5173\u200by\u200b\u8f74\u200b\u7684\u200b\u53c2\u6570\u200b\u53ea\u200b\u9700\u200b\u5c06\u200b\u4e0a\u8ff0\u200bx\u200b\u6362\u6210\u200by\u200b\u5373\u53ef\u200b</li> <li>opts.marginleft : \u200b\u5de6\u200b\u8fb9\u6846\u200b (in pixels)</li> <li>opts.marginright :\u200b\u53f3\u8fb9\u200b\u6846\u200b (in pixels)</li> <li>opts.margintop : \u200b\u4e0a\u200b\u8fb9\u6846\u200b (in pixels)</li> <li>opts.marginbottom : \u200b\u4e0b\u200b\u8fb9\u6846\u200b (in pixels)</li> <li>opts.lagent=[''] : \u200b\u663e\u793a\u200b\u56fe\u6807\u200b</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from visdom import Visdom\nimport numpy as np\n \nvis = Visdom(env='pytorchenv')\nvis.bar(X=np.random.rand(4, 2),\n        win='test1',\n        opts=dict(\n        stacked=False,  # \u200b\u662f\u5426\u200b\u5806\u53e0\u200b\u67f1\u5f62\u200b\uff08\u200b\u82e5\u200b\u4e3a\u200bFalse\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemo1\u200b\u6240\u793a\u200b\uff1b\u200b\u82e5\u200b\u4e3a\u200bTrue\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemo2\u200b\u6240\u793a\u200b\n        legend=['A', 'B'],   # \u200b\u56fe\u4f8b\u200b\u6807\u7b7e\u200b\u540d\u79f0\u200b\n        rownames=['top1', 'top5', 'top10', 'top20'],  # \u200b\u5217\u200b\u540d\u79f0\u200b\n        title='demo1',  # \u200b\u56fe\u8868\u200b\u6807\u9898\u200b\n        ylabel='rank-k  Error Rate',  # y\u200b\u8f74\u200b\u540d\u79f0\u200b\n        xtickmin=0.4,  # x\u200b\u8f74\u200b\u5de6\u200b\u7aef\u70b9\u200b\u8d77\u59cb\u200b\u4f4d\u7f6e\u200b\n        xtickstep=0.4  # \u200b\u6bcf\u4e2a\u200b\u67f1\u5f62\u200b\u95f4\u9694\u200b\u8ddd\u79bb\u200b\n    ))\n</pre> from visdom import Visdom import numpy as np   vis = Visdom(env='pytorchenv') vis.bar(X=np.random.rand(4, 2),         win='test1',         opts=dict(         stacked=False,  # \u200b\u662f\u5426\u200b\u5806\u53e0\u200b\u67f1\u5f62\u200b\uff08\u200b\u82e5\u200b\u4e3a\u200bFalse\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemo1\u200b\u6240\u793a\u200b\uff1b\u200b\u82e5\u200b\u4e3a\u200bTrue\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemo2\u200b\u6240\u793a\u200b         legend=['A', 'B'],   # \u200b\u56fe\u4f8b\u200b\u6807\u7b7e\u200b\u540d\u79f0\u200b         rownames=['top1', 'top5', 'top10', 'top20'],  # \u200b\u5217\u200b\u540d\u79f0\u200b         title='demo1',  # \u200b\u56fe\u8868\u200b\u6807\u9898\u200b         ylabel='rank-k  Error Rate',  # y\u200b\u8f74\u200b\u540d\u79f0\u200b         xtickmin=0.4,  # x\u200b\u8f74\u200b\u5de6\u200b\u7aef\u70b9\u200b\u8d77\u59cb\u200b\u4f4d\u7f6e\u200b         xtickstep=0.4  # \u200b\u6bcf\u4e2a\u200b\u67f1\u5f62\u200b\u95f4\u9694\u200b\u8ddd\u79bb\u200b     )) In\u00a0[\u00a0]: Copied! <pre>from visdom import Visdom\nimport numpy as np\n \nvis = Visdom(env='pytorchenv')\nvis.bar(X=np.random.rand(4, 2),\n        win='test2',\n        opts=dict(\n        stacked=True,  # \u200b\u662f\u5426\u200b\u5806\u53e0\u200b\u67f1\u5f62\u200b\uff08\u200b\u82e5\u200b\u4e3a\u200bFalse\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemoA\u200b\u6240\u793a\u200b\uff1b\u200b\u82e5\u200b\u4e3a\u200bTrue\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemoB\u200b\u6240\u793a\u200b\n        legend=['A', 'B'],   # \u200b\u56fe\u4f8b\u200b\u6807\u7b7e\u200b\u540d\u79f0\u200b\n        rownames=['top1', 'top5', 'top10', 'top20'],  # \u200b\u5217\u200b\u540d\u79f0\u200b\n        title='demo2',  # \u200b\u56fe\u8868\u200b\u6807\u9898\u200b\n        ylabel='rank-k  Error Rate',  # y\u200b\u8f74\u200b\u540d\u79f0\u200b\n        xtickmin=0.4,  # x\u200b\u8f74\u200b\u5de6\u200b\u7aef\u70b9\u200b\u8d77\u59cb\u200b\u4f4d\u7f6e\u200b\n        xtickstep=0.4  # \u200b\u6bcf\u4e2a\u200b\u67f1\u5f62\u200b\u95f4\u9694\u200b\u8ddd\u79bb\u200b\n    ))\n</pre> from visdom import Visdom import numpy as np   vis = Visdom(env='pytorchenv') vis.bar(X=np.random.rand(4, 2),         win='test2',         opts=dict(         stacked=True,  # \u200b\u662f\u5426\u200b\u5806\u53e0\u200b\u67f1\u5f62\u200b\uff08\u200b\u82e5\u200b\u4e3a\u200bFalse\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemoA\u200b\u6240\u793a\u200b\uff1b\u200b\u82e5\u200b\u4e3a\u200bTrue\uff0c\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200bdemoB\u200b\u6240\u793a\u200b         legend=['A', 'B'],   # \u200b\u56fe\u4f8b\u200b\u6807\u7b7e\u200b\u540d\u79f0\u200b         rownames=['top1', 'top5', 'top10', 'top20'],  # \u200b\u5217\u200b\u540d\u79f0\u200b         title='demo2',  # \u200b\u56fe\u8868\u200b\u6807\u9898\u200b         ylabel='rank-k  Error Rate',  # y\u200b\u8f74\u200b\u540d\u79f0\u200b         xtickmin=0.4,  # x\u200b\u8f74\u200b\u5de6\u200b\u7aef\u70b9\u200b\u8d77\u59cb\u200b\u4f4d\u7f6e\u200b         xtickstep=0.4  # \u200b\u6bcf\u4e2a\u200b\u67f1\u5f62\u200b\u95f4\u9694\u200b\u8ddd\u79bb\u200b     )) <p>\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a </p> In\u00a0[\u00a0]: Copied! <pre>import visdom  \nfrom PIL import Image  \nimport torchvision.transforms.functional as F  \n  \nvis = visdom.Visdom(env='pytorchenv')  \nimg = Image.open('img/Lenna.jpg')  \nimg_tensor = F.to_tensor(img)  # \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u4e3a\u200btensor\u200b\u7c7b\u578b\u200b\nprint(img_tensor.shape)  # \u200b\u8f93\u51fa\u200b\u56fe\u7247\u5927\u5c0f\u200b\uff0c\u200b\u53ef\u200b\u7701\u7565\u200b\nvis.image(img_tensor, win='photo')\n</pre> import visdom   from PIL import Image   import torchvision.transforms.functional as F      vis = visdom.Visdom(env='pytorchenv')   img = Image.open('img/Lenna.jpg')   img_tensor = F.to_tensor(img)  # \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u4e3a\u200btensor\u200b\u7c7b\u578b\u200b print(img_tensor.shape)  # \u200b\u8f93\u51fa\u200b\u56fe\u7247\u5927\u5c0f\u200b\uff0c\u200b\u53ef\u200b\u7701\u7565\u200b vis.image(img_tensor, win='photo') <p>\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a </p> <p>\u200b\u770b\u200b\u5b8c\u200b\u4ee5\u4e0a\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4f60\u200b\u662f\u5426\u200b\u4f9d\u7136\u200b\u6478\u4e0d\u7740\u5934\u8111\u200b\uff0c\u200b\u8fd8\u662f\u200b\u4e0d\u77e5\u200b\u8be5\u200b\u5982\u4f55\u200b\u8fd0\u7528\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u4e0e\u200b\u53c2\u6570\u200b\uff1f\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6211\u4eec\u200b\u4f1a\u200b\u7528\u200b\u4e00\u4e9b\u200b\u5b9e\u4f8b\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u638c\u63e1\u200b\u3002</p> <p>\u200b\u7ecf\u8fc7\u200b\u4e0a\u8ff0\u200b\u5b66\u4e60\u200b\uff0c\u200b\u76f8\u4fe1\u200b\u5927\u5bb6\u200b\u5df2\u7ecf\u200b\u5bf9\u200bVisdom\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u521d\u6b65\u200b\u7684\u200b\u4e86\u89e3\u200b\uff0c\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u8fd9\u90e8\u5206\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u5177\u4f53\u200b\u5730\u200b\u6848\u4f8b\u200b\u6765\u200b\u5e2e\u52a9\u200b\u5927\u5bb6\u200b\u901a\u8fc7\u200bVisdom\u200b\u66f4\u597d\u200b\u5730\u200b\u67e5\u770b\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u53d8\u5316\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># \u200b\u5355\u6761\u200b\u66f2\u7ebf\u200b\u7ed8\u5236\u200b\nimport visdom\nvis = visdom.Visdom(env=\"pytorchenv\")\n'''\u200b\u8d77\u70b9\u200b'''\nvis.line([0.],     #\u200b\u7b2c\u4e00\u4e2a\u200b\u70b9\u200b\u7684\u200bY\u200b\u5750\u6807\u200b\n         [0.],     #\u200b\u7b2c\u4e00\u4e2a\u200b\u70b9\u200b\u7684\u200bX\u200b\u5750\u6807\u200b\n         win='train loss',  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b\n         opts=dict(title = 'train_loss',xlabel='episodes',ylabel='loss') #\u200b\u56fe\u200b\u6807\u9898\u200b\u3001x\u200b\u8f74\u200b\u548c\u200bY\u200b\u8f74\u200b\u6807\u7b7e\u200b\n         ) #\u200b\u8bbe\u7f6e\u200b\u8d77\u70b9\u200b\n'''\u200b\u6a21\u578b\u200b\u6570\u636e\u200b'''\nvis.line([1.],[1.],       #\u200b\u4e0b\u200b\u4e00\u70b9\u200b\u7684\u200bY\u200b\u5750\u6807\u200b\u53ca\u200bX\u200b\u5750\u6807\u200b\n         win='train loss',  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b \u200b\u4e0e\u200b\u4e0a\u200b\u4e2a\u200b\u7a97\u53e3\u200b\u540c\u540d\u200b\u8868\u793a\u200b\u663e\u793a\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u8868\u683c\u200b\u91cc\u200b\n         update='append')  # \u200b\u6dfb\u52a0\u200b\u5230\u200b\u4e0a\u200b\u4e00\u4e2a\u70b9\u200b\u7684\u200b\u540e\u9762\u200b\n</pre> # \u200b\u5355\u6761\u200b\u66f2\u7ebf\u200b\u7ed8\u5236\u200b import visdom vis = visdom.Visdom(env=\"pytorchenv\") '''\u200b\u8d77\u70b9\u200b''' vis.line([0.],     #\u200b\u7b2c\u4e00\u4e2a\u200b\u70b9\u200b\u7684\u200bY\u200b\u5750\u6807\u200b          [0.],     #\u200b\u7b2c\u4e00\u4e2a\u200b\u70b9\u200b\u7684\u200bX\u200b\u5750\u6807\u200b          win='train loss',  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b          opts=dict(title = 'train_loss',xlabel='episodes',ylabel='loss') #\u200b\u56fe\u200b\u6807\u9898\u200b\u3001x\u200b\u8f74\u200b\u548c\u200bY\u200b\u8f74\u200b\u6807\u7b7e\u200b          ) #\u200b\u8bbe\u7f6e\u200b\u8d77\u70b9\u200b '''\u200b\u6a21\u578b\u200b\u6570\u636e\u200b''' vis.line([1.],[1.],       #\u200b\u4e0b\u200b\u4e00\u70b9\u200b\u7684\u200bY\u200b\u5750\u6807\u200b\u53ca\u200bX\u200b\u5750\u6807\u200b          win='train loss',  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b \u200b\u4e0e\u200b\u4e0a\u200b\u4e2a\u200b\u7a97\u53e3\u200b\u540c\u540d\u200b\u8868\u793a\u200b\u663e\u793a\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u8868\u683c\u200b\u91cc\u200b          update='append')  # \u200b\u6dfb\u52a0\u200b\u5230\u200b\u4e0a\u200b\u4e00\u4e2a\u70b9\u200b\u7684\u200b\u540e\u9762\u200b <p>\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a </p> <ul> <li>\u200b\u70b9\u51fb\u200b\u53f3\u4e0a\u89d2\u200b\u7684\u200b\u6807\u7b7e\u200b\u6309\u94ae\u200b\uff0c\u200b\u51fa\u73b0\u200b\u8be6\u7ec6\u200b\u7684\u200b\u5c5e\u6027\u200b\u4fe1\u606f\u200b\u3002</li> <li>\u200b\u9f20\u6807\u200b\u60ac\u6d6e\u200b\u5728\u200b\u56fe\u7247\u200b\u4e0a\u65b9\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u66f4\u200b\u591a\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5982\u200b\u653e\u5927\u200b\u3001\u200b\u7f29\u5c0f\u200b\u3001\u200b\u4e0b\u8f7d\u200b\u4e3a\u200bpng\u200b\u56fe\u7247\u200b\u7b49\u200b\u3002</li> <li>\u200b\u70b9\u51fb\u200b\u53f3\u4e0b\u89d2\u200b\u7684\u200b\u201cedit\u201d\uff0c\u200b\u8fd8\u200b\u53ef\u200b\u5bf9\u200b\u9875\u9762\u200b\u8fdb\u884c\u200b\u7f16\u8f91\u200b\u64cd\u4f5c\u200b\u3002</li> </ul> In\u00a0[\u00a0]: Copied! <pre># \u200b\u591a\u6761\u200b\u66f2\u7ebf\u200b\u7ed8\u5236\u200b \u200b\u5b9e\u9645\u4e0a\u200b\u5c31\u662f\u200b\u4f20\u5165\u200by\u200b\u503c\u65f6\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b\nvis = visdom.Visdom(env=\"pytorchenv\")\n\n'''\u200b\u8d77\u70b9\u200b'''\nvis.line([[0.0,0.0]],    # Y\u200b\u7684\u200b\u8d77\u59cb\u200b\u70b9\u200b\n          [0.],    # X\u200b\u7684\u200b\u8d77\u59cb\u200b\u70b9\u200b\n         win=\"test loss\",    #\u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b\n         opts=dict(title='test_loss')  # \u200b\u56fe\u50cf\u200b\u6807\u4f8b\u200b\n        )\n'''\u200b\u6a21\u578b\u200b\u6570\u636e\u200b'''\nvis.line([[1.1,1.5]],   # Y\u200b\u7684\u200b\u4e0b\u200b\u4e00\u4e2a\u70b9\u200b\n        [1.],   # X\u200b\u7684\u200b\u4e0b\u200b\u4e00\u4e2a\u70b9\u200b\n        win=\"test loss\",  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b\n        update='append')   # \u200b\u6dfb\u52a0\u200b\u5230\u200b\u4e0a\u200b\u4e00\u4e2a\u70b9\u200b\u540e\u9762\u200b\n</pre> # \u200b\u591a\u6761\u200b\u66f2\u7ebf\u200b\u7ed8\u5236\u200b \u200b\u5b9e\u9645\u4e0a\u200b\u5c31\u662f\u200b\u4f20\u5165\u200by\u200b\u503c\u65f6\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b vis = visdom.Visdom(env=\"pytorchenv\")  '''\u200b\u8d77\u70b9\u200b''' vis.line([[0.0,0.0]],    # Y\u200b\u7684\u200b\u8d77\u59cb\u200b\u70b9\u200b           [0.],    # X\u200b\u7684\u200b\u8d77\u59cb\u200b\u70b9\u200b          win=\"test loss\",    #\u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b          opts=dict(title='test_loss')  # \u200b\u56fe\u50cf\u200b\u6807\u4f8b\u200b         ) '''\u200b\u6a21\u578b\u200b\u6570\u636e\u200b''' vis.line([[1.1,1.5]],   # Y\u200b\u7684\u200b\u4e0b\u200b\u4e00\u4e2a\u70b9\u200b         [1.],   # X\u200b\u7684\u200b\u4e0b\u200b\u4e00\u4e2a\u70b9\u200b         win=\"test loss\",  # \u200b\u7a97\u53e3\u200b\u540d\u79f0\u200b         update='append')   # \u200b\u6dfb\u52a0\u200b\u5230\u200b\u4e0a\u200b\u4e00\u4e2a\u70b9\u200b\u540e\u9762\u200b <p>\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a </p> <p>\u200b\u4e3a\u200b\u65b9\u4fbf\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u81ea\u5e26\u200b\u7684\u200bMNIST\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u5c55\u793a\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>'''\n\u200b\u5bfc\u200b\u5165\u5e93\u200b\u6587\u4ef6\u200b\n'''\nimport  torch\nimport  torch.nn as nn\nimport  torch.nn.functional as F\nimport  torch.optim as optim\nfrom torchvision import datasets, transforms\nimport visdom\nimport numpy as np\n'''\n\u200b\u6784\u5efa\u200b\u7b80\u5355\u200b\u7684\u200b\u6a21\u578b\u200b:\u200b\u7b80\u5355\u200b\u7ebf\u6027\u200b\u5c42\u200b+Relu\u200b\u51fd\u6570\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\n'''\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(784, 200),\n            nn.ReLU(inplace=True),\n            nn.Linear(200, 200),\n            nn.ReLU(inplace=True),\n            nn.Linear(200, 10),\n            nn.ReLU(inplace=True))\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n'''\n\u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b\n'''\nbatch_size = 128\nlearning_rate = 0.01\nepochs = 10\n\n'''\n\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\n'''\ntrain_loader = torch.utils.data.DataLoader(datasets.MNIST(\n    'data', # \n    train=True,\n   download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.1307, ), (0.3081, ))])),\n                                           batch_size=batch_size,\n                                           shuffle=True)\ntest_loader = torch.utils.data.DataLoader(datasets.MNIST(\n    'data',\n    train=False,\n    transform=transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.1307, ), (0.3081, ))])),\n                                          batch_size=batch_size,\n                                          shuffle=True)\n</pre> ''' \u200b\u5bfc\u200b\u5165\u5e93\u200b\u6587\u4ef6\u200b ''' import  torch import  torch.nn as nn import  torch.nn.functional as F import  torch.optim as optim from torchvision import datasets, transforms import visdom import numpy as np ''' \u200b\u6784\u5efa\u200b\u7b80\u5355\u200b\u7684\u200b\u6a21\u578b\u200b:\u200b\u7b80\u5355\u200b\u7ebf\u6027\u200b\u5c42\u200b+Relu\u200b\u51fd\u6570\u200b\u7684\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b ''' class MLP(nn.Module):      def __init__(self):         super(MLP, self).__init__()         self.model = nn.Sequential(             nn.Linear(784, 200),             nn.ReLU(inplace=True),             nn.Linear(200, 200),             nn.ReLU(inplace=True),             nn.Linear(200, 10),             nn.ReLU(inplace=True))     def forward(self, x):         x = self.model(x)         return x  ''' \u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b ''' batch_size = 128 learning_rate = 0.01 epochs = 10  ''' \u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b ''' train_loader = torch.utils.data.DataLoader(datasets.MNIST(     'data', #      train=True,    download=True,     transform=transforms.Compose(         [transforms.ToTensor(),          transforms.Normalize((0.1307, ), (0.3081, ))])),                                            batch_size=batch_size,                                            shuffle=True) test_loader = torch.utils.data.DataLoader(datasets.MNIST(     'data',     train=False,     transform=transforms.Compose(         [transforms.ToTensor(),          transforms.Normalize((0.1307, ), (0.3081, ))])),                                           batch_size=batch_size,                                           shuffle=True) In\u00a0[\u00a0]: Copied! <pre># \u200b\u6ce8\u610f\u200b\u6b64\u5904\u200b\u521d\u59cb\u5316\u200bvisdom\u200b\u7c7b\u200b\nvis = visdom.Visdom(env=\"pytorchenv\")\n\n# \u200b\u7ed8\u5236\u200b\u8d77\u70b9\u200b\nvis.line([0.], [0.], win=\"train loss\", opts=dict(title='train_loss'))\ndevice = torch.device('cuda:0')  # \u200b\u6307\u5b9a\u200bGPU\nnet = MLP().to(device)  # \u200b\u521d\u59cb\u5316\u200b\u7f51\u7edc\u200b\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\ncriteon = nn.CrossEntropyLoss().to(device)\n\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.view(-1, 28 * 28)\n        data, target = data.to(device), target.cuda()\n        logits = net(data)\n        loss = criteon(logits, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        # print(w1.grad.norm(), w2.grad.norm())\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data = data.view(-1, 28 * 28)\n        data, target = data.to(device), target.cuda()\n        logits = net(data)\n        test_loss += criteon(logits, target).item()\n\n        pred = logits.argmax(dim=1)\n        correct += pred.eq(target).float().sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    # \u200b\u7ed8\u5236\u200bepoch\u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u635f\u5931\u200bloss\n    vis.line([test_loss], [epoch], win=\"train loss\", update='append') # win\u200b\u662f\u200b\u5fc5\u987b\u200b\u7684\u200b \n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset), correct / len(test_loader.dataset)))\n</pre> # \u200b\u6ce8\u610f\u200b\u6b64\u5904\u200b\u521d\u59cb\u5316\u200bvisdom\u200b\u7c7b\u200b vis = visdom.Visdom(env=\"pytorchenv\")  # \u200b\u7ed8\u5236\u200b\u8d77\u70b9\u200b vis.line([0.], [0.], win=\"train loss\", opts=dict(title='train_loss')) device = torch.device('cuda:0')  # \u200b\u6307\u5b9a\u200bGPU net = MLP().to(device)  # \u200b\u521d\u59cb\u5316\u200b\u7f51\u7edc\u200b optimizer = optim.SGD(net.parameters(), lr=learning_rate) criteon = nn.CrossEntropyLoss().to(device)  for epoch in range(epochs):     for batch_idx, (data, target) in enumerate(train_loader):         data = data.view(-1, 28 * 28)         data, target = data.to(device), target.cuda()         logits = net(data)         loss = criteon(logits, target)          optimizer.zero_grad()         loss.backward()         # print(w1.grad.norm(), w2.grad.norm())         optimizer.step()         if batch_idx % 100 == 0:             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(                 epoch, batch_idx * len(data), len(train_loader.dataset),                 100. * batch_idx / len(train_loader), loss.item()))      test_loss = 0     correct = 0     for data, target in test_loader:         data = data.view(-1, 28 * 28)         data, target = data.to(device), target.cuda()         logits = net(data)         test_loss += criteon(logits, target).item()          pred = logits.argmax(dim=1)         correct += pred.eq(target).float().sum().item()      test_loss /= len(test_loader.dataset)     # \u200b\u7ed8\u5236\u200bepoch\u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u635f\u5931\u200bloss     vis.line([test_loss], [epoch], win=\"train loss\", update='append') # win\u200b\u662f\u200b\u5fc5\u987b\u200b\u7684\u200b      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(     test_loss, correct, len(test_loader.dataset), correct / len(test_loader.dataset))) <p>\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a </p> <p>\u200b\u4ee5\u4e0a\u200b\u5185\u5bb9\u200b\u53ea\u662f\u200bVisdom\u200b\u5de5\u5177\u200b\u7684\u200b\u521d\u6b65\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u6709\u610f\u601d\u200b\u7684\u200b\u64cd\u4f5c\u200b\u7b49\u5f85\u200b\u5927\u5bb6\u200b\u5728\u200b\u5b9e\u9645\u200b\u4e2d\u200b\u63a2\u7d22\u200b~</p> <p>\u200b\u53c2\u8003\u200b\u94fe\u63a5\u200b\uff1a</p> <ol> <li>\u200b\u8f7b\u677e\u200b\u5b66\u200b Pytorch\u2013Visdom \u200b\u53ef\u89c6\u5316\u200b</li> <li>\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200bVisdom\u200b\u7684\u200b\u4f7f\u7528\u200b</li> </ol>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#1","title":"1 \u200b\u6982\u8ff0\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#2-visdom","title":"2 \u200b\u5b89\u88c5\u200b\u548c\u200b\u4f7f\u7528\u200bVisdom\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#3-visdom","title":"3 Visdom\u200b\u57fa\u672c\u77e5\u8bc6\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#31-env","title":"3.1 env\uff08\u200b\u73af\u5883\u200b\uff09\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#32-pane","title":"3.2 pane\uff08\u200b\u7a97\u683c\u200b\uff09\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#4-visdom","title":"4 \u200b\u4f7f\u7528\u200bVisdom\u200b\u7ed8\u56fe\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#41","title":"4.1 \u200b\u57fa\u672c\u200b\u7ed8\u5236\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u56f4\u7ed5\u200b\u5e38\u89c1\u200b\u7684\u200bline\u3001image\u3001text\u200b\u7b49\u200b\u64cd\u4f5c\u200b\u8fdb\u884c\u200b\u4ecb\u7ecd\u200b\u3002</p> <p>\u200b\u8fd8\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200bvisdom\u200b\u4ec5\u4ec5\u200b\u652f\u6301\u200bPyTorch\u200b\u7684\u200btensor\u200b\u548c\u200bnumpy\u200b\u7684\u200bndarray\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\uff0c\u200b\u4e0d\u200b\u652f\u6301\u200bPython\u200b\u4e2d\u200b\u7684\u200bint\u3001float\u200b\u7b49\u200b\u7c7b\u578b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u4f20\u5165\u200b\u524d\u200b\u5e94\u8be5\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\u662f\u200btensor\u200b\u6216\u200bnumpy\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u6765\u200b\u8fd0\u884c\u200b\u4e00\u4e0b\u200b\u6700\u200b\u57fa\u7840\u200b\u7684\u200b\u793a\u4f8b\u200b\u5427\u200b~\uff01</p>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#42","title":"4.2 \u200b\u5176\u4ed6\u200b\u56fe\u8868\u200b\u7ed8\u5236\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u4e00\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b<code>Visdom</code>\u200b\u7684\u200b\u5b9e\u4f8b\u200b<code>vis</code>\u200b\u652f\u6301\u200b\u4ee5\u4e0b\u200b\u753b\u56fe\u200b\u51fd\u6570\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u63a5\u53e3\u200b\u7531\u200b<code>Plotly</code>\u200b\u6240\u200b\u63d0\u4f9b\u200b\u3002</p> <ul> <li><code>vis.scatter</code>\u00a0: \u200b\u7ed8\u5236\u200b2D \u200b\u6216\u200b 3D \u200b\u6563\u70b9\u56fe\u200b</li> <li><code>vis.line</code>\u00a0: \u200b\u7ebf\u5f62\u200b\u56fe\u200b</li> <li><code>vis.stem</code>\u00a0: \u200b\u830e\u200b\u72b6\u56fe\u200b</li> <li><code>vis.heatmap</code>\u00a0: \u200b\u70ed\u200b\u529b\u56fe\u200b</li> <li><code>vis.bar</code>\u00a0: \u200b\u67f1\u72b6\u56fe\u200b</li> <li><code>vis.histogram</code>: \u200b\u76f4\u65b9\u56fe\u200b</li> <li><code>vis.boxplot</code>\u00a0: \u200b\u7bb1\u200b\u7ebf\u56fe\u200b</li> <li><code>vis.surf</code>\u00a0: \u200b\u66f2\u9762\u200b\u56fe\u200b</li> <li><code>vis.contour</code>\u00a0: \u200b\u7b49\u9ad8\u7ebf\u56fe\u200b</li> <li><code>vis.quiver</code>\u00a0: \u200b\u6298\u7ebf\u56fe\u200b</li> <li><code>vis.mesh</code>\u00a0: \u200b\u7f51\u683c\u200b\u56fe\u200b</li> <li><code>vis.dual_axis_lines</code>\u00a0: \u200b\u53cc\u200b y \u200b\u8f74\u7ebf\u200b\u56fe\u200b</li> </ul> <p>\u200b\u4e0b\u9762\u200b\u5c06\u200b\u4ee5\u200b\u67f1\u72b6\u56fe\u200b\u7684\u200b\u7ed8\u5236\u200b\u4e3a\u4f8b\u200b\u3002</p>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#43","title":"4.3 \u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bVisdom\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b</p>"},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#5-visdom","title":"5 \u200b\u5229\u7528\u200bVisdom\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#51","title":"5.1 \u200b\u7ed8\u5236\u200b\u5b9e\u65f6\u200b\u66f2\u7ebf\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E5%88%A9%E7%94%A8Visdom%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/#52","title":"5.2 \u200b\u521d\u8bc6\u200b\u53ef\u89c6\u5316\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%85%AD%E7%AB%A0%20PyTorch%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E4%BD%BF%E7%94%A8argparse%E8%B0%83%E5%8F%82/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>import argparse\nimport json\n# todo: \u200b\u5c1d\u8bd5\u200b\u4e0b\u200bjson\u200b\u6362\u6210\u200byaml\uff0cpickle\u200b\u7b49\u5e93\u200b\n</pre> import argparse import json # todo: \u200b\u5c1d\u8bd5\u200b\u4e0b\u200bjson\u200b\u6362\u6210\u200byaml\uff0cpickle\u200b\u7b49\u5e93\u200b In\u00a0[\u00a0]: Copied! <pre>class Options():\n    \"\"\"Class to manage options using parser and namespace.\n    \"\"\"\n    def __init__(self):\n        self.initialized = False\n\n    def add_arguments_parser(self, parser: argparse.ArgumentParser):\n        \"\"\"Add a set of arguments to the parser.\n        Parameters\n        ----------\n        parser : argparse.ArgumentParser\n            Parser to add arguments to.\n        Returns\n        -------\n        parser : argparse.ArgumentParser\n            Parser with added arguments.\n        \"\"\"\n        parser.add_argument('--batch_size', type=int, default=32, help=\"input batch size,default = 64\")\n        parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train for, default=10')\n        parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed\")\n        parser.add_argument(\"--class_num\", type=int, default=30, help=\"classification category,default 10\")\n        parser.add_argument(\"--log_path\", type=str, default=\"./runs/log\")\n        parser.add_argument(\"--img_size\", type=tuple, default=(224, 224), help=\"input image size\")\n        parser.add_argument(\"--show\", action=\"store_true\", default=False)\n        self.initialized = True\n\n        return parser\n\n    def _initialize_options(self):\n        \"\"\"Initialize a namespace that store options.\n        Returns\n        -------\n        opt: argparse.Namespace\n            Namespace with options.\n        \"\"\"\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.add_arguments_parser(parser)\n            self.parser = parser\n\n        else:\n            print(\"WARNING: Options was already initialized before\")\n\n        return self.parser.parse_args()\n\n    def parse(self):\n        \"\"\"Initialize a namespace that store options.\n        Returns\n        -------\n        opt: argparse.Namespace\n            Namespace with options.\n        \"\"\"\n        opt = self._initialize_options()\n        return opt\n\n    def print_options(self, opt: argparse.Namespace):\n        \"\"\"Print all options and the default values (if changed).\n        Parameters\n        ----------\n        opt : argparse.Namespace\n            Namespace with options to print.\n        \"\"\"\n        # create a new parser with default arguments\n        parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser = self.add_arguments_parser(parser)\n\n        message = '----------------- Options ---------------\\n'\n        for key, value in sorted(vars(opt).items()):\n            comment = ''\n            default = parser.get_default(key)\n            if value != default:\n                comment = f'(default {default})'\n            key, value = str(key), str(value)\n            message += f'{key}: {value} {comment}\\n'\n        print(message)\n\n    def save_options(self, opt: argparse.Namespace, path: str):\n        \"\"\"Save options to a json file.\n        Parameters\n        ----------\n        opt : argparse.Namespace\n            Namespace with options to save.\n        path : str\n            Path to save the options (.json extension will\n            be automatically added at the end if absent).\n        \"\"\"\n        if not path.endswith('.json'):\n            path += '.json'\n        with open(path, 'w') as f:\n            f.write(json.dumps(vars(opt), indent=4))\n\n    def load_options(self, path:str):\n        # bug:\u200b\u8fd9\u200b\u73a9\u610f\u200b\u5f53\u200b\u4f60\u200b\u7684\u200brequired = True\uff0c\u200b\u4f60\u200b\u5fc5\u987b\u200b\u8f93\u5165\u200b\u5fc5\u9700\u200b\u53c2\u6570\u200b\uff0c\u200b\u4e0d\u8fc7\u200bload\u200b\u7684\u8bdd\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200bload\u200b\u8fdb\u200b\u7f51\u7edc\u200b\uff0c\u200b\u4e0d\u8981\u200bload\u200b\u8fdb\u5165\u200bargparse\uff1f\n        \"\"\"Load options from a json file.\n        Parameters\n        ----------\n        path : str\n            Path to load the options (.json extension will\n            be automatically added at the end if absent).\n        Returns\n        -------\n        opt : argparse.Namespace\n            Namespace with loaded options.\n        \"\"\"\n        if not path.endswith('.json'):\n            path += '.json'\n        # init a new namespace with default arguments\n        parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        opt = self.add_arguments_parser(parser).parse_args([])\n\n        variables = json.load(open(path, 'r'))\n        for key, value in variables.items():\n            setattr(opt, key, value)\n        print(\"--------------load options finish----------------\")\n        return opt\n</pre> class Options():     \"\"\"Class to manage options using parser and namespace.     \"\"\"     def __init__(self):         self.initialized = False      def add_arguments_parser(self, parser: argparse.ArgumentParser):         \"\"\"Add a set of arguments to the parser.         Parameters         ----------         parser : argparse.ArgumentParser             Parser to add arguments to.         Returns         -------         parser : argparse.ArgumentParser             Parser with added arguments.         \"\"\"         parser.add_argument('--batch_size', type=int, default=32, help=\"input batch size,default = 64\")         parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train for, default=10')         parser.add_argument(\"--seed\", type=int, default=66, help=\"random seed\")         parser.add_argument(\"--class_num\", type=int, default=30, help=\"classification category,default 10\")         parser.add_argument(\"--log_path\", type=str, default=\"./runs/log\")         parser.add_argument(\"--img_size\", type=tuple, default=(224, 224), help=\"input image size\")         parser.add_argument(\"--show\", action=\"store_true\", default=False)         self.initialized = True          return parser      def _initialize_options(self):         \"\"\"Initialize a namespace that store options.         Returns         -------         opt: argparse.Namespace             Namespace with options.         \"\"\"         # initialize parser with basic options         if not self.initialized:             parser = argparse.ArgumentParser(                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)             parser = self.add_arguments_parser(parser)             self.parser = parser          else:             print(\"WARNING: Options was already initialized before\")          return self.parser.parse_args()      def parse(self):         \"\"\"Initialize a namespace that store options.         Returns         -------         opt: argparse.Namespace             Namespace with options.         \"\"\"         opt = self._initialize_options()         return opt      def print_options(self, opt: argparse.Namespace):         \"\"\"Print all options and the default values (if changed).         Parameters         ----------         opt : argparse.Namespace             Namespace with options to print.         \"\"\"         # create a new parser with default arguments         parser = argparse.ArgumentParser(                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)         parser = self.add_arguments_parser(parser)          message = '----------------- Options ---------------\\n'         for key, value in sorted(vars(opt).items()):             comment = ''             default = parser.get_default(key)             if value != default:                 comment = f'(default {default})'             key, value = str(key), str(value)             message += f'{key}: {value} {comment}\\n'         print(message)      def save_options(self, opt: argparse.Namespace, path: str):         \"\"\"Save options to a json file.         Parameters         ----------         opt : argparse.Namespace             Namespace with options to save.         path : str             Path to save the options (.json extension will             be automatically added at the end if absent).         \"\"\"         if not path.endswith('.json'):             path += '.json'         with open(path, 'w') as f:             f.write(json.dumps(vars(opt), indent=4))      def load_options(self, path:str):         # bug:\u200b\u8fd9\u200b\u73a9\u610f\u200b\u5f53\u200b\u4f60\u200b\u7684\u200brequired = True\uff0c\u200b\u4f60\u200b\u5fc5\u987b\u200b\u8f93\u5165\u200b\u5fc5\u9700\u200b\u53c2\u6570\u200b\uff0c\u200b\u4e0d\u8fc7\u200bload\u200b\u7684\u8bdd\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200bload\u200b\u8fdb\u200b\u7f51\u7edc\u200b\uff0c\u200b\u4e0d\u8981\u200bload\u200b\u8fdb\u5165\u200bargparse\uff1f         \"\"\"Load options from a json file.         Parameters         ----------         path : str             Path to load the options (.json extension will             be automatically added at the end if absent).         Returns         -------         opt : argparse.Namespace             Namespace with loaded options.         \"\"\"         if not path.endswith('.json'):             path += '.json'         # init a new namespace with default arguments         parser = argparse.ArgumentParser(                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)         opt = self.add_arguments_parser(parser).parse_args([])          variables = json.load(open(path, 'r'))         for key, value in variables.items():             setattr(opt, key, value)         print(\"--------------load options finish----------------\")         return opt In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    options = Options()\n    opt = options.load_options(\"./options.json\")\n    if not opt.show:\n        options.print_options(opt)\n</pre> if __name__ == '__main__':     options = Options()     opt = options.load_options(\"./options.json\")     if not opt.show:         options.print_options(opt)"},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/","title":"LSTM \u200b\u8be6\u89e3\u200b\u4ee3\u7801","text":"In\u00a0[\u00a0]: Copied! <pre># \u200b\u5c0f\u63d0\u793a\u200b\uff1a \u200b\u6bcf\u6b21\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b\u65f6\u8bf7\u200b\u5c3d\u91cf\u200b\u5b89\u88c5\u200b\u6a21\u5757\u200b\u8fdb\u884c\u200b\n# \u200b\u6bd4\u5982\u200b\u5b8c\u6574\u200b\u7684\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b MyLSTM\u200b\u4ee5\u4e0b\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\n# \u200b\u5355\u72ec\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b cell\uff0c\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u610f\u6599\u4e4b\u5916\u200b\u7684\u200b\u60c5\u51b5\u200b\n</pre> # \u200b\u5c0f\u63d0\u793a\u200b\uff1a \u200b\u6bcf\u6b21\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b\u65f6\u8bf7\u200b\u5c3d\u91cf\u200b\u5b89\u88c5\u200b\u6a21\u5757\u200b\u8fdb\u884c\u200b # \u200b\u6bd4\u5982\u200b\u5b8c\u6574\u200b\u7684\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b MyLSTM\u200b\u4ee5\u4e0b\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b # \u200b\u5355\u72ec\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b cell\uff0c\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u610f\u6599\u4e4b\u5916\u200b\u7684\u200b\u60c5\u51b5\u200b In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[1]: Copied! <pre># \u200b\u501f\u52a9\u200b torchtext \u200b\u52a0\u8f7d\u200b IMDB \u200b\u6570\u636e\u200b\u96c6\u200b\n</pre> # \u200b\u501f\u52a9\u200b torchtext \u200b\u52a0\u8f7d\u200b IMDB \u200b\u6570\u636e\u200b\u96c6\u200b In\u00a0[2]: Copied! <pre># dataset import\n# !pip install torchtext torchdata\nfrom torchtext.datasets import IMDB\nfrom torchtext.datasets.imdb import NUM_LINES\nfrom torchtext.data import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.functional import to_map_style_dataset\nimport torch\nimport torch.nn as nn\nfrom torch import utils\nimport torch.nn.functional as F\n\n# log \u200b\u4ee5\u53ca\u200b\u5de5\u5177\u200b\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport sys\nimport logging\nlogging.basicConfig(\n    level=logging.WARN, stream=sys.stdout, \\\n    format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\")\n\n# \u200b\u8bbe\u5907\u200b \u200b\u65e0\u200b\u663e\u5361\u200b\u4f1a\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b cpu\ndevice = 'cuda'\n</pre> # dataset import # !pip install torchtext torchdata from torchtext.datasets import IMDB from torchtext.datasets.imdb import NUM_LINES from torchtext.data import get_tokenizer from torchtext.vocab import build_vocab_from_iterator from torchtext.data.functional import to_map_style_dataset import torch import torch.nn as nn from torch import utils import torch.nn.functional as F  # log \u200b\u4ee5\u53ca\u200b\u5de5\u5177\u200b import numpy as np from tqdm import tqdm import os import sys import logging logging.basicConfig(     level=logging.WARN, stream=sys.stdout, \\     format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\")  # \u200b\u8bbe\u5907\u200b \u200b\u65e0\u200b\u663e\u5361\u200b\u4f1a\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b cpu device = 'cuda'  In\u00a0[3]: Copied! <pre>def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(1998)\n</pre> def seed_everything(seed=42):     os.environ['PYTHONHASHSEED'] = str(seed)     np.random.seed(seed)     torch.manual_seed(seed)     torch.cuda.manual_seed(seed)     torch.backends.cudnn.deterministic = True   seed_everything(1998) In\u00a0[4]: Copied! <pre>train_data_iter = IMDB(root=\"./data\", split=\"train\")\n</pre> train_data_iter = IMDB(root=\"./data\", split=\"train\")  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre># \u200b\u5bf9\u200b\u8f93\u5165\u200b\u6587\u672c\u200b\u8fdb\u884c\u200b\u5206\u5272\u200b\uff0c\u200b\u8fd4\u56de\u503c\u200b\u662f\u200b \u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\ndef yeild_tokens(train_data_iter, tokenizer):\n    for i, sample in enumerate(train_data_iter):\n        label, comment = sample\n        # \u200b\u6253\u5f00\u200b cell 4 \u200b\u4e2d\u200b\u7684\u200b\u6ce8\u91ca\u200b\u65f6\u8bf7\u200b\u6ce8\u610f\u200b\u5207\u6362\u200b\u6b64\u5904\u200b\u6ce8\u91ca\u200b\n        yield tokenizer(comment) \n        # return tokenizer(comment)\n</pre> # \u200b\u5bf9\u200b\u8f93\u5165\u200b\u6587\u672c\u200b\u8fdb\u884c\u200b\u5206\u5272\u200b\uff0c\u200b\u8fd4\u56de\u503c\u200b\u662f\u200b \u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b def yeild_tokens(train_data_iter, tokenizer):     for i, sample in enumerate(train_data_iter):         label, comment = sample         # \u200b\u6253\u5f00\u200b cell 4 \u200b\u4e2d\u200b\u7684\u200b\u6ce8\u91ca\u200b\u65f6\u8bf7\u200b\u6ce8\u610f\u200b\u5207\u6362\u200b\u6b64\u5904\u200b\u6ce8\u91ca\u200b         yield tokenizer(comment)          # return tokenizer(comment) In\u00a0[6]: Copied! <pre># \u200b\u8bf7\u200b\u6253\u5f00\u200b\u4e0b\u65b9\u200b\u6ce8\u91ca\u200b\u7406\u89e3\u200b yeild_tokens \n# x = yeild_tokens(train_data_iter, tokenizer)\n# x\n</pre> # \u200b\u8bf7\u200b\u6253\u5f00\u200b\u4e0b\u65b9\u200b\u6ce8\u91ca\u200b\u7406\u89e3\u200b yeild_tokens  # x = yeild_tokens(train_data_iter, tokenizer) # x In\u00a0[7]: Copied! <pre># \u200b\u5206\u8bcd\u200b\u3001\u200b\u6784\u5efa\u200b\u8bcd\u8868\u200b \u200b\u8fd9\u91cc\u200b\u7b2c\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8f83\u957f\u65f6\u95f4\u200b\ntokenizer = get_tokenizer(\"basic_english\")\n# \u200b\u53ea\u200b\u4f7f\u7528\u200b\u51fa\u73b0\u200b\u6b21\u6570\u200b\u5927\u4e8e\u200b20\u200b\u7684\u200btoken\nvocab = build_vocab_from_iterator(yeild_tokens(train_data_iter, tokenizer), min_freq=20, specials=[\"&lt;unk&gt;\"])\nvocab.set_default_index(0)  # \u200b\u7279\u6b8a\u200b\u7d22\u5f15\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\nprint(f'\u200b\u5355\u8bcd\u8868\u200b\u5927\u5c0f\u200b: {len(vocab)}')\n</pre> # \u200b\u5206\u8bcd\u200b\u3001\u200b\u6784\u5efa\u200b\u8bcd\u8868\u200b \u200b\u8fd9\u91cc\u200b\u7b2c\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8f83\u957f\u65f6\u95f4\u200b tokenizer = get_tokenizer(\"basic_english\") # \u200b\u53ea\u200b\u4f7f\u7528\u200b\u51fa\u73b0\u200b\u6b21\u6570\u200b\u5927\u4e8e\u200b20\u200b\u7684\u200btoken vocab = build_vocab_from_iterator(yeild_tokens(train_data_iter, tokenizer), min_freq=20, specials=[\"\"]) vocab.set_default_index(0)  # \u200b\u7279\u6b8a\u200b\u7d22\u5f15\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0 print(f'\u200b\u5355\u8bcd\u8868\u200b\u5927\u5c0f\u200b: {len(vocab)}') <pre>\u200b\u5355\u8bcd\u8868\u200b\u5927\u5c0f\u200b: 13351\n</pre> In\u00a0[8]: Copied! <pre># \u200b\u6784\u5efa\u200b\u8bcd\u200b\u5411\u91cf\u200b -- \u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b word2vec \u200b\u6216\u8005\u200b glovec \u200b\u7b49\u200b\u65b9\u5f0f\u200b\u6765\u200b\u66ff\u4ee3\u200b\u968f\u673a\u200b\u751f\u6210\u200b\nembedding = nn.Embedding(len(vocab), 64)\n# a = torch.LongTensor([0])\n# a\n# embedding(a)\n</pre> # \u200b\u6784\u5efa\u200b\u8bcd\u200b\u5411\u91cf\u200b -- \u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b word2vec \u200b\u6216\u8005\u200b glovec \u200b\u7b49\u200b\u65b9\u5f0f\u200b\u6765\u200b\u66ff\u4ee3\u200b\u968f\u673a\u200b\u751f\u6210\u200b embedding = nn.Embedding(len(vocab), 64) # a = torch.LongTensor([0]) # a # embedding(a) In\u00a0[9]: Copied! <pre># \u200b\u7531\u4e8e\u200b LSTM \u200b\u8fd9\u91cc\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b batch \u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\n# \u200b\u9488\u5bf9\u200b\u540c\u4e00\u200b batch \u200b\u4e2d\u200b \u200b\u53e5\u5b50\u200b\u957f\u5ea6\u200b\u4e0d\u540c\u200b\u7684\u200b\u60c5\u51b5\u200b\n# \u200b\u6bcf\u6b21\u200b\u90fd\u200b\u5bf9\u200b\u957f\u5ea6\u200b\u4e0d\u8db3\u200b\u7684\u200b\u53e5\u5b50\u200b\u8fdb\u884c\u200b padding\n# \u200b\u53e6\u5916\u200b\u5c06\u200b\u6807\u8bb0\u200b\u7531\u200b 1\u30012 \u200b\u4fee\u6539\u200b\u4e3a\u200b 0\u30011\ndef collate_fn(batch):\n    \"\"\"\n    \u200b\u5bf9\u200bDataLoader\u200b\u6240\u200b\u751f\u6210\u200b\u7684\u200bmini-batch\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b\n    \"\"\"\n    # print(batch)\n    target = []\n    token_index = []\n    max_length = 0  # \u200b\u6700\u5927\u200b\u7684\u200btoken\u200b\u957f\u5ea6\u200b\n    for i, (label, comment) in enumerate(batch):\n        tokens = tokenizer(comment)\n        # print(tokens)\n        # print(vocab(tokens))\n        token_index.append(vocab(tokens)) # \u200b\u5b57\u7b26\u200b\u5217\u8868\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7d22\u5f15\u200b\u5217\u8868\u200b\n        \n        # \u200b\u786e\u5b9a\u200b\u6700\u5927\u200b\u7684\u200b\u53e5\u5b50\u200b\u957f\u5ea6\u200b\n        if len(tokens) &gt; max_length:\n            max_length = len(tokens)\n        # \u200b\u8bbe\u5b9a\u200b\u76ee\u6807\u200b label 1\u200b\u6807\u8bb0\u200b\u4e3a\u200b 0  2\u200b\u6807\u8bb0\u200b\u4e3a\u200b 1\n        if label == 1:\n            target.append(0)\n        else:\n            target.append(1)\n    # print(token_index)\n    \n    # padding \u200b\u5230\u200b\u6700\u957f\u200b\u957f\u5ea6\u200b\n    token_index = [index + [0]*(max_length-len(index)) for index in token_index]\n    # \u200b\u8bcd\u5411\u200b\u91cf\u5316\u200b\n    token_index = embedding(torch.tensor(token_index).to(torch.int32))\n    # print(token_index.shape)\n    # one-hot\u200b\u63a5\u6536\u200b\u957f\u200b\u6574\u5f62\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8981\u200b\u8f6c\u6362\u200b\u4e3a\u200bint64\n    return (torch.tensor(target).to(torch.int64),token_index )\n</pre> # \u200b\u7531\u4e8e\u200b LSTM \u200b\u8fd9\u91cc\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b batch \u200b\u8fdb\u884c\u200b\u5904\u7406\u200b # \u200b\u9488\u5bf9\u200b\u540c\u4e00\u200b batch \u200b\u4e2d\u200b \u200b\u53e5\u5b50\u200b\u957f\u5ea6\u200b\u4e0d\u540c\u200b\u7684\u200b\u60c5\u51b5\u200b # \u200b\u6bcf\u6b21\u200b\u90fd\u200b\u5bf9\u200b\u957f\u5ea6\u200b\u4e0d\u8db3\u200b\u7684\u200b\u53e5\u5b50\u200b\u8fdb\u884c\u200b padding # \u200b\u53e6\u5916\u200b\u5c06\u200b\u6807\u8bb0\u200b\u7531\u200b 1\u30012 \u200b\u4fee\u6539\u200b\u4e3a\u200b 0\u30011 def collate_fn(batch):     \"\"\"     \u200b\u5bf9\u200bDataLoader\u200b\u6240\u200b\u751f\u6210\u200b\u7684\u200bmini-batch\u200b\u8fdb\u884c\u200b\u540e\u5904\u7406\u200b     \"\"\"     # print(batch)     target = []     token_index = []     max_length = 0  # \u200b\u6700\u5927\u200b\u7684\u200btoken\u200b\u957f\u5ea6\u200b     for i, (label, comment) in enumerate(batch):         tokens = tokenizer(comment)         # print(tokens)         # print(vocab(tokens))         token_index.append(vocab(tokens)) # \u200b\u5b57\u7b26\u200b\u5217\u8868\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7d22\u5f15\u200b\u5217\u8868\u200b                  # \u200b\u786e\u5b9a\u200b\u6700\u5927\u200b\u7684\u200b\u53e5\u5b50\u200b\u957f\u5ea6\u200b         if len(tokens) &gt; max_length:             max_length = len(tokens)         # \u200b\u8bbe\u5b9a\u200b\u76ee\u6807\u200b label 1\u200b\u6807\u8bb0\u200b\u4e3a\u200b 0  2\u200b\u6807\u8bb0\u200b\u4e3a\u200b 1         if label == 1:             target.append(0)         else:             target.append(1)     # print(token_index)          # padding \u200b\u5230\u200b\u6700\u957f\u200b\u957f\u5ea6\u200b     token_index = [index + [0]*(max_length-len(index)) for index in token_index]     # \u200b\u8bcd\u5411\u200b\u91cf\u5316\u200b     token_index = embedding(torch.tensor(token_index).to(torch.int32))     # print(token_index.shape)     # one-hot\u200b\u63a5\u6536\u200b\u957f\u200b\u6574\u5f62\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8981\u200b\u8f6c\u6362\u200b\u4e3a\u200bint64     return (torch.tensor(target).to(torch.int64),token_index )   In\u00a0[10]: Copied! <pre># \u200b\u7406\u89e3\u200b\u4e0a\u65b9\u200b\u4ee3\u7801\u200b\u8bf7\u200b\u6253\u5f00\u200b\u4e0b\u65b9\u200b\u6ce8\u91ca\u200b\uff0c\u200b\u5176\u4e2d\u200b batch_size &gt;= 2 \u200b\u65f6\u200b \u200b\u8bf7\u200b\u6ce8\u610f\u200b\u6700\u5927\u200b\u957f\u5ea6\u200b\u586b\u5145\u200b\u3002\n\n# for batch_index, (target, token_index) in enumerate(train_data_loader):\n#     print(batch_index)\n#     print(target)\n#     print(token_index)\n#     # (batch_size, seq_len, input_size)\n#     print(token_index.shape)\n#     break\n</pre> # \u200b\u7406\u89e3\u200b\u4e0a\u65b9\u200b\u4ee3\u7801\u200b\u8bf7\u200b\u6253\u5f00\u200b\u4e0b\u65b9\u200b\u6ce8\u91ca\u200b\uff0c\u200b\u5176\u4e2d\u200b batch_size &gt;= 2 \u200b\u65f6\u200b \u200b\u8bf7\u200b\u6ce8\u610f\u200b\u6700\u5927\u200b\u957f\u5ea6\u200b\u586b\u5145\u200b\u3002  # for batch_index, (target, token_index) in enumerate(train_data_loader): #     print(batch_index) #     print(target) #     print(token_index) #     # (batch_size, seq_len, input_size) #     print(token_index.shape) #     break  In\u00a0[11]: Copied! <pre># \u200b\u5b9a\u4e49\u200b\u8d85\u200b\u53c2\u6570\u200b\ninput_size = 64 #\nhidden_size = 128 \nnum_layers = 1\nnum_classes = 2\nbatch_size = 32\nmax_seq_len = 512\nlearning_rate = 0.01\n</pre> # \u200b\u5b9a\u4e49\u200b\u8d85\u200b\u53c2\u6570\u200b input_size = 64 # hidden_size = 128  num_layers = 1 num_classes = 2 batch_size = 32 max_seq_len = 512 learning_rate = 0.01 In\u00a0[12]: Copied! <pre># Train Dataloader\ntrain_data_iter = IMDB(root=\"data\", split=\"train\")\ntrain_data_loader = torch.utils.data.DataLoader(\n    to_map_style_dataset(train_data_iter), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n\n# Eval Dataloader\neval_data_iter = IMDB(root=\"data\", split=\"test\")\neval_data_loader = utils.data.DataLoader(\n    to_map_style_dataset(eval_data_iter), batch_size=batch_size, collate_fn=collate_fn)\n\n#to_map_style_dataset \u200b\u53ef\u4ee5\u200b\u81ea\u884c\u200b\u767e\u5ea6\u200b\n</pre> # Train Dataloader train_data_iter = IMDB(root=\"data\", split=\"train\") train_data_loader = torch.utils.data.DataLoader(     to_map_style_dataset(train_data_iter), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)  # Eval Dataloader eval_data_iter = IMDB(root=\"data\", split=\"test\") eval_data_loader = utils.data.DataLoader(     to_map_style_dataset(eval_data_iter), batch_size=batch_size, collate_fn=collate_fn)  #to_map_style_dataset \u200b\u53ef\u4ee5\u200b\u81ea\u884c\u200b\u767e\u5ea6\u200b In\u00a0[4]: Copied! <pre>hideen_size_temp = 3\n</pre> hideen_size_temp = 3 In\u00a0[5]: Copied! <pre>sigmoid = nn.Sigmoid()\nsigmoid\n</pre> sigmoid = nn.Sigmoid() sigmoid Out[5]: <pre>Sigmoid()</pre> In\u00a0[17]: Copied! <pre>hid = torch.ones(hideen_size_temp, hideen_size_temp)\nhid\n</pre> hid = torch.ones(hideen_size_temp, hideen_size_temp) hid Out[17]: <pre>tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])</pre> In\u00a0[18]: Copied! <pre>model =  nn.Linear(hideen_size_temp, hideen_size_temp)\nmodel\n</pre> model =  nn.Linear(hideen_size_temp, hideen_size_temp) model Out[18]: <pre>Linear(in_features=3, out_features=3, bias=True)</pre> In\u00a0[21]: Copied! <pre>mid_output = model(x)\nmid_output\n</pre> mid_output = model(x) mid_output Out[21]: <pre>tensor([[-0.2625,  0.3412, -0.5055],\n        [-0.2625,  0.3412, -0.5055]], grad_fn=&lt;AddmmBackward0&gt;)</pre> In\u00a0[22]: Copied! <pre>gate = sigmoid(mid_output)\ngate\n</pre> gate = sigmoid(mid_output) gate Out[22]: <pre>tensor([[0.4347, 0.5845, 0.3762],\n        [0.4347, 0.5845, 0.3762]], grad_fn=&lt;SigmoidBackward0&gt;)</pre> In\u00a0[23]: Copied! <pre>final_output = gate * mid_output\nfinal_output\n</pre> final_output = gate * mid_output final_output Out[23]: <pre>tensor([[-0.1141,  0.1994, -0.1902],\n        [-0.1141,  0.1994, -0.1902]], grad_fn=&lt;MulBackward0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre># \u200b\u518d\u6b21\u200b\u5b9a\u4e49\u200b\u8d85\u200b\u53c2\u6570\u200b \u200b\u907f\u514d\u200b\u5e72\u6270\u200b\u4e0b\u65b9\u200b\u8bad\u7ec3\u200b\ninput_size = 64 #\nhidden_size = 128 \nnum_layers = 1\nnum_classes = 2\nbatch_size = 32\nmax_seq_len = 512\nlearning_rate = 0.01\n</pre> # \u200b\u518d\u6b21\u200b\u5b9a\u4e49\u200b\u8d85\u200b\u53c2\u6570\u200b \u200b\u907f\u514d\u200b\u5e72\u6270\u200b\u4e0b\u65b9\u200b\u8bad\u7ec3\u200b input_size = 64 # hidden_size = 128  num_layers = 1 num_classes = 2 batch_size = 32 max_seq_len = 512 learning_rate = 0.01 \u200b\u57fa\u672c\u200b\u65e0\u9700\u200b\u4fee\u6539\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6548\u679c\u200b\u5c1a\u53ef\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u5b98\u65b9\u200b\u5b9e\u73b0\u200b\u7565\u6709\u200b\u6b20\u7f3a\u200b\uff0c\u200b\u4f46\u662f\u200b\u80fd\u200b\u8bc1\u660e\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6709\u6548\u200b\u7684\u200b\u6a21\u578b\u200b In\u00a0[13]: Copied! <pre># \u200b\u5b9a\u4e49\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        \"\"\"\n        args:\n            input_size: \u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\n            hidden_size: \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b\n            num_layers: \u200b\u51e0\u5c42\u200b\u7684\u200bLSTM\n            num_classes: \u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b 0 \u200b\u6216\u8005\u200b 1\n            \n        \"\"\"\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.fc_i = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_f = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_g = nn.Linear(input_size + hidden_size, hidden_size)\n        self.fc_o = nn.Linear(input_size + hidden_size, hidden_size)\n        self.sigmoid = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n        self.fc_out = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        # shape (batch_size, seq_len, input_size)\n        # print(x.shape)\n        h_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)\n        c_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)\n        # print(h_t.shape)\n        # print(c_t.shape)\n        combined = torch.cat((x, h_t), dim=2)\n        i_t = self.sigmoid(self.fc_i(combined))\n        f_t = self.sigmoid(self.fc_f(combined))\n        g_t = self.tanh(self.fc_g(combined))\n        o_t = self.sigmoid(self.fc_o(combined))\n        c_t = f_t * c_t + i_t * g_t\n        h_t = o_t * self.tanh(c_t)\n            \n#         print(x.shape)\n#         print(combined.shape)\n#         print(i_t.shape)\n#         print(f_t.shape)\n#         print(g_t.shape)\n#         print(o_t.shape)\n#         print(h_t.shape)\n        h_t = F.avg_pool2d(h_t, (h_t.shape[1],1)).squeeze()\n        out = self.fc_out(h_t)\n#         print(out.cpu().shape)\n        return out\n</pre> # \u200b\u5b9a\u4e49\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b class LSTM(nn.Module):     def __init__(self, input_size, hidden_size, num_layers, num_classes):         \"\"\"         args:             input_size: \u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b             hidden_size: \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b             num_layers: \u200b\u51e0\u5c42\u200b\u7684\u200bLSTM             num_classes: \u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b 0 \u200b\u6216\u8005\u200b 1                      \"\"\"         super(LSTM, self).__init__()         self.input_size = input_size         self.hidden_size = hidden_size         self.num_layers = num_layers         self.fc_i = nn.Linear(input_size + hidden_size, hidden_size)         self.fc_f = nn.Linear(input_size + hidden_size, hidden_size)         self.fc_g = nn.Linear(input_size + hidden_size, hidden_size)         self.fc_o = nn.Linear(input_size + hidden_size, hidden_size)         self.sigmoid = nn.Sigmoid()         self.tanh = nn.Tanh()         self.fc_out = nn.Linear(hidden_size, num_classes)     def forward(self, x):         # shape (batch_size, seq_len, input_size)         # print(x.shape)         h_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)         c_t = torch.zeros(x.size(0), x.size(1), self.hidden_size).to(x.device)         # print(h_t.shape)         # print(c_t.shape)         combined = torch.cat((x, h_t), dim=2)         i_t = self.sigmoid(self.fc_i(combined))         f_t = self.sigmoid(self.fc_f(combined))         g_t = self.tanh(self.fc_g(combined))         o_t = self.sigmoid(self.fc_o(combined))         c_t = f_t * c_t + i_t * g_t         h_t = o_t * self.tanh(c_t)              #         print(x.shape) #         print(combined.shape) #         print(i_t.shape) #         print(f_t.shape) #         print(g_t.shape) #         print(o_t.shape) #         print(h_t.shape)         h_t = F.avg_pool2d(h_t, (h_t.shape[1],1)).squeeze()         out = self.fc_out(h_t) #         print(out.cpu().shape)         return out In\u00a0[14]: Copied! <pre># \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u95ee\u9898\u200b\n# \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\u79cd\u5b50\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u7ed3\u679c\u200b\u53ef\u200b\u91cd\u590d\u200b\ntorch.manual_seed(2023)\n\n# \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b\nx = torch.randn(batch_size, max_seq_len, input_size).to(device)\ny = torch.randint(0, num_classes, (batch_size,)).to(device)\n\n# \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\nmodel = LSTM(input_size, hidden_size, num_layers, num_classes)\nmodel.to(device)\n# \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\nprint(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n# \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\noutput = model(x)\n# output\n</pre> # \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u95ee\u9898\u200b # \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\u79cd\u5b50\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u7ed3\u679c\u200b\u53ef\u200b\u91cd\u590d\u200b torch.manual_seed(2023)  # \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b x = torch.randn(batch_size, max_seq_len, input_size).to(device) y = torch.randint(0, num_classes, (batch_size,)).to(device)  # \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b model = LSTM(input_size, hidden_size, num_layers, num_classes) model.to(device) # \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b print(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))  # \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b output = model(x) # output <pre>\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a 99074\n</pre> In\u00a0[15]: Copied! <pre># \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\nmodel\n</pre> # \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b model Out[15]: <pre>LSTM(\n  (fc_i): Linear(in_features=192, out_features=128, bias=True)\n  (fc_f): Linear(in_features=192, out_features=128, bias=True)\n  (fc_g): Linear(in_features=192, out_features=128, bias=True)\n  (fc_o): Linear(in_features=192, out_features=128, bias=True)\n  (sigmoid): Sigmoid()\n  (tanh): Tanh()\n  (fc_out): Linear(in_features=128, out_features=2, bias=True)\n)</pre> In\u00a0[16]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncur_epoch = -1\nfor epoch_index in range(0,6):\n    num_batches = len(train_data_loader)\n    for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):\n        model.train()\n        optimizer.zero_grad()\n        target = target.to(device)\n        token_index = token_index.to(device)\n        step = num_batches*(epoch_index) + batch_index + 1   \n        logits = model(token_index)\n        loss = F.nll_loss(F.log_softmax(logits,dim=-1), target)\n        loss.backward()\n        optimizer.step()\n        if batch_index % 300 == 0:\n            cur_epoch = epoch_index\n            logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\")\n</pre> optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) cur_epoch = -1 for epoch_index in range(0,6):     num_batches = len(train_data_loader)     for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):         model.train()         optimizer.zero_grad()         target = target.to(device)         token_index = token_index.to(device)         step = num_batches*(epoch_index) + batch_index + 1            logits = model(token_index)         loss = F.nll_loss(F.log_softmax(logits,dim=-1), target)         loss.backward()         optimizer.step()         if batch_index % 300 == 0:             cur_epoch = epoch_index             logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\")  <pre>0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:37:03,079 (233408485:20) WARNING: epoch_index: 0, batch_index: 0, loss: 0.6886175274848938\n</pre> <pre>298it [00:07, 38.08it/s]</pre> <pre>2023-03-17 00:37:10,820 (233408485:20) WARNING: epoch_index: 0, batch_index: 300, loss: 0.6428654193878174\n</pre> <pre>599it [00:15, 39.36it/s]</pre> <pre>2023-03-17 00:37:18,390 (233408485:20) WARNING: epoch_index: 0, batch_index: 600, loss: 0.48170384764671326\n</pre> <pre>782it [00:20, 39.09it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:37:23,065 (233408485:20) WARNING: epoch_index: 1, batch_index: 0, loss: 0.44872817397117615\n</pre> <pre>300it [00:07, 37.30it/s]</pre> <pre>2023-03-17 00:37:30,612 (233408485:20) WARNING: epoch_index: 1, batch_index: 300, loss: 0.5459613800048828\n</pre> <pre>600it [00:15, 34.74it/s]</pre> <pre>2023-03-17 00:37:38,373 (233408485:20) WARNING: epoch_index: 1, batch_index: 600, loss: 0.41060134768486023\n</pre> <pre>782it [00:20, 39.04it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:37:43,096 (233408485:20) WARNING: epoch_index: 2, batch_index: 0, loss: 0.25612199306488037\n</pre> <pre>297it [00:07, 38.88it/s]</pre> <pre>2023-03-17 00:37:51,098 (233408485:20) WARNING: epoch_index: 2, batch_index: 300, loss: 0.33394932746887207\n</pre> <pre>596it [00:15, 41.05it/s]</pre> <pre>2023-03-17 00:37:58,503 (233408485:20) WARNING: epoch_index: 2, batch_index: 600, loss: 0.4152831733226776\n</pre> <pre>782it [00:19, 39.31it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:38:02,998 (233408485:20) WARNING: epoch_index: 3, batch_index: 0, loss: 0.31165584921836853\n</pre> <pre>296it [00:07, 41.12it/s]</pre> <pre>2023-03-17 00:38:10,474 (233408485:20) WARNING: epoch_index: 3, batch_index: 300, loss: 0.348602294921875\n</pre> <pre>600it [00:15, 35.43it/s]</pre> <pre>2023-03-17 00:38:18,298 (233408485:20) WARNING: epoch_index: 3, batch_index: 600, loss: 0.22047576308250427\n</pre> <pre>782it [00:20, 38.53it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:38:23,300 (233408485:20) WARNING: epoch_index: 4, batch_index: 0, loss: 0.3187304735183716\n</pre> <pre>300it [00:07, 39.59it/s]</pre> <pre>2023-03-17 00:38:31,194 (233408485:20) WARNING: epoch_index: 4, batch_index: 300, loss: 0.17088855803012848\n</pre> <pre>600it [00:15, 36.99it/s]</pre> <pre>2023-03-17 00:38:39,150 (233408485:20) WARNING: epoch_index: 4, batch_index: 600, loss: 0.4386771321296692\n</pre> <pre>782it [00:20, 38.19it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:38:43,787 (233408485:20) WARNING: epoch_index: 5, batch_index: 0, loss: 0.2063377946615219\n</pre> <pre>300it [00:07, 39.54it/s]</pre> <pre>2023-03-17 00:38:51,578 (233408485:20) WARNING: epoch_index: 5, batch_index: 300, loss: 0.3431491255760193\n</pre> <pre>600it [00:15, 41.39it/s]</pre> <pre>2023-03-17 00:38:59,081 (233408485:20) WARNING: epoch_index: 5, batch_index: 600, loss: 0.2605396807193756\n</pre> <pre>782it [00:19, 39.37it/s]\n</pre> In\u00a0[17]: Copied! <pre>model.eval()\ntotal_acc_account = 0\ntotal_account = 0\nfor eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):\n    eval_target = eval_target.to(device)\n    eval_token_index = eval_token_index.to(device)\n    total_account += eval_target.shape[0]\n    eval_logits = model(eval_token_index)\n    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n    eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target)\nlogging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\")\n</pre> model.eval() total_acc_account = 0 total_account = 0 for eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):     eval_target = eval_target.to(device)     eval_token_index = eval_token_index.to(device)     total_account += eval_target.shape[0]     eval_logits = model(eval_token_index)     total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()     eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target) logging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\") <pre>782it [00:10, 71.24it/s]</pre> <pre>2023-03-17 00:39:14,595 (612167170:11) WARNING: eval_loss: 0.6957253813743591, eval_acc: 0.86988\n</pre> <pre>\n</pre> \u200b\u53ef\u4ee5\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u65f6\u5019\u200b\u7684\u200b epoch \u200b\u6570\u5b57\u200b\uff0c\u200b\u5728\u200b\u5f53\u524d\u200b\u4ee3\u7801\u200b\u4e0b\u200b\uff0c3 \u200b\u8bad\u7ec3\u200b\u4e0d\u8db3\u200b\uff0c 6\u200b\u5bb9\u6613\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u4f46\u662f\u200b\u53ef\u4ee5\u200b\u8f85\u52a9\u200b\u8bc1\u660e\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u73b0\u200b\u6ca1\u6709\u200b\u95ee\u9898\u200b In\u00a0[18]: Copied! <pre>class LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        \"\"\"\n        args:\n            input_size: \u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\n            hidden_size: \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b\n            num_layers: \u200b\u51e0\u5c42\u200b\u7684\u200bLSTM\n            num_classes: \u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b 0 \u200b\u6216\u8005\u200b 1\n            \n        \"\"\"\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers=num_layers,batch_first=True,bidirectional=False)\n        self.fc_out = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        x,_ = self.lstm(x)\n        x = F.avg_pool2d(x, (x.shape[1],1)).squeeze()\n        out = self.fc_out(x)\n        return out\n</pre> class LSTM(nn.Module):     def __init__(self, input_size, hidden_size, num_layers, num_classes):         \"\"\"         args:             input_size: \u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b             hidden_size: \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b             num_layers: \u200b\u51e0\u5c42\u200b\u7684\u200bLSTM             num_classes: \u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b 0 \u200b\u6216\u8005\u200b 1                      \"\"\"         super(LSTM, self).__init__()         self.input_size = input_size         self.hidden_size = hidden_size         self.num_layers = num_layers         self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers=num_layers,batch_first=True,bidirectional=False)         self.fc_out = nn.Linear(hidden_size, num_classes)     def forward(self, x):         x,_ = self.lstm(x)         x = F.avg_pool2d(x, (x.shape[1],1)).squeeze()         out = self.fc_out(x)         return out In\u00a0[19]: Copied! <pre># \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u95ee\u9898\u200b\n# \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\u79cd\u5b50\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u7ed3\u679c\u200b\u53ef\u200b\u91cd\u590d\u200b\n\n\n\n# \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b\nx = torch.randn(batch_size, max_seq_len, input_size).to(device)\ny = torch.randint(0, num_classes, (batch_size,)).to(device)\n\n# \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\nmodel = LSTM(input_size, hidden_size, num_layers, num_classes)\nmodel.to(device)\n# \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\nprint(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n# \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\noutput = model(x)\n# output\n</pre> # \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u95ee\u9898\u200b # \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u6570\u200b\u79cd\u5b50\u200b\u4ee5\u200b\u4fdd\u8bc1\u200b\u7ed3\u679c\u200b\u53ef\u200b\u91cd\u590d\u200b    # \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b x = torch.randn(batch_size, max_seq_len, input_size).to(device) y = torch.randint(0, num_classes, (batch_size,)).to(device)  # \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b model = LSTM(input_size, hidden_size, num_layers, num_classes) model.to(device) # \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b print(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))  # \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b output = model(x) # output <pre>\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a 99586\n</pre> In\u00a0[20]: Copied! <pre># \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\nmodel\n</pre> # \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b model Out[20]: <pre>LSTM(\n  (lstm): LSTM(64, 128, batch_first=True)\n  (fc_out): Linear(in_features=128, out_features=2, bias=True)\n)</pre> In\u00a0[21]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncur_epoch = -1\nfor epoch_index in range(0,3):\n    num_batches = len(train_data_loader)\n    for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):\n        model.train()\n        optimizer.zero_grad()\n        target = target.to(device)\n        token_index = token_index.to(device)\n        step = num_batches*(epoch_index) + batch_index + 1   \n        logits = model(token_index)\u3011\n        loss = F.nll_loss(F.log_softmax(logits,dim=-1), target)\n        loss.backward()\n        # nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # \u200b\u68af\u5ea6\u200b\u7684\u200b\u6b63\u5219\u200b\u8fdb\u884c\u200b\u622a\u65ad\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u8bad\u7ec3\u200b\u7a33\u5b9a\u200b\n        optimizer.step()\n        if batch_index % 300 == 0:\n            cur_epoch = epoch_index\n            logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\")\n</pre> optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) cur_epoch = -1 for epoch_index in range(0,3):     num_batches = len(train_data_loader)     for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):         model.train()         optimizer.zero_grad()         target = target.to(device)         token_index = token_index.to(device)         step = num_batches*(epoch_index) + batch_index + 1            logits = model(token_index)\u3011         loss = F.nll_loss(F.log_softmax(logits,dim=-1), target)         loss.backward()         # nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # \u200b\u68af\u5ea6\u200b\u7684\u200b\u6b63\u5219\u200b\u8fdb\u884c\u200b\u622a\u65ad\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u8bad\u7ec3\u200b\u7a33\u5b9a\u200b         optimizer.step()         if batch_index % 300 == 0:             cur_epoch = epoch_index             logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\") <pre>0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:39:14,813 (3862857436:23) WARNING: epoch_index: 0, batch_index: 0, loss: 0.6966263055801392\n</pre> <pre>299it [00:08, 38.52it/s]</pre> <pre>2023-03-17 00:39:23,082 (3862857436:23) WARNING: epoch_index: 0, batch_index: 300, loss: 0.5508784055709839\n</pre> <pre>599it [00:16, 36.47it/s]</pre> <pre>2023-03-17 00:39:31,275 (3862857436:23) WARNING: epoch_index: 0, batch_index: 600, loss: 0.5803604125976562\n</pre> <pre>782it [00:21, 36.57it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:39:36,188 (3862857436:23) WARNING: epoch_index: 1, batch_index: 0, loss: 0.40404248237609863\n</pre> <pre>298it [00:08, 34.45it/s]</pre> <pre>2023-03-17 00:39:44,230 (3862857436:23) WARNING: epoch_index: 1, batch_index: 300, loss: 0.28521886467933655\n</pre> <pre>600it [00:16, 37.37it/s]</pre> <pre>2023-03-17 00:39:52,248 (3862857436:23) WARNING: epoch_index: 1, batch_index: 600, loss: 0.2420959770679474\n</pre> <pre>782it [00:20, 37.49it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:39:57,044 (3862857436:23) WARNING: epoch_index: 2, batch_index: 0, loss: 0.40587106347084045\n</pre> <pre>296it [00:08, 35.26it/s]</pre> <pre>2023-03-17 00:40:05,171 (3862857436:23) WARNING: epoch_index: 2, batch_index: 300, loss: 0.4198710024356842\n</pre> <pre>599it [00:16, 37.80it/s]</pre> <pre>2023-03-17 00:40:13,358 (3862857436:23) WARNING: epoch_index: 2, batch_index: 600, loss: 0.33481597900390625\n</pre> <pre>782it [00:21, 36.72it/s]\n</pre> In\u00a0[22]: Copied! <pre>model.eval()\ntotal_acc_account = 0\ntotal_account = 0\nfor eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):\n    eval_target = eval_target.to(device)\n    eval_token_index = eval_token_index.to(device)\n    total_account += eval_target.shape[0]\n    eval_logits = model(eval_token_index)\n    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n    eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target)\nlogging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\")\n</pre> model.eval() total_acc_account = 0 total_account = 0 for eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):     eval_target = eval_target.to(device)     eval_token_index = eval_token_index.to(device)     total_account += eval_target.shape[0]     eval_logits = model(eval_token_index)     total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()     eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target) logging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\") <pre>782it [00:10, 74.04it/s]</pre> <pre>2023-03-17 00:40:28,881 (612167170:11) WARNING: eval_loss: 0.6789069175720215, eval_acc: 0.8048\n</pre> <pre>\n</pre> IMDB \u200b\u6570\u636e\u200b\u6837\u672c\u200b\u957f\u5ea6\u200b\u5927\u6982\u200b\u5728\u200b 1000 \u200b\u5de6\u53f3\u200b\uff0c\u200b\u52a0\u4e4b\u200b\u8bcd\u200b\u5411\u91cf\u200b\u662f\u200b\u968f\u673a\u200b\u751f\u6210\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b RNN \u200b\u6a21\u578b\u200b\u4e0d\u591f\u200b\u7a33\u5b9a\u200b \u200b\u5bb9\u6613\u200b\u68af\u5ea6\u200b\u7206\u70b8\u200b\u548c\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\uff0c\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u8c03\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\u6216\u8005\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b \u200b\u53ef\u4ee5\u200b\u624b\u52a8\u200b\u8c03\u6574\u200b\u4e00\u4e0b\u200b\u5b66\u4e60\u200b\u7387\u200b\u548c\u200b\u8bad\u7ec3\u200b epoch\uff0c\u200b\u5982\u679c\u200b loss \u200b\u7a33\u5b9a\u200b\u5728\u200b 0.69 \u200b\u6216\u8005\u200b\u51fa\u73b0\u200b nan \u200b\u5219\u200b\u8868\u793a\u200b\u5b58\u5728\u200b\u95ee\u9898\u200b \u200b\u80fd\u200b\u4f4e\u4e8e\u200b 0.6 \u200b\u8bf4\u660e\u200b\u53ef\u4ee5\u200b\u7ee7\u7eed\u200b\u8dd1\u200b In\u00a0[33]: Copied! <pre>import torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n        super(RNN, self).__init__()\n\n        # Defining some parameters\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n\n        #Defining the layers\n        # RNN Layer\n        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n        # Fully connected layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n    \n    def forward(self, x):\n        \n        batch_size = x.size(0)\n\n        # Initializing hidden state for first input using method defined below\n        hidden = self.init_hidden(batch_size)\n\n        # Passing in the input and hidden state into the model and obtaining outputs\n        out, hidden = self.rnn(x, hidden)\n        \n        # Reshaping the outputs such that it can be fit into the fully connected layer\n        #out = out.contiguous().view(-1, self.hidden_dim)\n        out = F.avg_pool2d(out, (out.shape[1],1)).squeeze()\n        out = self.fc(out)\n        \n        return out\n    \n    def init_hidden(self, batch_size):\n        # This method generates the first hidden state of zeros which we'll use in the forward pass\n        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n        return hidden\n</pre> import torch.nn as nn  class RNN(nn.Module):     def __init__(self, input_size, output_size, hidden_dim, n_layers):         super(RNN, self).__init__()          # Defining some parameters         self.hidden_dim = hidden_dim         self.n_layers = n_layers          #Defining the layers         # RNN Layer         self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)            # Fully connected layer         self.fc = nn.Linear(hidden_dim, output_size)          def forward(self, x):                  batch_size = x.size(0)          # Initializing hidden state for first input using method defined below         hidden = self.init_hidden(batch_size)          # Passing in the input and hidden state into the model and obtaining outputs         out, hidden = self.rnn(x, hidden)                  # Reshaping the outputs such that it can be fit into the fully connected layer         #out = out.contiguous().view(-1, self.hidden_dim)         out = F.avg_pool2d(out, (out.shape[1],1)).squeeze()         out = self.fc(out)                  return out          def init_hidden(self, batch_size):         # This method generates the first hidden state of zeros which we'll use in the forward pass         # We'll send the tensor holding the hidden state to the device we specified earlier as well         hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)         return hidden  In\u00a0[34]: Copied! <pre># \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b\nx = torch.randn(batch_size, max_seq_len,input_size).to(device)\ny = torch.randint(0, num_classes, (batch_size,)).to(device)\nhidden = torch.zeros(batch_size, hidden_size).to(device)\n# \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\nmodel = RNN(input_size,  num_classes,hidden_size,1)\n\nmodel.to(device)\n# \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\nprint(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n# \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\noutput = model(x)\n# output\n</pre> # \u200b\u751f\u6210\u200b\u6d4b\u8bd5\u6570\u636e\u200b x = torch.randn(batch_size, max_seq_len,input_size).to(device) y = torch.randint(0, num_classes, (batch_size,)).to(device) hidden = torch.zeros(batch_size, hidden_size).to(device) # \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b model = RNN(input_size,  num_classes,hidden_size,1)  model.to(device) # \u200b\u6253\u5370\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b print(\"\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a\", sum(p.numel() for p in model.parameters() if p.requires_grad))  # \u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b output = model(x) # output <pre>\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff1a 25090\n</pre> In\u00a0[35]: Copied! <pre>model\n</pre> model Out[35]: <pre>RNN(\n  (rnn): RNN(64, 128, batch_first=True)\n  (fc): Linear(in_features=128, out_features=2, bias=True)\n)</pre> In\u00a0[38]: Copied! <pre># lr 0.01 \u200b\u5b58\u5728\u200b\u8dd1\u200b\u98de\u200b\u60c5\u51b5\u200b\n# 0.001 \u200b\u8bad\u7ec3\u200b 18 epoch\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncur_epoch = -1\nfor epoch_index in range(12,18):\n    num_batches = len(train_data_loader)\n    for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):\n        model.train()\n        optimizer.zero_grad()\n        target = target.to(device)\n        token_index = token_index.to(device)\n        step = num_batches*(epoch_index) + batch_index + 1   \n        output = model(token_index)\n        loss = F.nll_loss(F.log_softmax(output,dim=-1), target)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # \u200b\u68af\u5ea6\u200b\u7684\u200b\u6b63\u5219\u200b\u8fdb\u884c\u200b\u622a\u65ad\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u8bad\u7ec3\u200b\u7a33\u5b9a\u200b\n        optimizer.step()\n        if batch_index % 300 == 0:\n            cur_epoch = epoch_index\n            logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\")\n</pre> # lr 0.01 \u200b\u5b58\u5728\u200b\u8dd1\u200b\u98de\u200b\u60c5\u51b5\u200b # 0.001 \u200b\u8bad\u7ec3\u200b 18 epoch optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) cur_epoch = -1 for epoch_index in range(12,18):     num_batches = len(train_data_loader)     for batch_index, (target, token_index) in tqdm(enumerate(train_data_loader)):         model.train()         optimizer.zero_grad()         target = target.to(device)         token_index = token_index.to(device)         step = num_batches*(epoch_index) + batch_index + 1            output = model(token_index)         loss = F.nll_loss(F.log_softmax(output,dim=-1), target)         loss.backward()         nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # \u200b\u68af\u5ea6\u200b\u7684\u200b\u6b63\u5219\u200b\u8fdb\u884c\u200b\u622a\u65ad\u200b\uff0c\u200b\u4fdd\u8bc1\u200b\u8bad\u7ec3\u200b\u7a33\u5b9a\u200b         optimizer.step()         if batch_index % 300 == 0:             cur_epoch = epoch_index             logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, loss: {loss}\") <pre>0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:59:03,908 (1096697844:23) WARNING: epoch_index: 12, batch_index: 0, loss: 0.6250293254852295\n</pre> <pre>300it [00:06, 50.94it/s]</pre> <pre>2023-03-17 00:59:10,319 (1096697844:23) WARNING: epoch_index: 12, batch_index: 300, loss: 0.6724219918251038\n</pre> <pre>599it [00:12, 42.78it/s]</pre> <pre>2023-03-17 00:59:16,588 (1096697844:23) WARNING: epoch_index: 12, batch_index: 600, loss: 0.5623435378074646\n</pre> <pre>782it [00:16, 47.56it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:59:20,355 (1096697844:23) WARNING: epoch_index: 13, batch_index: 0, loss: 0.5873069167137146\n</pre> <pre>295it [00:06, 49.88it/s]</pre> <pre>2023-03-17 00:59:26,896 (1096697844:23) WARNING: epoch_index: 13, batch_index: 300, loss: 0.5667327642440796\n</pre> <pre>597it [00:12, 46.18it/s]</pre> <pre>2023-03-17 00:59:33,395 (1096697844:23) WARNING: epoch_index: 13, batch_index: 600, loss: 0.630550742149353\n</pre> <pre>782it [00:16, 47.39it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:59:36,858 (1096697844:23) WARNING: epoch_index: 14, batch_index: 0, loss: 0.5228468179702759\n</pre> <pre>300it [00:06, 43.12it/s]</pre> <pre>2023-03-17 00:59:42,946 (1096697844:23) WARNING: epoch_index: 14, batch_index: 300, loss: 0.46919119358062744\n</pre> <pre>598it [00:12, 44.54it/s]</pre> <pre>2023-03-17 00:59:49,330 (1096697844:23) WARNING: epoch_index: 14, batch_index: 600, loss: 0.6358293294906616\n</pre> <pre>782it [00:16, 47.77it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 00:59:53,238 (1096697844:23) WARNING: epoch_index: 15, batch_index: 0, loss: 0.6251819729804993\n</pre> <pre>297it [00:06, 52.90it/s]</pre> <pre>2023-03-17 00:59:59,394 (1096697844:23) WARNING: epoch_index: 15, batch_index: 300, loss: 0.5249137282371521\n</pre> <pre>597it [00:11, 49.78it/s]</pre> <pre>2023-03-17 01:00:05,039 (1096697844:23) WARNING: epoch_index: 15, batch_index: 600, loss: 0.6910139918327332\n</pre> <pre>782it [00:15, 51.67it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 01:00:08,375 (1096697844:23) WARNING: epoch_index: 16, batch_index: 0, loss: 0.5868373513221741\n</pre> <pre>299it [00:05, 52.95it/s]</pre> <pre>2023-03-17 01:00:14,215 (1096697844:23) WARNING: epoch_index: 16, batch_index: 300, loss: 0.6238973736763\n</pre> <pre>595it [00:11, 46.68it/s]</pre> <pre>2023-03-17 01:00:20,129 (1096697844:23) WARNING: epoch_index: 16, batch_index: 600, loss: 0.48795947432518005\n</pre> <pre>782it [00:15, 49.65it/s]\n0it [00:00, ?it/s]</pre> <pre>2023-03-17 01:00:24,120 (1096697844:23) WARNING: epoch_index: 17, batch_index: 0, loss: 0.5158190131187439\n</pre> <pre>296it [00:06, 48.67it/s]</pre> <pre>2023-03-17 01:00:30,500 (1096697844:23) WARNING: epoch_index: 17, batch_index: 300, loss: 0.7333455681800842\n</pre> <pre>596it [00:11, 56.56it/s]</pre> <pre>2023-03-17 01:00:36,137 (1096697844:23) WARNING: epoch_index: 17, batch_index: 600, loss: 0.5788255333900452\n</pre> <pre>782it [00:15, 50.98it/s]\n</pre> In\u00a0[39]: Copied! <pre>model.eval()\ntotal_acc_account = 0\ntotal_account = 0\nfor eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):\n    eval_target = eval_target.to(device)\n    eval_token_index = eval_token_index.to(device)\n    total_account += eval_target.shape[0]\n    eval_logits = model(eval_token_index)\n    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n    eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target)\nlogging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\")\n</pre> model.eval() total_acc_account = 0 total_account = 0 for eval_batch_index, (eval_target, eval_token_index) in tqdm(enumerate(eval_data_loader)):     eval_target = eval_target.to(device)     eval_token_index = eval_token_index.to(device)     total_account += eval_target.shape[0]     eval_logits = model(eval_token_index)     total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()     eval_loss = F.nll_loss(F.log_softmax(eval_logits,dim=-1), eval_target) logging.warning(f\"eval_loss: {eval_loss}, eval_acc: {total_acc_account / total_account}\") <pre>782it [00:08, 92.90it/s]</pre> <pre>2023-03-17 01:01:44,283 (612167170:11) WARNING: eval_loss: 1.3197190761566162, eval_acc: 0.67968\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u200b\u5f53\u524d\u60c5\u51b5\u200b\u4e0b\u200b log \u200b\u5b58\u5728\u200b\u8bad\u7ec3\u200b\u4e0d\u8db3\u200b\uff0c\u200b\u6709\u200b\u4e00\u5b9a\u200b\u63d0\u9ad8\u200b\u7a7a\u95f4\u200b\n</pre> # \u200b\u5f53\u524d\u60c5\u51b5\u200b\u4e0b\u200b log \u200b\u5b58\u5728\u200b\u8bad\u7ec3\u200b\u4e0d\u8db3\u200b\uff0c\u200b\u6709\u200b\u4e00\u5b9a\u200b\u63d0\u9ad8\u200b\u7a7a\u95f4\u200b In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#lstm","title":"LSTM \u200b\u8be6\u89e3\u200b\u4ee3\u7801\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#data-imdb","title":"Data - IMDB\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#model","title":"Model\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#gate","title":"Gate\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#mylstm","title":"MyLSTM\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#lstm-pytorch","title":"LSTM-Pytorch\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%9A%84%E8%A7%A3%E8%AF%BB/LSTM%E5%AE%9E%E6%88%98/#rnn","title":"RNN\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB/","title":"\u7b2c\u56db\u7ae0\u200b \u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014FashionMNIST\u200b\u65f6\u88c5\u200b\u5206\u7c7b","text":"<p>\u200b\u7ecf\u8fc7\u200b\u524d\u9762\u200b\u4e09\u7ae0\u200b\u5185\u5bb9\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a</p> <ul> <li>\u200b\u5bf9\u200bPyTorch\u200b\u6709\u200b\u4e86\u200b\u521d\u6b65\u200b\u7684\u200b\u8ba4\u8bc6\u200b</li> <li>\u200b\u5b66\u4f1a\u200b\u4e86\u200b\u5982\u4f55\u200b\u5b89\u88c5\u200bPyTorch\u200b\u4ee5\u53ca\u200b\u5bf9\u5e94\u200b\u7684\u200b\u7f16\u7a0b\u200b\u73af\u5883\u200b</li> <li>\u200b\u5b66\u4e60\u200b\u4e86\u200bPyTorch\u200b\u6700\u200b\u6838\u5fc3\u200b\u7684\u200b\u7406\u8bba\u200b\u57fa\u7840\u200b\uff08\u200b\u5f20\u91cf\u200b&amp;\u200b\u81ea\u52a8\u200b\u6c42\u5bfc\u200b\uff09</li> <li>\u200b\u68b3\u7406\u200b\u4e86\u200b\u5229\u7528\u200bPyTorch\u200b\u5b8c\u6210\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e3b\u8981\u200b\u6b65\u9aa4\u200b\u548c\u200b\u5bf9\u5e94\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b</li> </ul> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u6848\u4f8b\u200b\uff0c\u200b\u5c06\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u6240\u200b\u6d89\u53ca\u200b\u7684\u200bPyTorch\u200b\u5165\u95e8\u200b\u77e5\u8bc6\u200b\u4e32\u200b\u8d77\u6765\u200b\uff0c\u200b\u4fbf\u4e8e\u200b\u5927\u5bb6\u200b\u52a0\u6df1\u200b\u7406\u89e3\u200b\u3002\u200b\u540c\u65f6\u200b\u4e3a\u200b\u540e\u7eed\u200b\u7684\u200b\u8fdb\u9636\u200b\u5b66\u4e60\u200b\u6253\u200b\u597d\u200b\u57fa\u7840\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u5bf9\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u201c\u200b\u65f6\u88c5\u200b\u201d\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u4f7f\u7528\u200bFashionMNIST\u200b\u6570\u636e\u200b\u96c6\u200b\uff08https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion \uff09\u3002\u200b\u4e0a\u200b\u56fe\u200b\u7ed9\u51fa\u200b\u4e86\u200bFashionMNIST\u200b\u4e2d\u200b\u6570\u636e\u200b\u7684\u200b\u82e5\u5e72\u200b\u6837\u4f8b\u200b\u56fe\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u5c0f\u56fe\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u6837\u672c\u200b\u3002 FashionMNIST\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u5305\u542b\u200b\u5df2\u7ecf\u200b\u9884\u5148\u200b\u5212\u5206\u200b\u597d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bad\u7ec3\u200b\u96c6\u5171\u200b60,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u96c6\u5171\u200b10,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\u3002\u200b\u6bcf\u5f20\u200b\u56fe\u50cf\u200b\u5747\u200b\u4e3a\u200b\u5355\u901a\u9053\u200b\u9ed1\u767d\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b32*32pixel\uff0c\u200b\u5206\u5c5e\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u4e0b\u9762\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u8d77\u200b\u5c06\u200b\u7b2c\u4e09\u7ae0\u200b\u5404\u200b\u90e8\u5206\u200b\u5185\u5bb9\u200b\u9010\u6b65\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6765\u200b\u8dd1\u200b\u5b8c\u6574\u200b\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6d41\u7a0b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u7684\u200b\u5305\u200b</p> In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n</pre> import os import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader <p>\u200b\u914d\u7f6e\u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b\u548c\u200b\u8d85\u200b\u53c2\u6570\u200b</p> In\u00a0[2]: Copied! <pre># \u200b\u914d\u7f6e\u200bGPU\uff0c\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\n## \u200b\u65b9\u6848\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200bos.environ\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n# \u200b\u65b9\u6848\u200b\u4e8c\u200b\uff1a\u200b\u4f7f\u7528\u200b\u201cdevice\u201d\uff0c\u200b\u540e\u7eed\u200b\u5bf9\u8981\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u53d8\u91cf\u200b\u7528\u200b.to(device)\u200b\u5373\u53ef\u200b\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\n## \u200b\u914d\u7f6e\u200b\u5176\u4ed6\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200bbatch_size, num_workers, learning rate, \u200b\u4ee5\u53ca\u200b\u603b\u200b\u7684\u200bepochs\nbatch_size = 256\nnum_workers = 4   # \u200b\u5bf9\u4e8e\u200bWindows\u200b\u7528\u6237\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u5e94\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u591a\u7ebf\u7a0b\u200b\u9519\u8bef\u200b\nlr = 1e-4\nepochs = 20\n</pre> # \u200b\u914d\u7f6e\u200bGPU\uff0c\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b ## \u200b\u65b9\u6848\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200bos.environ os.environ['CUDA_VISIBLE_DEVICES'] = '0' # \u200b\u65b9\u6848\u200b\u4e8c\u200b\uff1a\u200b\u4f7f\u7528\u200b\u201cdevice\u201d\uff0c\u200b\u540e\u7eed\u200b\u5bf9\u8981\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u53d8\u91cf\u200b\u7528\u200b.to(device)\u200b\u5373\u53ef\u200b device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  ## \u200b\u914d\u7f6e\u200b\u5176\u4ed6\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200bbatch_size, num_workers, learning rate, \u200b\u4ee5\u53ca\u200b\u603b\u200b\u7684\u200bepochs batch_size = 256 num_workers = 4   # \u200b\u5bf9\u4e8e\u200bWindows\u200b\u7528\u6237\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u5e94\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\uff0c\u200b\u5426\u5219\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u591a\u7ebf\u7a0b\u200b\u9519\u8bef\u200b lr = 1e-4 epochs = 20 <p>\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u548c\u200b\u52a0\u8f7d\u200b \u200b\u8fd9\u91cc\u200b\u540c\u65f6\u200b\u5c55\u793a\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b:</p> <ul> <li>\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u4f7f\u7528\u200bPyTorch\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b</li> <li>\u200b\u4ece\u200b\u7f51\u7ad9\u200b\u4e0b\u8f7d\u200b\u4ee5\u200bcsv\u200b\u683c\u5f0f\u200b\u5b58\u50a8\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u8bfb\u5165\u200b\u5e76\u200b\u8f6c\u6210\u200b\u9884\u671f\u200b\u7684\u200b\u683c\u5f0f\u200b \u200b\u7b2c\u4e00\u79cd\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u65b9\u5f0f\u200b\u53ea\u200b\u9002\u7528\u200b\u4e8e\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5982\u200bMNIST\uff0cCIFAR10\u200b\u7b49\u200b\uff0cPyTorch\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u3002\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u5f80\u5f80\u200b\u9002\u7528\u200b\u4e8e\u200b\u5feb\u901f\u200b\u6d4b\u8bd5\u65b9\u6cd5\u200b\uff08\u200b\u6bd4\u5982\u200b\u6d4b\u8bd5\u200b\u4e0b\u200b\u67d0\u4e2a\u200bidea\u200b\u5728\u200bMNIST\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u662f\u5426\u200b\u6709\u6548\u200b\uff09 \u200b\u7b2c\u4e8c\u79cd\u200b\u6570\u636e\u200b\u8bfb\u5165\u200b\u65b9\u5f0f\u200b\u9700\u8981\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200bDataset\uff0c\u200b\u8fd9\u200b\u5bf9\u4e8e\u200bPyTorch\u200b\u5e94\u7528\u200b\u4e8e\u200b\u81ea\u5df1\u200b\u7684\u200b\u5de5\u4f5c\u200b\u4e2d\u200b\u5341\u5206\u200b\u91cd\u8981\u200b</li> </ul> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5fc5\u8981\u200b\u7684\u200b\u53d8\u6362\u200b\uff0c\u200b\u6bd4\u5982\u8bf4\u200b\u9700\u8981\u200b\u5c06\u200b\u56fe\u7247\u200b\u7edf\u4e00\u200b\u4e3a\u200b\u4e00\u81f4\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u540e\u7eed\u200b\u80fd\u591f\u200b\u8f93\u5165\u200b\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\uff1b\u200b\u9700\u8981\u200b\u5c06\u200b\u6570\u636e\u683c\u5f0f\u200b\u8f6c\u4e3a\u200bTensor\u200b\u7c7b\u200b\uff0c\u200b\u7b49\u7b49\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u5730\u200b\u501f\u52a9\u200btorchvision\u200b\u5305\u6765\u200b\u5b8c\u6210\u200b\uff0c\u200b\u8fd9\u662f\u200bPyTorch\u200b\u5b98\u65b9\u200b\u7528\u4e8e\u200b\u56fe\u50cf\u5904\u7406\u200b\u7684\u200b\u5de5\u5177\u200b\u5e93\u200b\uff0c\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e5f\u200b\u8981\u200b\u7528\u5230\u200b\u3002PyTorch\u200b\u7684\u200b\u4e00\u5927\u200b\u65b9\u4fbf\u200b\u4e4b\u5904\u200b\u5c31\u200b\u5728\u4e8e\u200b\u5b83\u200b\u662f\u200b\u4e00\u6574\u5957\u200b\u201c\u200b\u751f\u6001\u200b\u201d\uff0c\u200b\u6709\u7740\u200b\u5b98\u65b9\u200b\u548c\u200b\u7b2c\u4e09\u65b9\u200b\u5404\u4e2a\u9886\u57df\u200b\u7684\u200b\u652f\u6301\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u540e\u7eed\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u3002</p> In\u00a0[3]: Copied! <pre># \u200b\u9996\u5148\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\nfrom torchvision import transforms\n\nimage_size = 28\ndata_transform = transforms.Compose([\n    transforms.ToPILImage(),   # \u200b\u8fd9\u200b\u4e00\u6b65\u200b\u53d6\u51b3\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b\n    transforms.Resize(image_size),\n    transforms.ToTensor()\n])\n</pre> # \u200b\u9996\u5148\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u53d8\u6362\u200b from torchvision import transforms  image_size = 28 data_transform = transforms.Compose([     transforms.ToPILImage(),   # \u200b\u8fd9\u200b\u4e00\u6b65\u200b\u53d6\u51b3\u4e8e\u200b\u540e\u7eed\u200b\u7684\u200b\u6570\u636e\u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u5219\u200b\u4e0d\u200b\u9700\u8981\u200b     transforms.Resize(image_size),     transforms.ToTensor() ]) In\u00a0[5]: Copied! <pre>## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200btorchvision\u200b\u81ea\u5e26\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\nfrom torchvision import datasets\n\ntrain_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform)\ntest_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform)\n</pre> ## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e00\u200b\uff1a\u200b\u4f7f\u7528\u200btorchvision\u200b\u81ea\u5e26\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b from torchvision import datasets  train_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform) test_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform) <pre>/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n</pre> In\u00a0[4]: Copied! <pre>## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e8c\u200b\uff1a\u200b\u8bfb\u5165\u200bcsv\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u81ea\u884c\u200b\u6784\u5efa\u200bDataset\u200b\u7c7b\u200b\n# csv\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff1ahttps://www.kaggle.com/zalando-research/fashionmnist\nclass FMDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.images = df.iloc[:,1:].values.astype(np.uint8)\n        self.labels = df.iloc[:, 0].values\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].reshape(28,28,1)\n        label = int(self.labels[idx])\n        if self.transform is not None:\n            image = self.transform(image)\n        else:\n            image = torch.tensor(image/255., dtype=torch.float)\n        label = torch.tensor(label, dtype=torch.long)\n        return image, label\n\ntrain_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_train.csv\")\ntest_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_test.csv\")\ntrain_data = FMDataset(train_df, data_transform)\ntest_data = FMDataset(test_df, data_transform)\n</pre> ## \u200b\u8bfb\u53d6\u200b\u65b9\u5f0f\u200b\u4e8c\u200b\uff1a\u200b\u8bfb\u5165\u200bcsv\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u81ea\u884c\u200b\u6784\u5efa\u200bDataset\u200b\u7c7b\u200b # csv\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff1ahttps://www.kaggle.com/zalando-research/fashionmnist class FMDataset(Dataset):     def __init__(self, df, transform=None):         self.df = df         self.transform = transform         self.images = df.iloc[:,1:].values.astype(np.uint8)         self.labels = df.iloc[:, 0].values              def __len__(self):         return len(self.images)          def __getitem__(self, idx):         image = self.images[idx].reshape(28,28,1)         label = int(self.labels[idx])         if self.transform is not None:             image = self.transform(image)         else:             image = torch.tensor(image/255., dtype=torch.float)         label = torch.tensor(label, dtype=torch.long)         return image, label  train_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_train.csv\") test_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_test.csv\") train_data = FMDataset(train_df, data_transform) test_data = FMDataset(test_df, data_transform) <p>\u200b\u5728\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u5b9a\u4e49\u200bDataLoader\u200b\u7c7b\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b</p> In\u00a0[5]: Copied! <pre>train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n</pre> train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True) test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers) <p>\u200b\u8bfb\u5165\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u505a\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u53ef\u89c6\u5316\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4e3b\u8981\u200b\u662f\u200b\u9a8c\u8bc1\u200b\u6211\u4eec\u200b\u8bfb\u5165\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u5426\u200b\u6b63\u786e\u200b</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nimage, label = next(iter(train_loader))\nprint(image.shape, label.shape)\nplt.imshow(image[0][0], cmap=\"gray\")\n</pre> import matplotlib.pyplot as plt image, label = next(iter(train_loader)) print(image.shape, label.shape) plt.imshow(image[0][0], cmap=\"gray\") <pre>torch.Size([256, 1, 28, 28]) torch.Size([256])\n</pre> Out[6]: <pre>&lt;matplotlib.image.AxesImage at 0x7f19a043cc10&gt;</pre> <p>\u200b\u6a21\u578b\u200b\u8bbe\u8ba1\u200b \u200b\u7531\u4e8e\u200b\u4efb\u52a1\u200b\u8f83\u4e3a\u7b80\u5355\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u624b\u200b\u642d\u200b\u4e00\u4e2a\u200bCNN\uff0c\u200b\u800c\u200b\u4e0d\u200b\u8003\u8651\u200b\u5f53\u4e0b\u200b\u5404\u79cd\u200b\u6a21\u578b\u200b\u7684\u200b\u590d\u6742\u200b\u7ed3\u6784\u200b \u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u5230\u200bGPU\u200b\u4e0a\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b</p> In\u00a0[7]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout(0.3),\n            nn.Conv2d(32, 64, 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout(0.3)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64*4*4, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1, 64*4*4)\n        x = self.fc(x)\n        # x = nn.functional.normalize(x)\n        return x\n\nmodel = Net()\nmodel = model.cuda()\n# model = nn.DataParallel(model).cuda()   # \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u5199\u6cd5\u200b\uff0c\u200b\u4e4b\u540e\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u4f1a\u200b\u8fdb\u4e00\u6b65\u200b\u8bb2\u89e3\u200b\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         self.conv = nn.Sequential(             nn.Conv2d(1, 32, 5),             nn.ReLU(),             nn.MaxPool2d(2, stride=2),             nn.Dropout(0.3),             nn.Conv2d(32, 64, 5),             nn.ReLU(),             nn.MaxPool2d(2, stride=2),             nn.Dropout(0.3)         )         self.fc = nn.Sequential(             nn.Linear(64*4*4, 512),             nn.ReLU(),             nn.Linear(512, 10)         )              def forward(self, x):         x = self.conv(x)         x = x.view(-1, 64*4*4)         x = self.fc(x)         # x = nn.functional.normalize(x)         return x  model = Net() model = model.cuda() # model = nn.DataParallel(model).cuda()   # \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u5199\u6cd5\u200b\uff0c\u200b\u4e4b\u540e\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u4f1a\u200b\u8fdb\u4e00\u6b65\u200b\u8bb2\u89e3\u200b <p>\u200b\u8bbe\u5b9a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u4f7f\u7528\u200btorch.nn\u200b\u6a21\u5757\u200b\u81ea\u5e26\u200b\u7684\u200bCrossEntropy\u200b\u635f\u5931\u200b PyTorch\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u628a\u200b\u6574\u6570\u578b\u200b\u7684\u200blabel\u200b\u8f6c\u4e3a\u200bone-hot\u200b\u578b\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200bCE loss \u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u786e\u4fdd\u200blabel\u200b\u662f\u4ece\u200b0\u200b\u5f00\u59cb\u200b\u7684\u200b\uff0c\u200b\u540c\u65f6\u200b\u6a21\u578b\u200b\u4e0d\u200b\u52a0\u200bsoftmax\u200b\u5c42\u200b\uff08\u200b\u4f7f\u7528\u200blogits\u200b\u8ba1\u7b97\u200b\uff09,\u200b\u8fd9\u200b\u4e5f\u200b\u8bf4\u660e\u200b\u4e86\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u4e0d\u662f\u200b\u72ec\u7acb\u200b\u7684\u200b\uff0c\u200b\u9700\u8981\u200b\u901a\u76d8\u8003\u8651\u200b</p> In\u00a0[8]: Copied! <pre>criterion = nn.CrossEntropyLoss()\n# criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])\n</pre> criterion = nn.CrossEntropyLoss() # criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1]) In\u00a0[\u00a0]: Copied! <pre>?nn.CrossEntropyLoss # \u200b\u8fd9\u91cc\u200b\u65b9\u4fbf\u200b\u770b\u200b\u4e00\u4e0b\u200bweighting\u200b\u7b49\u200b\u7b56\u7565\u200b\n</pre> ?nn.CrossEntropyLoss # \u200b\u8fd9\u91cc\u200b\u65b9\u4fbf\u200b\u770b\u200b\u4e00\u4e0b\u200bweighting\u200b\u7b49\u200b\u7b56\u7565\u200b <p>\u200b\u8bbe\u5b9a\u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bAdam\u200b\u4f18\u5316\u200b\u5668\u200b</p> In\u00a0[9]: Copied! <pre>optimizer = optim.Adam(model.parameters(), lr=0.001)\n</pre> optimizer = optim.Adam(model.parameters(), lr=0.001) <p>\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\uff08\u200b\u9a8c\u8bc1\u200b\uff09 \u200b\u5404\u81ea\u200b\u5c01\u88c5\u200b\u6210\u200b\u51fd\u6570\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u540e\u7eed\u200b\u8c03\u7528\u200b \u200b\u5173\u6ce8\u200b\u4e24\u8005\u200b\u7684\u200b\u4e3b\u8981\u200b\u533a\u522b\u200b\uff1a</p> <ul> <li>\u200b\u6a21\u578b\u200b\u72b6\u6001\u200b\u8bbe\u7f6e\u200b</li> <li>\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u521d\u59cb\u5316\u200b\u4f18\u5316\u200b\u5668\u200b</li> <li>\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u5c06\u200bloss\u200b\u4f20\u200b\u56de\u5230\u200b\u7f51\u7edc\u200b</li> <li>\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u6bcf\u6b65\u200b\u66f4\u65b0\u200boptimizer</li> </ul> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6d4b\u8bd5\u200b\u6216\u200b\u9a8c\u8bc1\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u5206\u7c7b\u200b\u51c6\u786e\u7387\u200b</p> In\u00a0[10]: Copied! <pre>def train(epoch):\n    model.train()\n    train_loss = 0\n    for data, label in train_loader:\n        data, label = data.cuda(), label.cuda()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n    train_loss = train_loss/len(train_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n</pre> def train(epoch):     model.train()     train_loss = 0     for data, label in train_loader:         data, label = data.cuda(), label.cuda()         optimizer.zero_grad()         output = model(data)         loss = criterion(output, label)         loss.backward()         optimizer.step()         train_loss += loss.item()*data.size(0)     train_loss = train_loss/len(train_loader.dataset)     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss)) In\u00a0[11]: Copied! <pre>def val(epoch):       \n    model.eval()\n    val_loss = 0\n    gt_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data, label in test_loader:\n            data, label = data.cuda(), label.cuda()\n            output = model(data)\n            preds = torch.argmax(output, 1)\n            gt_labels.append(label.cpu().data.numpy())\n            pred_labels.append(preds.cpu().data.numpy())\n            loss = criterion(output, label)\n            val_loss += loss.item()*data.size(0)\n    val_loss = val_loss/len(test_loader.dataset)\n    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))\n</pre> def val(epoch):            model.eval()     val_loss = 0     gt_labels = []     pred_labels = []     with torch.no_grad():         for data, label in test_loader:             data, label = data.cuda(), label.cuda()             output = model(data)             preds = torch.argmax(output, 1)             gt_labels.append(label.cpu().data.numpy())             pred_labels.append(preds.cpu().data.numpy())             loss = criterion(output, label)             val_loss += loss.item()*data.size(0)     val_loss = val_loss/len(test_loader.dataset)     gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)     acc = np.sum(gt_labels==pred_labels)/len(pred_labels)     print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc)) In\u00a0[12]: Copied! <pre>for epoch in range(1, epochs+1):\n    train(epoch)\n    val(epoch)\n</pre> for epoch in range(1, epochs+1):     train(epoch)     val(epoch) <pre>/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.659050\nEpoch: 1 \tValidation Loss: 0.420328, Accuracy: 0.852000\nEpoch: 2 \tTraining Loss: 0.403703\nEpoch: 2 \tValidation Loss: 0.350373, Accuracy: 0.872300\nEpoch: 3 \tTraining Loss: 0.350197\nEpoch: 3 \tValidation Loss: 0.293053, Accuracy: 0.893200\nEpoch: 4 \tTraining Loss: 0.322463\nEpoch: 4 \tValidation Loss: 0.283335, Accuracy: 0.892300\nEpoch: 5 \tTraining Loss: 0.300117\nEpoch: 5 \tValidation Loss: 0.268653, Accuracy: 0.903500\nEpoch: 6 \tTraining Loss: 0.282179\nEpoch: 6 \tValidation Loss: 0.247219, Accuracy: 0.907200\nEpoch: 7 \tTraining Loss: 0.268283\nEpoch: 7 \tValidation Loss: 0.242937, Accuracy: 0.907800\nEpoch: 8 \tTraining Loss: 0.257615\nEpoch: 8 \tValidation Loss: 0.234324, Accuracy: 0.912200\nEpoch: 9 \tTraining Loss: 0.245795\nEpoch: 9 \tValidation Loss: 0.231515, Accuracy: 0.914100\nEpoch: 10 \tTraining Loss: 0.238739\nEpoch: 10 \tValidation Loss: 0.229616, Accuracy: 0.914400\nEpoch: 11 \tTraining Loss: 0.230499\nEpoch: 11 \tValidation Loss: 0.228124, Accuracy: 0.915200\nEpoch: 12 \tTraining Loss: 0.221574\nEpoch: 12 \tValidation Loss: 0.211928, Accuracy: 0.921200\nEpoch: 13 \tTraining Loss: 0.217924\nEpoch: 13 \tValidation Loss: 0.209744, Accuracy: 0.921700\nEpoch: 14 \tTraining Loss: 0.206033\nEpoch: 14 \tValidation Loss: 0.215477, Accuracy: 0.921400\nEpoch: 15 \tTraining Loss: 0.203349\nEpoch: 15 \tValidation Loss: 0.215550, Accuracy: 0.919400\nEpoch: 16 \tTraining Loss: 0.196319\nEpoch: 16 \tValidation Loss: 0.210800, Accuracy: 0.923700\nEpoch: 17 \tTraining Loss: 0.191969\nEpoch: 17 \tValidation Loss: 0.207266, Accuracy: 0.923700\nEpoch: 18 \tTraining Loss: 0.185466\nEpoch: 18 \tValidation Loss: 0.207138, Accuracy: 0.924200\nEpoch: 19 \tTraining Loss: 0.178241\nEpoch: 19 \tValidation Loss: 0.204093, Accuracy: 0.924900\nEpoch: 20 \tTraining Loss: 0.176674\nEpoch: 20 \tValidation Loss: 0.197495, Accuracy: 0.928300\n</pre> <p>\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b \u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200btorch.save\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u6216\u8005\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b \u200b\u8fd9\u90e8\u5206\u200b\u4f1a\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b</p> In\u00a0[\u00a0]: Copied! <pre>save_path = \"./FahionModel.pkl\"\ntorch.save(model, save_path)\n</pre> save_path = \"./FahionModel.pkl\" torch.save(model, save_path) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98/%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94FashionMNIST%E6%97%B6%E8%A3%85%E5%88%86%E7%B1%BB/#fashionmnist","title":"\u7b2c\u56db\u7ae0\u200b \u200b\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014FashionMNIST\u200b\u65f6\u88c5\u200b\u5206\u7c7b\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20PyTorch%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/%E6%9E%9C%E8%94%AC%E5%88%86%E7%B1%BB/%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%9E%9C%E8%94%AC%E5%88%86%E7%B1%BB/","title":"\u57fa\u7840\u200b\u5b9e\u6218\u200b\u2014\u2014\u200b\u679c\u852c\u200b\u5206\u7c7b","text":"In\u00a0[212]: Copied! <pre>!ls\n</pre> !ls <pre>archive.zip  fruit_vegetable_cls.ipynb\ttest  train  validation\r\n</pre> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u5730\u5740\u200b\uff1akaggle \u200b\u679c\u852c\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b</p> <p>\u200b\u767e\u5ea6\u7f51\u200b\u76d8\u200b\uff1a\u200b\u94fe\u63a5\u200b\uff1ahttps://pan.baidu.com/s/1q-fS2M97er1-769Ol-htSg \u200b\u63d0\u53d6\u200b\u7801\u200b\uff1at0ww</p> In\u00a0[213]: Copied! <pre># \u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u7684\u200b\u5305\u200b\nimport os\nimport torch\nimport timm\nfrom torchvision import transforms\nimport numpy as np\nimport torch.nn as nn\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> # \u200b\u5bfc\u5165\u200b\u5fc5\u8981\u200b\u7684\u200b\u5305\u200b import os import torch import timm from torchvision import transforms import numpy as np import torch.nn as nn import warnings warnings.filterwarnings(\"ignore\") import matplotlib.pyplot as plt %matplotlib inline In\u00a0[214]: Copied! <pre># \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u786e\u4fdd\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u590d\u73b0\u200b\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n</pre> # \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u786e\u4fdd\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u590d\u73b0\u200b def set_seed(seed):     torch.manual_seed(seed)     torch.cuda.manual_seed_all(seed)     random.seed(seed)     np.random.seed(seed)     torch.backends.cudnn.benchmark = False     torch.backends.cudnn.deterministic = True In\u00a0[210]: Copied! <pre># \u200b\u8bbe\u7f6e\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\ntrain_path = './train/'\nval_path = './validation/'\ntest_path = './test/'\n\n# \u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 32\n\n# num_workers wins\u200b\u7528\u6237\u200b\u53ea\u80fd\u200b\u6307\u5b9a\u200b\u4e3a\u200b0\nnum_workers = 4\nlr = 3e-4\nepochs = 10\n\n# \u200b\u8f93\u51fa\u200b\u7c7b\u522b\u200b\ncategories = os.listdir(train_path)\nprint(f\"Categories: {len(categories)}\")\n</pre> # \u200b\u8bbe\u7f6e\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b train_path = './train/' val_path = './validation/' test_path = './test/'  # \u200b\u8bbe\u7f6e\u200b\u8d85\u200b\u53c2\u6570\u200b device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") batch_size = 32  # num_workers wins\u200b\u7528\u6237\u200b\u53ea\u80fd\u200b\u6307\u5b9a\u200b\u4e3a\u200b0 num_workers = 4 lr = 3e-4 epochs = 10  # \u200b\u8f93\u51fa\u200b\u7c7b\u522b\u200b categories = os.listdir(train_path) print(f\"Categories: {len(categories)}\") <pre>Categories: 36\n</pre> In\u00a0[216]: Copied! <pre># \u200b\u4f7f\u7528\u200bimgaug\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\nimport imageio\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\nclass ImgAugTransform:\n    def __init__(self):\n        self.aug = iaa.Sequential([\n            iaa.Scale((224, 224)),\n            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n            iaa.Fliplr(0.5),\n            iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n            iaa.Sometimes(0.25,\n                          iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n                                     iaa.CoarseDropout(0.1, size_percent=0.5)])),\n            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n        ])\n\n    def __call__(self, img):\n        img = np.array(img)\n        return self.aug.augment_image(img).transpose(2,1,0)\n\n# linux\u200b\u7528\u6237\u200bnum_worker\u22600\u200b\u65f6\u200b\u4f7f\u7528\u200b\ndef worker_init_fn(worker_id):\n    imgaug.seed(np.random.get_state()[1][0] + worker_id)\n\ntfs = ImgAugTransform()\n</pre> # \u200b\u4f7f\u7528\u200bimgaug\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b import imageio import imgaug as ia from imgaug import augmenters as iaa  class ImgAugTransform:     def __init__(self):         self.aug = iaa.Sequential([             iaa.Scale((224, 224)),             iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),             iaa.Fliplr(0.5),             iaa.Affine(rotate=(-20, 20), mode='symmetric'),             iaa.Sometimes(0.25,                           iaa.OneOf([iaa.Dropout(p=(0, 0.1)),                                      iaa.CoarseDropout(0.1, size_percent=0.5)])),             iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)         ])      def __call__(self, img):         img = np.array(img)         return self.aug.augment_image(img).transpose(2,1,0)  # linux\u200b\u7528\u6237\u200bnum_worker\u22600\u200b\u65f6\u200b\u4f7f\u7528\u200b def worker_init_fn(worker_id):     imgaug.seed(np.random.get_state()[1][0] + worker_id)  tfs = ImgAugTransform() In\u00a0[220]: Copied! <pre>from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n# \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\ntrain_dataset = ImageFolder(train_path,transform=tfs)\nval_dataset = ImageFolder(val_path,transform=tfs)\ntest_dataset = ImageFolder(test_path,transform=tfs)\n\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=True)\nval_loader = DataLoader(val_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=False)\ntest_loader = DataLoader(test_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=False)\n\n# \u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b\nimages, labels = next(iter(train_loader))\nprint(images.shape)\nprint(type(images))\nprint(images[1].shape)\nplt.imshow(images[0].permute(1,2,0))\nplt.show()\n</pre> from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader from PIL import Image # \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b train_dataset = ImageFolder(train_path,transform=tfs) val_dataset = ImageFolder(val_path,transform=tfs) test_dataset = ImageFolder(test_path,transform=tfs)  train_loader = DataLoader(train_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=True) val_loader = DataLoader(val_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=False) test_loader = DataLoader(test_dataset,batch_size=batch_size,worker_init_fn=worker_init_fn,shuffle=False)  # \u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b images, labels = next(iter(train_loader)) print(images.shape) print(type(images)) print(images[1].shape) plt.imshow(images[0].permute(1,2,0)) plt.show() <pre>torch.Size([32, 3, 224, 224])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([3, 224, 224])\n</pre> In\u00a0[222]: Copied! <pre>import timm\n# \u200b\u4f7f\u7528\u200btimm\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nmodel = timm.create_model(\"resnet18\",num_classes=36,pretrained=True).to(device)\n</pre> import timm # \u200b\u4f7f\u7528\u200btimm\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b model = timm.create_model(\"resnet18\",num_classes=36,pretrained=True).to(device) In\u00a0[223]: Copied! <pre>avail_pretrained_models = timm.list_models(pretrained=True)\nlen(avail_pretrained_models)\n</pre> avail_pretrained_models = timm.list_models(pretrained=True) len(avail_pretrained_models) Out[223]: <pre>592</pre> In\u00a0[227]: Copied! <pre># \u200b\u4f7f\u7528\u200b\u901a\u914d\u7b26\u200b\u67e5\u627e\u200bresnet\u200b\u7cfb\u5217\u200b\nall_resnent_models = timm.list_models(\"*resnet*\")\nall_resnent_models\n</pre> # \u200b\u4f7f\u7528\u200b\u901a\u914d\u7b26\u200b\u67e5\u627e\u200bresnet\u200b\u7cfb\u5217\u200b all_resnent_models = timm.list_models(\"*resnet*\") all_resnent_models Out[227]: <pre>['cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'eca_resnet33ts',\n 'ecaresnet26t',\n 'ecaresnet50d',\n 'ecaresnet50d_pruned',\n 'ecaresnet50t',\n 'ecaresnet101d',\n 'ecaresnet101d_pruned',\n 'ecaresnet200d',\n 'ecaresnet269d',\n 'ecaresnetlight',\n 'ens_adv_inception_resnet_v2',\n 'gcresnet33ts',\n 'gcresnet50t',\n 'gluon_resnet18_v1b',\n 'gluon_resnet34_v1b',\n 'gluon_resnet50_v1b',\n 'gluon_resnet50_v1c',\n 'gluon_resnet50_v1d',\n 'gluon_resnet50_v1s',\n 'gluon_resnet101_v1b',\n 'gluon_resnet101_v1c',\n 'gluon_resnet101_v1d',\n 'gluon_resnet101_v1s',\n 'gluon_resnet152_v1b',\n 'gluon_resnet152_v1c',\n 'gluon_resnet152_v1d',\n 'gluon_resnet152_v1s',\n 'inception_resnet_v2',\n 'lambda_resnet26rpt_256',\n 'lambda_resnet26t',\n 'lambda_resnet50ts',\n 'legacy_seresnet18',\n 'legacy_seresnet34',\n 'legacy_seresnet50',\n 'legacy_seresnet101',\n 'legacy_seresnet152',\n 'nf_ecaresnet26',\n 'nf_ecaresnet50',\n 'nf_ecaresnet101',\n 'nf_resnet26',\n 'nf_resnet50',\n 'nf_resnet101',\n 'nf_seresnet26',\n 'nf_seresnet50',\n 'nf_seresnet101',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet26t',\n 'resnet32ts',\n 'resnet33ts',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50_gn',\n 'resnet50d',\n 'resnet50t',\n 'resnet51q',\n 'resnet61q',\n 'resnet101',\n 'resnet101d',\n 'resnet152',\n 'resnet152d',\n 'resnet200',\n 'resnet200d',\n 'resnetblur18',\n 'resnetblur50',\n 'resnetrs50',\n 'resnetrs101',\n 'resnetrs152',\n 'resnetrs200',\n 'resnetrs270',\n 'resnetrs350',\n 'resnetrs420',\n 'resnetv2_50',\n 'resnetv2_50d',\n 'resnetv2_50d_evob',\n 'resnetv2_50d_evos',\n 'resnetv2_50d_gn',\n 'resnetv2_50t',\n 'resnetv2_50x1_bit_distilled',\n 'resnetv2_50x1_bitm',\n 'resnetv2_50x1_bitm_in21k',\n 'resnetv2_50x3_bitm',\n 'resnetv2_50x3_bitm_in21k',\n 'resnetv2_101',\n 'resnetv2_101d',\n 'resnetv2_101x1_bitm',\n 'resnetv2_101x1_bitm_in21k',\n 'resnetv2_101x3_bitm',\n 'resnetv2_101x3_bitm_in21k',\n 'resnetv2_152',\n 'resnetv2_152d',\n 'resnetv2_152x2_bit_teacher',\n 'resnetv2_152x2_bit_teacher_384',\n 'resnetv2_152x2_bitm',\n 'resnetv2_152x2_bitm_in21k',\n 'resnetv2_152x4_bitm',\n 'resnetv2_152x4_bitm_in21k',\n 'seresnet18',\n 'seresnet33ts',\n 'seresnet34',\n 'seresnet50',\n 'seresnet50t',\n 'seresnet101',\n 'seresnet152',\n 'seresnet152d',\n 'seresnet200d',\n 'seresnet269d',\n 'skresnet18',\n 'skresnet34',\n 'skresnet50',\n 'skresnet50d',\n 'ssl_resnet18',\n 'ssl_resnet50',\n 'swsl_resnet18',\n 'swsl_resnet50',\n 'tresnet_l',\n 'tresnet_l_448',\n 'tresnet_m',\n 'tresnet_m_448',\n 'tresnet_m_miil_in21k',\n 'tresnet_xl',\n 'tresnet_xl_448',\n 'tv_resnet34',\n 'tv_resnet50',\n 'tv_resnet101',\n 'tv_resnet152',\n 'vit_base_resnet26d_224',\n 'vit_base_resnet50_224_in21k',\n 'vit_base_resnet50_384',\n 'vit_base_resnet50d_224',\n 'vit_small_resnet26d_224',\n 'vit_small_resnet50d_s16_224',\n 'wide_resnet50_2',\n 'wide_resnet101_2']</pre> In\u00a0[136]: Copied! <pre># \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u7684\u200bcfg\nmodel.default_cfg\n</pre> # \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u7684\u200bcfg model.default_cfg Out[136]: <pre>{'url': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n 'num_classes': 1000,\n 'input_size': (3, 224, 224),\n 'pool_size': (7, 7),\n 'crop_pct': 0.875,\n 'interpolation': 'bilinear',\n 'mean': (0.485, 0.456, 0.406),\n 'std': (0.229, 0.224, 0.225),\n 'first_conv': 'conv1',\n 'classifier': 'fc',\n 'architecture': 'resnet18'}</pre> In\u00a0[190]: Copied! <pre>from tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# \u200b\u8bbe\u7f6e\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=3e-4, betas=[0.9, 0.99], eps=1e-08, weight_decay=0.0)\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n# \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\ndef train(epoch,model,train_loader):\n    model.train()\n    train_loss = 0\n    for image, target in tqdm(train_loader):\n        image = image.float()\n        image = image.to(device)\n        target = target.to(device)\n        predict = model(image)\n        loss = criterion(predict, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        train_loss += loss.item() * image.size(0)\n    train_loss = train_loss / len(train_loader.dataset)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n</pre> from tqdm import tqdm from torch.optim import Adam from torch.optim.lr_scheduler import CosineAnnealingLR  # \u200b\u8bbe\u7f6e\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b criterion = nn.CrossEntropyLoss() optimizer = Adam(model.parameters(), lr=3e-4, betas=[0.9, 0.99], eps=1e-08, weight_decay=0.0) scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0) # \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b def train(epoch,model,train_loader):     model.train()     train_loss = 0     for image, target in tqdm(train_loader):         image = image.float()         image = image.to(device)         target = target.to(device)         predict = model(image)         loss = criterion(predict, target)          optimizer.zero_grad()         loss.backward()         optimizer.step()         scheduler.step()         train_loss += loss.item() * image.size(0)     train_loss = train_loss / len(train_loader.dataset)     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss)) In\u00a0[225]: Copied! <pre># \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u9a8c\u8bc1\u200b\ndef val(epoch,model,val_dataloader):\n    model.eval()\n    val_loss = 0\n    gt_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data, label in tqdm(val_dataloader):\n            data, label = data.float().to(device), label.to(device)\n            output = model(data)\n            preds = torch.argmax(output, 1)\n            gt_labels.append(label.cpu().data.numpy())\n            pred_labels.append(preds.cpu().data.numpy())\n            loss = criterion(output, label)\n            val_loss += loss.item() * data.size(0)\n    val_loss = val_loss / len(val_dataloader)\n    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n    acc = np.sum(gt_labels == pred_labels) / len(pred_labels)\n    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))\n</pre> # \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u9a8c\u8bc1\u200b def val(epoch,model,val_dataloader):     model.eval()     val_loss = 0     gt_labels = []     pred_labels = []     with torch.no_grad():         for data, label in tqdm(val_dataloader):             data, label = data.float().to(device), label.to(device)             output = model(data)             preds = torch.argmax(output, 1)             gt_labels.append(label.cpu().data.numpy())             pred_labels.append(preds.cpu().data.numpy())             loss = criterion(output, label)             val_loss += loss.item() * data.size(0)     val_loss = val_loss / len(val_dataloader)     gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)     acc = np.sum(gt_labels == pred_labels) / len(pred_labels)     print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc)) In\u00a0[226]: Copied! <pre># \u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u9a8c\u8bc1\u200b\ndef test(epoch,model,test_dataloader):\n    model.eval()\n    test_loss = 0\n    gt_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data, label in tqdm(test_dataloader):\n            data, label = data.float().to(device), label.to(device)\n            output = model(data)\n            preds = torch.argmax(output, 1)\n            gt_labels.append(label.cpu().data.numpy())\n            pred_labels.append(preds.cpu().data.numpy())\n    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n    acc = np.sum(gt_labels == pred_labels) / len(pred_labels)\n    print('Epoch: {} \\tTest  Accuracy: {:6f}'.format(epoch, acc))\n</pre> # \u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u9a8c\u8bc1\u200b def test(epoch,model,test_dataloader):     model.eval()     test_loss = 0     gt_labels = []     pred_labels = []     with torch.no_grad():         for data, label in tqdm(test_dataloader):             data, label = data.float().to(device), label.to(device)             output = model(data)             preds = torch.argmax(output, 1)             gt_labels.append(label.cpu().data.numpy())             pred_labels.append(preds.cpu().data.numpy())     gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)     acc = np.sum(gt_labels == pred_labels) / len(pred_labels)     print('Epoch: {} \\tTest  Accuracy: {:6f}'.format(epoch, acc)) <p>\u200b\u4ee5\u4e0b\u200b\u4ec5\u200b\u5c55\u793a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u9a8c\u8bc1\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u4fdd\u5b58\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u9700\u6c42\u200b\u8fdb\u884c\u200b\u4fdd\u5b58\u200b</p> In\u00a0[211]: Copied! <pre>for epoch in range(epochs):\n    train(epoch,model,train_loader)\n    val(epoch,model,val_loader)\n    test(epoch,model,test_loader)\n</pre> for epoch in range(epochs):     train(epoch,model,train_loader)     val(epoch,model,val_loader)     test(epoch,model,test_loader) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:50&lt;00:00,  1.74s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 1.017386\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:27&lt;00:00,  2.49s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 27.227383, Accuracy: 0.746439\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:26&lt;00:00,  2.20s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.738162\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:44&lt;00:00,  1.68s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.844879\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:28&lt;00:00,  2.57s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 10.465725, Accuracy: 0.897436\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:26&lt;00:00,  2.20s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.905292\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:44&lt;00:00,  1.68s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.582476\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:22&lt;00:00,  2.03s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 15.013344, Accuracy: 0.871795\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:24&lt;00:00,  2.06s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.844011\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:44&lt;00:00,  1.67s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.632940\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:23&lt;00:00,  2.17s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 7.386183, Accuracy: 0.928775\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:22&lt;00:00,  1.91s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.908078\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:39&lt;00:00,  1.63s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.415200\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:26&lt;00:00,  2.40s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 13.273417, Accuracy: 0.857550\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:26&lt;00:00,  2.18s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.874652\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:40&lt;00:00,  1.64s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.544968\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:24&lt;00:00,  2.20s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 6.059515, Accuracy: 0.940171\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:24&lt;00:00,  2.01s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.933148\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:47&lt;00:00,  1.71s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.350375\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:22&lt;00:00,  2.02s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 11.914146, Accuracy: 0.897436\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:23&lt;00:00,  1.97s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.899721\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:37&lt;00:00,  1.61s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.459682\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:24&lt;00:00,  2.23s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 6.115790, Accuracy: 0.943020\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:25&lt;00:00,  2.09s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.935933\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:31&lt;00:00,  1.55s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.261108\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:24&lt;00:00,  2.23s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 7.643102, Accuracy: 0.923077\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:24&lt;00:00,  2.06s/it]\n</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.927577\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [02:40&lt;00:00,  1.64s/it]\n</pre> <pre>Epoch: 1 \tTraining Loss: 0.404539\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:24&lt;00:00,  2.27s/it]\n</pre> <pre>Epoch: 1 \tValidation Loss: 7.167813, Accuracy: 0.934473\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:27&lt;00:00,  2.28s/it]</pre> <pre>Epoch: 1 \tTest  Accuracy: 0.941504\n</pre> <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/","title":"\u591a\u884c\u200b\u683c\u5f0f","text":"In\u00a0[305]: Copied! <pre># \u200b\u589e\u52a0\u200b\uff0c\u200b\u51cf\u5c11\u200b\uff0c\u200b\u526a\u5207\u200b\uff0c\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5220\u9664\u200b\u7b49\u200b\n# a, b, x, s, dd\n</pre> # \u200b\u589e\u52a0\u200b\uff0c\u200b\u51cf\u5c11\u200b\uff0c\u200b\u526a\u5207\u200b\uff0c\u200b\u4fdd\u5b58\u200b\uff0c\u200b\u5220\u9664\u200b\u7b49\u200b # a, b, x, s, dd In\u00a0[301]: Copied! <pre># \u200b\u5408\u5e76\u200b\uff0c\u200b\u6267\u884c\u200b\n# shift+M shift+Enter\n</pre> # \u200b\u5408\u5e76\u200b\uff0c\u200b\u6267\u884c\u200b # shift+M shift+Enter In\u00a0[304]: Copied! <pre># \u200b\u663e\u793a\u200b\u884c\u200b\u6570\u200b\uff0c\u200b\u5207\u6362\u200bmarkdown/code\n# l, m/y\n</pre> # \u200b\u663e\u793a\u200b\u884c\u200b\u6570\u200b\uff0c\u200b\u5207\u6362\u200bmarkdown/code # l, m/y <p>\u200b\u659c\u4f53\u200b</p> <p>\u200b\u659c\u4f53\u200b2</p> <p>\u200b\u52a0\u7c97\u200b\u5b57\u4f53\u200b</p> <p><code>import numpy as np # \u200b\u5355\u884c\u200b\u4ee3\u7801\u200b</code></p> <pre># \u200b\u591a\u884c\u200b\u683c\u5f0f\u200b\n\n{\n    \"name\": \"Yam\",\n    \"age\": 22\n}\n</pre> <p>\u200b\u5f15\u7528\u200b</p> <p>\u200b\u5217\u8868\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u4e00\u4e2a\u200b</li> <li>\u200b\u7b2c\u4e8c\u4e2a\u200b</li> </ul> <p>TODO\uff1a</p> <ul> <li>\u200b\u7b2c\u4e00\u4e2a\u200b</li> <li>\u200b\u7b2c\u4e8c\u4e2a\u200b</li> </ul> <p>\u200b\u56fe\u7247\u200b\uff1a</p> <p>\u200b\u94fe\u63a5\u200b</p> <p>\u200b\u8868\u683c\u200b\uff1a</p> name age Yam 22 In\u00a0[117]: Copied! <pre>!ls\n</pre> !ls <pre>Basic.ipynb    NumPy.ipynb    data           exit\nNotebook.ipynb __pycache__    demo.py        tianchi\n</pre> In\u00a0[11]: Copied! <pre>!head data/table.txt\n</pre> !head data/table.txt <pre>organization\testablished\tceo\nGoogle\t1998\tSundar Pichai\nMicrosoft\t1975\tSatya Nadella\nNokia\t1865\tRajeev Suri</pre> In\u00a0[12]: Copied! <pre>!ls | wc -l\n</pre> !ls | wc -l <pre>       8\n</pre> In\u00a0[13]: Copied! <pre>!cat demo.py\n</pre> !cat demo.py <pre>def add(a: int, b: int) -&gt; int:\n    return a + b\n\n\na = 10\nb = 10\nres = add(a, b)\nprint(res)\n</pre> In\u00a0[18]: Copied! <pre>!python3.8 demo.py\n</pre> !python3.8 demo.py <pre>20\n</pre> In\u00a0[17]: Copied! <pre>!which python\n</pre> !which python <pre>/usr/bin/python\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install ...\n</pre> !pip install ... In\u00a0[108]: Copied! <pre># \u200b\u5217\u51fa\u200b\u9b54\u6cd5\u200b\u65b9\u6cd5\u200b\n%lsmagic\n</pre> # \u200b\u5217\u51fa\u200b\u9b54\u6cd5\u200b\u65b9\u6cd5\u200b %lsmagic Out[108]: <pre>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.</pre> In\u00a0[109]: Copied! <pre># \u200b\u8bbe\u7f6e\u200b\u73af\u5883\u53d8\u91cf\u200b\n%env PROFILE=dev\n</pre> # \u200b\u8bbe\u7f6e\u200b\u73af\u5883\u53d8\u91cf\u200b %env PROFILE=dev <pre>env: PROFILE=dev\n</pre> In\u00a0[114]: Copied! <pre>!echo $PROFILE\n</pre> !echo $PROFILE <pre>dev\n</pre> In\u00a0[115]: Copied! <pre>import os\n</pre> import os In\u00a0[116]: Copied! <pre>os.environ.get(\"PROFILE\")\n</pre> os.environ.get(\"PROFILE\") Out[116]: <pre>'dev'</pre> <p>\u200b\u6700\u200b\u6709\u7528\u200b/\u200b\u5e38\u7528\u200b\u7684\u200b\u662f\u200btimeit\uff0c\u200b\u7528\u6765\u200b\u8bc4\u4f30\u200b\u8fd0\u884c\u200b\u6548\u7387\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u7528\u200b\u4e24\u4e2a\u200b\u6392\u5e8f\u200b\u7684\u200b\u4f8b\u5b50\u200b\u6765\u200b\u8bf4\u660e\u200b\u3002</p> In\u00a0[307]: Copied! <pre>def choose_sort(arr: list) -&gt; list:\n    length = len(arr)\n    for i in range(length):\n        min_idx = i\n        for j in range(i, length):\n            if arr[j] &lt; arr[min_idx]:\n                min_idx = j\n        if min_idx != i:\n            arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    return arr\n\ndef quick_sort(arr: list) -&gt; list:\n    \n    def qs(arr, l, r):\n        if l &lt; r:\n            p = partion(arr, l, r)\n            qs(arr, l, p-1)\n            qs(arr, p, r)\n\n    def partion(arr: list, l: int, r: int) -&gt; int:\n        pivot = arr[l + (r-l)//2]\n        while l&lt;= r:\n            while arr[l] &lt; pivot:\n                l += 1\n            while arr[r] &gt; pivot:\n                r -= 1\n            if l &lt;= r:\n                arr[l], arr[r] = arr[r], arr[l]\n                l += 1\n                r -= 1\n        return l\n    l, r = 0, len(arr) - 1\n    qs(arr, l, r)\n    return arr\n</pre> def choose_sort(arr: list) -&gt; list:     length = len(arr)     for i in range(length):         min_idx = i         for j in range(i, length):             if arr[j] &lt; arr[min_idx]:                 min_idx = j         if min_idx != i:             arr[i], arr[min_idx] = arr[min_idx], arr[i]     return arr  def quick_sort(arr: list) -&gt; list:          def qs(arr, l, r):         if l &lt; r:             p = partion(arr, l, r)             qs(arr, l, p-1)             qs(arr, p, r)      def partion(arr: list, l: int, r: int) -&gt; int:         pivot = arr[l + (r-l)//2]         while l&lt;= r:             while arr[l] &lt; pivot:                 l += 1             while arr[r] &gt; pivot:                 r -= 1             if l &lt;= r:                 arr[l], arr[r] = arr[r], arr[l]                 l += 1                 r -= 1         return l     l, r = 0, len(arr) - 1     qs(arr, l, r)     return arr In\u00a0[308]: Copied! <pre>import random\n</pre> import random In\u00a0[309]: Copied! <pre>lst = [random.randint(1, 1000) for i in range(1000)]\n</pre> lst = [random.randint(1, 1000) for i in range(1000)] In\u00a0[288]: Copied! <pre>%timeit choose_sort(lst)\n</pre> %timeit choose_sort(lst) <pre>43 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[289]: Copied! <pre>%timeit quick_sort(lst)\n</pre> %timeit quick_sort(lst) <pre>1.79 ms \u00b1 51.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</pre> In\u00a0[290]: Copied! <pre>%timeit sorted(lst)\n</pre> %timeit sorted(lst) <pre>7.65 \u00b5s \u00b1 270 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n</pre> In\u00a0[315]: Copied! <pre># \u200b\u4e24\u4e2a\u200b%\u200b\u8868\u793a\u200b\u591a\u884c\u200b\uff0c\u200b\u4e14\u200b %%timeie \u200b\u5fc5\u987b\u200b\u653e\u5728\u200b\u7b2c\u4e00\u884c\u200b\n</pre> # \u200b\u4e24\u4e2a\u200b%\u200b\u8868\u793a\u200b\u591a\u884c\u200b\uff0c\u200b\u4e14\u200b %%timeie \u200b\u5fc5\u987b\u200b\u653e\u5728\u200b\u7b2c\u4e00\u884c\u200b In\u00a0[314]: Copied! <pre>%%timeit\n\nchoose_sort(lst)\nsorted(lst)\n</pre> %%timeit  choose_sort(lst) sorted(lst) <pre>44.8 ms \u00b1 5.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u6309\u200b\u6267\u884c\u200b\u4ee3\u7801\u200b\u5355\u884c\u200b\u5206\u6790\u200b\uff0c\u200b\u5185\u5b58\u200b\u5206\u6790\u200b\u7b49\u200b\u529f\u80fd\u200b\u3002\u200b\u5177\u4f53\u200b\u53ef\u200b\u67e5\u770b\u200b\u53c2\u8003\u6587\u732e\u200b\u30102\u3011\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u968f\u610f\u200b\u6267\u884c\u200b\u4efb\u610f\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u4f60\u200b\u5728\u200b\u8349\u7a3f\u7eb8\u200b\u4e0a\u200b\u6253\u8349\u7a3f\u200b\u4e00\u6837\u200b\u3002</p> In\u00a0[292]: Copied! <pre>1 + 2\n</pre> 1 + 2 Out[292]: <pre>3</pre> In\u00a0[293]: Copied! <pre>3 ** 3\n</pre> 3 ** 3 Out[293]: <pre>27</pre> In\u00a0[1]: Copied! <pre>x = [1, 2, 3]\ny = [4, 5, 6]\n</pre> x = [1, 2, 3] y = [4, 5, 6] In\u00a0[2]: Copied! <pre>sorted(y + x)\n</pre> sorted(y + x) Out[2]: <pre>[1, 2, 3, 4, 5, 6]</pre> In\u00a0[4]: Copied! <pre># \u200b\u968f\u65f6\u968f\u5730\u200b\u53ef\u200b\u67e5\u770b\u200b\u63a5\u53e3\u200b\u6587\u6863\u200b\nx.append?\n</pre> # \u200b\u968f\u65f6\u968f\u5730\u200b\u53ef\u200b\u67e5\u770b\u200b\u63a5\u53e3\u200b\u6587\u6863\u200b x.append? In\u00a0[\u00a0]: Copied! <pre># tab \u200b\u8865\u5168\u200b\nx.\n</pre> # tab \u200b\u8865\u5168\u200b x. <p>\u200b\u518d\u6765\u70b9\u200b\u6587\u672c\u200b\uff0c\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5b83\u200b\u652f\u6301\u200bMarkdown\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u4e5f\u200b\u652f\u6301\u200bLaTeX\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u5b66\u516c\u5f0f\u200b\u3002</p> <p>$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\\\ e^{i \\pi} = 0 $$</p> In\u00a0[1]: Copied! <pre>def read_file(file_path: str, num: int = -1, sep: str = \"\\t\"):\n    \"\"\"\n    Parameters\n    -----------\n    file_path: The file path.\n    num: How many lines would read, default is -1, means all lines.\n    sep: seperator for the line.\n    \n    Return\n    ------\n    A generator of the splited item.\n    \"\"\"\n    n = 0\n    with open(file_path, \"r\") as f:\n        for i, line in enumerate(f):\n            if num == -1 or n &lt; num:\n                yield line.strip().split(sep)\n            else:\n                break\n            n += 1\n</pre> def read_file(file_path: str, num: int = -1, sep: str = \"\\t\"):     \"\"\"     Parameters     -----------     file_path: The file path.     num: How many lines would read, default is -1, means all lines.     sep: seperator for the line.          Return     ------     A generator of the splited item.     \"\"\"     n = 0     with open(file_path, \"r\") as f:         for i, line in enumerate(f):             if num == -1 or n &lt; num:                 yield line.strip().split(sep)             else:                 break             n += 1 In\u00a0[9]: Copied! <pre>class Task:\n    \"\"\"\n    A Company sort task.\n    \"\"\"\n    \n    def __init__(self, file_path: str):\n        self.data = self._load_file(file_path)\n    \n    def _load_file(self, file_path: str) -&gt; list:\n        res = []\n        lines = read_file(file_path)\n        for i, item in enumerate(lines):\n            if i == 0:\n                continue\n            company = {\n                \"org\": item[0],\n                \"est\": int(item[1]),\n                \"ceo\": item[2]\n            }\n            res.append(company)\n        return res\n    \n    def order_by(self, field: str = \"est\") -&gt; list:\n        return sorted(self.data, key=lambda x: x.get(field, \"est\"))\n</pre> class Task:     \"\"\"     A Company sort task.     \"\"\"          def __init__(self, file_path: str):         self.data = self._load_file(file_path)          def _load_file(self, file_path: str) -&gt; list:         res = []         lines = read_file(file_path)         for i, item in enumerate(lines):             if i == 0:                 continue             company = {                 \"org\": item[0],                 \"est\": int(item[1]),                 \"ceo\": item[2]             }             res.append(company)         return res          def order_by(self, field: str = \"est\") -&gt; list:         return sorted(self.data, key=lambda x: x.get(field, \"est\")) In\u00a0[10]: Copied! <pre>task = Task(\"data/table.txt\")\n</pre> task = Task(\"data/table.txt\") In\u00a0[11]: Copied! <pre>task.order_by()\n</pre> task.order_by() Out[11]: <pre>[{'org': 'Nokia', 'est': 1865, 'ceo': 'Rajeev Suri'},\n {'org': 'Microsoft', 'est': 1975, 'ceo': 'Satya Nadella'},\n {'org': 'Google', 'est': 1998, 'ceo': 'Sundar Pichai'}]</pre> In\u00a0[7]: Copied! <pre>task.order_by(\"org\")\n</pre> task.order_by(\"org\") Out[7]: <pre>[{'org': 'Google', 'est': 1998, 'ceo': 'Sundar Pichai'},\n {'org': 'Microsoft', 'est': 1975, 'ceo': 'Satya Nadella'},\n {'org': 'Nokia', 'est': 1865, 'ceo': 'Rajeev Suri'}]</pre> <p>\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5bfc\u5165\u200b\u5916\u90e8\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</p> In\u00a0[105]: Copied! <pre>from demo import add\n</pre> from demo import add <pre>20\n</pre> In\u00a0[106]: Copied! <pre>add(3, 4)\n</pre> add(3, 4) Out[106]: <pre>7</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/#bash","title":"Bash\u200b\u547d\u4ee4\u200b\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/#magic","title":"Magic\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/#or","title":"\u7b14\u8bb0\u672c\u200b or \u200b\u8349\u7a3f\u7eb8\u200b\uff1f\u00b6","text":""},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/","title":"\u518d\u200b\u591a\u4e00\u70b9\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5c06\u200bNotebook\u200b\u4f5c\u4e3a\u200b\u65e5\u5e38\u200b\u5de5\u4f5c\u200b\u7684\u200b\u5de5\u5177\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5199\u200b\u5404\u79cd\u200b\u6a21\u578b\u200b\u3001\u200b\u4e1a\u52a1\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u8bfb\u53d6\u200b table.txt \u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u6309\u200b\u521b\u5efa\u200b\u65f6\u95f4\u200b\u6392\u5e8f\u200b\u3002\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u7b80\u5355\u200b\u4efb\u52a1\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5148\u5199\u200b\u4e00\u4e2a\u200b\u8bfb\u200b\u6587\u672c\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u518d\u200b\u5199\u4e2a\u200b\u5173\u4e8e\u200b\u8fd9\u4e2a\u200b\u4efb\u52a1\u200b\u7684\u200b\u7c7b\u200b\u3002</p>"},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<ul> <li>\u200b\u8bf7\u200b\u81ea\u884c\u200b\u5c1d\u8bd5\u200b\u672c\u200b\u8282\u200b\u6d89\u53ca\u200b\u5230\u200b\u7684\u200b\u548c\u200b\u76f8\u5173\u200b\u7684\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4fee\u6539\u200b\u5176\u4e2d\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</li> <li>\u200b\u8bf7\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u5b66\u4e60\u200b\u7b14\u8bb0\u200b\uff08\u200b\u6587\u672c\u200b+\u200b\u4ee3\u7801\u200b\uff09\u200b\u4ee5\u200bNotebook\u200b\u8bb0\u5f55\u200b\u3002</li> </ul>"},{"location":"notebook/%E7%AC%AC%E9%9B%B6%E7%AB%A0%20%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/","title":"\u53c2\u8003\u200b\u548c\u200b\u8d44\u6599\u200b\u00b6","text":"<ul> <li>\u30101\u3011Built-in magic commands \u2014 IPython 8.1.1 documentation</li> <li>\u30102\u3011Jake VanderPlas\uff0c\u300aPythoon \u200b\u6570\u636e\u200b\u79d1\u5b66\u200b\u624b\u518c\u200b\u300b\uff0c\u200b\u4eba\u6c11\u90ae\u7535\u51fa\u7248\u793e\u200b\uff0c2018\u200b\u5e74\u200b2\u200b\u6708\u200b</li> </ul>"}]}